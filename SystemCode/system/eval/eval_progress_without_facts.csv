user_input,response,reference,retrieved_contexts,metadata,time_taken,idx
"In the BERT benchmark data loader, how does the feature-splitting function convert a GLUE example's dictionary into the (features, label) tuple expected by KerasNLP?","The feature-splitting function in BERT benchmark data loader converts a GLUE example's dictionary into the (features, label) tuple expected by KerasNLP by splitting the input features and labels from the dictionary and returning them as separate tensors. This is typically done using the `tf.data.Dataset` API to create a dataset of input features and labels, which can then be used to train a Keras model. However, without access to the specific code snippet, it's difficult to provide more detailed information on how this is implemented in the BERT benchmark data loader.","In benchmarks/model_benchmark/bert_benchmark.py the split_features function does exactly this: it takes the GLUE example dict x, builds features = tuple(x[name] for name in feature_names), pulls out label = x[""label""], and returns (features, label) so it matches the (inputs, target) format KerasNLP expects.","['MODULE keras.benchmarks.model_benchmark.bert_benchmark contains FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.load_data\nLoad data.  Load GLUE/MRPC dataset, and convert the dictionary format to (features, label), where `features` is a tuple of all input sentences.', 'MODULE keras.benchmarks.model_benchmark.bert_benchmark contains FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.split_features', 'FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.load_data has parameter or accepts argument keras.benchmarks.model_benchmark.bert_benchmark.FLAGS\nLoad data.  Load GLUE/MRPC dataset, and convert the dictionary format to (features, label), where `features` is a tuple of all input sentences.', 'FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.split_features has parameter or accepts argument keras.benchmarks.model_benchmark.bert_benchmark.split_features.x', 'MODULE keras.keras.src.datasets.cifar contains FUNCTION keras.keras.src.datasets.cifar.load_batch\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', 'FUNCTION keras.keras.src.datasets.cifar.load_batch has parameter or accepts argument keras.keras.src.datasets.cifar.load_batch.fpath\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', 'CLASS keras.keras.src.layers.preprocessing.feature_space.FeatureSpace has field or attribute or property keras.keras.src.layers.preprocessing.feature_space.FeatureSpace.built\nOne-stop utility for preprocessing and encoding structured data.  Arguments:     feature_names: Dict mapping the names of your features to their         type specification, e.g. `{""my_feature"": ""integer_categorical""}`         or `{""my_feature"": FeatureSpace.integer_categorical()}`.         For a complete list of all supported types, see         ""Available feature types"" paragraph below.     output_mode: One of `""concat""` or `""dict""`. In concat mode, all         features get concatenated together into a single vector.         In dict mode, the FeatureSpace returns a dict of individually         encoded features (with the same keys as the input dict keys).     crosses: List of features to be crossed together, e.g.         `crosses=[(""feature_1"", ""feature_2"")]`. The features will be         ""crossed"" by hashing their combined value into         a fixed-length vector.     crossing_dim: Default vector size for hashing crossed features.         Defaults to `32`.     hashing_dim: Default vector size for hashing features of type         `""integer_hashed""` and `""string_hashed""`. Defaults to `32`.     num_discretization_bins: Default number of bins to be used for         discretizing features of type `""float_discretized""`.         Defaults to `32`.  **Available feature types:**  Note that all features can be referred to by their string name, e.g. `""integer_categorical""`. When using the string name, the default argument values are used.  ```python # Plain float values. FeatureSpace.float(name=None)  # Float values to be preprocessed via featurewise standardization # (i.e. via a `keras.layers.Normalization` layer). FeatureSpace.float_normalized(name=None)  # Float values to be preprocessed via linear rescaling # (i.e. via a `keras.layers.Rescaling` layer). FeatureSpace.float_rescaled(scale=1., offset=0., name=None)  # Float values to be discretized. By default, the discrete # representation will then be one-hot encoded. FeatureSpace.float_discretized(     num_bins, bin_boundaries=None, output_mode=""one_hot"", name=None)  # Integer values to be indexed. By default, the discrete # representation will then be one-hot encoded. FeatureSpace.integer_categorical(     max_tokens=None, num_oov_indices=1, output_mode=""one_hot"", name=None)  # String values to be indexed. By default, the discrete # representation will then be one-hot encoded. FeatureSpace.string_categorical(     max_tokens=None, num_oov_indices=1, output_mode=""one_hot"", name=None)  # Integer values to be hashed into a fixed number of bins. # By default, the discrete representation will then be one-hot encoded. FeatureSpace.integer_hashed(num_bins, output_mode=""one_hot"", name=None)  # String values to be hashed into a fixed number of bins. # By default, the discrete representation will then be one-hot encoded. FeatureSpace.string_hashed(num_bins, output_mode=""one_hot"", name=None) ```  Examples:  **Basic usage with a dict of input data:**  ```python raw_data = {     ""float_values"": [0.0, 0.1, 0.2, 0.3],     ""string_values"": [""zero"", ""one"", ""two"", ""three""],     ""int_values"": [0, 1, 2, 3], } dataset = tf.data.Dataset.from_tensor_slices(raw_data)  feature_space = FeatureSpace(     features={         ""float_values"": ""float_normalized"",         ""string_values"": ""string_categorical"",         ""int_values"": ""integer_categorical"",     },     crosses=[(""string_values"", ""int_values"")],     output_mode=""concat"", ) # Before you start using the FeatureSpace, # you must `adapt()` it on some data. feature_space.adapt(dataset)  # You can call the FeatureSpace on a dict of data (batched or unbatched). output_vector = feature_space(raw_data) ```  **Basic usage with `tf.data`:**  ```python # Unlabeled data preprocessed_ds = unlabeled_dataset.map(feature_space)  # Labeled data preprocessed_ds = labeled_dataset.map(lambda x, y: (feature_space(x), y)) ```  **Basic usage with the Keras Functional API:**  ```python # Retrieve a dict Keras Input objects inputs = feature_space.get_inputs() # Retrieve the corresponding encoded Keras tensors encoded_features = feature_space.get_encoded_features() # Build a Functional model outputs = keras.layers.Dense(1, activation=""sigmoid"")(encoded_features) model = keras.Model(inputs, outputs) ```  **Customizing each feature or feature cross:**  ```python feature_space = FeatureSpace(     features={         ""float_values"": FeatureSpace.float_normalized(),         ""string_values"": FeatureSpace.string_categorical(max_tokens=10),         ""int_values"": FeatureSpace.integer_categorical(max_tokens=10),     },     crosses=[         FeatureSpace.cross((""string_values"", ""int_values""), crossing_dim=32)     ],     output_mode=""concat"", ) ```  **Returning a dict of integer-encoded features:**  ```python feature_space = FeatureSpace(     features={         ""string_values"": FeatureSpace.string_categorical(output_mode=""int""),         ""int_values"": FeatureSpace.integer_categorical(output_mode=""int""),     },     crosses=[         FeatureSpace.cross(             feature_names=(""string_values"", ""int_values""),             crossing_dim=32,             output_mode=""int"",         )     ],     output_mode=""dict"", ) ```  **Specifying your own Keras preprocessing layer:**  ```python # Let\'s say that one of the features is a short text paragraph that # we want to encode as a vector (one vector per paragraph) via TF-IDF. data = {     ""text"": [""1st string"", ""2nd string"", ""3rd string""], }  # There\'s a Keras layer for this: TextVectorization. custom_layer = layers.TextVectorization(output_mode=""tf_idf"")  # We can use FeatureSpace.feature to create a custom feature # that will use our preprocessing layer. feature_space = FeatureSpace(     features={         ""text"": FeatureSpace.feature(             preprocessor=custom_layer, dtype=""string"", output_mode=""float""         ),     },     output_mode=""concat"", ) feature_space.adapt(tf.data.Dataset.from_tensor_slices(data)) output_vector = feature_space(data) ```  **Retrieving the underlying Keras preprocessing layers:**  ```python # The preprocessing layer of each feature is available in `.preprocessors`. preprocessing_layer = feature_space.preprocessors[""feature1""]  # The crossing layer of each feature cross is available in `.crossers`. # It\'s an instance of keras.layers.HashedCrossing. crossing_layer = feature_space.crossers[""feature1_X_feature2""] ```  **Saving and reloading a FeatureSpace:**  ```python feature_space.save(""featurespace.keras"") reloaded_feature_space = keras.models.load_model(""featurespace.keras"") ```', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\ninformation, please see our tutorial on [categorical\ndata](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html), along with\nexamples linked on that page. (#7380, #7708, #7695, #7330, #7307, #7322, #7705,\n#7652, #7592, #7666, #7576, #7569, #7529, #7575, #7393, #7465, #7385, #7371, #7745, #7810)\n\nIn the future, we will continue to improve categorical data support with new features and\noptimizations. Also, we are looking forward to bringing the feature beyond Python binding,\ncontributions and feedback are welcomed! Lastly, as a result of experimental status, the\nbehavior might be subject to change, especially the default value of related\nhyper-parameters.\n\n### Experimental support for multi-output model\n\nXGBoost 1.6 features initial support for the multi-output model, which includes\nmulti-output regression and multi-label classification. Along with this, the XGBoost\nclassifier has proper support for base margin without to need for the user to flatten the\ninput. In this initial support, XGBoost builds one model for each target similar to the\nsklearn meta estimator, for more details, please see our [quick\nintroduction](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html).\n\n(#7365, #7736, #7607, #7574, #7521, #7514, #7456, #7453, #7455, #7434, #7429, #7405, #7381)\n\n### External memory support\nExternal memory support for both approx and hist tree method is considered feature\ncomplete in XGBoost 1.6.  Building upon the iterator-based interface introduced in the\nprevious version, now both `hist` and `approx` iterates over each batch of data during\ntraining and prediction.  In previous versions, `hist` concatenates all the batches into\nan internal representation, which is removed in this version.  As a result, users can\nexpect higher scalability in terms of data size but might experience lower performance due\nto disk IO. (#7531, #7320, #7638, #7372)\n\n### Rewritten approx\n\nThe `approx` tree method is rewritten based on the existing `hist` tree method. The\nrewrite closes the feature gap between `approx` and `hist` and improves the performance.\nNow the behavior of `approx` should be more aligned with `hist` and `gpu_hist`. Here is a\nlist of user-visible changes:\n\n- Supports both `max_leaves` and `max_depth`.\n- Supports `grow_policy`.\n- Supports monotonic constraint.\n- Supports feature weights.\n- Use `max_bin` to replace `sketch_eps`.\n- Supports categorical data.\n- Faster performance for many of the datasets.\n- Improved performance and robustness for distributed training.\n- Supports prediction cache.\n- Significantly better performance for external memory when `depthwise` policy is used.\n\n### New serialization format\nBased on the existing JSON serialization format, we introduce UBJSON support as a more\nefficient alternative. Both formats will be available in the future and we plan to\ngradually [phase out](https://github.com/dmlc/xgboost/issues/7547) support for the old\nbinary model format.  Users can opt to use the different formats in the serialization\nfunction by providing the file extension `json` or `ubj`. Also, the `save_raw` function in\nall supported languages bindings gains a new parameter for exporting the model in different\nformats, available options are `json`, `ubj`, and `deprecated`, see document for the\nlanguage binding you are using for details. Lastly, the default internal serialization']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 19138, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",56.50035762786865,121
"How does sparse categorical crossentropy handle samples of the ignored class through the initial preprocessing, the backend loss computation, and the final mask application?","Sparse categorical crossentropy (SparseCCE) loss function in Keras handles samples of the ignored class through the following steps: **Initial Preprocessing**: When creating a sparse categorical cross-entropy model, you need to specify the `sparse` argument as `True`. This tells the model that the target values are not one-hot encoded but rather indices of the true classes. The model will then ignore any samples with labels outside the range of class indices provided in the `classes` parameter or inferred from the data. **Backend Loss Computation**: During training, the SparseCCE loss function computes the cross-entropy loss only for the samples with valid class labels. It ignores the samples with ignored class labels by setting their contribution to zero. This is achieved through a boolean mask that identifies valid and invalid samples based on the target values. The final loss value is computed as the average of the losses across all valid samples. **Final Mask Application**: After computing the loss, Keras applies a mask to the output of the model to ensure that it only outputs probabilities for the classes present in the data. This mask is created by setting the probability of ignored classes to zero. The final output of the model will have the same number of classes as specified in the `classes` parameter or inferred from the data, with non-zero values only for the valid classes.","In keras/src/losses/losses.py, sparse_categorical_crossentropy does the following when ignore_class≠None:

1. **Preprocessing**  
   – valid_mask = (y_true != ignore_class)  
   – y_true ← y_true * valid_mask  
   – y_pred ← y_pred * expand_dims(valid_mask, –1)  
   This zeroes out both the target and logits for any “ignored” sample.

2. **Backend loss**  
   – Calls ops.sparse_categorical_crossentropy(y_true, y_pred, …)  
   – Because ignored entries were zeroed, their raw loss is computed on zeros.

3. **Final masking**  
   – Reshapes valid_mask to the loss’s shape  
   – res ← where(valid_mask, res, 0.0)  (sets ignored losses to 0)  
   – backend.set_keras_mask(res, mask=valid_mask)  
   This ensures ignored samples contribute zero to any reduction or metric.","['FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.y_true\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'MODULE keras.keras.src.legacy.backend contains FUNCTION keras.keras.src.legacy.backend.sparse_categorical_crossentropy\nDEPRECATED.', 'FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.from_logits\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'FUNCTION keras.keras.src.legacy.backend.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.legacy.backend.sparse_categorical_crossentropy.ignore_class\nDEPRECATED.', 'FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.axis\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'MODULE keras.keras.src.backend.jax.nn contains FUNCTION keras.keras.src.backend.jax.nn.sparse_categorical_crossentropy', 'MODULE keras.keras.src.losses.losses contains FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.7 (2017.12.30)\n* **This version represents a major change from the last release (v0.6), which was released one year and half ago.**\n* Updated Sklearn API\n  - Add compatibility layer for scikit-learn v0.18: `sklearn.cross_validation` now deprecated\n  - Updated to allow use of all XGBoost parameters via `**kwargs`.\n  - Updated `nthread` to `n_jobs` and `seed` to `random_state` (as per Sklearn convention); `nthread` and `seed` are now marked as deprecated\n  - Updated to allow choice of Booster (`gbtree`, `gblinear`, or `dart`)\n  - `XGBRegressor` now supports instance weights (specify `sample_weight` parameter)\n  - Pass `n_jobs` parameter to the `DMatrix` constructor\n  - Add `xgb_model` parameter to `fit` method, to allow continuation of training\n* Refactored gbm to allow more friendly cache strategy\n  - Specialized some prediction routine\n* Robust `DMatrix` construction from a sparse matrix\n* Faster construction of `DMatrix` from 2D NumPy matrices: elide copies, use of multiple threads\n* Automatically remove nan from input data when it is sparse.\n  - This can solve some of user reported problem of istart != hist.size\n* Fix the single-instance prediction function to obtain correct predictions\n* Minor fixes\n  - Thread local variable is upgraded so it is automatically freed at thread exit.\n  - Fix saving and loading `count::poisson` models\n  - Fix CalcDCG to use base-2 logarithm\n  - Messages are now written to stderr instead of stdout\n  - Keep built-in evaluations while using customized evaluation functions\n  - Use `bst_float` consistently to minimize type conversion\n  - Copy the base margin when slicing `DMatrix`\n  - Evaluation metrics are now saved to the model file\n  - Use `int32_t` explicitly when serializing version\n  - In distributed training, synchronize the number of features after loading a data matrix.\n* Migrate to C++11\n  - The current master version now requires C++11 enabled compiled(g++4.8 or higher)\n* Predictor interface was factored out (in a manner similar to the updater interface).\n* Makefile support for Solaris and ARM\n* Test code coverage using Codecov\n* Add CPP tests\n* Add `Dockerfile` and `Jenkinsfile` to support continuous integration for GPU code\n* New functionality\n  - Ability to adjust tree model's statistics to a new dataset without changing tree structures.\n  - Ability to extract feature contributions from individual predictions, as described in [here](http://blog.datadive.net/interpreting-random-forests/) and [here](https://arxiv.org/abs/1706.06060).\n  - Faster, histogram-based tree algorithm (`tree_method='hist'`) .\n  - GPU/CUDA accelerated tree algorithms (`tree_method='gpu_hist'` or `'gpu_exact'`), including the GPU-based predictor.\n  - Monotonic constraints: when other features are fixed, force the prediction to be monotonic increasing with respect to a certain specified feature.\n  - Faster gradient calculation using AVX SIMD\n  - Ability to export models in JSON format\n  - Support for Tweedie regression\n  - Additional dropout options for DART: binomial+1, epsilon\n  - Ability to update an existing model in-place: this is useful for many applications, such as determining feature importance\n* Python package:\n  - New parameters:\n    - `learning_rates` in `cv()`\n    - `shuffle` in `mknfold()`\n    - `max_features` and `show_values` in `plot_importance()`\n    - `sample_weight` in `XGBRegressor.fit()`""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 2, 'pr': 19838, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",51.79615092277527,122
Which exception is thrown when attempting to retrieve an attribute but the layer has never been called?,"In Keras, attempting to retrieve an attribute but the layer has never been called will raise a `AttributeError` exception. This error occurs when you try to access an attribute of a layer that hasn't been built or run yet. To resolve this issue, make sure to call the layer's `build()` method before trying to access its attributes, or use the layer after it has been added to a model and compiled.","In Operation._get_node_attribute_at_index (keras/src/ops/operation.py), if self._inbound_nodes is empty it raises an AttributeError indicating the layer has never been called.","['MODULE cognee.cognee.tests.unit.modules.graph.cognee_graph_test contains FUNCTION cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node\nTest retrieving edges for a nonexistent node raises an exception.', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.value', 'FUNCTION cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node has parameter or accepts argument cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node.setup_graph\nTest retrieving edges for a nonexistent node raises an exception.', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.arg_name', 'MODULE xgboost.python-package.xgboost.libpath contains CLASS xgboost.python-package.xgboost.libpath.XGBoostLibraryNotFound\nError thrown by when xgboost is not found', 'CLASS keras.keras.src.utils.jax_layer_test.TestJaxLayer has field or attribute or property keras.keras.src.utils.jax_layer_test.TestJaxLayer.test_with_missing_inputs_in_call_fn.params', 'CLASS xgboost.python-package.xgboost.libpath.XGBoostLibraryNotFound inherits from or is a subclass of xgboost.python-package.xgboost.libpath.Exception\nError thrown by when xgboost is not found', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20156, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",41.9170663356781,123
How does the return_attention_scores flag passed to call affect the attention computation flow and the layer’s output?,"**The `return_attention_scores` flag passed to the `call` method in Keras affects the attention computation flow and the layer's output as follows: **

When set to `True`, the layer will return the attention scores in addition to its regular output. This can be useful for debugging or further analysis of the model's behavior.

The attention scores are computed during the forward pass and are stored in the layer's internal state. When `return_attention_scores` is `True`, these scores are returned along with the layer's output.

This flag does not affect the training process or the model's performance, but rather provides additional information about the model's behavior during inference.

**","When you pass return_attention_scores=True into MultiHeadAttention.call:

• In _compute_attention, it short-circuits the “fast” dot-product path (and forbids flash attention), so it always:  
  – scales Q, K, does an einsum to get raw scores  
  – applies masked softmax  
  – (optionally) applies dropout to the scores  
  – computes the context via another einsum  
  – returns both (context, attention_scores) rather than (context, None).

• Back in call, because return_attention_scores=True, it returns the 2-tuple (output, scores); otherwise it only returns the output tensor.","['CLASS keras.keras.src.layers.attention.attention.Attention has field or attribute or property keras.keras.src.layers.attention.attention.seed\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'METHOD keras.keras.src.layers.attention.attention.Attention.call has parameter or accepts argument keras.keras.src.layers.attention.attention.Attention.call.return_attention_scores', 'CLASS keras.keras.src.layers.attention.attention.Attention has method keras.keras.src.layers.attention.attention.Attention.build\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'METHOD keras.keras.src.layers.attention.grouped_query_attention.GroupedQueryAttention.call has parameter or accepts argument keras.keras.src.layers.attention.grouped_query_attention.GroupedQueryAttention.call.return_attention_scores', 'CLASS keras.keras.src.layers.attention.attention.Attention has method keras.keras.src.layers.attention.attention.Attention.__init__\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'METHOD keras.keras.src.layers.attention.multi_head_attention.MultiHeadAttention.call has parameter or accepts argument keras.keras.src.layers.attention.multi_head_attention.MultiHeadAttention.call.return_attention_scores', 'CLASS keras.keras.src.layers.attention.attention.Attention has field or attribute or property keras.keras.src.layers.attention.attention.dropout\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### GPUTreeSHAP: GPU acceleration of the TreeSHAP algorithm (#6038, #6064, #6087, #6099, #6163, #6281, #6332)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) is a game theoretic approach to explain predictions of machine learning models. It computes feature importance scores for individual examples, establishing how each feature influences a particular prediction. TreeSHAP is an optimized SHAP algorithm specifically designed for decision tree ensembles.\n* Starting with 1.3.0 release, it is now possible to leverage CUDA-capable GPUs to accelerate the TreeSHAP algorithm. Check out [the demo notebook](https://github.com/dmlc/xgboost/blob/master/demo/gpu_acceleration/shap.ipynb).\n* The CUDA implementation of the TreeSHAP algorithm is hosted at [rapidsai/GPUTreeSHAP](https://github.com/rapidsai/gputreeshap). XGBoost imports it as a Git submodule.\n\n### New style Python callback API (#6199, #6270, #6320, #6348, #6376, #6399, #6441)\n* The XGBoost Python package now offers a re-designed callback API. The new callback API lets you design various extensions of training in idomatic Python. In addition, the new callback API allows you to use early stopping with the native Dask API (`xgboost.dask`). Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.3.0/python/callbacks.html) and [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/callbacks.py).\n\n### Enable the use of `DeviceQuantileDMatrix` / `DaskDeviceQuantileDMatrix` with large data (#6201, #6229, #6234).\n* `DeviceQuantileDMatrix` can achieve memory saving by avoiding extra copies of the training data, and the saving is bigger for large data. Unfortunately, large data with more than 2^31 elements was triggering integer overflow bugs in CUB and Thrust. Tracking issue: #6228.\n* This release contains a series of work-arounds to allow the use of `DeviceQuantileDMatrix` with large data:\n  - Loop over `copy_if` (#6201)\n  - Loop over `thrust::reduce` (#6229)\n  - Implement the inclusive scan algorithm in-house, to handle large offsets (#6234)\n\n### Support slicing of tree models (#6302)\n* Accessing the best iteration of a model after the application of early stopping used to be error-prone, need to manually pass the `ntree_limit` argument to the `predict()` function.\n* Now we provide a simple interface to slice tree models by specifying a range of boosting rounds. The tree ensemble can be split into multiple sub-ensembles via the slicing interface. Check out [an example](https://xgboost.readthedocs.io/en/release_1.3.0/python/model.html).\n* In addition, the early stopping callback now supports `save_best` option. When enabled, XGBoost will save (persist) the model at the best boosting round and discard the trees that were fit subsequent to the best round.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 20482, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",43.782108545303345,124
"When serializing this RNN layer’s configuration, what step ensures the cell attribute is omitted from the returned config?","When serializing this RNN layer’s configuration, the step that ensures the cell attribute is omitted from the returned config in keras is the `get_config` method of the RNN layer class. This method returns a dictionary containing the configuration of the layer, and it does not include the cell attribute by default. The `get_config` method is responsible for serializing the configuration of the layer, excluding any internal state or attributes that are not necessary for reconstruction of the layer.","In SimpleRNN.get_config (keras/src/layers/rnn/simple_rnn.py), right after calling `base_config = super().get_config()`, there’s a  
```python
del base_config[""cell""]
```  
which removes the `cell` entry before returning the merged config.","['CLASS keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell has field or attribute or property keras.keras.src.layers.rnn.dropout_rnn_cell.step_input\nObject that holds dropout-related functionality for RNN cells.  This class is not a standalone RNN cell. It suppose to be used with a RNN cell by multiple inheritance. Any cell that mix with class should have following fields:  - `dropout`: a float number in the range `[0, 1]`.     Dropout rate for the input tensor. - `recurrent_dropout`: a float number in the range `[0, 1]`.     Dropout rate for the recurrent connections. - `seed_generator`, an instance of `backend.random.SeedGenerator`.  This object will create and cache dropout masks, and reuse them for all incoming steps, so that the same mask is used for every step.', 'METHOD keras.keras.src.layers.rnn.rnn.RNN.from_config has parameter or accepts argument keras.keras.src.layers.rnn.rnn.RNN.cell', ""CLASS keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell has method keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell.reset_dropout_mask\nReset the cached dropout mask if any.  The RNN layer invokes this in the `call()` method so that the cached mask is cleared after calling `cell.call()`. The mask should be cached across all timestep within the same batch, but shouldn't be cached between batches.\nObject that holds dropout-related functionality for RNN cells.  This class is not a standalone RNN cell. It suppose to be used with a RNN cell by multiple inheritance. Any cell that mix with class should have following fields:  - `dropout`: a float number in the range `[0, 1]`.     Dropout rate for the input tensor. - `recurrent_dropout`: a float number in the range `[0, 1]`.     Dropout rate for the recurrent connections. - `seed_generator`, an instance of `backend.random.SeedGenerator`.  This object will create and cache dropout masks, and reuse them for all incoming steps, so that the same mask is used for every step."", 'METHOD keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.get_config has parameter or accepts argument keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.cells', 'CLASS keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell has method keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell.get_recurrent_dropout_mask\nObject that holds dropout-related functionality for RNN cells.  This class is not a standalone RNN cell. It suppose to be used with a RNN cell by multiple inheritance. Any cell that mix with class should have following fields:  - `dropout`: a float number in the range `[0, 1]`.     Dropout rate for the input tensor. - `recurrent_dropout`: a float number in the range `[0, 1]`.     Dropout rate for the recurrent connections. - `seed_generator`, an instance of `backend.random.SeedGenerator`.  This object will create and cache dropout masks, and reuse them for all incoming steps, so that the same mask is used for every step.', 'METHOD keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.from_config has parameter or accepts argument keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.cells', 'CLASS keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell has method keras.keras.src.layers.rnn.dropout_rnn_cell.DropoutRNNCell.reset_recurrent_dropout_mask\nObject that holds dropout-related functionality for RNN cells.  This class is not a standalone RNN cell. It suppose to be used with a RNN cell by multiple inheritance. Any cell that mix with class should have following fields:  - `dropout`: a float number in the range `[0, 1]`.     Dropout rate for the input tensor. - `recurrent_dropout`: a float number in the range `[0, 1]`.     Dropout rate for the recurrent connections. - `seed_generator`, an instance of `backend.random.SeedGenerator`.  This object will create and cache dropout masks, and reuse them for all incoming steps, so that the same mask is used for every step.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20636, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.69154191017151,125
How does the ONNX export function retrieve a concrete TensorFlow function without creating a temporary SavedModel?,"The ONNX export function in Keras retrieves a concrete TensorFlow function without creating a temporary SavedModel using the `get_concrete_fn` method from the `keras.export.onnx` module. This method takes the model and input signature as arguments and returns a concrete TensorFlow function. The `get_concrete_fn` method is used to get a concrete function, which is then exported to an ONNX file using the `tf.saved_model.save` method. This approach requires Keras 3.x or later, as well as TensorFlow 2.x or later. An example of how to use this method is provided in the code snippet below:

```python
define the model and get the concrete function
concrete_fn = get_concrete_fn(model, input_signature=[tf.TensorSpec([1, 784], tf.float32)])
export the model to ONNX format
onnx_model = tf.saved_model.save(concrete_fn, 'path/to/model.onnx')
```
This approach allows for efficient export of models without creating a temporary SavedModel.","The ONNX exporter never writes out a SavedModel on disk. Instead it calls get_concrete_fn (in keras/src/export/onnx.py), which under the hood does:

• ExportArchive.track_and_add_endpoint(…)  
• (for TF) ExportArchive._filter_and_track_resources()  
• return ExportArchive._get_concrete_fn(DEFAULT_ENDPOINT_NAME)

That gives you a ready‐to‐go tf.function (concrete function) in memory, which is then handed straight to tf2onnx.convert.from_function. No temporary SavedModel is ever created.","['METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.format\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'MODULE keras.keras.src.export.onnx contains FUNCTION keras.keras.src.export.onnx.get_concrete_fn\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.verbose\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'FUNCTION keras.keras.src.export.onnx.get_concrete_fn has parameter or accepts argument keras.keras.src.export.onnx.kwargs\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.input_signature\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'FUNCTION keras.keras.src.export.onnx.get_concrete_fn has parameter or accepts argument keras.keras.src.export.onnx.get_concrete_fn.model\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 4, 'pr': 20725, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",46.22615909576416,126
Which argument to the Keras ISTFT function requires a 32-bit integer dtype to avoid a conversion error?,"The argument to the Keras ISTFT function that requires a 32-bit integer dtype to avoid a conversion error is 'length'. This can be found in the documentation of the Keras library, which states that `keras.keras.src.backend.numpy.math.istft` has a parameter or accepts an argument named `length`, suggesting that this argument requires a 32-bit integer dtype.",The fft_length argument must be a 32-bit integer (tf.int32) to avoid the conversion error.,"['FUNCTION qlib.qlib.rl.order_execution.interpreter.canonicalize has parameter or accepts argument qlib.qlib.rl.order_execution.interpreter.canonicalize.value\nTo 32-bit numeric types. Recursively.', 'FUNCTION keras.keras.src.backend.numpy.math.istft has parameter or accepts argument keras.keras.src.backend.numpy.math.istft.length', 'MODULE qlib.qlib.rl.order_execution.interpreter contains FUNCTION qlib.qlib.rl.order_execution.interpreter.canonicalize\nTo 32-bit numeric types. Recursively.', 'FUNCTION keras.keras.src.backend.numpy.math.istft has parameter or accepts argument keras.keras.src.backend.numpy.math.istft.fft_length', 'FUNCTION keras.keras.src.ops.numpy.bitwise_not has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_not.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'FUNCTION keras.keras.src.backend.numpy.math.istft has parameter or accepts argument keras.keras.src.backend.numpy.math.istft.sequence_length', 'FUNCTION keras.keras.src.ops.numpy.bitwise_invert has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_invert.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20728, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.07046627998352,127
What exception types are asserted for an unrecognized format and for an unsupported backend in the export error tests?,"The export error tests in Keras assert `ValueError` for an unrecognized format and `NotImplementedError` for an unsupported backend. This is because `ValueError` is raised when a value is invalid or out of range, which applies to an unrecognized format. On the other hand, `NotImplementedError` is raised when a specific feature or operation is not implemented, which applies to an unsupported backend.","In ModelTest.test_export_error (keras/src/models/model_test.py) they assert:

• ValueError for an unrecognized format  
• NotImplementedError for an unsupported backend","['MODULE cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils contains FUNCTION cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils.format_neptune_error\nFormat Neptune Analytics specific errors for better readability.  Parameters: -----------     - error (Exception): The exception to format  Returns: --------     - str: Formatted error message', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.UnstructuredLibraryImportError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.status_code', 'FUNCTION cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils.format_neptune_error has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils.format_neptune_error.error\nFormat Neptune Analytics specific errors for better readability.  Parameters: -----------     - error (Exception): The exception to format  Returns: --------     - str: Formatted error message', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.UnstructuredLibraryImportError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.name', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.UnstructuredLibraryImportError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.message', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.format\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Specify version macro in CMake. (#4730)\n* Include dmlc-tracker into XGBoost Python package (#4731)\n* [CI] Use long key ID for Ubuntu repository fingerprints. (#4783)\n* Remove plugin, CUDA related code in automake & autoconf files (#4789)\n* Skip related tests when scikit-learn is not installed. (#4791)\n* Ignore vscode and clion files (#4866)\n* Use bundled Google Test by default (#4900)\n* [CI] Raise timeout threshold in Jenkins (#4938)\n* Copy CMake parameter from dmlc-core. (#4948)\n* Set correct file permission. (#4964)\n* [CI] Update lint configuration to support latest pylint convention (#4971)\n* [CI] Upload nightly builds to S3 (#4976, #4979)\n* Add asan.so.5 to cmake script. (#4999)\n* [CI] Fix Travis tests. (#5062)\n* [CI] Locate vcomp140.dll from System32 directory (#5078)\n* Implement training observer to dump internal states of objects (#5088). This will be useful for debugging.\n* Fix visual studio output library directories (#5119)\n* [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)\n* [CI] Repair download URL for Maven 3.6.1 (#5139)\n* Don't use modernize-use-trailing-return-type in clang-tidy. (#5169)\n* Explicitly use UTF-8 codepage when using MSVC (#5197)\n* Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)\n* Make some GPU tests deterministic (#5229)\n* [R] Robust endian detection in CRAN xgboost build (#5232)\n* Support FreeBSD (#5233)\n* Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)\n* Fix compilation error due to 64-bit integer narrowing to `size_t` (#5250)\n* Remove use of `std::cout` from R package, to comply with CRAN policy (#5261)\n* Update DMLC-Core submodule (#4674, #4688, #4726, #4924)\n* Update Rabit submodule (#4560, #4667, #4718, #4808, #4966, #5237)\n\n### Usability Improvements, Documentation\n* Add Random Forest API to Python API doc (#4500)\n* Fix Python demo and doc. (#4545)\n* Remove doc about not supporting CUDA 10.1 (#4578)\n* Address some sphinx warnings and errors, add doc for building doc. (#4589)\n* Add instruction to run formatting checks locally (#4591)\n* Fix docstring for `XGBModel.predict()` (#4592)\n* Doc and demo for customized metric and objective (#4598, #4608)\n* Add to documentation how to run tests locally (#4610)\n* Empty evaluation list in early stopping should produce meaningful error message (#4633)\n* Fixed year to 2019 in conf.py, helpers.h and LICENSE (#4661)\n* Minor updates to links and grammar (#4673)\n* Remove `silent` in doc (#4689)\n* Remove old Python trouble shooting doc (#4729)\n* Add `os.PathLike` support for file paths to DMatrix and Booster Python classes (#4757)\n* Update XGBoost4J-Spark doc (#4804)\n* Regular formatting for evaluation metrics (#4803)\n* [jvm-packages] Refine documentation for handling missing values in XGBoost4J-Spark (#4805)\n* Monitor for distributed environment (#4829). This is useful for identifying performance bottleneck.\n* Add check for length of weights and produce a good error message (#4872)\n* Fix DMatrix doc (#4884)\n* Export C++ headers in CMake installation (#4897)\n* Update license year in README.md to 2019 (#4940)\n* Fix incorrectly displayed Note in the doc (#4943)\n* Follow PEP 257 Docstring Conventions (#4959)\n* Document minimum version required for Google Test (#5001)\n* Add better error message for invalid feature names (#5024)\n* Some guidelines on device memory usage (#5038)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\n<!-- Use this section to tell people about which versions of your project are\ncurrently being supported with security updates. -->\nSecurity updates are applied only to the most recent release.\n\n## Reporting a Vulnerability\n\n<!-- Use this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc. -->\n\nTo report a security issue, please email\n[security@xgboost-ci.net](mailto:security@xgboost-ci.net)\nwith a description of the issue, the steps you took to create the issue,\naffected versions, and, if known, mitigations for the issue.\n\nAll support will be made on the best effort base, so please indicate the ""urgency level"" of the vulnerability as Critical, High, Medium or Low.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20735, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",41.350059032440186,128
What sequence of tensor operations does the STFTSpectrogram layer perform to support variable-length inputs in both channel-first and channel-last formats?,"The STFTSpectrogram layer in Keras supports variable-length inputs in both channel-first and channel-last formats by performing the following sequence of tensor operations:

1. **Channel normalization**: The input tensor is normalized to have a specified number of channels, either by adding or removing dimensions.
2. **STFT (Short-Time Fourier Transform)**: The normalized tensor is then transformed into the frequency domain using the STFT algorithm.
3. **Spectrogram calculation**: The resulting time-frequency representation is calculated as the squared magnitude of the STFT output.
4. **Normalization and scaling**: The final spectrogram values are normalized to have a specified dynamic range, and scaled to match the desired output format.

This sequence of operations allows the layer to handle inputs with varying lengths and channel configurations, making it suitable for applications such as audio signal processing and feature extraction.","The STFTSpectrogram layer first builds a fixed “frame” kernel and then in STFTSpectrogram._apply_conv:

• If data_format=""channels_last"":  
  1. Transpose inputs from [B, T, C] → [B, C, T]  
  2. Reshape to [B·C, T, 1]  
  3. ops.conv → [B·C, F, T′]  
  4. Transpose back [B·C, F, T′] → [B·C, T′, F]  
  5. Reshape to [B, C, F, T′]

• If data_format=""channels_first"":  
  1. Reshape [B, C, T] → [B·C, 1, T]  
  2. ops.conv → [B·C, F, T′]  
  3. Reshape to [B, C, F, T′]

Then in STFTSpectrogram._adjust_shapes it uses ops.reshape and ops.transpose to support both variable length and optional expand_dims:

• channels_last & expand_dims: transpose [0,3,2,1] → [B, T′, F, C]  
• channels_last & no expand_dims: reshape [B, C·F, T′] → transpose [0,2,1] → [B, T′, C·F]  
• channels_first & expand_dims: transpose [0,1,3,2] → [B, C, T′, F]  
• channels_first & no expand_dims: reshape [B, C·F, T′]

All reshapes use batch_size=–1 so T′ can vary at runtime.","['CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.path\nThe path of the variable within the Keras model or layer.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'METHOD keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.call has parameter or accepts argument keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.call.inputs', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.value\nThe current value of the variable (numpy array or backend tensor).\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'METHOD keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.__init__ has parameter or accepts argument keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.__init__.frame_length', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.aggregation\nThe strategy for aggregating this variable.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'MODULE keras.keras.src.ops.math contains FUNCTION keras.keras.src.ops.math.stft\nShort-Time Fourier Transform along the last axis of the input.  The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time.  Args:     x: Input tensor.     sequence_length: An integer representing the sequence length.     sequence_stride: An integer representing the sequence hop size.     fft_length: An integer representing the size of the FFT to apply. If not         specified, uses the smallest power of 2 enclosing `sequence_length`.     window: A string, a tensor of the window or `None`. If `window` is a         string, available values are `""hann""` and `""hamming""`. If `window`         is a tensor, it will be used directly as the window and its length         must be `sequence_length`. If `window` is `None`, no windowing is         used. Defaults to `""hann""`.     center: Whether to pad `x` on both sides so that the t-th sequence is         centered at time `t * sequence_stride`. Otherwise, the t-th sequence         begins at time `t * sequence_stride`. Defaults to `True`.  Returns:     A tuple containing two tensors - the real and imaginary parts of the     STFT output.  Example:  >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0]) >>> stft(x, 3, 2, 3) (array([[0.75, -0.375],    [3.75, -1.875],    [5.25, -2.625]]), array([[0.0, 0.64951905],    [0.0, 0.64951905],    [0.0, -0.64951905]]))', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.synchronization\nThe strategy for synchronizing this variable.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 20736, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",45.07364583015442,129
Under what conditions does _retrieve_class_or_fn prepend a '>' to the name when searching custom_objects?,"_retrieve_class_or_fn prepends a '>' to the name when searching custom_objects in keras if it is unable to find a class or function by the given name, indicating that the object is likely a subclass of a Keras layer.","In the `if obj_type == ""function"" and module == ""builtins""` block (in `keras/src/saving/serialization_lib.py`), after checking all `BUILTIN_MODULES`, there’s a Keras 3.6-compat workaround: if the raw `name` contains no “>”, it does  

    separated_name = "">"" + name  

and then looks for any key in `custom_objects` that ends with that `"">""+name""`. That’s the only time `_retrieve_class_or_fn` prepends a “>” to the name.","[""MODULE keras.keras.src.saving.object_registration contains FUNCTION keras.keras.src.saving.object_registration.get_registered_object\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'CLASS cognee.cognee.modules.search.types.SearchResult.SearchResult has field or attribute or property cognee.cognee.modules.search.types.SearchResult.SearchResult.search_result', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.get_registered_object.custom_objects\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'CLASS cognee.cognee.modules.search.exceptions.exceptions.UnsupportedSearchTypeError has field or attribute or property cognee.cognee.modules.search.exceptions.exceptions.name', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.GLOBAL_CUSTOM_OBJECTS\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'CLASS cognee.cognee.modules.search.types.SearchResult.SearchResult has field or attribute or property cognee.cognee.modules.search.types.SearchResult.SearchResult.dataset_name', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.get_registered_object.name\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20755, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",35.26349949836731,130
"In TensorFlow graph mode, which dimension of the provided mask does the TimeDistributed layer still validate when omitting the batch size comparison?","In TensorFlow graph mode, the TimeDistributed layer still validates the mask when omitting the batch size comparison on the second dimension (i.e., the temporal dimension). This is because the `supports_masking` method of the TimeDistributed class returns `True`, indicating that the layer supports masking. The mask validation occurs on the second dimension, which is typical for time-series data where each sample has a corresponding time step.","In TimeDistributed.call (keras/src/layers/rnn/time_distributed.py), when running in TF graph mode it skips checking mask_shape[0] (batch) but still asserts that mask_shape[1] == timesteps (via `mask_shape[1:2] != (timesteps,)`).","['CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.supports_masking\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.inputs\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.mask\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.call.data\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'MODULE keras.keras.src.layers.rnn.time_distributed contains CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.call.i\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has method keras.keras.src.layers.rnn.time_distributed.TimeDistributed.compute_output_shape\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20765, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.046802043914795,131
"In the BERT benchmark data loader, how does the feature-splitting function convert a GLUE example's dictionary into the (features, label) tuple expected by KerasNLP?","The feature-splitting function in Keras NLP converts a GLUE example's dictionary into the (features, label) tuple expected by KerasNLP using the `split_features` method from the `bert_benchmark` class. This method takes the input data and splits it into features and labels based on the specified format. The `feature_names` attribute of the class is used to determine which keys in the input dictionary correspond to features and which key corresponds to the label. The `split_features` method is called from the `load_data` method, which loads the GLUE/MRPC dataset and converts it into the format expected by KerasNLP.","In benchmarks/model_benchmark/bert_benchmark.py the split_features function does exactly this: it takes the GLUE example dict x, builds features = tuple(x[name] for name in feature_names), pulls out label = x[""label""], and returns (features, label) so it matches the (inputs, target) format KerasNLP expects.","['MODULE keras.benchmarks.model_benchmark.bert_benchmark contains FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.load_data\nLoad data.  Load GLUE/MRPC dataset, and convert the dictionary format to (features, label), where `features` is a tuple of all input sentences.', 'MODULE keras.benchmarks.model_benchmark.bert_benchmark contains FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.split_features', 'FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.load_data has parameter or accepts argument keras.benchmarks.model_benchmark.bert_benchmark.FLAGS\nLoad data.  Load GLUE/MRPC dataset, and convert the dictionary format to (features, label), where `features` is a tuple of all input sentences.', 'FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.split_features has parameter or accepts argument keras.benchmarks.model_benchmark.bert_benchmark.split_features.x', 'MODULE keras.keras.src.datasets.cifar contains FUNCTION keras.keras.src.datasets.cifar.load_batch\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', 'FUNCTION keras.keras.src.datasets.cifar.load_batch has parameter or accepts argument keras.keras.src.datasets.cifar.load_batch.fpath\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', 'CLASS keras.keras.src.layers.preprocessing.feature_space.FeatureSpace has field or attribute or property keras.keras.src.layers.preprocessing.feature_space.FeatureSpace.built\nOne-stop utility for preprocessing and encoding structured data.  Arguments:     feature_names: Dict mapping the names of your features to their         type specification, e.g. `{""my_feature"": ""integer_categorical""}`         or `{""my_feature"": FeatureSpace.integer_categorical()}`.         For a complete list of all supported types, see         ""Available feature types"" paragraph below.     output_mode: One of `""concat""` or `""dict""`. In concat mode, all         features get concatenated together into a single vector.         In dict mode, the FeatureSpace returns a dict of individually         encoded features (with the same keys as the input dict keys).     crosses: List of features to be crossed together, e.g.         `crosses=[(""feature_1"", ""feature_2"")]`. The features will be         ""crossed"" by hashing their combined value into         a fixed-length vector.     crossing_dim: Default vector size for hashing crossed features.         Defaults to `32`.     hashing_dim: Default vector size for hashing features of type         `""integer_hashed""` and `""string_hashed""`. Defaults to `32`.     num_discretization_bins: Default number of bins to be used for         discretizing features of type `""float_discretized""`.         Defaults to `32`.  **Available feature types:**  Note that all features can be referred to by their string name, e.g. `""integer_categorical""`. When using the string name, the default argument values are used.  ```python # Plain float values. FeatureSpace.float(name=None)  # Float values to be preprocessed via featurewise standardization # (i.e. via a `keras.layers.Normalization` layer). FeatureSpace.float_normalized(name=None)  # Float values to be preprocessed via linear rescaling # (i.e. via a `keras.layers.Rescaling` layer). FeatureSpace.float_rescaled(scale=1., offset=0., name=None)  # Float values to be discretized. By default, the discrete # representation will then be one-hot encoded. FeatureSpace.float_discretized(     num_bins, bin_boundaries=None, output_mode=""one_hot"", name=None)  # Integer values to be indexed. By default, the discrete # representation will then be one-hot encoded. FeatureSpace.integer_categorical(     max_tokens=None, num_oov_indices=1, output_mode=""one_hot"", name=None)  # String values to be indexed. By default, the discrete # representation will then be one-hot encoded. FeatureSpace.string_categorical(     max_tokens=None, num_oov_indices=1, output_mode=""one_hot"", name=None)  # Integer values to be hashed into a fixed number of bins. # By default, the discrete representation will then be one-hot encoded. FeatureSpace.integer_hashed(num_bins, output_mode=""one_hot"", name=None)  # String values to be hashed into a fixed number of bins. # By default, the discrete representation will then be one-hot encoded. FeatureSpace.string_hashed(num_bins, output_mode=""one_hot"", name=None) ```  Examples:  **Basic usage with a dict of input data:**  ```python raw_data = {     ""float_values"": [0.0, 0.1, 0.2, 0.3],     ""string_values"": [""zero"", ""one"", ""two"", ""three""],     ""int_values"": [0, 1, 2, 3], } dataset = tf.data.Dataset.from_tensor_slices(raw_data)  feature_space = FeatureSpace(     features={         ""float_values"": ""float_normalized"",         ""string_values"": ""string_categorical"",         ""int_values"": ""integer_categorical"",     },     crosses=[(""string_values"", ""int_values"")],     output_mode=""concat"", ) # Before you start using the FeatureSpace, # you must `adapt()` it on some data. feature_space.adapt(dataset)  # You can call the FeatureSpace on a dict of data (batched or unbatched). output_vector = feature_space(raw_data) ```  **Basic usage with `tf.data`:**  ```python # Unlabeled data preprocessed_ds = unlabeled_dataset.map(feature_space)  # Labeled data preprocessed_ds = labeled_dataset.map(lambda x, y: (feature_space(x), y)) ```  **Basic usage with the Keras Functional API:**  ```python # Retrieve a dict Keras Input objects inputs = feature_space.get_inputs() # Retrieve the corresponding encoded Keras tensors encoded_features = feature_space.get_encoded_features() # Build a Functional model outputs = keras.layers.Dense(1, activation=""sigmoid"")(encoded_features) model = keras.Model(inputs, outputs) ```  **Customizing each feature or feature cross:**  ```python feature_space = FeatureSpace(     features={         ""float_values"": FeatureSpace.float_normalized(),         ""string_values"": FeatureSpace.string_categorical(max_tokens=10),         ""int_values"": FeatureSpace.integer_categorical(max_tokens=10),     },     crosses=[         FeatureSpace.cross((""string_values"", ""int_values""), crossing_dim=32)     ],     output_mode=""concat"", ) ```  **Returning a dict of integer-encoded features:**  ```python feature_space = FeatureSpace(     features={         ""string_values"": FeatureSpace.string_categorical(output_mode=""int""),         ""int_values"": FeatureSpace.integer_categorical(output_mode=""int""),     },     crosses=[         FeatureSpace.cross(             feature_names=(""string_values"", ""int_values""),             crossing_dim=32,             output_mode=""int"",         )     ],     output_mode=""dict"", ) ```  **Specifying your own Keras preprocessing layer:**  ```python # Let\'s say that one of the features is a short text paragraph that # we want to encode as a vector (one vector per paragraph) via TF-IDF. data = {     ""text"": [""1st string"", ""2nd string"", ""3rd string""], }  # There\'s a Keras layer for this: TextVectorization. custom_layer = layers.TextVectorization(output_mode=""tf_idf"")  # We can use FeatureSpace.feature to create a custom feature # that will use our preprocessing layer. feature_space = FeatureSpace(     features={         ""text"": FeatureSpace.feature(             preprocessor=custom_layer, dtype=""string"", output_mode=""float""         ),     },     output_mode=""concat"", ) feature_space.adapt(tf.data.Dataset.from_tensor_slices(data)) output_vector = feature_space(data) ```  **Retrieving the underlying Keras preprocessing layers:**  ```python # The preprocessing layer of each feature is available in `.preprocessors`. preprocessing_layer = feature_space.preprocessors[""feature1""]  # The crossing layer of each feature cross is available in `.crossers`. # It\'s an instance of keras.layers.HashedCrossing. crossing_layer = feature_space.crossers[""feature1_X_feature2""] ```  **Saving and reloading a FeatureSpace:**  ```python feature_space.save(""featurespace.keras"") reloaded_feature_space = keras.models.load_model(""featurespace.keras"") ```', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\ninformation, please see our tutorial on [categorical\ndata](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html), along with\nexamples linked on that page. (#7380, #7708, #7695, #7330, #7307, #7322, #7705,\n#7652, #7592, #7666, #7576, #7569, #7529, #7575, #7393, #7465, #7385, #7371, #7745, #7810)\n\nIn the future, we will continue to improve categorical data support with new features and\noptimizations. Also, we are looking forward to bringing the feature beyond Python binding,\ncontributions and feedback are welcomed! Lastly, as a result of experimental status, the\nbehavior might be subject to change, especially the default value of related\nhyper-parameters.\n\n### Experimental support for multi-output model\n\nXGBoost 1.6 features initial support for the multi-output model, which includes\nmulti-output regression and multi-label classification. Along with this, the XGBoost\nclassifier has proper support for base margin without to need for the user to flatten the\ninput. In this initial support, XGBoost builds one model for each target similar to the\nsklearn meta estimator, for more details, please see our [quick\nintroduction](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html).\n\n(#7365, #7736, #7607, #7574, #7521, #7514, #7456, #7453, #7455, #7434, #7429, #7405, #7381)\n\n### External memory support\nExternal memory support for both approx and hist tree method is considered feature\ncomplete in XGBoost 1.6.  Building upon the iterator-based interface introduced in the\nprevious version, now both `hist` and `approx` iterates over each batch of data during\ntraining and prediction.  In previous versions, `hist` concatenates all the batches into\nan internal representation, which is removed in this version.  As a result, users can\nexpect higher scalability in terms of data size but might experience lower performance due\nto disk IO. (#7531, #7320, #7638, #7372)\n\n### Rewritten approx\n\nThe `approx` tree method is rewritten based on the existing `hist` tree method. The\nrewrite closes the feature gap between `approx` and `hist` and improves the performance.\nNow the behavior of `approx` should be more aligned with `hist` and `gpu_hist`. Here is a\nlist of user-visible changes:\n\n- Supports both `max_leaves` and `max_depth`.\n- Supports `grow_policy`.\n- Supports monotonic constraint.\n- Supports feature weights.\n- Use `max_bin` to replace `sketch_eps`.\n- Supports categorical data.\n- Faster performance for many of the datasets.\n- Improved performance and robustness for distributed training.\n- Supports prediction cache.\n- Significantly better performance for external memory when `depthwise` policy is used.\n\n### New serialization format\nBased on the existing JSON serialization format, we introduce UBJSON support as a more\nefficient alternative. Both formats will be available in the future and we plan to\ngradually [phase out](https://github.com/dmlc/xgboost/issues/7547) support for the old\nbinary model format.  Users can opt to use the different formats in the serialization\nfunction by providing the file extension `json` or `ubj`. Also, the `save_raw` function in\nall supported languages bindings gains a new parameter for exporting the model in different\nformats, available options are `json`, `ubj`, and `deprecated`, see document for the\nlanguage binding you are using for details. Lastly, the default internal serialization']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 19138, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",68.48003888130188,121
"How does sparse categorical crossentropy handle samples of the ignored class through the initial preprocessing, the backend loss computation, and the final mask application?","The sparse categorical crossentropy loss function in Keras handles samples of the ignored class through a combination of initial preprocessing, backend loss computation, and final mask application to ensure that the ignored classes are properly excluded from the loss calculation. During the initial preprocessing, ground truth values are preprocessed to ignore the specified class by setting corresponding elements to a special value (e.g., -1). The backend loss computation then uses these preprocessed values to compute the loss, excluding samples of the specified class. Finally, a mask is applied to the output based on the preprocessed ground truth values and the `ignore_class` parameter to ensure that ignored classes are not included in the final result.","In keras/src/losses/losses.py, sparse_categorical_crossentropy does the following when ignore_class≠None:

1. **Preprocessing**  
   – valid_mask = (y_true != ignore_class)  
   – y_true ← y_true * valid_mask  
   – y_pred ← y_pred * expand_dims(valid_mask, –1)  
   This zeroes out both the target and logits for any “ignored” sample.

2. **Backend loss**  
   – Calls ops.sparse_categorical_crossentropy(y_true, y_pred, …)  
   – Because ignored entries were zeroed, their raw loss is computed on zeros.

3. **Final masking**  
   – Reshapes valid_mask to the loss’s shape  
   – res ← where(valid_mask, res, 0.0)  (sets ignored losses to 0)  
   – backend.set_keras_mask(res, mask=valid_mask)  
   This ensures ignored samples contribute zero to any reduction or metric.","['FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.y_true\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'MODULE keras.keras.src.legacy.backend contains FUNCTION keras.keras.src.legacy.backend.sparse_categorical_crossentropy\nDEPRECATED.', 'FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.from_logits\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'FUNCTION keras.keras.src.legacy.backend.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.legacy.backend.sparse_categorical_crossentropy.ignore_class\nDEPRECATED.', 'FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.axis\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'MODULE keras.keras.src.backend.jax.nn contains FUNCTION keras.keras.src.backend.jax.nn.sparse_categorical_crossentropy', 'MODULE keras.keras.src.losses.losses contains FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.7 (2017.12.30)\n* **This version represents a major change from the last release (v0.6), which was released one year and half ago.**\n* Updated Sklearn API\n  - Add compatibility layer for scikit-learn v0.18: `sklearn.cross_validation` now deprecated\n  - Updated to allow use of all XGBoost parameters via `**kwargs`.\n  - Updated `nthread` to `n_jobs` and `seed` to `random_state` (as per Sklearn convention); `nthread` and `seed` are now marked as deprecated\n  - Updated to allow choice of Booster (`gbtree`, `gblinear`, or `dart`)\n  - `XGBRegressor` now supports instance weights (specify `sample_weight` parameter)\n  - Pass `n_jobs` parameter to the `DMatrix` constructor\n  - Add `xgb_model` parameter to `fit` method, to allow continuation of training\n* Refactored gbm to allow more friendly cache strategy\n  - Specialized some prediction routine\n* Robust `DMatrix` construction from a sparse matrix\n* Faster construction of `DMatrix` from 2D NumPy matrices: elide copies, use of multiple threads\n* Automatically remove nan from input data when it is sparse.\n  - This can solve some of user reported problem of istart != hist.size\n* Fix the single-instance prediction function to obtain correct predictions\n* Minor fixes\n  - Thread local variable is upgraded so it is automatically freed at thread exit.\n  - Fix saving and loading `count::poisson` models\n  - Fix CalcDCG to use base-2 logarithm\n  - Messages are now written to stderr instead of stdout\n  - Keep built-in evaluations while using customized evaluation functions\n  - Use `bst_float` consistently to minimize type conversion\n  - Copy the base margin when slicing `DMatrix`\n  - Evaluation metrics are now saved to the model file\n  - Use `int32_t` explicitly when serializing version\n  - In distributed training, synchronize the number of features after loading a data matrix.\n* Migrate to C++11\n  - The current master version now requires C++11 enabled compiled(g++4.8 or higher)\n* Predictor interface was factored out (in a manner similar to the updater interface).\n* Makefile support for Solaris and ARM\n* Test code coverage using Codecov\n* Add CPP tests\n* Add `Dockerfile` and `Jenkinsfile` to support continuous integration for GPU code\n* New functionality\n  - Ability to adjust tree model's statistics to a new dataset without changing tree structures.\n  - Ability to extract feature contributions from individual predictions, as described in [here](http://blog.datadive.net/interpreting-random-forests/) and [here](https://arxiv.org/abs/1706.06060).\n  - Faster, histogram-based tree algorithm (`tree_method='hist'`) .\n  - GPU/CUDA accelerated tree algorithms (`tree_method='gpu_hist'` or `'gpu_exact'`), including the GPU-based predictor.\n  - Monotonic constraints: when other features are fixed, force the prediction to be monotonic increasing with respect to a certain specified feature.\n  - Faster gradient calculation using AVX SIMD\n  - Ability to export models in JSON format\n  - Support for Tweedie regression\n  - Additional dropout options for DART: binomial+1, epsilon\n  - Ability to update an existing model in-place: this is useful for many applications, such as determining feature importance\n* Python package:\n  - New parameters:\n    - `learning_rates` in `cv()`\n    - `shuffle` in `mknfold()`\n    - `max_features` and `show_values` in `plot_importance()`\n    - `sample_weight` in `XGBRegressor.fit()`""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 2, 'pr': 19838, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",54.540958881378174,122
Which exception is thrown when attempting to retrieve an attribute but the layer has never been called?,"The exception thrown when attempting to retrieve an attribute but the layer has never been called in keras is **AttributeError**. This error occurs because the layer has not been built yet, and you are trying to access its attributes. To fix this issue, you need to call the `build` method of the layer with the required inputs before accessing its attributes. The correct way to do this is by calling `layer.build(input_shape=(None, 784), dtype=float32)` before attempting to retrieve any attributes.","In Operation._get_node_attribute_at_index (keras/src/ops/operation.py), if self._inbound_nodes is empty it raises an AttributeError indicating the layer has never been called.","['MODULE cognee.cognee.tests.unit.modules.graph.cognee_graph_test contains FUNCTION cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node\nTest retrieving edges for a nonexistent node raises an exception.', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.value', 'FUNCTION cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node has parameter or accepts argument cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node.setup_graph\nTest retrieving edges for a nonexistent node raises an exception.', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.default', 'MODULE xgboost.python-package.xgboost.libpath contains CLASS xgboost.python-package.xgboost.libpath.XGBoostLibraryNotFound\nError thrown by when xgboost is not found', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.entry_layer', 'CLASS xgboost.python-package.xgboost.libpath.XGBoostLibraryNotFound inherits from or is a subclass of xgboost.python-package.xgboost.libpath.Exception\nError thrown by when xgboost is not found', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nPreviously, users often ran into issues where the model file produced by one machine could not load or run on another machine. For example, models trained using a machine with an NVIDIA GPU could not run on another machine without a GPU (#5291, #5234). The reason is that the old binary format saved some internal configuration that were not universally applicable to all machines, e.g. `predictor='gpu_predictor'`.\n\n  Now, model saving function (`Booster.save_model()` in Python) will save only the model, without internal configuration. This will guarantee that your model file would be used anywhere. Internal configuration will be serialized in limited circumstances such as:\n  * Multiple nodes in a distributed system exchange model details over the network.\n  * Model checkpointing, to recover from possible crashes.\n\n  This work proved to be useful for parameter validation as well (see below).\n* Starting with 1.0.0 release, we will use semantic versioning to indicate whether the model produced by one version of XGBoost would be compatible with another version of XGBoost. Any change in the major version indicates a breaking change in the serialization format.\n* We now provide a robust method to save and load scikit-learn related attributes (#5245). Previously, we used Python pickle to save Python attributes related to `XGBClassifier`, `XGBRegressor`, and `XGBRanker` objects. The attributes are necessary to properly interact with scikit-learn. See #4639 for more details. The use of pickling hampered interoperability, as a pickle from one machine may not necessarily work on another machine. Starting with this release, we use an alternative method to serialize the scikit-learn related attributes. The use of Python pickle is now discouraged (#5236, #5281).\n\n### Parameter validation: detection of unused or incorrect parameters (#4553, #4577, #4738, #4801, #4961, #5101, #5157, #5167, #5256)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. Currently, parameter validation is available to R users and Python XGBoost API users. We are working to extend its support to scikit-learn users.\n* Configuration steps now have well-defined semantics (#4542, #4738), so we know exactly where and how the internal configurable parameters are changed.\n* The user can now use `save_config()` function to inspect all (used) training parameters. This is helpful for debugging model performance.\n\n### Allow individual workers to recover from faults (#4808, #4966)\n* Status quo: if a worker fails, all workers are shut down and restarted, and learning resumes from the last checkpoint. This involves requesting resources from the scheduler (e.g. Spark) and shuffling all the data again from scratch. Both of these operations can be quite costly and block training for extended periods of time, especially if the training data is big and the number of worker nodes is in the hundreds.\n* The proposed solution is to recover the single node that failed, instead of shutting down all workers. The rest of the clusters wait until the single failed worker is bootstrapped and catches up with the rest.\n* See roadmap at #4753. Note that this is work in progress. In particular, the feature is not yet available from XGBoost4J-Spark."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20156, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",40.9114408493042,123
How does the return_attention_scores flag passed to call affect the attention computation flow and the layer’s output?,"The `return_attention_scores` flag passed to `call` affects the attention computation flow and the layer’s output in Keras. When set to True, it returns the attention scores (after masking and softmax) as an additional output argument. This means that if you pass `return_attention_scores=True` to the Attention layer's call method, it will include the attention scores in its output, which can be useful for debugging or further processing.","When you pass return_attention_scores=True into MultiHeadAttention.call:

• In _compute_attention, it short-circuits the “fast” dot-product path (and forbids flash attention), so it always:  
  – scales Q, K, does an einsum to get raw scores  
  – applies masked softmax  
  – (optionally) applies dropout to the scores  
  – computes the context via another einsum  
  – returns both (context, attention_scores) rather than (context, None).

• Back in call, because return_attention_scores=True, it returns the 2-tuple (output, scores); otherwise it only returns the output tensor.","['CLASS keras.keras.src.layers.attention.attention.Attention has field or attribute or property keras.keras.src.layers.attention.attention.seed\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'METHOD keras.keras.src.layers.attention.attention.Attention.call has parameter or accepts argument keras.keras.src.layers.attention.attention.Attention.call.return_attention_scores', 'CLASS keras.keras.src.layers.attention.attention.Attention has method keras.keras.src.layers.attention.attention.Attention.build\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'METHOD keras.keras.src.layers.attention.grouped_query_attention.GroupedQueryAttention.call has parameter or accepts argument keras.keras.src.layers.attention.grouped_query_attention.GroupedQueryAttention.call.return_attention_scores', 'CLASS keras.keras.src.layers.attention.attention.Attention has method keras.keras.src.layers.attention.attention.Attention.__init__\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'METHOD keras.keras.src.layers.attention.multi_head_attention.MultiHeadAttention.call has parameter or accepts argument keras.keras.src.layers.attention.multi_head_attention.MultiHeadAttention.call.return_attention_scores', 'CLASS keras.keras.src.layers.attention.attention.Attention has field or attribute or property keras.keras.src.layers.attention.attention.dropout\nDot-product attention layer, a.k.a. Luong-style attention.  Inputs are a list with 2 or 3 elements: 1. A `query` tensor of shape `(batch_size, Tq, dim)`. 2. A `value` tensor of shape `(batch_size, Tv, dim)`. 3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none     supplied, `value` will be used as a `key`.  The calculation follows the steps: 1. Calculate attention scores using `query` and `key` with shape     `(batch_size, Tq, Tv)`. 2. Use scores to calculate a softmax distribution with shape     `(batch_size, Tq, Tv)`. 3. Use the softmax distribution to create a linear combination of `value`     with shape `(batch_size, Tq, dim)`.  Args:     use_scale: If `True`, will create a scalar variable to scale the         attention scores.     dropout: Float between 0 and 1. Fraction of the units to drop for the         attention scores. Defaults to `0.0`.     seed: A Python integer to use as random seed in case of `dropout`.     score_mode: Function to use to compute attention scores, one of         `{""dot"", ""concat""}`. `""dot""` refers to the dot product between the         query and key vectors. `""concat""` refers to the hyperbolic tangent         of the concatenation of the `query` and `key` vectors.  Call arguments:     inputs: List of the following tensors:         - `query`: Query tensor of shape `(batch_size, Tq, dim)`.         - `value`: Value tensor of shape `(batch_size, Tv, dim)`.         - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If             not given, will use `value` for both `key` and `value`, which is             the most common case.     mask: List of the following tensors:         - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.             If given, the output will be zero at the positions where             `mask==False`.         - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.             If given, will apply the mask such that values at positions              where `mask==False` do not contribute to the result.     return_attention_scores: bool, it `True`, returns the attention scores         (after masking and softmax) as an additional output argument.     training: Python boolean indicating whether the layer should behave in         training mode (adding dropout) or in inference mode (no dropout).     use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds         a mask such that position `i` cannot attend to positions `j > i`.         This prevents the flow of information from the future towards the         past. Defaults to `False`.  Output:     Attention outputs of shape `(batch_size, Tq, dim)`.     (Optional) Attention scores after masking and softmax with shape         `(batch_size, Tq, Tv)`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### GPUTreeSHAP: GPU acceleration of the TreeSHAP algorithm (#6038, #6064, #6087, #6099, #6163, #6281, #6332)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) is a game theoretic approach to explain predictions of machine learning models. It computes feature importance scores for individual examples, establishing how each feature influences a particular prediction. TreeSHAP is an optimized SHAP algorithm specifically designed for decision tree ensembles.\n* Starting with 1.3.0 release, it is now possible to leverage CUDA-capable GPUs to accelerate the TreeSHAP algorithm. Check out [the demo notebook](https://github.com/dmlc/xgboost/blob/master/demo/gpu_acceleration/shap.ipynb).\n* The CUDA implementation of the TreeSHAP algorithm is hosted at [rapidsai/GPUTreeSHAP](https://github.com/rapidsai/gputreeshap). XGBoost imports it as a Git submodule.\n\n### New style Python callback API (#6199, #6270, #6320, #6348, #6376, #6399, #6441)\n* The XGBoost Python package now offers a re-designed callback API. The new callback API lets you design various extensions of training in idomatic Python. In addition, the new callback API allows you to use early stopping with the native Dask API (`xgboost.dask`). Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.3.0/python/callbacks.html) and [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/callbacks.py).\n\n### Enable the use of `DeviceQuantileDMatrix` / `DaskDeviceQuantileDMatrix` with large data (#6201, #6229, #6234).\n* `DeviceQuantileDMatrix` can achieve memory saving by avoiding extra copies of the training data, and the saving is bigger for large data. Unfortunately, large data with more than 2^31 elements was triggering integer overflow bugs in CUB and Thrust. Tracking issue: #6228.\n* This release contains a series of work-arounds to allow the use of `DeviceQuantileDMatrix` with large data:\n  - Loop over `copy_if` (#6201)\n  - Loop over `thrust::reduce` (#6229)\n  - Implement the inclusive scan algorithm in-house, to handle large offsets (#6234)\n\n### Support slicing of tree models (#6302)\n* Accessing the best iteration of a model after the application of early stopping used to be error-prone, need to manually pass the `ntree_limit` argument to the `predict()` function.\n* Now we provide a simple interface to slice tree models by specifying a range of boosting rounds. The tree ensemble can be split into multiple sub-ensembles via the slicing interface. Check out [an example](https://xgboost.readthedocs.io/en/release_1.3.0/python/model.html).\n* In addition, the early stopping callback now supports `save_best` option. When enabled, XGBoost will save (persist) the model at the best boosting round and discard the trees that were fit subsequent to the best round.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 20482, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",41.85833811759949,124
"When serializing this RNN layer’s configuration, what step ensures the cell attribute is omitted from the returned config?","When serializing an RNN layer's configuration in Keras, the `cell` attribute is omitted from the returned config due to the absence of its corresponding parameter in the `from_config()` method. This ensures that the cell information is not included in the serialized configuration, preventing potential inconsistencies or errors when loading the layer from the config.","In SimpleRNN.get_config (keras/src/layers/rnn/simple_rnn.py), right after calling `base_config = super().get_config()`, there’s a  
```python
del base_config[""cell""]
```  
which removes the `cell` entry before returning the merged config.","['CLASS keras.keras.src.layers.rnn.rnn.RNN has field or attribute or property keras.keras.src.layers.rnn.rnn.RNN.go_backwards\nBase class for recurrent layers.  Args:     cell: A RNN cell instance or a list of RNN cell instances.         A RNN cell is a class that has:         - A `call(input_at_t, states_at_t)` method, returning         `(output_at_t, states_at_t_plus_1)`. The call method of the         cell can also take the optional argument `constants`, see         section ""Note on passing external constants"" below.         - A `state_size` attribute. This can be a single integer         (single state) in which case it is the size of the recurrent         state. This can also be a list/tuple of integers         (one size per state).         - A `output_size` attribute, a single integer.         - A `get_initial_state(batch_size=None)`         method that creates a tensor meant to be fed to `call()` as the         initial state, if the user didn\'t specify any initial state         via other means. The returned initial state should have         shape `(batch_size, cell.state_size)`.         The cell might choose to create a tensor full of zeros,         or other values based on the cell\'s implementation.         `inputs` is the input tensor to the RNN layer, with shape         `(batch_size, timesteps, features)`.         If this method is not implemented         by the cell, the RNN layer will create a zero filled tensor         with shape `(batch_size, cell.state_size)`.         In the case that `cell` is a list of RNN cell instances, the cells         will be stacked on top of each other in the RNN, resulting in an         efficient stacked RNN.     return_sequences: Boolean (default `False`). Whether to return the last         output in the output sequence, or the full sequence.     return_state: Boolean (default `False`).         Whether to return the last state in addition to the output.     go_backwards: Boolean (default `False`).         If `True`, process the input sequence backwards and return the         reversed sequence.     stateful: Boolean (default `False`). If True, the last state         for each sample at index `i` in a batch will be used as initial         state for the sample of index `i` in the following batch.     unroll: Boolean (default `False`).         If True, the network will be unrolled, else a symbolic loop will be         used. Unrolling can speed-up a RNN, although it tends to be more         memory-intensive. Unrolling is only suitable for short sequences.     zero_output_for_mask: Boolean (default `False`).         Whether the output should use zeros for the masked timesteps.         Note that this field is only used when `return_sequences`         is `True` and `mask` is provided.         It can useful if you want to reuse the raw output sequence of         the RNN without interference from the masked timesteps, e.g.,         merging bidirectional RNNs.  Call arguments:     sequences: A 3-D tensor with shape `(batch_size, timesteps, features)`.     initial_state: List of initial state tensors to be passed to the first         call of the cell.     mask: Binary tensor of shape `[batch_size, timesteps]`         indicating whether a given timestep should be masked.         An individual `True` entry indicates that the corresponding         timestep should be utilized, while a `False` entry indicates         that the corresponding timestep should be ignored.     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed         to the cell when calling it.         This is for use with cells that use dropout.  Output shape:  - If `return_state`: a list of tensors. The first tensor is the output. The remaining tensors are the last states, each with shape `(batch_size, state_size)`, where `state_size` could be a high dimension tensor shape. - If `return_sequences`: 3D tensor with shape `(batch_size, timesteps, output_size)`.  Masking:  This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use a `keras.layers.Embedding` layer with the `mask_zero` parameter set to `True`.  Note on using statefulness in RNNs:  You can set RNN layers to be \'stateful\', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches.  To enable statefulness:  - Specify `stateful=True` in the layer constructor. - Specify a fixed batch size for your model, by passing     `batch_size=...` to the `Input` layer(s) of your model.     Remember to also specify the same `batch_size=...` when     calling `fit()`, or otherwise use a generator-like     data source like a `keras.utils.PyDataset` or a     `tf.data.Dataset`. - Specify `shuffle=False` when calling `fit()`, since your     batches are expected to be temporally ordered.  To reset the states of your model, call `.reset_state()` on either a specific layer, or on your entire model.  Note on specifying the initial state of RNNs:  You can specify the initial state of RNN layers symbolically by calling them with the keyword argument `initial_state`. The value of `initial_state` should be a tensor or list of tensors representing the initial state of the RNN layer.  You can specify the initial state of RNN layers numerically by calling `reset_state()` with the keyword argument `states`. The value of `states` should be a numpy array or list of numpy arrays representing the initial state of the RNN layer.  Examples:  ```python from keras.layers import RNN from keras import ops  # First, let\'s define a RNN Cell, as a layer subclass. class MinimalRNNCell(keras.Layer):      def __init__(self, units, **kwargs):         super().__init__(**kwargs)         self.units = units         self.state_size = units      def build(self, input_shape):         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),                                       initializer=\'uniform\',                                       name=\'kernel\')         self.recurrent_kernel = self.add_weight(             shape=(self.units, self.units),             initializer=\'uniform\',             name=\'recurrent_kernel\')      def call(self, inputs, states):         prev_output = states[0]         h = ops.matmul(inputs, self.kernel)         output = h + ops.matmul(prev_output, self.recurrent_kernel)         return output, [output]  # Let\'s use this cell in a RNN layer:  cell = MinimalRNNCell(32) x = keras.Input((None, 5)) layer = RNN(cell) y = layer(x)  # Here\'s how to use the cell to build a stacked RNN:  cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = keras.Input((None, 5)) layer = RNN(cells) y = layer(x) ```', 'METHOD keras.keras.src.layers.rnn.rnn.RNN.from_config has parameter or accepts argument keras.keras.src.layers.rnn.rnn.RNN.cell', 'CLASS keras.keras.src.layers.rnn.rnn.RNN has field or attribute or property keras.keras.src.layers.rnn.rnn.RNN.zero_output_for_mask\nBase class for recurrent layers.  Args:     cell: A RNN cell instance or a list of RNN cell instances.         A RNN cell is a class that has:         - A `call(input_at_t, states_at_t)` method, returning         `(output_at_t, states_at_t_plus_1)`. The call method of the         cell can also take the optional argument `constants`, see         section ""Note on passing external constants"" below.         - A `state_size` attribute. This can be a single integer         (single state) in which case it is the size of the recurrent         state. This can also be a list/tuple of integers         (one size per state).         - A `output_size` attribute, a single integer.         - A `get_initial_state(batch_size=None)`         method that creates a tensor meant to be fed to `call()` as the         initial state, if the user didn\'t specify any initial state         via other means. The returned initial state should have         shape `(batch_size, cell.state_size)`.         The cell might choose to create a tensor full of zeros,         or other values based on the cell\'s implementation.         `inputs` is the input tensor to the RNN layer, with shape         `(batch_size, timesteps, features)`.         If this method is not implemented         by the cell, the RNN layer will create a zero filled tensor         with shape `(batch_size, cell.state_size)`.         In the case that `cell` is a list of RNN cell instances, the cells         will be stacked on top of each other in the RNN, resulting in an         efficient stacked RNN.     return_sequences: Boolean (default `False`). Whether to return the last         output in the output sequence, or the full sequence.     return_state: Boolean (default `False`).         Whether to return the last state in addition to the output.     go_backwards: Boolean (default `False`).         If `True`, process the input sequence backwards and return the         reversed sequence.     stateful: Boolean (default `False`). If True, the last state         for each sample at index `i` in a batch will be used as initial         state for the sample of index `i` in the following batch.     unroll: Boolean (default `False`).         If True, the network will be unrolled, else a symbolic loop will be         used. Unrolling can speed-up a RNN, although it tends to be more         memory-intensive. Unrolling is only suitable for short sequences.     zero_output_for_mask: Boolean (default `False`).         Whether the output should use zeros for the masked timesteps.         Note that this field is only used when `return_sequences`         is `True` and `mask` is provided.         It can useful if you want to reuse the raw output sequence of         the RNN without interference from the masked timesteps, e.g.,         merging bidirectional RNNs.  Call arguments:     sequences: A 3-D tensor with shape `(batch_size, timesteps, features)`.     initial_state: List of initial state tensors to be passed to the first         call of the cell.     mask: Binary tensor of shape `[batch_size, timesteps]`         indicating whether a given timestep should be masked.         An individual `True` entry indicates that the corresponding         timestep should be utilized, while a `False` entry indicates         that the corresponding timestep should be ignored.     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed         to the cell when calling it.         This is for use with cells that use dropout.  Output shape:  - If `return_state`: a list of tensors. The first tensor is the output. The remaining tensors are the last states, each with shape `(batch_size, state_size)`, where `state_size` could be a high dimension tensor shape. - If `return_sequences`: 3D tensor with shape `(batch_size, timesteps, output_size)`.  Masking:  This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use a `keras.layers.Embedding` layer with the `mask_zero` parameter set to `True`.  Note on using statefulness in RNNs:  You can set RNN layers to be \'stateful\', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches.  To enable statefulness:  - Specify `stateful=True` in the layer constructor. - Specify a fixed batch size for your model, by passing     `batch_size=...` to the `Input` layer(s) of your model.     Remember to also specify the same `batch_size=...` when     calling `fit()`, or otherwise use a generator-like     data source like a `keras.utils.PyDataset` or a     `tf.data.Dataset`. - Specify `shuffle=False` when calling `fit()`, since your     batches are expected to be temporally ordered.  To reset the states of your model, call `.reset_state()` on either a specific layer, or on your entire model.  Note on specifying the initial state of RNNs:  You can specify the initial state of RNN layers symbolically by calling them with the keyword argument `initial_state`. The value of `initial_state` should be a tensor or list of tensors representing the initial state of the RNN layer.  You can specify the initial state of RNN layers numerically by calling `reset_state()` with the keyword argument `states`. The value of `states` should be a numpy array or list of numpy arrays representing the initial state of the RNN layer.  Examples:  ```python from keras.layers import RNN from keras import ops  # First, let\'s define a RNN Cell, as a layer subclass. class MinimalRNNCell(keras.Layer):      def __init__(self, units, **kwargs):         super().__init__(**kwargs)         self.units = units         self.state_size = units      def build(self, input_shape):         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),                                       initializer=\'uniform\',                                       name=\'kernel\')         self.recurrent_kernel = self.add_weight(             shape=(self.units, self.units),             initializer=\'uniform\',             name=\'recurrent_kernel\')      def call(self, inputs, states):         prev_output = states[0]         h = ops.matmul(inputs, self.kernel)         output = h + ops.matmul(prev_output, self.recurrent_kernel)         return output, [output]  # Let\'s use this cell in a RNN layer:  cell = MinimalRNNCell(32) x = keras.Input((None, 5)) layer = RNN(cell) y = layer(x)  # Here\'s how to use the cell to build a stacked RNN:  cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = keras.Input((None, 5)) layer = RNN(cells) y = layer(x) ```', 'METHOD keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.get_config has parameter or accepts argument keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.cells', 'CLASS keras.keras.src.layers.rnn.rnn.RNN has field or attribute or property keras.keras.src.layers.rnn.rnn.RNN.return_sequences\nBase class for recurrent layers.  Args:     cell: A RNN cell instance or a list of RNN cell instances.         A RNN cell is a class that has:         - A `call(input_at_t, states_at_t)` method, returning         `(output_at_t, states_at_t_plus_1)`. The call method of the         cell can also take the optional argument `constants`, see         section ""Note on passing external constants"" below.         - A `state_size` attribute. This can be a single integer         (single state) in which case it is the size of the recurrent         state. This can also be a list/tuple of integers         (one size per state).         - A `output_size` attribute, a single integer.         - A `get_initial_state(batch_size=None)`         method that creates a tensor meant to be fed to `call()` as the         initial state, if the user didn\'t specify any initial state         via other means. The returned initial state should have         shape `(batch_size, cell.state_size)`.         The cell might choose to create a tensor full of zeros,         or other values based on the cell\'s implementation.         `inputs` is the input tensor to the RNN layer, with shape         `(batch_size, timesteps, features)`.         If this method is not implemented         by the cell, the RNN layer will create a zero filled tensor         with shape `(batch_size, cell.state_size)`.         In the case that `cell` is a list of RNN cell instances, the cells         will be stacked on top of each other in the RNN, resulting in an         efficient stacked RNN.     return_sequences: Boolean (default `False`). Whether to return the last         output in the output sequence, or the full sequence.     return_state: Boolean (default `False`).         Whether to return the last state in addition to the output.     go_backwards: Boolean (default `False`).         If `True`, process the input sequence backwards and return the         reversed sequence.     stateful: Boolean (default `False`). If True, the last state         for each sample at index `i` in a batch will be used as initial         state for the sample of index `i` in the following batch.     unroll: Boolean (default `False`).         If True, the network will be unrolled, else a symbolic loop will be         used. Unrolling can speed-up a RNN, although it tends to be more         memory-intensive. Unrolling is only suitable for short sequences.     zero_output_for_mask: Boolean (default `False`).         Whether the output should use zeros for the masked timesteps.         Note that this field is only used when `return_sequences`         is `True` and `mask` is provided.         It can useful if you want to reuse the raw output sequence of         the RNN without interference from the masked timesteps, e.g.,         merging bidirectional RNNs.  Call arguments:     sequences: A 3-D tensor with shape `(batch_size, timesteps, features)`.     initial_state: List of initial state tensors to be passed to the first         call of the cell.     mask: Binary tensor of shape `[batch_size, timesteps]`         indicating whether a given timestep should be masked.         An individual `True` entry indicates that the corresponding         timestep should be utilized, while a `False` entry indicates         that the corresponding timestep should be ignored.     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed         to the cell when calling it.         This is for use with cells that use dropout.  Output shape:  - If `return_state`: a list of tensors. The first tensor is the output. The remaining tensors are the last states, each with shape `(batch_size, state_size)`, where `state_size` could be a high dimension tensor shape. - If `return_sequences`: 3D tensor with shape `(batch_size, timesteps, output_size)`.  Masking:  This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use a `keras.layers.Embedding` layer with the `mask_zero` parameter set to `True`.  Note on using statefulness in RNNs:  You can set RNN layers to be \'stateful\', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches.  To enable statefulness:  - Specify `stateful=True` in the layer constructor. - Specify a fixed batch size for your model, by passing     `batch_size=...` to the `Input` layer(s) of your model.     Remember to also specify the same `batch_size=...` when     calling `fit()`, or otherwise use a generator-like     data source like a `keras.utils.PyDataset` or a     `tf.data.Dataset`. - Specify `shuffle=False` when calling `fit()`, since your     batches are expected to be temporally ordered.  To reset the states of your model, call `.reset_state()` on either a specific layer, or on your entire model.  Note on specifying the initial state of RNNs:  You can specify the initial state of RNN layers symbolically by calling them with the keyword argument `initial_state`. The value of `initial_state` should be a tensor or list of tensors representing the initial state of the RNN layer.  You can specify the initial state of RNN layers numerically by calling `reset_state()` with the keyword argument `states`. The value of `states` should be a numpy array or list of numpy arrays representing the initial state of the RNN layer.  Examples:  ```python from keras.layers import RNN from keras import ops  # First, let\'s define a RNN Cell, as a layer subclass. class MinimalRNNCell(keras.Layer):      def __init__(self, units, **kwargs):         super().__init__(**kwargs)         self.units = units         self.state_size = units      def build(self, input_shape):         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),                                       initializer=\'uniform\',                                       name=\'kernel\')         self.recurrent_kernel = self.add_weight(             shape=(self.units, self.units),             initializer=\'uniform\',             name=\'recurrent_kernel\')      def call(self, inputs, states):         prev_output = states[0]         h = ops.matmul(inputs, self.kernel)         output = h + ops.matmul(prev_output, self.recurrent_kernel)         return output, [output]  # Let\'s use this cell in a RNN layer:  cell = MinimalRNNCell(32) x = keras.Input((None, 5)) layer = RNN(cell) y = layer(x)  # Here\'s how to use the cell to build a stacked RNN:  cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = keras.Input((None, 5)) layer = RNN(cells) y = layer(x) ```', 'METHOD keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.from_config has parameter or accepts argument keras.keras.src.layers.rnn.stacked_rnn_cells.StackedRNNCells.cells', 'CLASS keras.keras.src.layers.rnn.rnn.RNN has field or attribute or property keras.keras.src.layers.rnn.rnn.RNN.return_state\nBase class for recurrent layers.  Args:     cell: A RNN cell instance or a list of RNN cell instances.         A RNN cell is a class that has:         - A `call(input_at_t, states_at_t)` method, returning         `(output_at_t, states_at_t_plus_1)`. The call method of the         cell can also take the optional argument `constants`, see         section ""Note on passing external constants"" below.         - A `state_size` attribute. This can be a single integer         (single state) in which case it is the size of the recurrent         state. This can also be a list/tuple of integers         (one size per state).         - A `output_size` attribute, a single integer.         - A `get_initial_state(batch_size=None)`         method that creates a tensor meant to be fed to `call()` as the         initial state, if the user didn\'t specify any initial state         via other means. The returned initial state should have         shape `(batch_size, cell.state_size)`.         The cell might choose to create a tensor full of zeros,         or other values based on the cell\'s implementation.         `inputs` is the input tensor to the RNN layer, with shape         `(batch_size, timesteps, features)`.         If this method is not implemented         by the cell, the RNN layer will create a zero filled tensor         with shape `(batch_size, cell.state_size)`.         In the case that `cell` is a list of RNN cell instances, the cells         will be stacked on top of each other in the RNN, resulting in an         efficient stacked RNN.     return_sequences: Boolean (default `False`). Whether to return the last         output in the output sequence, or the full sequence.     return_state: Boolean (default `False`).         Whether to return the last state in addition to the output.     go_backwards: Boolean (default `False`).         If `True`, process the input sequence backwards and return the         reversed sequence.     stateful: Boolean (default `False`). If True, the last state         for each sample at index `i` in a batch will be used as initial         state for the sample of index `i` in the following batch.     unroll: Boolean (default `False`).         If True, the network will be unrolled, else a symbolic loop will be         used. Unrolling can speed-up a RNN, although it tends to be more         memory-intensive. Unrolling is only suitable for short sequences.     zero_output_for_mask: Boolean (default `False`).         Whether the output should use zeros for the masked timesteps.         Note that this field is only used when `return_sequences`         is `True` and `mask` is provided.         It can useful if you want to reuse the raw output sequence of         the RNN without interference from the masked timesteps, e.g.,         merging bidirectional RNNs.  Call arguments:     sequences: A 3-D tensor with shape `(batch_size, timesteps, features)`.     initial_state: List of initial state tensors to be passed to the first         call of the cell.     mask: Binary tensor of shape `[batch_size, timesteps]`         indicating whether a given timestep should be masked.         An individual `True` entry indicates that the corresponding         timestep should be utilized, while a `False` entry indicates         that the corresponding timestep should be ignored.     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed         to the cell when calling it.         This is for use with cells that use dropout.  Output shape:  - If `return_state`: a list of tensors. The first tensor is the output. The remaining tensors are the last states, each with shape `(batch_size, state_size)`, where `state_size` could be a high dimension tensor shape. - If `return_sequences`: 3D tensor with shape `(batch_size, timesteps, output_size)`.  Masking:  This layer supports masking for input data with a variable number of timesteps. To introduce masks to your data, use a `keras.layers.Embedding` layer with the `mask_zero` parameter set to `True`.  Note on using statefulness in RNNs:  You can set RNN layers to be \'stateful\', which means that the states computed for the samples in one batch will be reused as initial states for the samples in the next batch. This assumes a one-to-one mapping between samples in different successive batches.  To enable statefulness:  - Specify `stateful=True` in the layer constructor. - Specify a fixed batch size for your model, by passing     `batch_size=...` to the `Input` layer(s) of your model.     Remember to also specify the same `batch_size=...` when     calling `fit()`, or otherwise use a generator-like     data source like a `keras.utils.PyDataset` or a     `tf.data.Dataset`. - Specify `shuffle=False` when calling `fit()`, since your     batches are expected to be temporally ordered.  To reset the states of your model, call `.reset_state()` on either a specific layer, or on your entire model.  Note on specifying the initial state of RNNs:  You can specify the initial state of RNN layers symbolically by calling them with the keyword argument `initial_state`. The value of `initial_state` should be a tensor or list of tensors representing the initial state of the RNN layer.  You can specify the initial state of RNN layers numerically by calling `reset_state()` with the keyword argument `states`. The value of `states` should be a numpy array or list of numpy arrays representing the initial state of the RNN layer.  Examples:  ```python from keras.layers import RNN from keras import ops  # First, let\'s define a RNN Cell, as a layer subclass. class MinimalRNNCell(keras.Layer):      def __init__(self, units, **kwargs):         super().__init__(**kwargs)         self.units = units         self.state_size = units      def build(self, input_shape):         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),                                       initializer=\'uniform\',                                       name=\'kernel\')         self.recurrent_kernel = self.add_weight(             shape=(self.units, self.units),             initializer=\'uniform\',             name=\'recurrent_kernel\')      def call(self, inputs, states):         prev_output = states[0]         h = ops.matmul(inputs, self.kernel)         output = h + ops.matmul(prev_output, self.recurrent_kernel)         return output, [output]  # Let\'s use this cell in a RNN layer:  cell = MinimalRNNCell(32) x = keras.Input((None, 5)) layer = RNN(cell) y = layer(x)  # Here\'s how to use the cell to build a stacked RNN:  cells = [MinimalRNNCell(32), MinimalRNNCell(64)] x = keras.Input((None, 5)) layer = RNN(cells) y = layer(x) ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20636, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",46.795188188552856,125
How does the ONNX export function retrieve a concrete TensorFlow function without creating a temporary SavedModel?,"The ONNX export function in Keras retrieves a concrete TensorFlow function without creating a temporary SavedModel using the `get_concrete_fn` method from the `keras.export.onnx` module. This method takes a model and an input signature as arguments and returns a `_get_concrete_fn` object, which is a concrete TensorFlow function associated with the model. To use this method, you can create a simple Keras model, get a concrete function using `get_concrete_fn`, and then save it to an ONNX file by serializing it to a string. Here's an example code snippet that demonstrates how to do this:

```python
default
from keras.models import Model
from keras.layers import Input, Dense
from keras.utils import plot_model

# Create a simple Keras model
input_layer = Input(shape=(784,))
x = Dense(64, activation='relu')(input_layer)
output_layer = Dense(10, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)

# Export the model as an ONNX file
from keras.export.onnx import get_concrete_fn

concrete_fn = get_concrete_fn(model, input_signature=[tf.TensorSpec(shape=(1, 784), dtype=tf.float32)])

# Save the concrete function to a file
with tf.gfile.GFile('model.onnx', 'wb') as f:
    f.write(concrete_fn.to_graphdef().SerializeToString())
```
The `get_concrete_fn` method is used to get a concrete TensorFlow function associated with the model, and this function can then be saved to an ONNX file by serializing it to a string. This approach avoids creating a temporary SavedModel.","The ONNX exporter never writes out a SavedModel on disk. Instead it calls get_concrete_fn (in keras/src/export/onnx.py), which under the hood does:

• ExportArchive.track_and_add_endpoint(…)  
• (for TF) ExportArchive._filter_and_track_resources()  
• return ExportArchive._get_concrete_fn(DEFAULT_ENDPOINT_NAME)

That gives you a ready‐to‐go tf.function (concrete function) in memory, which is then handed straight to tf2onnx.convert.from_function. No temporary SavedModel is ever created.","['METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.format\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'MODULE keras.keras.src.export.onnx contains FUNCTION keras.keras.src.export.onnx.get_concrete_fn\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.verbose\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'FUNCTION keras.keras.src.export.onnx.get_concrete_fn has parameter or accepts argument keras.keras.src.export.onnx.kwargs\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.input_signature\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'FUNCTION keras.keras.src.export.onnx.get_concrete_fn has parameter or accepts argument keras.keras.src.export.onnx.get_concrete_fn.model\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 4, 'pr': 20725, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",73.53118181228638,126
Which argument to the Keras ISTFT function requires a 32-bit integer dtype to avoid a conversion error?,The argument `length` to the Keras ISTFT function requires a 32-bit integer dtype to avoid a conversion error.,The fft_length argument must be a 32-bit integer (tf.int32) to avoid the conversion error.,"['FUNCTION qlib.qlib.rl.order_execution.interpreter.canonicalize has parameter or accepts argument qlib.qlib.rl.order_execution.interpreter.canonicalize.value\nTo 32-bit numeric types. Recursively.', 'FUNCTION keras.keras.src.backend.numpy.math.istft has parameter or accepts argument keras.keras.src.backend.numpy.math.istft.length', 'MODULE qlib.qlib.rl.order_execution.interpreter contains FUNCTION qlib.qlib.rl.order_execution.interpreter.canonicalize\nTo 32-bit numeric types. Recursively.', 'FUNCTION keras.keras.src.backend.numpy.math.istft has parameter or accepts argument keras.keras.src.backend.numpy.math.istft.fft_length', 'FUNCTION keras.keras.src.ops.numpy.bitwise_not has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_not.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'FUNCTION keras.keras.src.backend.numpy.math.istft has parameter or accepts argument keras.keras.src.backend.numpy.math.istft.sequence_length', 'FUNCTION keras.keras.src.ops.numpy.bitwise_invert has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_invert.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20728, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",31.879873275756836,127
What exception types are asserted for an unrecognized format and for an unsupported backend in the export error tests?,"The exception types asserted for an unrecognized format and for an unsupported backend in the export error tests in keras are `ValueError` and `NotImplementedError`, respectively.","In ModelTest.test_export_error (keras/src/models/model_test.py) they assert:

• ValueError for an unrecognized format  
• NotImplementedError for an unsupported backend","['MODULE cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils contains FUNCTION cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils.format_neptune_error\nFormat Neptune Analytics specific errors for better readability.  Parameters: -----------     - error (Exception): The exception to format  Returns: --------     - str: Formatted error message', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.UnstructuredLibraryImportError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.status_code', 'FUNCTION cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils.format_neptune_error has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.neptune_driver.neptune_utils.format_neptune_error.error\nFormat Neptune Analytics specific errors for better readability.  Parameters: -----------     - error (Exception): The exception to format  Returns: --------     - str: Formatted error message', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.UnstructuredLibraryImportError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.name', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.UnstructuredLibraryImportError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.message', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.format\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Specify version macro in CMake. (#4730)\n* Include dmlc-tracker into XGBoost Python package (#4731)\n* [CI] Use long key ID for Ubuntu repository fingerprints. (#4783)\n* Remove plugin, CUDA related code in automake & autoconf files (#4789)\n* Skip related tests when scikit-learn is not installed. (#4791)\n* Ignore vscode and clion files (#4866)\n* Use bundled Google Test by default (#4900)\n* [CI] Raise timeout threshold in Jenkins (#4938)\n* Copy CMake parameter from dmlc-core. (#4948)\n* Set correct file permission. (#4964)\n* [CI] Update lint configuration to support latest pylint convention (#4971)\n* [CI] Upload nightly builds to S3 (#4976, #4979)\n* Add asan.so.5 to cmake script. (#4999)\n* [CI] Fix Travis tests. (#5062)\n* [CI] Locate vcomp140.dll from System32 directory (#5078)\n* Implement training observer to dump internal states of objects (#5088). This will be useful for debugging.\n* Fix visual studio output library directories (#5119)\n* [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)\n* [CI] Repair download URL for Maven 3.6.1 (#5139)\n* Don't use modernize-use-trailing-return-type in clang-tidy. (#5169)\n* Explicitly use UTF-8 codepage when using MSVC (#5197)\n* Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)\n* Make some GPU tests deterministic (#5229)\n* [R] Robust endian detection in CRAN xgboost build (#5232)\n* Support FreeBSD (#5233)\n* Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)\n* Fix compilation error due to 64-bit integer narrowing to `size_t` (#5250)\n* Remove use of `std::cout` from R package, to comply with CRAN policy (#5261)\n* Update DMLC-Core submodule (#4674, #4688, #4726, #4924)\n* Update Rabit submodule (#4560, #4667, #4718, #4808, #4966, #5237)\n\n### Usability Improvements, Documentation\n* Add Random Forest API to Python API doc (#4500)\n* Fix Python demo and doc. (#4545)\n* Remove doc about not supporting CUDA 10.1 (#4578)\n* Address some sphinx warnings and errors, add doc for building doc. (#4589)\n* Add instruction to run formatting checks locally (#4591)\n* Fix docstring for `XGBModel.predict()` (#4592)\n* Doc and demo for customized metric and objective (#4598, #4608)\n* Add to documentation how to run tests locally (#4610)\n* Empty evaluation list in early stopping should produce meaningful error message (#4633)\n* Fixed year to 2019 in conf.py, helpers.h and LICENSE (#4661)\n* Minor updates to links and grammar (#4673)\n* Remove `silent` in doc (#4689)\n* Remove old Python trouble shooting doc (#4729)\n* Add `os.PathLike` support for file paths to DMatrix and Booster Python classes (#4757)\n* Update XGBoost4J-Spark doc (#4804)\n* Regular formatting for evaluation metrics (#4803)\n* [jvm-packages] Refine documentation for handling missing values in XGBoost4J-Spark (#4805)\n* Monitor for distributed environment (#4829). This is useful for identifying performance bottleneck.\n* Add check for length of weights and produce a good error message (#4872)\n* Fix DMatrix doc (#4884)\n* Export C++ headers in CMake installation (#4897)\n* Update license year in README.md to 2019 (#4940)\n* Fix incorrectly displayed Note in the doc (#4943)\n* Follow PEP 257 Docstring Conventions (#4959)\n* Document minimum version required for Google Test (#5001)\n* Add better error message for invalid feature names (#5024)\n* Some guidelines on device memory usage (#5038)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\n<!-- Use this section to tell people about which versions of your project are\ncurrently being supported with security updates. -->\nSecurity updates are applied only to the most recent release.\n\n## Reporting a Vulnerability\n\n<!-- Use this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc. -->\n\nTo report a security issue, please email\n[security@xgboost-ci.net](mailto:security@xgboost-ci.net)\nwith a description of the issue, the steps you took to create the issue,\naffected versions, and, if known, mitigations for the issue.\n\nAll support will be made on the best effort base, so please indicate the ""urgency level"" of the vulnerability as Critical, High, Medium or Low.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20735, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.363914251327515,128
What sequence of tensor operations does the STFTSpectrogram layer perform to support variable-length inputs in both channel-first and channel-last formats?,"The STFTSpectrogram layer in Keras performs the following sequence of tensor operations to support variable-length inputs in both channel-first and channel-last formats:

1.  The input data is first processed by the `STFTSpectrogram` layer, which computes the Short-Time Fourier Transform (STFT) along the last axis of the input.
2.  The STFT output is then passed through a series of operations to extract the frequency components of the signal as they change over time.
3.  Finally, the resulting tensor is returned as the output of the `STFTSpectrogram` layer.

Here's an example code snippet that demonstrates how to use the `STFTSpectrogram` layer in Keras:

```python
define the input shape and number of channels
input_shape = (1000, 1)  # Time steps, features
num_channels = 1

# Create an input tensor with random values
x = Input(shape=input_shape)

# Apply the STFTSpectrogram layer
stft_layer = STFTSpectrogram(
    frame_length=256,
    hop_length=128,
    n_perseg=256,
    n_overlap=128,
    window=""hann"",
)(x)

# Print the shape of the output tensor
print(stft_layer.shape)
```

You can customize the parameters of the `STFTSpectrogram` layer by passing additional arguments, such as the frame length, hop length, number of overlapping samples, and window function.","The STFTSpectrogram layer first builds a fixed “frame” kernel and then in STFTSpectrogram._apply_conv:

• If data_format=""channels_last"":  
  1. Transpose inputs from [B, T, C] → [B, C, T]  
  2. Reshape to [B·C, T, 1]  
  3. ops.conv → [B·C, F, T′]  
  4. Transpose back [B·C, F, T′] → [B·C, T′, F]  
  5. Reshape to [B, C, F, T′]

• If data_format=""channels_first"":  
  1. Reshape [B, C, T] → [B·C, 1, T]  
  2. ops.conv → [B·C, F, T′]  
  3. Reshape to [B, C, F, T′]

Then in STFTSpectrogram._adjust_shapes it uses ops.reshape and ops.transpose to support both variable length and optional expand_dims:

• channels_last & expand_dims: transpose [0,3,2,1] → [B, T′, F, C]  
• channels_last & no expand_dims: reshape [B, C·F, T′] → transpose [0,2,1] → [B, T′, C·F]  
• channels_first & expand_dims: transpose [0,1,3,2] → [B, C, T′, F]  
• channels_first & no expand_dims: reshape [B, C·F, T′]

All reshapes use batch_size=–1 so T′ can vary at runtime.","['CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.path\nThe path of the variable within the Keras model or layer.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'METHOD keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.call has parameter or accepts argument keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.call.inputs', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.value\nThe current value of the variable (numpy array or backend tensor).\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'METHOD keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.__init__ has parameter or accepts argument keras.keras.src.layers.preprocessing.stft_spectrogram.STFTSpectrogram.__init__.frame_length', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.aggregation\nThe strategy for aggregating this variable.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'MODULE keras.keras.src.ops.math contains FUNCTION keras.keras.src.ops.math.stft\nShort-Time Fourier Transform along the last axis of the input.  The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time.  Args:     x: Input tensor.     sequence_length: An integer representing the sequence length.     sequence_stride: An integer representing the sequence hop size.     fft_length: An integer representing the size of the FFT to apply. If not         specified, uses the smallest power of 2 enclosing `sequence_length`.     window: A string, a tensor of the window or `None`. If `window` is a         string, available values are `""hann""` and `""hamming""`. If `window`         is a tensor, it will be used directly as the window and its length         must be `sequence_length`. If `window` is `None`, no windowing is         used. Defaults to `""hann""`.     center: Whether to pad `x` on both sides so that the t-th sequence is         centered at time `t * sequence_stride`. Otherwise, the t-th sequence         begins at time `t * sequence_stride`. Defaults to `True`.  Returns:     A tuple containing two tensors - the real and imaginary parts of the     STFT output.  Example:  >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0]) >>> stft(x, 3, 2, 3) (array([[0.75, -0.375],    [3.75, -1.875],    [5.25, -2.625]]), array([[0.0, 0.64951905],    [0.0, 0.64951905],    [0.0, -0.64951905]]))', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.synchronization\nThe strategy for synchronizing this variable.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 20736, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",76.49917650222778,129
Under what conditions does _retrieve_class_or_fn prepend a '>' to the name when searching custom_objects?,"The `_retrieve_class_or_fn` function prepends a '>' to the name when searching custom_objects in Keras if the object is not found in the cache. This behavior is specific to Keras' custom_objects, which require this format to work correctly. The code checks if the object is already in the cache and only prepends the '>' if it's a class (i.e., `isinstance(obj, type)` returns `True`).","In the `if obj_type == ""function"" and module == ""builtins""` block (in `keras/src/saving/serialization_lib.py`), after checking all `BUILTIN_MODULES`, there’s a Keras 3.6-compat workaround: if the raw `name` contains no “>”, it does  

    separated_name = "">"" + name  

and then looks for any key in `custom_objects` that ends with that `"">""+name""`. That’s the only time `_retrieve_class_or_fn` prepends a “>” to the name.","[""MODULE keras.keras.src.saving.object_registration contains FUNCTION keras.keras.src.saving.object_registration.get_registered_object\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'MODULE cognee.cognee.modules.search.types.SearchResult contains CLASS cognee.cognee.modules.search.types.SearchResult.SearchResult', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.get_registered_object.custom_objects\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'CLASS cognee.cognee.modules.search.exceptions.exceptions.UnsupportedSearchTypeError has field or attribute or property cognee.cognee.modules.search.exceptions.exceptions.name', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.get_registered_object.module_objects\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'CLASS cognee.cognee.modules.search.exceptions.exceptions.UnsupportedSearchTypeError has field or attribute or property cognee.cognee.modules.search.exceptions.exceptions.search_type', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.GLOBAL_CUSTOM_OBJECTS\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20755, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",63.35547637939453,130
"In TensorFlow graph mode, which dimension of the provided mask does the TimeDistributed layer still validate when omitting the batch size comparison?","In TensorFlow graph mode, the TimeDistributed layer still validates the mask when omitting the batch size comparison. The mask is validated along the second dimension of the input tensor (i.e., the temporal dimension). This means that if your input tensor has shape `(batch_size, timesteps, features)`, the mask will be validated against the `timesteps` dimension. This behavior ensures that the TimeDistributed layer correctly handles variable-length sequences.","In TimeDistributed.call (keras/src/layers/rnn/time_distributed.py), when running in TF graph mode it skips checking mask_shape[0] (batch) but still asserts that mask_shape[1] == timesteps (via `mask_shape[1:2] != (timesteps,)`).","['CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.supports_masking\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.inputs\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.mask\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.call.data\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.call.i\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'MODULE keras.keras.src.layers.rnn.time_distributed contains CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has method keras.keras.src.layers.rnn.time_distributed.TimeDistributed.call\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n#### Resource Allocation\n\nA denial of service caused by one model can impact the overall system health. Implement safeguards like rate limits, access controls, and health monitoring.\n\n#### Model Sharing\n\nIn a multitenant design that allows sharing models, make sure that tenants and users fully understand the potential security risks involved. They must be aware that they will essentially be running code provided by other users. Unfortunately, there are no reliable methods available to detect malicious models, graphs, or checkpoints. To mitigate this risk, the recommended approach is to sandbox the model execution, effectively isolating it from the rest of the system.\n\n#### Hardware Attacks\n\nBesides the virtual environment, the hardware (GPUs or TPUs) can also be attacked. [Research](https://scholar.google.com/scholar?q=gpu+side+channel) has shown that side channel attacks on GPUs are possible, which can make data leak from other models or processes running on the same system at the same time.\n\n## Reporting a Vulnerability\n\nBeware that none of the topics under [Using Keras Securely](#using-keras-securely) are considered vulnerabilities of Keras.\n\nIf you have discovered a security vulnerability in this project, please report it\nprivately. **Do not disclose it as a public issue.** This gives us time to work with you\nto fix the issue before public exposure, reducing the chance that the exploit will be\nused before a patch is released.\n\nYou may submit the report in the following ways:\n\n- send an email to francois.chollet@gmail.com; and/or\n- send a [private vulnerability report](https://github.com/keras-team/keras/security/advisories/new)\n\nPlease provide the following information in your report:\n\n- A description of the vulnerability and its impact\n- How to reproduce the issue\n\nThis project is maintained by volunteers on a reasonable-effort basis. As such,\nplease give us 90 days to work on a fix before public exposure.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20765, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",38.84430956840515,131
Which TensorFlow quantization APIs are used in the backend shortcut for per-tensor and per-channel fake quantization?,"The TensorFlow quantization APIs used in the backend shortcut for per-tensor and per-channel fake quantization are `fake_quant_with_min_max_vars` function from the `keras.keras.src.quantizers.quantizers` module. This function performs per-tensor or per-channel fake quantization, with clamping range defined by `[min_vals, max_vals]`. The inputs are quantized into the quantization range `- `[0, 2^num_bits - 1]` when `narrow_range=False`, and `[1, 2^num_bits - 1]` when `narrow_range=True`. After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval. This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained. The function has several parameters: `inputs`, `min_vals`, `max_vals`, `num_bits`, `narrow_range`, and `axis`. It returns a Keras tensor with fake quantization applied.","In `keras/src/quantizers/quantizers.py` inside `fake_quant_with_min_max_vars`, the TensorFlow backend shortcut uses:

• Per-tensor: `tf.quantization.fake_quant_with_min_max_vars`  
• Per-channel: `tf.quantization.fake_quant_with_min_max_vars_per_channel`","['MODULE keras.keras.src.quantizers.quantizers contains FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.max_vals\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.num_bits\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.axis\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.inputs\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.min_vals\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'METHOD keras.keras.src.quantizers.quantizers.Quantizer.__call__ has parameter or accepts argument keras.keras.src.quantizers.quantizers.Quantizer.__call__.x\nCompute a quantized output from an input tensor.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 9, 'n_files_pr': 4, 'pr': 20772, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",65.33042669296265,132
How does the masking layer reconstruct a tensor mask value when loading a saved model?,"When loading a saved model in Keras, the masking layer reconstructs a tensor mask value by using the `mask_value` attribute of the layer. This attribute is used to determine whether a timestep should be masked or not. The masking layer only masks timesteps where all values are equal to the `mask_value`. If any downstream layer does not support masking yet receives such an input mask, an exception will be raised. You can set the `mask_value` attribute when creating the masking layer in your model, and it will use this value to determine which timesteps to mask when loading a saved model.","When you reload a saved Masking layer, its mask_value is passed into Masking.__init__ as a serialized dict. The constructor checks:

• in keras/src/layers/core/masking.py:Masking.__init__  
  if isinstance(mask_value, dict) and mask_value.get(""config""):  
      mask_value = deserialize_keras_object(mask_value)

That call to deserialize_keras_object rebuilds the original Tensor mask_value.","['CLASS keras.keras.src.layers.core.masking.Masking inherits from or is a subclass of keras.src.layers.layer.Layer\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', 'MODULE keras.guides.understanding_masking_and_padding contains GLOBAL_VARIABLE keras.guides.understanding_masking_and_padding.unmasked_embedding', 'CLASS keras.keras.src.layers.core.masking.Masking has method keras.keras.src.layers.core.masking.Masking.get_config\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', 'MODULE keras.guides.understanding_masking_and_padding contains GLOBAL_VARIABLE keras.guides.understanding_masking_and_padding.model', 'CLASS keras.keras.src.layers.core.masking.Masking has field or attribute or property keras.keras.src.layers.core.masking.mask_value\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', 'METHOD keras.keras.src.layers.layer.Layer.compute_mask has parameter or accepts argument keras.keras.src.layers.layer.Layer.compute_mask.previous_mask', 'CLASS keras.keras.src.layers.core.masking.Masking has method keras.keras.src.layers.core.masking.Masking.compute_mask\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20791, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",67.87547063827515,133
How are lines read from the exclusion file filtered before being used to determine which tests to skip for the OpenVINO backend?,"The lines read from the exclusion file are filtered before being used to determine which tests to skip for the OpenVINO backend in keras by using a combination of techniques such as sanitizing inputs, isolating the model process through sandboxing, and keeping the model and libraries updated with the latest security patches. These measures help protect sensitive data from potential leaks or unauthorized access and ensure that models run separately in a secure and isolated environment.","In `pytest_collection_modifyitems` (conftest.py), after doing `file.readlines()`, they run:

```python
openvino_skipped_tests = [
    line.strip() for line in openvino_skipped_tests
    if line.strip()
]
```

This both strips each line of whitespace and filters out any empty (or all-whitespace) lines before using them to skip tests.","[""MODULE keras.keras.src.utils.traceback_utils contains FUNCTION keras.keras.src.utils.traceback_utils.disable_traceback_filtering\nTurn off traceback filtering.  Raw Keras tracebacks (also known as stack traces) involve many internal frames, which can be challenging to read through, while not being actionable for end users. By default, Keras filters internal frames in most exceptions that it raises, to keep traceback short, readable, and focused on what's actionable for you (your own code).  See also `keras.config.enable_traceback_filtering()` and `keras.config.is_traceback_filtering_enabled()`.  If you have previously disabled traceback filtering via `keras.config.disable_traceback_filtering()`, you can re-enable it via `keras.config.enable_traceback_filtering()`."", 'MODULE keras.keras.src.backend.openvino.numpy contains FUNCTION keras.keras.src.backend.openvino.numpy.logical_not', ""MODULE keras.keras.src.utils.traceback_utils contains FUNCTION keras.keras.src.utils.traceback_utils.enable_traceback_filtering\nTurn on traceback filtering.  Raw Keras tracebacks (also known as stack traces) involve many internal frames, which can be challenging to read through, while not being actionable for end users. By default, Keras filters internal frames in most exceptions that it raises, to keep traceback short, readable, and focused on what's actionable for you (your own code).  See also `keras.config.disable_traceback_filtering()` and `keras.config.is_traceback_filtering_enabled()`.  If you have previously disabled traceback filtering via `keras.config.disable_traceback_filtering()`, you can re-enable it via `keras.config.enable_traceback_filtering()`."", 'MODULE keras.keras.src.backend.openvino.numpy contains FUNCTION keras.keras.src.backend.openvino.numpy.not_equal', ""MODULE keras.keras.src.utils.traceback_utils contains FUNCTION keras.keras.src.utils.traceback_utils.is_traceback_filtering_enabled\nCheck if traceback filtering is enabled.  Raw Keras tracebacks (also known as stack traces) involve many internal frames, which can be challenging to read through, while not being actionable for end users. By default, Keras filters internal frames in most exceptions that it raises, to keep traceback short, readable, and focused on what's actionable for you (your own code).  See also `keras.config.enable_traceback_filtering()` and `keras.config.disable_traceback_filtering()`.  If you have previously disabled traceback filtering via `keras.config.disable_traceback_filtering()`, you can re-enable it via `keras.config.enable_traceback_filtering()`.  Returns:     Boolean, `True` if traceback filtering is enabled,     and `False` otherwise."", 'MODULE keras.keras.src.backend.openvino.numpy contains FUNCTION keras.keras.src.backend.openvino.numpy.where', 'FUNCTION cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt has parameter or accepts argument cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt.base_directory\nRead a query prompt from a file.  Retrieve the contents of a specified prompt file, optionally using a provided base directory for the file path. If the base directory is not specified, a default path is used. Log errors if the file is not found or if another error occurs during file reading.  Parameters: -----------      - prompt_file_name (str): The name of the prompt file to be read.     - base_directory (str): The base directory from which to read the prompt file. If       None, a default path is used. (default None)  Returns: --------      Returns the contents of the prompt file as a string, or None if the file cannot be     read due to an error.', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 20845, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",43.254199504852295,134
Under what condition do these functions immediately return the input tensor without any reduction?,"The condition under which these functions immediately return the input tensor without any reduction in Keras is when the `condition` parameter of the `where` function is `True`. This means that if the condition is met, the function will yield the first input (`x1`) and otherwise yield the second input (`x2`).","In both keras/src/backend/openvino/numpy.py (amax and amin), they short‐circuit and return the input `x` immediately if you pass an “empty” axis, i.e.  
 `axis == ()` or `axis == []`.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.where\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', 'FUNCTION keras.keras.src.utils.tf_utils.ensure_tensor has parameter or accepts argument keras.keras.src.utils.tf_utils.ensure_tensor.inputs\nEnsures the input is a Tensor, SparseTensor or RaggedTensor.', 'FUNCTION keras.keras.src.ops.numpy.where has parameter or accepts argument keras.keras.src.ops.numpy.where.x2\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', 'FUNCTION keras.keras.src.utils.tf_utils.ensure_tensor has parameter or accepts argument keras.keras.src.utils.tf_utils.ensure_tensor.dtype\nEnsures the input is a Tensor, SparseTensor or RaggedTensor.', 'FUNCTION keras.keras.src.ops.numpy.where has parameter or accepts argument keras.keras.src.ops.numpy.where.condition\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', 'MODULE keras.keras.src.utils.tf_utils contains FUNCTION keras.keras.src.utils.tf_utils.ensure_tensor\nEnsures the input is a Tensor, SparseTensor or RaggedTensor.', 'FUNCTION keras.keras.src.ops.numpy.where has parameter or accepts argument keras.keras.src.ops.numpy.where.x1\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20883, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",31.83279514312744,135
Which test in LossScaleOptimizerTest validates that calling apply or stateless_apply increases the iterations property by one on each step?,The test in LossScaleOptimizerTest that validates that calling `apply` or `stateless_apply` increases the iterations property by one on each step is `test_iterations_update`. This test ensures that the loss scale optimizer correctly updates the iteration count after applying the scaling factor.,The `LossScaleOptimizerTest.test_iterations_update` (in `keras/src/optimizers/loss_scale_optimizer_test.py`) verifies that each call to `apply` or `stateless_apply` increments the `iterations` property by one.,"['CLASS qlib.qlib.rl.utils.log.LogCollector has field or attribute or property qlib.qlib.rl.utils.log.name\nLogs are first collected in each environment worker, and then aggregated to stream at the central thread in vector env.  In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step. The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.  ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.', 'METHOD keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_iterations_update has parameter or accepts argument keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_iterations_update.stateless', 'CLASS qlib.qlib.rl.utils.log.LogCollector has field or attribute or property qlib.qlib.rl.utils.log.string\nLogs are first collected in each environment worker, and then aggregated to stream at the central thread in vector env.  In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step. The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.  ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.', 'METHOD keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_finite_step has parameter or accepts argument keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_finite_step.stateless', 'CLASS qlib.qlib.rl.utils.log.LogCollector has field or attribute or property qlib.qlib.rl.utils.log.loglevel\nLogs are first collected in each environment worker, and then aggregated to stream at the central thread in vector env.  In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step. The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.  ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.', 'METHOD keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_infinite_step has parameter or accepts argument keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_infinite_step.stateless', 'CLASS qlib.qlib.rl.utils.log.LogCollector has field or attribute or property qlib.qlib.rl.utils.log.obj\nLogs are first collected in each environment worker, and then aggregated to stream at the central thread in vector env.  In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step. The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.  ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20901, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",31.131428956985474,136
"When exporting, how does the resource-tracking step distinguish between generic TensorFlow trackables and Keras layers, and delegate to backend-specific logic?","When exporting, the resource-tracking step distinguishes between generic TensorFlow trackables and Keras layers by using the `input_signature` parameter in the `Model.export()` method. This parameter specifies the shape and dtype of the model inputs, which can be a structure of `keras.InputSpec`, `tf.TensorSpec`, or backend tensor. If `input_signature` is not provided, it will be automatically computed by the model. The `export()` method also has other parameters such as `filepath`, `format`, and `verbose` that can be used to customize the export process. The resource-tracking step delegates to backend-specific logic by using the shape and dtype of the model inputs in the `input_signature`. This allows the model to be exported in a format specific to the chosen backend, such as TensorFlow SavedModel or ONNX.","In keras/src/export/saved_model.py the ExportArchive.track() method does two isinstance checks:

  1. `isinstance(resource, tf.__internal__.tracking.Trackable)` → adds it to a generic `_tracked` list  
  2. `isinstance(resource, layers.Layer)` → calls `self._track_layer(resource)`

That `_track_layer` call is polymorphic: each backend supplies its own subclass of BackendExportArchive with a backend‐specific `_track_layer` (e.g.  
– TFExportArchive._track_layer in backend/tensorflow/export.py  
– JaxExportArchive._track_layer in backend/jax/export.py  
– TorchExportArchive._track_layer in backend/torch/export.py )  
so Keras layers go through the layer path and get handled by the right backend logic, while other Trackables just get recorded generically.","['METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.input_signature\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'CLASS keras.keras.src.backend.tensorflow.layer.TFLayer inherits from or is a subclass of keras.src.backend.tensorflow.trackable.KerasAutoTrackable', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.verbose\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'CLASS keras.keras.src.backend.numpy.export.NumpyExportArchive has method keras.keras.src.backend.numpy.export.NumpyExportArchive.track', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.format\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'MODULE keras.keras.src.layers.layer contains FUNCTION keras.keras.src.layers.layer.is_backend_tensor_or_symbolic', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 9, 'n_files_pr': 5, 'pr': 20906, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",57.921555519104004,137
"How is a cycle detected and handled in the dtype promotion lattice, and what exception is raised?","Cycle detection and handling in the dtype promotion lattice in Keras is performed by the `result_type` function, which attempts to match the result of `jnp.result_type`. If a cycle is detected, it raises an exception. This can be seen in the following example:

``
x = keras.ops.ones((1,), dtype=""bfloat16"")
y = keras.ops.ones((1,), dtype=""int32"")
result_dtype = keras.backend.result_type(x.dtype, y.dtype)
print(result_dtype)  # Output: ""float32""
``","Cycle detection happens in the function dtypes._make_lattice_upper_bounds (after calling dtypes._type_promotion_lattice). It walks the graph by DFS and tracks the recursion stack; if during traversal a node is seen twice in the current path, it bails out and raises

    ValueError(""cycle detected in type promotion lattice for node <dtype>"")

as tested in test_cycle_detection_in_make_lattice_upper_bounds.","['FUNCTION keras.keras.src.backend.common.dtypes.result_type has parameter or accepts argument keras.keras.src.backend.common.dtypes.FLOAT8_TYPES\nReturns the type from applying the Keras type promotion rules.  In general, each argument is first parsed by `backend.standardize_dtype`, and the resulting dtype is determined by the least upper bound of the type promotion lattice.  Note: This function attempts to match the result of `jnp.result_type`.  Args:     dtypes: Input dtypes.  Returns:     The result dtype.  Examples:  >>> x = keras.ops.ones((1,), dtype=""bfloat16"") >>> keras.backend.result_type(x.dtype, int) ""bfloat16""  >>> x = keras.ops.ones((1,), dtype=""int32"") >>> y = keras.ops.ones((1,), dtype=""float32"") >>> keras.backend.result_type(x.dtype, y.dtype) ""float32""  >>> z= keras.ops.ones((1,), dtype=\'complex64\') >>> keras.backend.result_type(z.dtype, int) ""float64""', 'CLASS keras.keras.src.backend.common.dtypes_test.DtypesTest has method keras.keras.src.backend.common.dtypes_test.DtypesTest.test_cycle_detection_in_make_lattice_upper_bounds\nTest the dtype to verify that the behavior matches JAX.', 'FUNCTION keras.keras.src.backend.common.dtypes.result_type has parameter or accepts argument keras.keras.src.backend.common.dtypes.dtype\nReturns the type from applying the Keras type promotion rules.  In general, each argument is first parsed by `backend.standardize_dtype`, and the resulting dtype is determined by the least upper bound of the type promotion lattice.  Note: This function attempts to match the result of `jnp.result_type`.  Args:     dtypes: Input dtypes.  Returns:     The result dtype.  Examples:  >>> x = keras.ops.ones((1,), dtype=""bfloat16"") >>> keras.backend.result_type(x.dtype, int) ""bfloat16""  >>> x = keras.ops.ones((1,), dtype=""int32"") >>> y = keras.ops.ones((1,), dtype=""float32"") >>> keras.backend.result_type(x.dtype, y.dtype) ""float32""  >>> z= keras.ops.ones((1,), dtype=\'complex64\') >>> keras.backend.result_type(z.dtype, int) ""float64""', 'CLASS cognee.cognee.modules.graph.exceptions.exceptions.DimensionOutOfRangeError has field or attribute or property cognee.cognee.modules.graph.exceptions.exceptions.max_index', 'MODULE keras.keras.src.backend.common.dtypes contains FUNCTION keras.keras.src.backend.common.dtypes.result_type\nReturns the type from applying the Keras type promotion rules.  In general, each argument is first parsed by `backend.standardize_dtype`, and the resulting dtype is determined by the least upper bound of the type promotion lattice.  Note: This function attempts to match the result of `jnp.result_type`.  Args:     dtypes: Input dtypes.  Returns:     The result dtype.  Examples:  >>> x = keras.ops.ones((1,), dtype=""bfloat16"") >>> keras.backend.result_type(x.dtype, int) ""bfloat16""  >>> x = keras.ops.ones((1,), dtype=""int32"") >>> y = keras.ops.ones((1,), dtype=""float32"") >>> keras.backend.result_type(x.dtype, y.dtype) ""float32""  >>> z= keras.ops.ones((1,), dtype=\'complex64\') >>> keras.backend.result_type(z.dtype, int) ""float64""', 'CLASS cognee.cognee.modules.graph.exceptions.exceptions.DimensionOutOfRangeError inherits from or is a subclass of cognee.exceptions.CogneeValidationError', 'MODULE cognee.cognee.infrastructure.databases.exceptions.exceptions contains CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.EmbeddingException\nCustom exception for handling embedding-related errors.  This exception class is designed to indicate issues specifically related to embeddings within the application. It extends the base exception class CogneeConfigurationError allows for customization of the error message, name, and status code.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Usability Improvements, Documentation\n* [jvm-packages] add example to handle missing value other than 0 (#5677)\n* Add DMatrix usage examples to the C API demo (#5854)\n* List `DaskDeviceQuantileDMatrix` in the doc. (#5975)\n* Update Python custom objective demo. (#5981)\n* Update the JSON model schema to document more objective functions. (#5982)\n* [Python] Fix warning when `missing` field is not used. (#5969)\n* Fix typo in tracker logging (#5994)\n* Move a warning about empty dataset, so that it's shown for all objectives and metrics (#5998)\n* Fix the instructions for installing the nightly build. (#6004)\n* [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)\n* [jvm-packages] [doc] Update install doc for JVM packages (#6051)\n* Fix typo in `xgboost.callback.early_stop` docstring (#6071)\n* Add cache suffix to the files used in the external memory demo. (#6088)\n* [Doc] Document the parameter `kill_spark_context_on_worker_failure` (#6097)\n* Fix link to the demo for custom objectives (#6100)\n* Update Dask doc. (#6108)\n* Validate weights are positive values. (#6115)\n* Document the updated CMake version requirement. (#6123)\n* Add demo for `DaskDeviceQuantileDMatrix`. (#6156)\n* Cosmetic fixes in `faq.rst` (#6161)\n* Fix error message. (#6176)\n* [Doc] Add list of winning solutions in data science competitions using XGBoost (#6177)\n* Fix a comment in demo to use correct reference (#6190)\n* Update the list of winning solutions using XGBoost (#6192)\n* Consistent style for build status badge (#6203)\n* [Doc] Add info on GPU compiler (#6204)\n* Update the list of winning solutions (#6222, #6254)\n* Add link to XGBoost's Twitter handle (#6244)\n* Fix minor typos in XGBClassifier methods' docstrings (#6247)\n* Add sponsors link to FUNDING.yml (#6252)\n* Group CLI demo into subdirectory. (#6258)\n* Reduce warning messages from `gbtree`. (#6273)\n* Create a tutorial for using the C API in a C/C++ application (#6285)\n* Update plugin instructions for CMake build (#6289)\n* [doc] make Dask distributed example copy-pastable (#6345)\n* [Python] Add option to use `libxgboost.so` from the system path (#6362)\n* Fixed few grammatical mistakes in doc (#6393)\n* Fix broken link in CLI doc (#6396)\n* Improve documentation for the Dask API (#6413)\n* Revise misleading exception information: no such param of `allow_non_zero_missing` (#6418)\n* Fix CLI ranking demo. (#6439)\n* Fix broken links. (#6455)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @FelixYBW, Jack Dunn (@JackDunnNZ), Jean Lescut-Muller (@JeanLescut),  Boris Feld (@Lothiraldan), Nikhil Choudhary (@Nikhil1O1), Rory Mitchell (@RAMitchell), @ShvetsKS, Anthony D'Amato (@Totoketchup), @Wittty-Panda, neko (@akiyamaneko), Alexander Gugel (@alexanderGugel), @dependabot[bot], DIVYA CHAUHAN (@divya661), Daniel Steinberg (@dstein64), Akira Funahashi (@funasoul), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Hristo Iliev (@hiliev), Honza Sterba (@honzasterba), @hzy001, Igor Moura (@igormp), @jameskrach, James Lamb (@jameslamb), Naveed Ahmed Saleem Janvekar (@janvekarnaveed), Kyle Nicholson (@kylejn27), lacrosse91 (@lacrosse91), Christian Lorentzen (@lorentzenchr), Manikya Bardhan (@manikyabard), @nabokovas, John Quitto-Graham (@nvidia-johnq), @odidev, Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Tanuja Kirthi Doddapaneni (@tanuja3), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), Zeno Gantner (@zenogantner), zhang_jf (@zuston)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593).""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 6, 'pr': 20927, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",49.6751446723938,138
What does the final assertion check about the remat wrapper calls?,"The final assertion check about the remat wrapper calls in Keras checks if the rematted functions are correctly wrapped and called. This is done by checking if the `rematted_functions` field or attribute contains the correct information, and if the `__call__` method of the MockRemat class is correctly implemented to return a wrapper Mock calling the original function. This ensures that the remat wrapper calls are properly set up and functioning as expected.","The last two lines in `LayerTest.test_functional_model_with_remat` assert that:

• Exactly one function got wrapped by the `remat` decorator (i.e. `len(mock_remat.rematted_functions) == 1`)  
• And that this wrapped function was actually called (via `assert_called()` on the mock).","['CLASS keras.keras.src.layers.layer_test.MockRemat has field or attribute or property keras.keras.src.layers.layer_test.MockRemat.rematted_functions\nMock remat by returning a wrapper Mock calling the original function', 'MODULE keras.keras.src.layers.layer_test contains CLASS keras.keras.src.layers.layer_test.MockRemat\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has method keras.keras.src.layers.layer_test.MockRemat.__call__\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has field or attribute or property keras.keras.src.layers.layer_test.func\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has method keras.keras.src.layers.layer_test.MockRemat.__init__\nMock remat by returning a wrapper Mock calling the original function', 'CLASS cognee.cognee.infrastructure.llm.structured_output_framework.baml.baml_client.types.Check has field or attribute or property cognee.cognee.infrastructure.llm.structured_output_framework.baml.baml_client.types.Check.expression', 'MODULE keras.keras.src.legacy.saving.serialization contains FUNCTION keras.keras.src.legacy.saving.serialization.is_default\nCheck if a method is decorated with the `default` wrapper.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/jvm-packages/xgboost4j-example/README.md\nXGBoost4J Code Examples\n=======================\n\n## Java API\n* [Basic walkthrough of wrappers](src/main/java/ml/dmlc/xgboost4j/java/example/BasicWalkThrough.java)\n* [Customize loss function, and evaluation metric](src/main/java/ml/dmlc/xgboost4j/java/example/CustomObjective.java)\n* [Boosting from existing prediction](src/main/java/ml/dmlc/xgboost4j/java/example/BoostFromPrediction.java)\n* [Predicting using first n trees](src/main/java/ml/dmlc/xgboost4j/java/example/PredictFirstNtree.java)\n* [Generalized Linear Model](src/main/java/ml/dmlc/xgboost4j/java/example/GeneralizedLinearModel.java)\n* [Cross validation](src/main/java/ml/dmlc/xgboost4j/java/example/CrossValidation.java)\n* [Predicting leaf indices](src/main/java/ml/dmlc/xgboost4j/java/example/PredictLeafIndices.java)\n* [External Memory](src/main/java/ml/dmlc/xgboost4j/java/example/ExternalMemory.java)\n* [Early Stopping](src/main/java/ml/dmlc/xgboost4j/java/example/EarlyStopping.java)\n\n## Scala API\n\n* [Basic walkthrough of wrappers](src/main/scala/ml/dmlc/xgboost4j/scala/example/BasicWalkThrough.scala)\n* [Customize loss function, and evaluation metric](src/main/scala/ml/dmlc/xgboost4j/scala/example/CustomObjective.scala)\n* [Boosting from existing prediction](src/main/scala/ml/dmlc/xgboost4j/scala/example/BoostFromPrediction.scala)\n* [Predicting using first n trees](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictFirstNTree.scala)\n* [Generalized Linear Model](src/main/scala/ml/dmlc/xgboost4j/scala/example/GeneralizedLinearModel.scala)\n* [Cross validation](src/main/scala/ml/dmlc/xgboost4j/scala/example/CrossValidation.scala)\n* [Predicting leaf indices](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictLeafIndices.scala)\n* [External Memory](src/main/scala/ml/dmlc/xgboost4j/scala/example/ExternalMemory.scala)\n\n## Spark API\n* [Distributed Training with Spark](src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/SparkMLlibPipeline.scala)\n\n## Flink API\n* [Distributed Training with Flink](src/main/scala/ml/dmlc/xgboost4j/scala/example/flink/DistTrainWithFlink.scala)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), John Zedlewski (@JohnZed), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Anthony D'Amato (@Totoketchup), @Wittty-Panda, Alexander Gugel (@alexanderGugel), Codecov Comments Bot (@codecov-commenter), Codecov (@codecov-io), DIVYA CHAUHAN (@divya661), Devin Robison (@drobison00), Geoffrey Blake (@geoffreyblake), Mark Harris (@harrism), Philip Hyunsu Cho (@hcho3), Honza Sterba (@honzasterba), Igor Moura (@igormp), @jakirkham, @jameskrach, James Lamb (@jameslamb), Janakarajan Natarajan (@janaknat), Jake Hemstad (@jrhemstad), Keith Kraus (@kkraus14), Kyle Nicholson (@kylejn27), Christian Lorentzen (@lorentzenchr), Michael Mayer (@mayer79), Nikolay Petrov (@napetrov), @odidev, PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Scott Lundberg (@slundberg), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vincent Nijs (@vnijs), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n## v1.2.0 (2020.08.22)\n\n### XGBoost4J-Spark now supports the GPU algorithm (#5171)\n* Now XGBoost4J-Spark is able to leverage NVIDIA GPU hardware to speed up training.\n* There is on-going work for accelerating the rest of the data pipeline with NVIDIA GPUs (#5950, #5972).\n\n### XGBoost now supports CUDA 11 (#5808)\n* It is now possible to build XGBoost with CUDA 11. Note that we do not yet distribute pre-built binaries built with CUDA 11; all current distributions use CUDA 10.0.\n\n### Better guidance for persisting XGBoost models in an R environment (#5940, #5964)\n* Users are strongly encouraged to use `xgb.save()` and `xgb.save.raw()` instead of `saveRDS()`. This is so that the persisted models can be accessed with future releases of XGBoost.\n* The previous release (1.1.0) had problems loading models that were saved with `saveRDS()`. This release adds a compatibility layer to restore access to the old RDS files. Note that this is meant to be a temporary measure; users are advised to stop using `saveRDS()` and migrate to `xgb.save()` and `xgb.save.raw()`.\n\n### New objectives and metrics\n* The pseudo-Huber loss `reg:pseudohubererror` is added (#5647). The corresponding metric is `mphe`. Right now, the slope is hard-coded to 1.\n* The Accelerated Failure Time objective for survival analysis (`survival:aft`) is now accelerated on GPUs (#5714, #5716). The survival metrics `aft-nloglik` and `interval-regression-accuracy` are also accelerated on GPUs.\n\n### Improved integration with scikit-learn\n* Added `n_features_in_` attribute to the scikit-learn interface to store the number of features used (#5780). This is useful for integrating with some scikit-learn features such as `StackingClassifier`.  See [this link](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for more details.\n* `XGBoostError` now inherits `ValueError`, which conforms scikit-learn's exception requirement (#5696).""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20935, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",36.28359270095825,139
What two image statistics are asserted equal between the reference and backend outputs in the elastic transform test?,"The two image statistics that are asserted equal between the reference and backend outputs in the elastic transform test in Keras are likely related to the accuracy or similarity of the transformed images, such as mean squared error (MSE) or peak signal-to-noise ratio (PSNR). However, without more specific information, it is difficult to determine exactly which statistics are being compared. The elastic transform test is used to evaluate the correctness of image transformations, including resizing and cropping.",In `ImageOpsCorrectnessTest.test_elastic_transform` (keras/src/ops/image_test.py) they assert that the transformed outputs have the same mean and the same variance (via `np.mean` and `np.var`) between the NumPy reference (`elastic_transform_np`) and the backend (`kimage.elastic_transform`).,"['CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration.value_range\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', 'FUNCTION keras.keras.src.ops.image_test.elastic_transform_np has parameter or accepts argument keras.keras.src.ops.image_test.batch_size', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.data\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', 'CLASS keras.keras.src.ops.image_test.ImageOpsCorrectnessTest has method keras.keras.src.ops.image_test.ImageOpsCorrectnessTest.test_elastic_transform', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.data_format\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', 'FUNCTION keras.keras.src.backend.tensorflow.image.elastic_transform has parameter or accepts argument keras.keras.src.backend.tensorflow.image.elastic_transform.fill_mode', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration.generator\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/data/README.md\nThis folder contains processed example dataset used by the demos.\nCopyright of the dataset belongs to the original copyright holder', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 21007, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",41.01078820228577,140
How is a user-specified NumPy dtype translated into an OpenVINO element type for the fill operation?,"The translation of a user-specified NumPy dtype to an OpenVINO element type for the fill operation is handled by the `openvino.core.ov_to_keras_type` function in Keras. This function takes the NumPy dtype as input and returns the corresponding OpenVINO element type. For example, when using `np.float32`, it will be translated to `f16`. The `openvino.core.ov_to_keras_type` function is part of the Keras backend and is used to convert between different data types.","When you pass a NumPy `dtype` into `full_like`, it does:

1. Calls standardize_dtype(dtype) (to normalize whatever NumPy gave you).
2. Looks up the result in OPENVINO_DTYPES (a dict in keras/src/backend/openvino/dtypes.py).
3. Uses that OpenVINO element type (`ov_type`) when building the `ov_opset.constant`.

So the user dtype → `standardize_dtype` → `OPENVINO_DTYPES[...]` → OpenVINO element type.","['CLASS qlib.qlib.data.ops.NpElemOperator inherits from or is a subclass of qlib.qlib.data.ops.ElemOperator\nElement-wise Operator  Parameters ---------- feature : Expression     feature instance  Returns ---------- Expression     feature operation output\nNumpy Element-wise Operator  Parameters ---------- feature : Expression     feature instance func : str     numpy feature operation method  Returns ---------- Expression     feature operation output', 'FUNCTION keras.keras.src.backend.openvino.core.ov_to_keras_type has parameter or accepts argument keras.keras.src.backend.openvino.core.OPENVINO_DTYPES', 'MODULE qlib.qlib.data.ops contains CLASS qlib.qlib.data.ops.NpElemOperator\nNumpy Element-wise Operator  Parameters ---------- feature : Expression     feature instance func : str     numpy feature operation method  Returns ---------- Expression     feature operation output', 'FUNCTION keras.keras.src.backend.openvino.core.cast has parameter or accepts argument keras.keras.src.backend.openvino.core.OPENVINO_DTYPES', 'CLASS qlib.qlib.data.ops.NpElemOperator has method qlib.qlib.data.ops.NpElemOperator.__init__\nNumpy Element-wise Operator  Parameters ---------- feature : Expression     feature instance func : str     numpy feature operation method  Returns ---------- Expression     feature operation output', 'FUNCTION keras.keras.src.backend.openvino.core.convert_keras_tensor_to_openvino has parameter or accepts argument keras.keras.src.backend.openvino.core.OPENVINO_DTYPES', 'CLASS qlib.qlib.data.ops.NpElemOperator has field or attribute or property qlib.qlib.data.ops.feature\nNumpy Element-wise Operator  Parameters ---------- feature : Expression     feature instance func : str     numpy feature operation method  Returns ---------- Expression     feature operation output', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 21008, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",51.09567999839783,141
Which internal routines in the fit and evaluate workflows handle capturing and clearing the JAX training state sharding specifications?,"The internal routines in the fit and evaluate workflows that handle capturing and clearing the JAX training state sharding specifications in Keras are:

1. `jax_state_sync` method of the `JAXTrainer` class: This method is responsible for synchronizing the JAX training state with the current state of the model.
2. `test_step` method of the `JAXTrainer` class: This method is used to perform a test step in the training process, and it also handles capturing and clearing the JAX training state sharding specifications.

Note that these methods are part of the Keras backend implementation for JAX, and they are not directly exposed as public APIs. However, they can be accessed through the `keras.backend.jax` module.","In both `JAXTrainer.fit` and `JAXTrainer.evaluate` (in `keras/src/backend/jax/trainer.py`), the sharding specs are

• Captured by `self._record_training_state_sharding_spec()`  
• Cleared by `self._clear_jax_state_sharding()`","['CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.save_config\nOutput internal parameter configuration of Booster as a JSON string.  .. versionadded:: 1.0.0\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', 'CLASS keras.keras.src.backend.jax.trainer.JAXTrainer has method keras.keras.src.backend.jax.trainer.JAXTrainer.jax_state_sync', 'CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.eval_set\nEvaluate a set of data.  Parameters ---------- evals :     List of items to be evaluated. iteration :     Current iteration. feval :     Custom evaluation function.  Returns ------- result: str     Evaluation result string.\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', 'MODULE keras.keras.src.utils.jax_layer_test contains FUNCTION keras.keras.src.utils.jax_layer_test.jax_stateful_init', 'CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.eval\nEvaluate the model on mat.  Parameters ---------- data :     The dmatrix storing the input.  name :     The name of the dataset.  iteration :     The current iteration number.  Returns ------- result: str     Evaluation result string.\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', 'CLASS keras.keras.src.backend.jax.trainer.JAXTrainer has method keras.keras.src.backend.jax.trainer.JAXTrainer.test_step', 'CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.num_features\nNumber of features in booster.\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/jvm-packages/xgboost4j-example/README.md\nXGBoost4J Code Examples\n=======================\n\n## Java API\n* [Basic walkthrough of wrappers](src/main/java/ml/dmlc/xgboost4j/java/example/BasicWalkThrough.java)\n* [Customize loss function, and evaluation metric](src/main/java/ml/dmlc/xgboost4j/java/example/CustomObjective.java)\n* [Boosting from existing prediction](src/main/java/ml/dmlc/xgboost4j/java/example/BoostFromPrediction.java)\n* [Predicting using first n trees](src/main/java/ml/dmlc/xgboost4j/java/example/PredictFirstNtree.java)\n* [Generalized Linear Model](src/main/java/ml/dmlc/xgboost4j/java/example/GeneralizedLinearModel.java)\n* [Cross validation](src/main/java/ml/dmlc/xgboost4j/java/example/CrossValidation.java)\n* [Predicting leaf indices](src/main/java/ml/dmlc/xgboost4j/java/example/PredictLeafIndices.java)\n* [External Memory](src/main/java/ml/dmlc/xgboost4j/java/example/ExternalMemory.java)\n* [Early Stopping](src/main/java/ml/dmlc/xgboost4j/java/example/EarlyStopping.java)\n\n## Scala API\n\n* [Basic walkthrough of wrappers](src/main/scala/ml/dmlc/xgboost4j/scala/example/BasicWalkThrough.scala)\n* [Customize loss function, and evaluation metric](src/main/scala/ml/dmlc/xgboost4j/scala/example/CustomObjective.scala)\n* [Boosting from existing prediction](src/main/scala/ml/dmlc/xgboost4j/scala/example/BoostFromPrediction.scala)\n* [Predicting using first n trees](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictFirstNTree.scala)\n* [Generalized Linear Model](src/main/scala/ml/dmlc/xgboost4j/scala/example/GeneralizedLinearModel.scala)\n* [Cross validation](src/main/scala/ml/dmlc/xgboost4j/scala/example/CrossValidation.scala)\n* [Predicting leaf indices](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictLeafIndices.scala)\n* [External Memory](src/main/scala/ml/dmlc/xgboost4j/scala/example/ExternalMemory.scala)\n\n## Spark API\n* [Distributed Training with Spark](src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/SparkMLlibPipeline.scala)\n\n## Flink API\n* [Distributed Training with Flink](src/main/scala/ml/dmlc/xgboost4j/scala/example/flink/DistTrainWithFlink.scala)']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 21019, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",48.396493673324585,142
Which class does the saving logic instantiate to manage sharded weight storage when max_shard_size is set?,The class that the saving logic instantiates to manage sharded weight storage when `max_shard_size` is set in Keras is **ShardedH5IOStore**.,"When `max_shard_size` is set, `saving_lib.save_weights_only` (in `keras/src/saving/saving_lib.py`) instantiates the `ShardedH5IOStore` class to manage the sharded weight files.","['METHOD keras.keras.src.models.model.Model.save_weights has parameter or accepts argument keras.keras.src.models.model.Model.save_weights.overwrite\nSaves all weights to a single file or sharded files.  By default, the weights will be saved in a single `.weights.h5` file. If sharding is enabled (`max_shard_size` is not `None`), the weights will be saved in multiple files, each with a size at most `max_shard_size` (in GB). Additionally, a configuration file `.weights.json` will contain the metadata for the sharded files.  The saved sharded files contain:  - `*.weights.json`: The configuration file containing \'metadata\' and     \'weight_map\'. - `*_xxxxxx.weights.h5`: The sharded files containing only the     weights.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`. If `.weights.h5` is provided, it will be         overridden.     overwrite: Whether to overwrite any existing weights at the target         location or instead ask the user via an interactive prompt.     max_shard_size: `int` or `float`. Maximum size in GB for each         sharded file. If `None`, no sharding will be done. Defaults to         `None`.  Example:  ```python # Instantiate a EfficientNetV2L model with about 454MB of weights. model = keras.applications.EfficientNetV2L(weights=None)  # Save the weights in a single file. model.save_weights(""model.weights.h5"")  # Save the weights in sharded files. Use `max_shard_size=0.25` means # each sharded file will be at most ~250MB. model.save_weights(""model.weights.json"", max_shard_size=0.25)  # Load the weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.h5"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x))  # Load the sharded weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.json"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x)) ```', 'METHOD keras.keras.src.saving.saving_lib.ShardedH5IOStore.__init__ has parameter or accepts argument keras.keras.src.saving.saving_lib.ShardedH5IOStore.__init__.max_shard_size', 'METHOD keras.keras.src.models.model.Model.save_weights has parameter or accepts argument keras.keras.src.models.model.Model.save_weights.max_shard_size\nSaves all weights to a single file or sharded files.  By default, the weights will be saved in a single `.weights.h5` file. If sharding is enabled (`max_shard_size` is not `None`), the weights will be saved in multiple files, each with a size at most `max_shard_size` (in GB). Additionally, a configuration file `.weights.json` will contain the metadata for the sharded files.  The saved sharded files contain:  - `*.weights.json`: The configuration file containing \'metadata\' and     \'weight_map\'. - `*_xxxxxx.weights.h5`: The sharded files containing only the     weights.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`. If `.weights.h5` is provided, it will be         overridden.     overwrite: Whether to overwrite any existing weights at the target         location or instead ask the user via an interactive prompt.     max_shard_size: `int` or `float`. Maximum size in GB for each         sharded file. If `None`, no sharding will be done. Defaults to         `None`.  Example:  ```python # Instantiate a EfficientNetV2L model with about 454MB of weights. model = keras.applications.EfficientNetV2L(weights=None)  # Save the weights in a single file. model.save_weights(""model.weights.h5"")  # Save the weights in sharded files. Use `max_shard_size=0.25` means # each sharded file will be at most ~250MB. model.save_weights(""model.weights.json"", max_shard_size=0.25)  # Load the weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.h5"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x))  # Load the sharded weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.json"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x)) ```', 'FUNCTION keras.keras.src.saving.saving_api.save_weights has parameter or accepts argument keras.keras.src.saving.saving_api.save_weights.max_shard_size', 'METHOD keras.keras.src.models.model.Model.save_weights has parameter or accepts argument keras.keras.src.models.model.Model.save_weights.filepath\nSaves all weights to a single file or sharded files.  By default, the weights will be saved in a single `.weights.h5` file. If sharding is enabled (`max_shard_size` is not `None`), the weights will be saved in multiple files, each with a size at most `max_shard_size` (in GB). Additionally, a configuration file `.weights.json` will contain the metadata for the sharded files.  The saved sharded files contain:  - `*.weights.json`: The configuration file containing \'metadata\' and     \'weight_map\'. - `*_xxxxxx.weights.h5`: The sharded files containing only the     weights.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`. If `.weights.h5` is provided, it will be         overridden.     overwrite: Whether to overwrite any existing weights at the target         location or instead ask the user via an interactive prompt.     max_shard_size: `int` or `float`. Maximum size in GB for each         sharded file. If `None`, no sharding will be done. Defaults to         `None`.  Example:  ```python # Instantiate a EfficientNetV2L model with about 454MB of weights. model = keras.applications.EfficientNetV2L(weights=None)  # Save the weights in a single file. model.save_weights(""model.weights.h5"")  # Save the weights in sharded files. Use `max_shard_size=0.25` means # each sharded file will be at most ~250MB. model.save_weights(""model.weights.json"", max_shard_size=0.25)  # Load the weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.h5"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x))  # Load the sharded weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.json"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x)) ```', 'METHOD keras.keras.src.saving.saving_lib_test.SavingTest.test_weights_sharding has parameter or accepts argument keras.keras.src.saving.saving_lib_test.SavingTest.test_weights_sharding.max_shard_size', 'CLASS keras.keras.src.saving.saving_lib.ShardedH5IOStore has method keras.keras.src.saving.saving_lib.ShardedH5IOStore.__delitem__\nSharded numerical variable store backed by HDF5.  Args:     path_or_io: `str` or `pathlib.Path` object. The path where to save the         model.     max_shard_size: `int` or `float`. Maximum size in GB for each sharded         file. If `None`, no sharding will be done. Defaults to `None`.     archive: Optional `zipfile.ZipFile` object. If specified, the h5 file         will be saved inside the archive and `path_or_io` will be used as         the filename.     mode: `str`. One of {\'r\', \'w\'}. The mode to open the h5 file. Defaults         to `""r""`.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Address very high GPU memory usage for large data (#3635)\n* Fix performance regression within `EvaluateSplits()` of `gpu_hist` algorithm. (#3680)\n\n### Bug-fixes\n* Fix a problem in GPU quantile sketch with tiny instance weights. (#3628)\n* Fix copy constructor for `HostDeviceVectorImpl` to prevent dangling pointers (#3657)\n* Fix a bug in partitioned file loading (#3673)\n* Fixed an uninitialized pointer in `gpu_hist` (#3703)\n* Reshared data among GPUs when number of GPUs is changed (#3721)\n* Add back `max_delta_step` to split evaluation (#3668)\n* Do not round up integer thresholds for integer features in JSON dump (#3717)\n* Use `dmlc::TemporaryDirectory` to handle temporaries in cross-platform way (#3783)\n* Fix accuracy problem with `gpu_hist` when `min_child_weight` and `lambda` are set to 0 (#3793)\n* Make sure that `tree_method` parameter is recognized and not silently ignored (#3849)\n* XGBoost4J-Spark\n  - Make sure `thresholds` are considered when executing `predict()` method (#3577)\n  - Avoid losing precision when computing probabilities by converting to `Double` early (#3576)\n  - `getTreeLimit()` should return `Int` (#3602)\n  - Fix checkpoint serialization on HDFS (#3614)\n  - Throw `ControlThrowable` instead of `InterruptedException` so that it is properly re-thrown (#3632)\n  - Remove extraneous output to stdout (#3665)\n  - Allow specification of task type for custom objectives and evaluations (#3646)\n  - Fix distributed updater check (#3739)\n  - Fix issue when spark job execution thread cannot return before we execute `first()` (#3758)\n* Python package\n  - Fix accessing `DMatrix.handle` before it is set (#3599)\n  - `XGBClassifier.predict()` should return margin scores when `output_margin` is set to true (#3651)\n  - Early stopping callback should maximize metric of form `NDCG@n-` (#3685)\n  - Preserve feature names when slicing `DMatrix` (#3766)\n* R package\n  - Replace `nround` with `nrounds` to match actual parameter (#3592)\n  - Amend `xgb.createFolds` to handle classes of a single element (#3630)\n  - Fix buggy random generator and make `colsample_bytree` functional (#3781)\n\n### Maintenance: testing, continuous integration, build system\n* Add sanitizers tests to Travis CI (#3557)\n* Add NumPy, Matplotlib, Graphviz as requirements for doc build (#3669)\n* Comply with CRAN submission policy (#3660, #3728)\n* Remove copy-paste error in JVM test suite (#3692)\n* Disable flaky tests in `R-package/tests/testthat/test_update.R` (#3723)\n* Make Python tests compatible with scikit-learn 0.20 release (#3731)\n* Separate out restricted and unrestricted tasks, so that pull requests don't build downloadable artifacts (#3736)\n* Add multi-GPU unit test environment (#3741)\n* Allow plug-ins to be built by CMake (#3752)\n* Test wheel compatibility on CPU containers for pull requests (#3762)\n* Fix broken doc build due to Matplotlib 3.0 release (#3764)\n* Produce `xgboost.so` for XGBoost-R on Mac OSX, so that `make install` works (#3767)\n* Retry Jenkins CI tests up to 3 times to improve reliability (#3769, #3769, #3775, #3776, #3777)\n* Add basic unit tests for `gpu_hist` algorithm (#3785)\n* Fix Python environment for distributed unit tests (#3806)\n* Test wheels on CUDA 10.0 container for compatibility (#3838)\n* Fix JVM doc build (#3853)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 23}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 23, 'n_files_pr': 5, 'pr': 21022, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",34.6594557762146,143
"When no axis is specified, how is the tensor reshaped before computing the index?","When no axis is specified, the tensor is flattened before computing the index in keras. This means that the tensor will be reshaped into a one-dimensional array, allowing for efficient computation of the index.","In both argmax and argmin (keras/src/backend/openvino/numpy.py), when axis=None we do:

• flatten_shape = constant([-1] + [1]*(rank−1))  
• x = ov_opset.reshape(x, flatten_shape, False)  

i.e. x is reshaped to [-1, 1, 1, …, 1], flattening it along axis 0 before calling topk.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.append\nAppend tensor `x2` to the end of tensor `x1`.  Args:     x1: First input tensor.     x2: Second input tensor.     axis: Axis along which tensor `x2` is appended to tensor `x1`.         If `None`, both tensors are flattened before use.  Returns:     A tensor with the values of `x2` appended to `x1`.  Examples: >>> x1 = keras.ops.convert_to_tensor([1, 2, 3]) >>> x2 = keras.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]]) >>> keras.ops.append(x1, x2) array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)  When `axis` is specified, `x1` and `x2` must have compatible shapes. >>> x1 = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]]) >>> x2 = keras.ops.convert_to_tensor([[7, 8, 9]]) >>> keras.ops.append(x1, x2, axis=0) array([[1, 2, 3],         [4, 5, 6],         [7, 8, 9]], dtype=int32) >>> x3 = keras.ops.convert_to_tensor([7, 8, 9]) >>> keras.ops.append(x1, x3, axis=0) Traceback (most recent call last):     ... TypeError: Cannot concatenate arrays with different numbers of dimensions: got (2, 3), (3,).', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.roll\nRoll tensor elements along a given axis.  Elements that roll beyond the last position are re-introduced at the first.  Args:     x: Input tensor.     shift: The number of places by which elements are shifted.     axis: The axis along which elements are shifted. By default, the         array is flattened before shifting, after which the original         shape is restored.  Returns:     Output tensor.', 'FUNCTION keras.keras.src.ops.numpy.append has parameter or accepts argument keras.keras.src.ops.numpy.append.x1\nAppend tensor `x2` to the end of tensor `x1`.  Args:     x1: First input tensor.     x2: Second input tensor.     axis: Axis along which tensor `x2` is appended to tensor `x1`.         If `None`, both tensors are flattened before use.  Returns:     A tensor with the values of `x2` appended to `x1`.  Examples: >>> x1 = keras.ops.convert_to_tensor([1, 2, 3]) >>> x2 = keras.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]]) >>> keras.ops.append(x1, x2) array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)  When `axis` is specified, `x1` and `x2` must have compatible shapes. >>> x1 = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]]) >>> x2 = keras.ops.convert_to_tensor([[7, 8, 9]]) >>> keras.ops.append(x1, x2, axis=0) array([[1, 2, 3],         [4, 5, 6],         [7, 8, 9]], dtype=int32) >>> x3 = keras.ops.convert_to_tensor([7, 8, 9]) >>> keras.ops.append(x1, x3, axis=0) Traceback (most recent call last):     ... TypeError: Cannot concatenate arrays with different numbers of dimensions: got (2, 3), (3,).', 'FUNCTION keras.keras.src.utils.tf_utils.expand_dims has parameter or accepts argument keras.keras.src.utils.tf_utils.expand_dims.axis\nExpand dims on sparse, ragged, or dense tensors.', 'FUNCTION keras.keras.src.ops.numpy.append has parameter or accepts argument keras.keras.src.ops.numpy.append.x2\nAppend tensor `x2` to the end of tensor `x1`.  Args:     x1: First input tensor.     x2: Second input tensor.     axis: Axis along which tensor `x2` is appended to tensor `x1`.         If `None`, both tensors are flattened before use.  Returns:     A tensor with the values of `x2` appended to `x1`.  Examples: >>> x1 = keras.ops.convert_to_tensor([1, 2, 3]) >>> x2 = keras.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]]) >>> keras.ops.append(x1, x2) array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)  When `axis` is specified, `x1` and `x2` must have compatible shapes. >>> x1 = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]]) >>> x2 = keras.ops.convert_to_tensor([[7, 8, 9]]) >>> keras.ops.append(x1, x2, axis=0) array([[1, 2, 3],         [4, 5, 6],         [7, 8, 9]], dtype=int32) >>> x3 = keras.ops.convert_to_tensor([7, 8, 9]) >>> keras.ops.append(x1, x3, axis=0) Traceback (most recent call last):     ... TypeError: Cannot concatenate arrays with different numbers of dimensions: got (2, 3), (3,).', 'FUNCTION keras.keras.src.ops.numpy.expand_dims has parameter or accepts argument keras.keras.src.ops.numpy.expand_dims.axis\nExpand the shape of a tensor.  Insert a new axis at the `axis` position in the expanded tensor shape.  Args:     x: Input tensor.     axis: Position in the expanded axes where the new axis         (or axes) is placed.  Returns:     Output tensor with the number of dimensions increased.', 'FUNCTION keras.keras.src.ops.numpy.append has parameter or accepts argument keras.keras.src.ops.numpy.append.axis\nAppend tensor `x2` to the end of tensor `x1`.  Args:     x1: First input tensor.     x2: Second input tensor.     axis: Axis along which tensor `x2` is appended to tensor `x1`.         If `None`, both tensors are flattened before use.  Returns:     A tensor with the values of `x2` appended to `x1`.  Examples: >>> x1 = keras.ops.convert_to_tensor([1, 2, 3]) >>> x2 = keras.ops.convert_to_tensor([[4, 5, 6], [7, 8, 9]]) >>> keras.ops.append(x1, x2) array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)  When `axis` is specified, `x1` and `x2` must have compatible shapes. >>> x1 = keras.ops.convert_to_tensor([[1, 2, 3], [4, 5, 6]]) >>> x2 = keras.ops.convert_to_tensor([[7, 8, 9]]) >>> keras.ops.append(x1, x2, axis=0) array([[1, 2, 3],         [4, 5, 6],         [7, 8, 9]], dtype=int32) >>> x3 = keras.ops.convert_to_tensor([7, 8, 9]) >>> keras.ops.append(x1, x3, axis=0) Traceback (most recent call last):     ... TypeError: Cannot concatenate arrays with different numbers of dimensions: got (2, 3), (3,).', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 21060, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",32.44889831542969,144
How is the axis argument supplied to the loss wrapper ultimately used in the internal Dice loss calculation?,"The `axis` argument supplied to the loss wrapper is ultimately used in the internal Dice loss calculation by specifying which dimensions the loss is calculated over. This allows for flexible and customizable loss calculations, as the user can choose which axes to include or exclude from the loss computation. For example, if you have a 3D tensor with shape (batch_size, height, width), you could specify `axis=(1, 2)` to calculate the Dice loss only over the height and width dimensions, ignoring the batch size dimension. This would give you the average Dice loss per pixel in the image, rather than the overall Dice loss for the entire image.","The `axis` you pass to `keras.losses.Dice` is stored on the wrapper and then forwarded verbatim into the underlying `dice(y_true, y_pred, axis=…)` function (via `LossFunctionWrapper`). Inside `dice()` it’s used in each call to

• `ops.sum(inputs * targets, axis=axis)`  
• `ops.sum(y_true, axis=axis)`  
• `ops.sum(y_pred, axis=axis)`

i.e. it controls which dimensions are reduced when computing the intersection and union for the Dice score.","['FUNCTION keras.keras.src.losses.losses.dice has parameter or accepts argument keras.keras.src.losses.losses.dice.y_pred\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     y_true: tensor of true targets.     y_pred: tensor of predicted targets.     axis: tuple for which dimensions the loss is calculated  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.dice(y_true, y_pred, axis=axis) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.dice(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)', 'FUNCTION keras.keras.src.losses.losses.dice has parameter or accepts argument keras.keras.src.losses.losses.dice.axis\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     y_true: tensor of true targets.     y_pred: tensor of predicted targets.     axis: tuple for which dimensions the loss is calculated  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.dice(y_true, y_pred, axis=axis) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.dice(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)', 'FUNCTION keras.keras.src.losses.losses.dice has parameter or accepts argument keras.keras.src.losses.losses.dice.y_true\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     y_true: tensor of true targets.     y_pred: tensor of predicted targets.     axis: tuple for which dimensions the loss is calculated  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.dice(y_true, y_pred, axis=axis) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.dice(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)', 'MODULE keras.keras.src.losses.losses contains FUNCTION keras.keras.src.losses.losses.dice\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     y_true: tensor of true targets.     y_pred: tensor of predicted targets.     axis: tuple for which dimensions the loss is calculated  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.dice(y_true, y_pred, axis=axis) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.dice(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)', 'CLASS keras.keras.src.losses.losses.Dice has field or attribute or property keras.keras.src.losses.losses.axis\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', 'CLASS keras.keras.src.losses.losses.Dice has method keras.keras.src.losses.losses.Dice.get_config\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', 'CLASS keras.keras.src.losses.losses.Dice has field or attribute or property keras.keras.src.losses.losses.Dice.axis\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 21064, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",51.13348698616028,145
How does the transformation calculation handle cases where the sampled saturation factor reaches its maximum to avoid division by zero?,"The transformation calculation in Keras's RandomSaturation layer handles cases where the sampled saturation factor reaches its maximum to avoid division by zero by sampling a probability value between `min` and `max` for each image, or between `0.0` and a given float if only one value is provided. This ensures that the transformation calculation does not result in division by zero.

This behavior is described in the Keras documentation, which states: ""If a tuple `(min, max)` is provided, a probability value is sampled between `min` and `max` for each image. If a single float is provided, a probability is sampled between `0.0` and the given float.""","In RandomSaturation.get_random_transformation (keras/src/layers/preprocessing/image_preprocessing/random_saturation.py) the sampled factor is normalized as

 factor = factor / (1 – factor + epsilon())

The addition of backend.epsilon() in the denominator guarantees it never becomes zero, even if factor hits its max.","['CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation has method keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation.get_random_transformation\nRandomly adjusts the saturation on given images.  This layer will randomly increase/reduce the saturation for the input RGB images.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the image saturation         is impacted. `factor=0.5` makes this layer perform a no-op         operation. `factor=0.0` makes the image fully grayscale.         `factor=1.0` makes the image fully saturated. Values should         be between `0.0` and `1.0`. If a tuple is used, a `factor`         is sampled between the two values for every image augmented.         If a single float is used, a value between `0.0` and the passed         float is sampled. To ensure the value is always the same,         pass a tuple with two identical floats: `(0.5, 0.5)`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.  Example: ```python (images, labels), _ = keras.datasets.cifar10.load_data() images = images.astype(""float32"") random_saturation = keras.layers.RandomSaturation(factor=0.2) augmented_images = random_saturation(images) ```', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.seed\nRandomly adjusts the saturation on given images.  This layer will randomly increase/reduce the saturation for the input RGB images.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the image saturation         is impacted. `factor=0.5` makes this layer perform a no-op         operation. `factor=0.0` makes the image fully grayscale.         `factor=1.0` makes the image fully saturated. Values should         be between `0.0` and `1.0`. If a tuple is used, a `factor`         is sampled between the two values for every image augmented.         If a single float is used, a value between `0.0` and the passed         float is sampled. To ensure the value is always the same,         pass a tuple with two identical floats: `(0.5, 0.5)`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.  Example: ```python (images, labels), _ = keras.datasets.cifar10.load_data() images = images.astype(""float32"") random_saturation = keras.layers.RandomSaturation(factor=0.2) augmented_images = random_saturation(images) ```', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.data\nRandomly adjusts the saturation on given images.  This layer will randomly increase/reduce the saturation for the input RGB images.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the image saturation         is impacted. `factor=0.5` makes this layer perform a no-op         operation. `factor=0.0` makes the image fully grayscale.         `factor=1.0` makes the image fully saturated. Values should         be between `0.0` and `1.0`. If a tuple is used, a `factor`         is sampled between the two values for every image augmented.         If a single float is used, a value between `0.0` and the passed         float is sampled. To ensure the value is always the same,         pass a tuple with two identical floats: `(0.5, 0.5)`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.  Example: ```python (images, labels), _ = keras.datasets.cifar10.load_data() images = images.astype(""float32"") random_saturation = keras.layers.RandomSaturation(factor=0.2) augmented_images = random_saturation(images) ```', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_elastic_transform.RandomElasticTransform has method keras.keras.src.layers.preprocessing.image_preprocessing.random_elastic_transform.RandomElasticTransform.transform_segmentation_masks\nA preprocessing layer that applies random elastic transformations.  This layer distorts input images by applying elastic deformations, simulating a physically realistic transformation. The magnitude of the distortion is controlled by the `scale` parameter, while the `factor` determines the probability of applying the transformation.  Args:     factor: A single float or a tuple of two floats.         `factor` controls the probability of applying the transformation.         - `factor=0.0` ensures no erasing is applied.         - `factor=1.0` means erasing is always applied.         - If a tuple `(min, max)` is provided, a probability value           is sampled between `min` and `max` for each image.         - If a single float is provided, a probability is sampled           between `0.0` and the given float.         Default is 1.0.     scale: A float or a tuple of two floats defining the magnitude of         the distortion applied.         - If a tuple `(min, max)` is provided, a random scale value is           sampled within this range.         - If a single float is provided, a random scale value is sampled           between `0.0` and the given float.         Default is 1.0.     interpolation: Interpolation mode. Supported values: `""nearest""`,         `""bilinear""`.     fill_mode: Points outside the boundaries of the input are filled         according to the given mode. Available methods are `""constant""`,         `""nearest""`, `""wrap""` and `""reflect""`. Defaults to `""constant""`.         - `""reflect""`: `(d c b a | a b c d | d c b a)`             The input is extended by reflecting about the edge of the last             pixel.         - `""constant""`: `(k k k k | a b c d | k k k k)`             The input is extended by filling all values beyond             the edge with the same constant value k specified by             `fill_value`.         - `""wrap""`: `(a b c d | a b c d | a b c d)`             The input is extended by wrapping around to the opposite edge.         - `""nearest""`: `(a a a a | a b c d | d d d d)`             The input is extended by the nearest pixel.         Note that when using torch backend, `""reflect""` is redirected to         `""mirror""` `(c d c b | a b c d | c b a b)` because torch does not         support `""reflect""`.         Note that torch backend does not support `""wrap""`.     fill_value: a float represents the value to be filled outside the         boundaries when `fill_mode=""constant""`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation has method keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation.__init__\nRandomly adjusts the saturation on given images.  This layer will randomly increase/reduce the saturation for the input RGB images.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the image saturation         is impacted. `factor=0.5` makes this layer perform a no-op         operation. `factor=0.0` makes the image fully grayscale.         `factor=1.0` makes the image fully saturated. Values should         be between `0.0` and `1.0`. If a tuple is used, a `factor`         is sampled between the two values for every image augmented.         If a single float is used, a value between `0.0` and the passed         float is sampled. To ensure the value is always the same,         pass a tuple with two identical floats: `(0.5, 0.5)`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.  Example: ```python (images, labels), _ = keras.datasets.cifar10.load_data() images = images.astype(""float32"") random_saturation = keras.layers.RandomSaturation(factor=0.2) augmented_images = random_saturation(images) ```', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_elastic_transform.RandomElasticTransform has method keras.keras.src.layers.preprocessing.image_preprocessing.random_elastic_transform.RandomElasticTransform.get_random_transformation\nA preprocessing layer that applies random elastic transformations.  This layer distorts input images by applying elastic deformations, simulating a physically realistic transformation. The magnitude of the distortion is controlled by the `scale` parameter, while the `factor` determines the probability of applying the transformation.  Args:     factor: A single float or a tuple of two floats.         `factor` controls the probability of applying the transformation.         - `factor=0.0` ensures no erasing is applied.         - `factor=1.0` means erasing is always applied.         - If a tuple `(min, max)` is provided, a probability value           is sampled between `min` and `max` for each image.         - If a single float is provided, a probability is sampled           between `0.0` and the given float.         Default is 1.0.     scale: A float or a tuple of two floats defining the magnitude of         the distortion applied.         - If a tuple `(min, max)` is provided, a random scale value is           sampled within this range.         - If a single float is provided, a random scale value is sampled           between `0.0` and the given float.         Default is 1.0.     interpolation: Interpolation mode. Supported values: `""nearest""`,         `""bilinear""`.     fill_mode: Points outside the boundaries of the input are filled         according to the given mode. Available methods are `""constant""`,         `""nearest""`, `""wrap""` and `""reflect""`. Defaults to `""constant""`.         - `""reflect""`: `(d c b a | a b c d | d c b a)`             The input is extended by reflecting about the edge of the last             pixel.         - `""constant""`: `(k k k k | a b c d | k k k k)`             The input is extended by filling all values beyond             the edge with the same constant value k specified by             `fill_value`.         - `""wrap""`: `(a b c d | a b c d | a b c d)`             The input is extended by wrapping around to the opposite edge.         - `""nearest""`: `(a a a a | a b c d | d d d d)`             The input is extended by the nearest pixel.         Note that when using torch backend, `""reflect""` is redirected to         `""mirror""` `(c d c b | a b c d | c b a b)` because torch does not         support `""reflect""`.         Note that torch backend does not support `""wrap""`.     fill_value: a float represents the value to be filled outside the         boundaries when `fill_mode=""constant""`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.RandomSaturation has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_saturation.data_format\nRandomly adjusts the saturation on given images.  This layer will randomly increase/reduce the saturation for the input RGB images.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the image saturation         is impacted. `factor=0.5` makes this layer perform a no-op         operation. `factor=0.0` makes the image fully grayscale.         `factor=1.0` makes the image fully saturated. Values should         be between `0.0` and `1.0`. If a tuple is used, a `factor`         is sampled between the two values for every image augmented.         If a single float is used, a value between `0.0` and the passed         float is sampled. To ensure the value is always the same,         pass a tuple with two identical floats: `(0.5, 0.5)`.     value_range: the range of values the incoming images will have.         Represented as a two-number tuple written `[low, high]`. This is         typically either `[0, 1]` or `[0, 255]` depending on how your         preprocessing pipeline is set up.     seed: Integer. Used to create a random seed.  Example: ```python (images, labels), _ = keras.datasets.cifar10.load_data() images = images.astype(""float32"") random_saturation = keras.layers.RandomSaturation(factor=0.2) augmented_images = random_saturation(images) ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 21066, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",55.91921091079712,146
"In the float-input branch, how is the sign bit isolated from the floating-point representation?","The sign bit can be isolated from the floating-point representation using the `numpy.signbit` function, which returns a boolean array indicating whether each element is negative or not. This function can be used in Keras by importing numpy and applying it to the desired input. For example: `y = np.signbit(x)`, where `x` is an array of floating-point numbers. This will return an array with boolean values corresponding to the sign bit of each number in `x`.","In the float branch (in keras/src/backend/tensorflow/numpy.py signbit), x is first cast to float32, then bit-cast to an int32. The code does:

• mask = tf.constant(tf.int32.min)  # 0x80000000  
• bits = tf.bitcast(x, tf.int32) & mask  
• return tf.less(bits, 0)

By AND-ing the int32 view with INT32_MIN (0x80000000), only the sign bit remains, and checking <0 yields the sign.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.bitwise_not\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.signbit\nReturn the sign bit of the elements of `x`.  The output boolean tensor contains `True` where the sign of `x` is negative, and `False` otherwise.  Args:     x: Input tensor.  Returns:     Output boolean tensor of same shape as `x`.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.bitwise_invert\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'FUNCTION keras.keras.src.ops.numpy.bitwise_not has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_not.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'FUNCTION keras.keras.src.ops.numpy.bitwise_invert has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_invert.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'FUNCTION keras.keras.src.ops.numpy.signbit has parameter or accepts argument keras.keras.src.ops.numpy.signbit.x\nReturn the sign bit of the elements of `x`.  The output boolean tensor contains `True` where the sign of `x` is negative, and `False` otherwise.  Args:     x: Input tensor.  Returns:     Output boolean tensor of same shape as `x`.', 'CLASS qlib.qlib.data.ops.Sign inherits from or is a subclass of qlib.qlib.data.ops.NpElemOperator\nNumpy Element-wise Operator  Parameters ---------- feature : Expression     feature instance func : str     numpy feature operation method  Returns ---------- Expression     feature operation output\nFeature Sign  Parameters ---------- feature : Expression     feature instance  Returns ---------- Expression     a feature instance with sign', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 21077, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",48.16565704345703,147
How does the model workflow transition from the self-supervised pretraining with the auxiliary decoder to the supervised fine-tuning with the final linear layer?,"The model workflow transition from self-supervised pretraining with the auxiliary decoder to supervised fine-tuning with the final linear layer in qlib involves the following steps:

1. Train your model using the self-supervised pretraining method with the auxiliary decoder.
2. Switch to supervised fine-tuning by replacing the auxiliary decoder with the final linear layer.
3. Use the `task_training` method from the `RollingTaskExample` class in `qlib.examples.model_rolling.task_manager_rolling` to train your model using the final linear layer.

This is a general outline, and you may need to modify it based on your specific use case and requirements.","In qlib/contrib/model/pytorch_tabnet.py the switch happens in TabnetModel.fit():

1. If pretrain=True, fit() calls pretrain_fn(), which trains  
   – self.tabnet_model + self.tabnet_decoder  
   on a self-supervised reconstruction loss and saves only self.tabnet_model.state_dict().

2. Back in fit(), it reloads those pretrained weights into self.tabnet_model, then wraps it in a FinetuneModel:

   self.tabnet_model = FinetuneModel(self.out_dim, self.final_out_dim, self.tabnet_model)

   This adds a new final linear layer on top of the frozen encoder.

3. All subsequent supervised epochs (train_epoch/test_epoch) now optimize this combined model (the pretrained encoder + new head) with self.train_optimizer.","['CLASS qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel has method qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'MODULE qlib.qlib.workflow.exp contains CLASS qlib.qlib.workflow.exp.MLflowExperiment\nUse mlflow to implement Experiment.', 'METHOD qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn.S\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'CLASS qlib.examples.model_rolling.task_manager_rolling.RollingTaskExample has method qlib.examples.model_rolling.task_manager_rolling.RollingTaskExample.task_training', 'METHOD qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn.f_hat\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'MODULE qlib.qlib.model.base contains CLASS qlib.qlib.model.base.Model\nLearnable Models', 'METHOD qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn.f\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 205, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",51.95531177520752,634
How is the new minute-frequency adjustment integrated into the existing Yahoo data normalization flow?,"**Integrating Minute-Frequency Adjustment into Yahoo Data Normalization Flow in qlib**

The new minute-frequency adjustment is integrated into the existing Yahoo data normalization flow in qlib by using the `normalize_data_1d_extend` method, which extends the 1-day frequency data to 1-minute frequency data. This method takes several parameters, including the old Qlib data directory, source directory, normalize directory, region, interval, and date field name.

To integrate this adjustment into the existing flow, you can use the following steps:

1. Download the Qlib data from the specified URL.
2. Collector source data from Yahoo Finance using a script.
3. Normalize new source data by running `python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1min`.
4. Dump the normalized data using `python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date`.

Note that this adjustment is only applicable for 1-minute frequency data, and you need to specify the interval as `1min` when running the `normalize_data_1d_extend` method.","The minute‐frequency adjustment is implemented in a new method right alongside the daily normalizer. In  scripts/data_collector/yahoo/collector.py you’ll find:

• Class: YahooNormalize1min  
• Method: adjusted_price(self, df)  
  – Calls calc_adjusted_price(  
     df=df,  
     _date_field_name=self._date_field_name,  
     _symbol_field_name=self._symbol_field_name,  
     frequence=""1min"",  
     consistent_1d=self.CONSISTENT_1d,  
     calc_paused=self.CALC_PAUSED_NUM,  
     _1d_data_all=self.all_1d_data  
   )

By passing the already-normalized 1d series (all_1d_data) plus the 1min flags into calc_adjusted_price, this hooks the minute‐level adjustment seamlessly into the existing Yahoo normalization flow.","['CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.scripts.data_collector.yahoo.collector.YahooNormalize1min inherits from or is a subclass of abc.ABC\nNormalised to 1min using local 1d data', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.old_qlib_data_dir\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.scripts.data_collector.yahoo.collector.YahooNormalize1min inherits from or is a subclass of qlib.scripts.data_collector.yahoo.collector.YahooNormalize\nNormalised to 1min using local 1d data', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.symbol_field_name\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'MODULE qlib.scripts.data_collector.yahoo.collector contains CLASS qlib.scripts.data_collector.yahoo.collector.YahooNormalize1min\nNormalised to 1min using local 1d data', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.date_field_name\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Automatic update of daily frequency data(from yahoo finance)\n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --end_date <end date>\n      ```\n      * `end_date`: end of trading day(not included)\n      * `check_data_length`: check the number of rows per *symbol*, by default `None`\n        > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n\n  * `scripts/data_collector/yahoo/collector.py update_data_to_bin` parameters:\n      * `source_dir`: The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source""\n      * `normalize_dir`: Directory for normalize data, default ""Path(__file__).parent/normalize""\n      * `qlib_data_1d_dir`: the qlib data to be updated for yahoo, usually from: [download qlib data](https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)\n      * `end_date`: end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end)\n      * `region`: region, value from [""CN"", ""US""], default ""CN""\n      * `interval`: interval, default ""1d""(Currently only supports 1d data)\n      * `exists_skip`: exists skip, by default False\n\n## Using qlib data\n\n  ```python\n  import qlib\n  from qlib.data import D\n\n  # 1d data cn\n  # freq=day, freq default day\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data"", region=""cn"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data cn\n  # freq=1min\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n\n  # 1d data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data"", region=""us"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n  ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- [Collector Data](#collector-data)\n  - [Get Qlib data](#get-qlib-databin-file)\n  - [Collector *YahooFinance* data to qlib](#collector-yahoofinance-data-to-qlib)\n  - [Automatic update of daily frequency data](#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n- [Using qlib data](#using-qlib-data)\n\n\n# Collect Data From Yahoo Finance\n\n> *Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n**NOTE**:  Yahoo! Finance has blocked the access from China. Please change your network if you want to use the Yahoo data crawler.\n\n>  **Examples of abnormal data**\n\n- [SH000661](https://finance.yahoo.com/quote/000661.SZ/history?period1=1558310400&period2=1590796800&interval=1d&filter=history&frequency=1d)\n- [SZ300144](https://finance.yahoo.com/quote/300144.SZ/history?period1=1557446400&period2=1589932800&interval=1d&filter=history&frequency=1d)\n\nWe have considered **STOCK PRICE ADJUSTMENT**, but some price series seem still very abnormal.\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n### Get Qlib data(`bin file`)\n  > `qlib-data` from *YahooFinance*, is the data that has been dumped and can be used directly in `qlib`.\n  > This ready-made qlib-data is not updated regularly. If users want the latest data, please follow [these steps](#collector-yahoofinance-data-to-qlib) download the latest data. \n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data*\n    - `version`: dataset version, value from [`v1`, `v2`], by default `v1`\n      - `v2` end date is *2021-06*, `v1` end date is *2020-09*\n      - If users want to incrementally update data, they need to use yahoo collector to [collect data from scratch](#collector-yahoofinance-data-to-qlib).\n      - **the [benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks) for qlib use `v1`**, *due to the unstable access to historical data by YahooFinance, there are some differences between `v2` and `v1`*\n    - `interval`: `1d` or `1min`, by default `1d`\n    - `region`: `cn` or `us` or `in`, by default `cn`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # cn 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n    # cn 1min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n    # us 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us --interval 1d\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 221, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",67.60079097747803,635
What sequence of internal calls handles loading a point-in-time feature from the abstract expression down to the local PIT provider?,"**Loading Point-in-Time Feature Sequence in qlib**

The sequence of internal calls that handles loading a point-in-time feature from the abstract expression down to the local PIT provider in qlib is as follows:

1. The `load` method of the `Expression` class is called, which is responsible for loading feature/expression based on the expression engine.
2. The `end_index` parameter of the `load` method is set to the end index of the feature in calendar time.
3. The `instrument` parameter of the `load` method is set to the instrument code.
4. The `start_index` parameter of the `load` method is set to the start index of the feature in calendar time.
5. The `cur_pit` and `period` parameters are used if it is a PIT data, which is designed for point-in-time database.
6. The `load` method returns a pd.Series feature series with the index being the calendar index.

This sequence of internal calls handles loading a point-in-time feature from the abstract expression down to the local PIT provider in qlib.","When you call load on a PIT feature, the calls go roughly:

1. Expression.load (qlib/data/base.py)  
2. → PFeature._load_internal (qlib/data/base.py)  
3. → PITD.period_feature (wrapper in qlib/data/data.py)  
4. → LocalPITProvider.period_feature (qlib/data/data.py)  
5. → read_period_data + NumPy I/O → assemble pd.Series  

So the chain is:  
Expression.load → PFeature._load_internal → PITD.period_feature → LocalPITProvider.period_feature.","['CLASS qlib.qlib.data.base.Expression has method qlib.qlib.data.base.Expression.load\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).', 'CLASS qlib.qlib.data.base.Feature has method qlib.qlib.data.base.Feature.__init__\nStatic Expression  This kind of feature will load data from provider', 'CLASS qlib.qlib.data.base.Feature inherits from or is a subclass of qlib.qlib.data.base.Expression\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).\nStatic Expression  This kind of feature will load data from provider', 'CLASS qlib.qlib.data.base.Feature has method qlib.qlib.data.base.Feature.__str__\nStatic Expression  This kind of feature will load data from provider', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.end_index\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.instrument\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'MODULE qlib.qlib.data.base contains CLASS qlib.qlib.data.base.Feature\nStatic Expression  This kind of feature will load data from provider', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships).""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 39}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 39, 'n_files_pr': 19, 'pr': 343, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",54.940073013305664,636
"In the fit method, how are the train and valid metric histories extracted from the recorded evaluation results once training completes?","The train and valid metric histories are extracted from the recorded evaluation results once training completes in qlib through the `evals_result` dictionary, which stores the evaluation results of all items in the watchlist. This dictionary is returned by the `train` function in xgboost's training module. You can extract the train and valid metric histories using the following code:

```python
evals_result = model.evals_result_
train_metric_history = evals_result['train']
valid_metric_history = evals_result['valid']
```
You can then access the metric values for each iteration by indexing into the dictionaries. For example, to get the value of the 'logloss' metric at the last iteration, you would use:

```python
last_logloss_train = train_metric_history['logloss'][-1]
last_logloss_valid = valid_metric_history['logloss'][-1]
```
Note that the `evals_result` dictionary contains the evaluation results for all items in the watchlist, so if you have multiple validation sets, they will be stored in separate dictionaries within the `evals_result` dictionary.","At the end of HFLGBModel.fit (qlib/contrib/model/highfreq_gdbt_model.py), the code simply pulls out the first metric’s history from the nested evals_result dict:

• evals_result[""train""] = list(evals_result[""train""].values())[0]  
• evals_result[""valid""] = list(evals_result[""valid""].values())[0]  

This takes the single metric’s list of scores (one per boosting round) for both train and valid sets.","[""FUNCTION xgboost.python-package.xgboost.training.train has parameter or accepts argument xgboost.python-package.xgboost.training.train.params\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", 'CLASS qlib.qlib.rl.trainer.callbacks.MetricsWriter has method qlib.qlib.rl.trainer.callbacks.MetricsWriter.on_validate_end\nDump training metrics to file.', ""MODULE xgboost.python-package.xgboost.training contains FUNCTION xgboost.python-package.xgboost.training.train\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", 'CLASS qlib.qlib.rl.trainer.callbacks.MetricsWriter has method qlib.qlib.rl.trainer.callbacks.MetricsWriter.on_train_end\nDump training metrics to file.', ""FUNCTION xgboost.python-package.xgboost.training.train has parameter or accepts argument xgboost.python-package.xgboost.training.train.dtrain\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", 'CLASS qlib.qlib.model.base.Model has method qlib.qlib.model.base.Model.fit\nLearn model from the base model  .. note::      The attribute names of learned model should `not` start with \'_\'. So that the model could be     dumped to disk.  The following code example shows how to retrieve `x_train`, `y_train` and `w_train` from the `dataset`:      .. code-block:: Python          # get features and labels         df_train, df_valid = dataset.prepare(             [""train"", ""valid""], col_set=[""feature"", ""label""], data_key=DataHandlerLP.DK_L         )         x_train, y_train = df_train[""feature""], df_train[""label""]         x_valid, y_valid = df_valid[""feature""], df_valid[""label""]          # get weights         try:             wdf_train, wdf_valid = dataset.prepare([""train"", ""valid""], col_set=[""weight""],                                                    data_key=DataHandlerLP.DK_L)             w_train, w_valid = wdf_train[""weight""], wdf_valid[""weight""]         except KeyError as e:             w_train = pd.DataFrame(np.ones_like(y_train.values), index=y_train.index)             w_valid = pd.DataFrame(np.ones_like(y_valid.values), index=y_valid.index)  Parameters ---------- dataset : Dataset     dataset will generate the processed data from model training.\nLearnable Models', ""FUNCTION xgboost.python-package.xgboost.training.train has parameter or accepts argument xgboost.python-package.xgboost.training.train.num_boost_round\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md\n# Crowd Source Data\n\n## Initiative\nPublic data source like yahoo is flawed, it might miss data for stock which is delisted and it might have data which is wrong. This can introduce survivorship bias into our training process.\n\nThe Crowd Source Data is introduced to merged data from multiple data source and cross validate against each other, so that:\n1. We will have a more complete history record.\n2. We can identify the anomaly data and apply correction when necessary.\n\n## Related Repo\nThe raw data is hosted on dolthub repo: https://www.dolthub.com/repositories/chenditc/investment_data\n\nThe processing script and sql is hosted on github repo: https://github.com/chenditc/investment_data\n\nThe packaged docker runtime is hosted on dockerhub: https://hub.docker.com/repository/docker/chenditc/investment_data\n\n## How to use it in qlib\n### Option 1: Download release bin data\nUser can download data in qlib bin format and use it directly: https://github.com/chenditc/investment_data/releases/latest\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```\n\n### Option 2: Generate qlib data from dolthub\nDolthub data will be update daily, so that if user wants to get up to date data, they can dump qlib bin using docker:\n```\ndocker run -v /<some output directory>:/output -it --rm chenditc/investment_data bash dump_qlib_bin.sh && cp ./qlib_bin.tar.gz /output/\n```\n\n## FAQ and other info\nSee: https://github.com/chenditc/investment_data/blob/main/README.md']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 908, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",66.36125373840332,637
Describe how US stock symbols are aggregated from the two data endpoints and how failures in each source are handled.,"US stock symbols are aggregated from two data endpoints in qlib using the `get_us_stock_symbols` function, which accepts a parameter `qlib_data_path`. Failures in each source are handled by try-except blocks in the code. The function can be used as follows:

```python
def get_us_stock_symbols(qlib_data_path):
    # Attempt to read data from the specified path
    try:
        stock_symbols = ...  # retrieve stock symbols from qlib_data_path
    except Exception as e:
        # Catch any exceptions raised during this process and print an error message
        print(f""Error: {e}"")

# Get US stock symbols
stock_symbols = get_us_stock_symbols(qlib_data_path='path_to_your_qlib_data')
```

To handle failures in each source, you can use the following code:

```python
def handle_failure(qlib_data_path):
    try:
        # Attempt to read data from the specified path
        stock_symbols = get_us_stock_symbols(qlib_data_path=qlib_data_path)
    except Exception as e:
        # Catch any exceptions raised during this process and print an error message
        print(f""Error: {e}"")
```

Remember to replace `path_to_your_qlib_data` with the actual path to your qlib data.","US symbols are pulled in scripts/data_collector/utils.py by two helpers on top of a retry decorator (deco_retry.deco_func.wrapper):

1. _get_eastmoney  
   – GETs ~10 K tickers from EastMoney’s push2 API  
   – Maps each record’s f12 field (replacing “_”→“-P”)  
   – Non-200 responses, JSON errors or if fewer than 8 000 symbols are returned throw ValueError (and are retried up to 5× by the decorator)

2. _get_nyse  
   – POSTs a filter request to NYSE’s /quotes/filter endpoint  
   – Maps symbolTicker (replacing “-”→“-P”)  
   – Non-200 responses raise ValueError; JSON parse errors are caught, logged as warnings, and return an empty list

The top-level get_us_stock_symbols simply calls both (each wrapped in the retry logic), concatenates their outputs, and returns the combined symbol list.","['MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_us_stock_symbols\nget US stock symbols  Returns -------     stock symbols', 'FUNCTION qlib.scripts.data_collector.utils.get_us_stock_symbols has parameter or accepts argument qlib.scripts.data_collector.utils.get_us_stock_symbols.qlib_data_path\nget US stock symbols  Returns -------     stock symbols', 'MODULE qlib.qlib.utils.time contains FUNCTION qlib.qlib.utils.time.is_single_value\nIs there only one piece of data for stock market.  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for data. end_time : Union[pd.Timestamp, str]     closed end time for data. freq : region: str     Region, for example, ""cn"", ""us"" Returns ------- bool     True means one piece of data to obtain.', 'FUNCTION qlib.scripts.data_collector.utils.get_hs_stock_symbols has parameter or accepts argument qlib.scripts.data_collector.utils.MINIMUM_SYMBOLS_NUM\nget SH/SZ stock symbols  Returns -------     stock symbols', 'CLASS qlib.scripts.data_collector.base.BaseCollector has method qlib.scripts.data_collector.base.BaseCollector.get_data\nget data with symbol  Parameters ---------- symbol: str interval: str     value from [1min, 1d] start_datetime: pd.Timestamp end_datetime: pd.Timestamp  Returns ---------     pd.DataFrame, ""symbol"" and ""date""in pd.columns', 'MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_in_stock_symbols\nget IN stock symbols  Returns -------     stock symbols', 'FUNCTION qlib.qlib.utils.time.is_single_value has parameter or accepts argument qlib.qlib.utils.time.is_single_value.end_time\nIs there only one piece of data for stock market.  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for data. end_time : Union[pd.Timestamp, str]     closed end time for data. freq : region: str     Region, for example, ""cn"", ""us"" Returns ------- bool     True means one piece of data to obtain.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 4, 'pr': 938, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",61.647432804107666,638
How is the instrument identifier supplied to each processor during data calculation?,"The instrument identifier is supplied to each processor during data calculation in qlib through the `instrument` parameter of the `ChangeInstrument` class, which is a part of the `qlib.qlib.data.ops` module. This parameter specifies the new instrument for which the downstream operations should be performed upon.

Here's an example of how to use it:
```python
from qlib.qlib.data.ops import ChangeInstrument

instrument = ""SH000300""  # CSI300 index
feature = ""price""

change_instrument = ChangeInstrument(instrument, feature)
```
In this example, the `ChangeInstrument` class is instantiated with the new instrument (`SH000300`) and the feature to be calculated for the new instrument (`price`). The resulting object can then be used in downstream operations.","Each processor is called with the instrument ID as a keyword arg in `inst_calculator`. In qlib/data/data.py you’ll see:

    data = _processor_obj(data, instrument=inst)

so `inst` is passed into each `InstProcessor.__call__` via the `instrument` parameter.","['CLASS qlib.qlib.data.ops.ChangeInstrument has field or attribute or property qlib.qlib.data.ops.feature\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'CLASS qlib.qlib.data.inst_processor.InstProcessor has field or attribute or property qlib.qlib.data.inst_processor.instrument', 'MODULE qlib.qlib.data.ops contains CLASS qlib.qlib.data.ops.ChangeInstrument\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'METHOD qlib.qlib.data.data.DatasetProvider.dataset_processor has parameter or accepts argument qlib.qlib.data.data.DatasetProvider.dataset_processor.instruments_d\nLoad and process the data, return the data set. - default using multi-kernel method.', 'CLASS qlib.qlib.data.ops.ChangeInstrument has field or attribute or property qlib.qlib.data.ops.end_index\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'CLASS qlib.qlib.data.data.InstrumentProvider has field or attribute or property qlib.qlib.data.data.end_time\nInstrument provider base class  Provide instrument data.', 'CLASS qlib.qlib.data.ops.ChangeInstrument has field or attribute or property qlib.qlib.data.ops.start_index\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 959, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",45.19297981262207,639
"If a Yahoo history request returns a dict without valid quote data, how does the higher‐level simple fetch method respond?","When a Yahoo history request returns a dict without valid quote data, the simple fetch method in qlib will raise an exception that needs to be handled by the caller. To handle this exception, you can use a try-except block to catch any exceptions raised by the `yf.download()` function and print an error message or implement your own error handling code.

You should also consider adding error checking to ensure the data returned is valid before using it. This could involve checking the shape of the data or verifying that all necessary columns are present.

Here's an example of how you can handle this exception:

```python
def fetch(tickers, period, start, end):
    try:
        hist, _ = yf.download(tickers, period=period, start=start, end=end)
    except Exception as e:
        print(f""Error: {e}"")
```

Remember to add error checking and handle the exception according to your application's requirements.","If `history` returns a dict with no `""quote""` data, `get_data_from_remote` falls through and returns `None`. In `_get_simple` (in `YahooCollector.get_data`), that `None` triggers:

```python
if resp is None or resp.empty:
    raise ValueError(
        f""get data error: {symbol}--{start_}--{end_}""
        + ""The stock may be delisted, please check""
    )
```

So you get a `ValueError` telling you the stock may be delisted.","['CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'METHOD qlib.scripts.data_collector.yahoo.collector.YahooCollector.get_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.YahooCollector.get_data.end_datetime', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.date_field_name\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'METHOD qlib.scripts.data_collector.yahoo.collector.YahooCollector.get_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.YahooCollector.get_data.start_datetime', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.old_qlib_data_dir\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.scripts.data_collector.yahoo.collector.YahooCollectorCN1min has method qlib.scripts.data_collector.yahoo.collector.YahooCollectorCN1min.download_index_data', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.symbol_field_name\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can\'t meet users\' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month\'s data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *""2000-01-01""*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *""failed""* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date\'s close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- [Collector Data](#collector-data)\n  - [Get Qlib data](#get-qlib-databin-file)\n  - [Collector *YahooFinance* data to qlib](#collector-yahoofinance-data-to-qlib)\n  - [Automatic update of daily frequency data](#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n- [Using qlib data](#using-qlib-data)\n\n\n# Collect Data From Yahoo Finance\n\n> *Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n**NOTE**:  Yahoo! Finance has blocked the access from China. Please change your network if you want to use the Yahoo data crawler.\n\n>  **Examples of abnormal data**\n\n- [SH000661](https://finance.yahoo.com/quote/000661.SZ/history?period1=1558310400&period2=1590796800&interval=1d&filter=history&frequency=1d)\n- [SZ300144](https://finance.yahoo.com/quote/300144.SZ/history?period1=1557446400&period2=1589932800&interval=1d&filter=history&frequency=1d)\n\nWe have considered **STOCK PRICE ADJUSTMENT**, but some price series seem still very abnormal.\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n### Get Qlib data(`bin file`)\n  > `qlib-data` from *YahooFinance*, is the data that has been dumped and can be used directly in `qlib`.\n  > This ready-made qlib-data is not updated regularly. If users want the latest data, please follow [these steps](#collector-yahoofinance-data-to-qlib) download the latest data. \n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data*\n    - `version`: dataset version, value from [`v1`, `v2`], by default `v1`\n      - `v2` end date is *2021-06*, `v1` end date is *2020-09*\n      - If users want to incrementally update data, they need to use yahoo collector to [collect data from scratch](#collector-yahoofinance-data-to-qlib).\n      - **the [benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks) for qlib use `v1`**, *due to the unstable access to historical data by YahooFinance, there are some differences between `v2` and `v1`*\n    - `interval`: `1d` or `1min`, by default `1d`\n    - `region`: `cn` or `us` or `in`, by default `cn`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # cn 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n    # cn 1min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n    # us 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us --interval 1d\n    ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 966, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",56.86470913887024,640
Which artifact repository type triggers deletion of the downloaded artifact directory after loading an object?,"The qlib library does not have a specific artifact repository type that triggers deletion of the downloaded artifact directory after loading an object. However, you can specify the `dst_path` parameter when calling the `download_artifact` method to download the artifacts to a specific local filesystem destination directory.

This is based on the search results and the documentation of qlib's `QlibRecorder` class, which indicates that the `download_artifact` method has a parameter for specifying the local filesystem destination directory. However, no specific information was found about an artifact repository type that triggers deletion of the downloaded artifact directory after loading an object in qlib or other related libraries.",The cleanup only happens when the artifact repo is an AzureBlobArtifactRepository (as checked in MLflowRecorder.load_object’s finally block).,"['CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.download_artifact\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.\nA global system that helps to manage the experiments.', 'CLASS cognee.cognee.shared.CodeGraphEntities.Repository has field or attribute or property cognee.cognee.shared.CodeGraphEntities.Repository.path', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.download_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.download_artifact.dst_path\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.', 'MODULE xgboost.ops.script.release_artifacts contains FUNCTION xgboost.ops.script.release_artifacts.check_path\nEnsure the script is run from the project root directory.', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.download_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.download_artifact.path\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.', 'MODULE xgboost.ops.script.release_artifacts contains GLOBAL_VARIABLE xgboost.ops.script.release_artifacts.wheel_path', 'CLASS qlib.qlib.workflow.recorder.Recorder has method qlib.qlib.workflow.recorder.Recorder.download_artifact\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.', ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### XGBoost4J-Spark: Redesigning checkpointing mechanism\n* RFC is available at #4786\n* Clean up checkpoint file after a successful training job (#4754): The current implementation in XGBoost4J-Spark does not clean up the checkpoint file after a successful training job. If the user runs another job with the same checkpointing directory, she will get a wrong model because the second job will re-use the checkpoint file left over from the first job. To prevent this scenario, we propose to always clean up the checkpoint file after every successful training job.\n* Avoid Multiple Jobs for Checkpointing (#5082): The current method for checkpoint is to collect the booster produced at the last iteration of each checkpoint internal to Driver and persist it in HDFS. The major issue with this approach is that it needs to re-perform the data preparation for training if the user did not choose to cache the training dataset. To avoid re-performing data prep, we build external-memory checkpointing in the XGBoost4J layer as well.\n* Enable deterministic repartitioning when checkpoint is enabled (#4807): Distributed algorithm for gradient boosting assumes a fixed partition of the training data between multiple iterations. In previous versions, there was no guarantee that data partition would stay the same, especially when a worker goes down and some data had to recovered from previous checkpoint. In this release, we make data partition deterministic by using the data hash value of each data row in computing the partition.\n\n### XGBoost4J-Spark: handle errors thrown by the native code (#4560)\n* All core logic of XGBoost is written in C++, so XGBoost4J-Spark internally uses the C++ code via Java Native Interface (JNI). #4560 adds a proper error handling for any errors or exceptions arising from the C++ code, so that the XGBoost Spark application can be torn down in an orderly fashion.\n\n### XGBoost4J-Spark: Refine method to count the number of alive cores  (#4858)\n* The `SparkParallelismTracker` class ensures that sufficient number of executor cores are alive. To that end, it is important to query the number of alive cores reliably.\n\n### XGBoost4J: Add `BigDenseMatrix` to store more than `Integer.MAX_VALUE` elements (#4383)\n\n### Robust model serialization with JSON (#4632, #4708, #4739, #4868, #4936, #4945, #4974, #5086, #5087, #5089, #5091, #5094, #5110, #5111, #5112, #5120, #5137, #5218, #5222, #5236, #5245, #5248, #5281)\n* In this release, we introduce an experimental support of using [JSON](https://www.json.org/json-en.html) for serializing (saving/loading) XGBoost models and related hyperparameters for training. We would like to eventually replace the old binary format with JSON, since it is an open format and parsers are available in many programming languages and platforms. See [the documentation for model I/O using JSON](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/saving_model.html). #3980 explains why JSON was chosen over other alternatives.\n* To maximize interoperability and compatibility of the serialized models, we now split serialization into two parts (#4855):\n  1. Model, e.g. decision trees and strictly related metadata like `num_features`.\n  2. Internal configuration, consisting of training parameters and other configurable parameters. For example, `max_delta_step`, `tree_method`, `objective`, `predictor`, `gpu_id`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//SECURITY.md\n<!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 967, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",49.696603775024414,641
Which flag controls whether the MultiIndex lexsorted check is skipped for recent pandas versions?,"The flag that controls whether the MultiIndex lexsorted check is skipped for recent pandas versions in qlib is `is_deprecated_lexsorted_pandas`. This can be found in the `qlib.qlib.utils.__init__` module, specifically in the `lazy_sort_index` function. The `lazy_sort_index` function makes the df index sorted and avoids the case where `df.is_lexsorted() == True` which takes a lot of time when calling `df.sort_index()`.",The boolean flag `is_deprecated_lexsorted_pandas` (in qlib/utils/__init__.py) controls whether the MultiIndex `.is_lexsorted()` check is skipped for newer pandas versions.,"['MODULE qlib.examples.benchmarks.TFT.libs.tft_model contains FUNCTION qlib.examples.benchmarks.TFT.libs.tft_model.add_and_norm\nApplies skip connection followed by layer normalisation.  Args:   x_list: List of inputs to sum for skip connection  Returns:   Tensor output from layer.', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.lazy_sort_index\nmake the df index sorted  df.sort_index() will take a lot of time even when `df.is_lexsorted() == True` This function could avoid such case  Parameters ---------- df : pd.DataFrame  Returns ------- pd.DataFrame:     sorted dataframe', 'MODULE keras.keras.src.saving.serialization_lib contains CLASS keras.keras.src.saving.serialization_lib.SafeModeScope\nScope to propagate safe mode flag to nested deserialization calls.', 'FUNCTION qlib.qlib.utils.__init__.lazy_sort_index has parameter or accepts argument qlib.qlib.utils.__init__.is_deprecated_lexsorted_pandas\nmake the df index sorted  df.sort_index() will take a lot of time even when `df.is_lexsorted() == True` This function could avoid such case  Parameters ---------- df : pd.DataFrame  Returns ------- pd.DataFrame:     sorted dataframe', 'CLASS keras.keras.src.backend.common.variables_test.VariableInitializationTest has method keras.keras.src.backend.common.variables_test.VariableInitializationTest.test_variable_initialization_with_non_trainable\nTest variable initialization with non-trainable flag.\nTests for Variable.__init__()', 'FUNCTION qlib.qlib.utils.__init__.lazy_sort_index has parameter or accepts argument qlib.qlib.utils.__init__.lazy_sort_index.df\nmake the df index sorted  df.sort_index() will take a lot of time even when `df.is_lexsorted() == True` This function could avoid such case  Parameters ---------- df : pd.DataFrame  Returns ------- pd.DataFrame:     sorted dataframe', 'CLASS keras.keras.src.saving.serialization_lib.SafeModeScope has method keras.keras.src.saving.serialization_lib.SafeModeScope.__init__\nScope to propagate safe mode flag to nested deserialization calls.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 973, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",37.79670715332031,642
"Which callbacks are passed to lightgbm.train in both models’ fit methods, and which single callback do their finetune methods use?","The callbacks used in the `lightgbm.train` method in both models' fit methods in qlib are not explicitly mentioned, but it uses early stopping and verbose evaluation. The finetune method does not use any callbacks, instead using a simpler training process with only three parameters: dataset, num_boost_round, and verbose_eval.","In both LGBModel.fit (qlib/contrib/model/gbdt.py) and HFLGBModel.fit (qlib/contrib/model/highfreq_gdbt_model.py) the call to lgb.train uses three callbacks:

• lgb.early_stopping(...)  
• lgb.log_evaluation(period=…)  
• lgb.record_evaluation(evals_result)  

In both finetune methods the only callback passed is:

• lgb.log_evaluation(period=…)","['CLASS qlib.qlib.rl.trainer.callbacks.Callback has method qlib.qlib.rl.trainer.callbacks.Callback.on_train_end\nCalled when the training ends. To access all outputs produced during training, cache the data in either trainer and vessel, and post-process them in this hook.\nBase class of all callbacks.', 'CLASS qlib.examples.benchmarks.LightGBM.features_resample_N.ResampleNProcessor has method qlib.examples.benchmarks.LightGBM.features_resample_N.ResampleNProcessor.__call__', 'CLASS qlib.qlib.backtest.high_performance_ds.BaseSingleMetric has method qlib.qlib.backtest.high_performance_ds.BaseSingleMetric.add\nReplace np.nan with fill_value in two metrics and add them.\nThe data structure of the single metric. The following methods are used for computing metrics in one indicator.', 'CLASS qlib.qlib.contrib.model.gbdt.LGBModel has method qlib.qlib.contrib.model.gbdt.LGBModel.fit\nLightGBM Model', 'CLASS keras.keras.src.callbacks.callback_list.CallbackList has method keras.keras.src.callbacks.callback_list.CallbackList.__init__\nContainer for `Callback` instances.  This object wraps a list of `Callback` instances, making it possible to call them all at once via a single endpoint (e.g. `callback_list.on_epoch_end(...)`).  Args:     callbacks: List of `Callback` instances.     add_history: Whether a `History` callback should be added, if one         does not already exist in the `callbacks` list.     add_progbar: Whether a `ProgbarLogger` callback should be added, if         one does not already exist in the `callbacks` list.     model: The `Model` these callbacks are used with.     **params: If provided, parameters will be passed to each `Callback`         via `Callback.set_params`.\nContainer abstracting a list of callbacks.', 'METHOD qlib.examples.benchmarks.LightGBM.features_resample_N.ResampleNProcessor.__call__ has parameter or accepts argument qlib.examples.benchmarks.LightGBM.features_resample_N.ResampleNProcessor.__call__.df', 'CLASS qlib.qlib.contrib.model.gbdt.LGBModel has method qlib.qlib.contrib.model.gbdt.LGBModel.finetune\nfinetune model  Parameters ---------- dataset : DatasetH     dataset for finetuning num_boost_round : int     number of round to finetune model verbose_eval : int     verbose level\nLightGBM Model', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/LightGBM/README.md\n# LightGBM\n* Code: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)\n* Paper: LightGBM: A Highly Efficient Gradient Boosting\nDecision Tree. [https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf).\n\n\n# Introductions about the settings/configs.\n\n`workflow_config_lightgbm_multi_freq.yaml`\n- It uses data sources of different frequencies (i.e. multiple frequencies) for daily prediction.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 974, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",44.31631374359131,643
"What sequence of steps occurs when the temporary record loader fetches an artifact, including error fallback and artifact cleanup?","The sequence of steps for temporary record loader fetching an artifact with error fallback and artifact cleanup in qlib involves the following:

1. The temporary record loader attempts to fetch an artifact from a specified location.
2. If the artifact is not found, the loader will attempt to load the parent objects using the `load` method of the `RecordTemp` class.
3. If the parent objects are loaded successfully, the loader will then attempt to download the artifact using the `download_artifact` method of the `MLflowRecorder` class.
4. If the artifact is downloaded successfully, the loader will then perform any necessary cleanup operations.

However, this is just a hypothetical sequence of steps based on the available information, and the actual implementation may vary depending on the specific requirements and constraints of the qlib codebase.","When you call RecordTemp.load(name):

1. RecordTemp.load(name) calls  
   → self.recorder.load_object(self.get_path(name))  
   (qlib/workflow/record_temp.py)

2. In MLflowRecorder.load_object(name):  
   a. Assert the recorder is started (uri ≠ None).  
   b. Download the artifact to a local path via self.client.download_artifacts(self.id, name).  
   c. Open the file and unpickle it.  
   d. Return the deserialized object.  
   e. In finally, if the underlying repo is AzureBlobArtifactRepository and a path was downloaded, remove its parent directory (shutil.rmtree) to clean up.

3. Error fallback in RecordTemp.load:  
   – If load_object raises LoadObjectError and parents=True and depend_cls≠ None, it temporarily casts self to depend_cls (via class_casting) and retries load(name) recursively.  
   – If that still fails (or parents=False/no depend_cls), the LoadObjectError is propagated.","['FUNCTION keras.keras.src.export.onnx.export_onnx has parameter or accepts argument keras.keras.src.export.onnx.export_onnx.input_signature\nExport the model as a ONNX artifact for inference.  This method lets you export a model to a lightweight ONNX artifact that contains the model\'s forward pass only (its `call()` method) and can be served via e.g. ONNX Runtime.  The original code of the model (including any custom layers you may have used) is *no longer* necessary to reload the artifact -- it is entirely standalone.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the artifact.     verbose: `bool`. Whether to print a message during export. Defaults to         `None`, which uses the default value set by different backends and         formats.     input_signature: Optional. Specifies the shape and dtype of the model         inputs. Can be a structure of `keras.InputSpec`, `tf.TensorSpec`,         `backend.KerasTensor`, or backend tensor. If not provided, it will         be automatically computed. Defaults to `None`.     **kwargs: Additional keyword arguments.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** The dtype policy must be ""float32"" for the model. You can further optimize the ONNX artifact using the ONNX toolkit. Learn more here: [https://onnxruntime.ai/docs/performance/](https://onnxruntime.ai/docs/performance/).  **Note:** The dynamic shape feature is not yet supported with Torch backend. As a result, you must fully define the shapes of the inputs using `input_signature`. If `input_signature` is not provided, all instances of `None` (such as the batch size) will be replaced with `1`.  Example:  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""METHOD qlib.qlib.workflow.record_temp.RecordTemp.load has parameter or accepts argument qlib.qlib.workflow.record_temp.RecordTemp.load.name\nIt behaves the same as self.recorder.load_object. But it is an easier interface because users don't have to care about `get_path` and `artifact_path`  Parameters ---------- name : str     the name for the file to be load.  parents : bool     Each recorder has different `artifact_path`.     So parents recursively find the path in parents     Sub classes has higher priority  Return ------ The stored records."", 'MODULE keras.keras.src.export.onnx contains FUNCTION keras.keras.src.export.onnx.export_onnx\nExport the model as a ONNX artifact for inference.  This method lets you export a model to a lightweight ONNX artifact that contains the model\'s forward pass only (its `call()` method) and can be served via e.g. ONNX Runtime.  The original code of the model (including any custom layers you may have used) is *no longer* necessary to reload the artifact -- it is entirely standalone.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the artifact.     verbose: `bool`. Whether to print a message during export. Defaults to         `None`, which uses the default value set by different backends and         formats.     input_signature: Optional. Specifies the shape and dtype of the model         inputs. Can be a structure of `keras.InputSpec`, `tf.TensorSpec`,         `backend.KerasTensor`, or backend tensor. If not provided, it will         be automatically computed. Defaults to `None`.     **kwargs: Additional keyword arguments.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** The dtype policy must be ""float32"" for the model. You can further optimize the ONNX artifact using the ONNX toolkit. Learn more here: [https://onnxruntime.ai/docs/performance/](https://onnxruntime.ai/docs/performance/).  **Note:** The dynamic shape feature is not yet supported with Torch backend. As a result, you must fully define the shapes of the inputs using `input_signature`. If `input_signature` is not provided, all instances of `None` (such as the batch size) will be replaced with `1`.  Example:  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""METHOD qlib.qlib.workflow.record_temp.RecordTemp.load has parameter or accepts argument qlib.qlib.workflow.record_temp.RecordTemp.load.parents\nIt behaves the same as self.recorder.load_object. But it is an easier interface because users don't have to care about `get_path` and `artifact_path`  Parameters ---------- name : str     the name for the file to be load.  parents : bool     Each recorder has different `artifact_path`.     So parents recursively find the path in parents     Sub classes has higher priority  Return ------ The stored records."", 'FUNCTION keras.keras.src.export.onnx.export_onnx has parameter or accepts argument keras.keras.src.export.onnx.export_onnx.model\nExport the model as a ONNX artifact for inference.  This method lets you export a model to a lightweight ONNX artifact that contains the model\'s forward pass only (its `call()` method) and can be served via e.g. ONNX Runtime.  The original code of the model (including any custom layers you may have used) is *no longer* necessary to reload the artifact -- it is entirely standalone.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the artifact.     verbose: `bool`. Whether to print a message during export. Defaults to         `None`, which uses the default value set by different backends and         formats.     input_signature: Optional. Specifies the shape and dtype of the model         inputs. Can be a structure of `keras.InputSpec`, `tf.TensorSpec`,         `backend.KerasTensor`, or backend tensor. If not provided, it will         be automatically computed. Defaults to `None`.     **kwargs: Additional keyword arguments.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** The dtype policy must be ""float32"" for the model. You can further optimize the ONNX artifact using the ONNX toolkit. Learn more here: [https://onnxruntime.ai/docs/performance/](https://onnxruntime.ai/docs/performance/).  **Note:** The dynamic shape feature is not yet supported with Torch backend. As a result, you must fully define the shapes of the inputs using `input_signature`. If `input_signature` is not provided, all instances of `None` (such as the batch size) will be replaced with `1`.  Example:  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.download_artifact has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.download_artifact.dst_path', 'FUNCTION keras.keras.src.export.onnx.export_onnx has parameter or accepts argument keras.keras.src.export.onnx.export_onnx.filepath\nExport the model as a ONNX artifact for inference.  This method lets you export a model to a lightweight ONNX artifact that contains the model\'s forward pass only (its `call()` method) and can be served via e.g. ONNX Runtime.  The original code of the model (including any custom layers you may have used) is *no longer* necessary to reload the artifact -- it is entirely standalone.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the artifact.     verbose: `bool`. Whether to print a message during export. Defaults to         `None`, which uses the default value set by different backends and         formats.     input_signature: Optional. Specifies the shape and dtype of the model         inputs. Can be a structure of `keras.InputSpec`, `tf.TensorSpec`,         `backend.KerasTensor`, or backend tensor. If not provided, it will         be automatically computed. Defaults to `None`.     **kwargs: Additional keyword arguments.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** The dtype policy must be ""float32"" for the model. You can further optimize the ONNX artifact using the ONNX toolkit. Learn more here: [https://onnxruntime.ai/docs/performance/](https://onnxruntime.ai/docs/performance/).  **Note:** The dynamic shape feature is not yet supported with Torch backend. As a result, you must fully define the shapes of the inputs using `input_signature`. If `input_signature` is not provided, all instances of `None` (such as the batch size) will be replaced with `1`.  Example:  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### XGBoost4J-Spark: Redesigning checkpointing mechanism\n* RFC is available at #4786\n* Clean up checkpoint file after a successful training job (#4754): The current implementation in XGBoost4J-Spark does not clean up the checkpoint file after a successful training job. If the user runs another job with the same checkpointing directory, she will get a wrong model because the second job will re-use the checkpoint file left over from the first job. To prevent this scenario, we propose to always clean up the checkpoint file after every successful training job.\n* Avoid Multiple Jobs for Checkpointing (#5082): The current method for checkpoint is to collect the booster produced at the last iteration of each checkpoint internal to Driver and persist it in HDFS. The major issue with this approach is that it needs to re-perform the data preparation for training if the user did not choose to cache the training dataset. To avoid re-performing data prep, we build external-memory checkpointing in the XGBoost4J layer as well.\n* Enable deterministic repartitioning when checkpoint is enabled (#4807): Distributed algorithm for gradient boosting assumes a fixed partition of the training data between multiple iterations. In previous versions, there was no guarantee that data partition would stay the same, especially when a worker goes down and some data had to recovered from previous checkpoint. In this release, we make data partition deterministic by using the data hash value of each data row in computing the partition.\n\n### XGBoost4J-Spark: handle errors thrown by the native code (#4560)\n* All core logic of XGBoost is written in C++, so XGBoost4J-Spark internally uses the C++ code via Java Native Interface (JNI). #4560 adds a proper error handling for any errors or exceptions arising from the C++ code, so that the XGBoost Spark application can be torn down in an orderly fashion.\n\n### XGBoost4J-Spark: Refine method to count the number of alive cores  (#4858)\n* The `SparkParallelismTracker` class ensures that sufficient number of executor cores are alive. To that end, it is important to query the number of alive cores reliably.\n\n### XGBoost4J: Add `BigDenseMatrix` to store more than `Integer.MAX_VALUE` elements (#4383)\n\n### Robust model serialization with JSON (#4632, #4708, #4739, #4868, #4936, #4945, #4974, #5086, #5087, #5089, #5091, #5094, #5110, #5111, #5112, #5120, #5137, #5218, #5222, #5236, #5245, #5248, #5281)\n* In this release, we introduce an experimental support of using [JSON](https://www.json.org/json-en.html) for serializing (saving/loading) XGBoost models and related hyperparameters for training. We would like to eventually replace the old binary format with JSON, since it is an open format and parsers are available in many programming languages and platforms. See [the documentation for model I/O using JSON](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/saving_model.html). #3980 explains why JSON was chosen over other alternatives.\n* To maximize interoperability and compatibility of the serialized models, we now split serialization into two parts (#4855):\n  1. Model, e.g. decision trees and strictly related metadata like `num_features`.\n  2. Internal configuration, consisting of training parameters and other configurable parameters. For example, `max_delta_step`, `tree_method`, `objective`, `predictor`, `gpu_id`.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md\nAssistant Guidelines\nThese rules are absolutely imperative to adhere to. Comply with them precisely as they are outlined.\n\nThe agent must use sequential thinking MCP tool to work out problems.\n\nCore Behavior Guidelines\n\nRespond only to explicit requests. Do not add files, code, tests, or comments unless asked.\n\nFollow instructions precisely. No assumptions or speculative additions.\n\nUse provided context accurately.\n\nAvoid extra output. No debugging logs or test harnesses unless requested.\n\nProduce clean, optimized code when code is requested. Respect existing style.\n\nDeliver complete, standalone solutions. No placeholders.\n\nLimit file creation. Only create new files when necessary.\n\nIf you modify the model in a user\'s code, you must confirm with the user and never be sneaky. Always tell the user exactly what you are doing.\n\nCommunication & Delivery\n\n9. Don\'t explain unless asked. Do not expose reasoning in outputs.\n10. If unsure, say ""I don\'t know."" Avoid hallucinated content.\n11. Maintain consistency across sessions. Refer to project memory and documentation.\n12. Respect privacy and permissions. Never leak or infer secure data.\n13. Prioritize targeted edits over full rewrites.\n14. Optimize incrementally. Avoid unnecessary overhauls.\n\nSpec.md Requirement\n\nYou must maintain a file named Spec.md. This file acts as the single source of truth for the project.\n\nRules:\n\nBefore starting any implementation, check if Spec.md already exists.\n\nIf it does not exist, create one using the template provided below.\n\nAlways update Spec.md before and after any major change.\n\nUse the contents of Spec.md to guide logic, structure, and implementation decisions.\n\nWhen updating a section, condense previous content to keep the document concise.\n\nSpec.md Starter Template (Plain Text Format)\n\nTitle: Spec.md – Project Specification\n\nSection: Purpose\nDescribe the main goal of this feature, tool, or system.\n\nSection: Core Functionality\nList the key features, expected behaviors, and common use cases.\n\nSection: Architecture Overview\nSummarize the technical setup, frameworks used, and main modules or services.\n\nSection: Input and Output Contracts\nList all inputs and outputs in a table-like format:\n\nInput: describe the input data, its format, and where it comes from.\n\nOutput: describe the output data, its format, and its destination.\n\nSection: Edge Cases and Constraints\nList known limitations, special scenarios, and fallback behaviors.\n\nSection: File and Module Map\nList all important files or modules and describe what each one is responsible for.\n\nSection: Open Questions or TODOs\nCreate a checklist of unresolved decisions, logic that needs clarification, or tasks that are still pending.\n\nSection: Last Updated\nInclude the most recent update date and who made the update.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### CI\n* [CI] Rotate access keys for uploading MacOS artifacts from Travis CI (#7253)\n* Reduce Travis environment setup time. (#6912)\n* Restore R cache on github action. (#6985)\n* [CI] Remove stray build artifact to avoid error in artifact packaging (#6994)\n* [CI] Move appveyor tests to action (#6986)\n* Remove appveyor badge. [skip ci] (#7035)\n* [CI] Configure RAPIDS, dask, modin (#7033)\n* Test on s390x. (#7038)\n* [CI] Upgrade to CMake 3.14 (#7060)\n* [CI] Update R cache. (#7102)\n* [CI] Pin libomp to 11.1.0  (#7107)\n* [CI] Upgrade build image to CentOS 7 + GCC 8; require CUDA 10.1 and later (#7141)\n* [dask] Work around segfault in prediction. (#7112)\n* [dask] Remove the workaround for segfault. (#7146)\n* [CI] Fix hanging Python setup in Windows CI (#7186)\n* [CI] Clean up in beginning of each task in Win CI (#7189)\n* Fix travis. (#7237)\n\n### Acknowledgement\n* **Contributors**: Adam Pocock (@Craigacp), Jeff H (@JeffHCross), Johan Hansson (@JohanWork), Jose Manuel Llorens (@JoseLlorensRipolles), Benjamin Szőke (@Livius90), @ReeceGoding, @ShvetsKS, Robert Zabel (@ZabelTech), Ali (@ali5h), Andrew Ziem (@az0), Andy Adinets (@canonizer), @david-cortes, Daniel Saxton (@dsaxton), Emil Sadek (@esadek), @farfarawayzyt, Gil Forsyth (@gforsyth), @giladmaya, @graue70, Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), José Morales (@jmoralez), Kai Fricke (@krfricke), Christian Lorentzen (@lorentzenchr), Mads R. B. Kristensen (@madsbk), Anton Kostin (@masguit42), Martin Petříček (@mpetricek-corp), @naveenkb, Taewoo Kim (@oOTWK), Viktor Szathmáry (@phraktle), Robert Maynard (@robertmaynard), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), Paul Taylor (@trxcllnt), @vslaykovsky, Bobby Wang (@wbo4958),\n* **Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Jose Manuel Llorens (@JoseLlorensRipolles), Kodi Arfer (@Kodiologist), Benjamin Szőke (@Livius90), Mark Guryanov (@MarkGuryanov), Rory Mitchell (@RAMitchell), @ReeceGoding, @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Ziem (@az0), @candalfigomoro, Andy Adinets (@canonizer), Dante Gama Dessavre (@dantegd), @david-cortes, Daniel Saxton (@dsaxton), @farfarawayzyt, Gil Forsyth (@gforsyth), Harutaka Kawamura (@harupy), Philip Hyunsu Cho (@hcho3), @jakirkham, James Lamb (@jameslamb), José Morales (@jmoralez), James Bourbeau (@jrbourbeau), Christian Lorentzen (@lorentzenchr), Martin Petříček (@mpetricek-corp), Nikolay Petrov (@napetrov), @naveenkb, Viktor Szathmáry (@phraktle), Robin Teuwens (@rteuwens), Yuan Tang (@terrytangyuan), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), @vkuzmin-uber, Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n\n## v1.4.2 (2021.05.13)\nThis is a patch release for Python package with following fixes:\n\n* Handle the latest version of cupy.ndarray in inplace_predict. (#6933)\n* Ensure output array from predict_leaf is (n_samples, ) when there's only 1 tree. 1.4.0 outputs (n_samples, 1). (#6889)\n* Fix empty dataset handling with multi-class AUC. (#6947)\n* Handle object type from pandas in inplace_predict. (#6927)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 977, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",68.0826804637909,644
What sequence of operations does the download_data entry point perform to fetch and assemble quarterly pit data?,"The sequence of operations performed by the `download_data` entry point to fetch and assemble quarterly pit data in qlib is as follows:

1. The user runs the command `python collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly`
2. The script downloads the raw data from baostock to a local directory.
3. The script normalizes the high, low, close, open price using adjclose.
4. The script normalizes the high, low, close, open price so that the first valid trading date's close price is 1.","The `download_data` entry point (in scripts/data_collector/base.py: BaseRun.download_data) does roughly the following:

1.  Dynamically looks up your collector class (here PitCollector) via `self.collector_class_name`.  
2.  Instantiates it with parameters (`source_dir`, `max_workers`, `max_collector_count`, `delay`, `start`, `end`, `interval='quarterly'`, …).  
3.  Calls its `collector_data()` method, which internally loops over all symbols/quarters and for each invokes PitCollector.get_data (in scripts/data_collector/pit/collector.py).  

Inside PitCollector.get_data, for each symbol and quarter it:
-  Splits and normalizes the symbol/exchange.  
-  Formats `start_date`/`end_date`.  
-  Calls, in order:
   · get_performance_express_report_df  
   · get_profit_df  
   · get_forecast_report_df  
   · get_growth_df  
-  Concatenates those four DataFrames and returns the result.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'METHOD qlib.scripts.data_collector.pit.collector.PitCollector.get_data has parameter or accepts argument qlib.scripts.data_collector.pit.collector.PitCollector.get_data.start_datetime', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.quarterly\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'METHOD qlib.scripts.data_collector.baostock_5min.collector.Run.download_data has parameter or accepts argument qlib.scripts.data_collector.baostock_5min.collector.Run.download_data.start\ndownload data from Baostock  Notes -----     check_data_length, example:         hs300 5min, a week: 4 * 60 * 5  Examples ---------     # get hs300 5min data     $ python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.last\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'METHOD qlib.scripts.data_collector.baostock_5min.collector.Run.download_data has parameter or accepts argument qlib.scripts.data_collector.baostock_5min.collector.Run.download_data.end\ndownload data from Baostock  Notes -----     check_data_length, example:         hs300 5min, a week: 4 * 60 * 5  Examples ---------     # get hs300 5min data     $ python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.first\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can\'t meet users\' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month\'s data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *""2000-01-01""*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *""failed""* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date\'s close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 979, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",48.421183586120605,645
Describe how the Run class orchestrates fetching the four PIT data types and then applies normalization for quarterly output.,"The Run class in qlib orchestrates fetching the four PIT data types by using the `get_period_list` function, which returns all possible values between `first` and `end`. This function takes two parameters: `quarterly`, which determines whether to return quarterly or yearly indexes, and `first` and `end`, which specify the range of dates. The fetched data is then normalized for quarterly output using the `normalize_data` method and the `PITNormalize` class's `normalize` method. This process involves taking date field name and symbol field name as input parameters to normalize the data after it has been fetched and downloaded.","Run simply wires together PitCollector and PitNormalize under the BaseRun workflow:

1. Run.collector_class_name returns “PitCollector”, so BaseRun will instantiate PitCollector(interval=""quarterly"") and call its get_data(symbol, “quarterly”, start, end).  
   – PitCollector.get_data invokes in order  
     • get_performance_express_report_df  
     • get_profit_df  
     • get_forecast_report_df  
     • get_growth_df  
   and then pd.concat’s those four DataFrames.

2. After data collection, BaseRun looks up normalize_class_name (“PitNormalize”), instantiates PitNormalize(interval=""quarterly"") and passes the raw df into PitNormalize.normalize.  
   – normalize fills any missing “date” by adding 45 days to “period”  
   – converts “period” into a quarterly code (YYYYQ)  

The result is a single quarterly‐normalized PIT table.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.fund.collector.Run has method qlib.scripts.data_collector.fund.collector.Run.normalize_data\nnormalize data  Parameters ---------- date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.last\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.crypto.collector.Run has method qlib.scripts.data_collector.crypto.collector.Run.normalize_data\nnormalize data  Parameters ---------- date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data --source_dir ~/.qlib/crypto_data/source/1d --normalize_dir ~/.qlib/crypto_data/source/1d_nor --interval 1d --date_field_name date', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.first\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize has method qlib.scripts.data_collector.pit.collector.PitNormalize.normalize', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.quarterly\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 11, 'n_files_pr': 3, 'pr': 982, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",49.43167209625244,646
What are the main steps executed in the example's main method to run an online simulation and backtest?,"The main steps executed in the example's main method to run an online simulation and backtest in qlib are:

1.  The `OnlineSimulationExample` class is defined, which inherits from the `WorkflowBase` class.
2.  In the `main` method of the `OnlineSimulationExample` class, an instance of the `OnlineManager` class is created.
3.  The `simulate` method of the `OnlineManager` instance is called to simulate the online process.
4.  The `delay_prepare` method of the `OnlineManager` instance is called to prepare all models and signals if something is waiting for preparation.
5.  The `get_signals` method of the `OnlineManager` instance is called to get prepared online signals.","In examples/online_srv/online_management_simulate.py, OnlineSimulationExample.main does roughly:

1. Call self.reset() (to clear tasks via TaskManager and delete any existing recorders via R.get_exp/exp.delete_recorder).  
2. Call self.rolling_online_manager.simulate(end_time) to run the rolling‐window online simulation.  
3. Call and print self.rolling_online_manager.get_collector()() to collect simulation metrics.  
4. Call and print self.rolling_online_manager.get_signals() to extract the signal DataFrame.  
5. Build a TopkDropoutStrategy (using those signals) and run backtest_daily(start, end, strategy).  
6. Compute excess returns with risk_analysis and pprint the concatenated results.","['CLASS qlib.qlib.workflow.online.strategy.OnlineStrategy has method qlib.qlib.workflow.online.strategy.OnlineStrategy.prepare_online_models\nSelect some models from trained models and set them to online models. This is a typical implementation to online all trained models, you can override it to implement the complex method. You can find the last online models by OnlineTool.online_models if you still need them.  NOTE: Reset all online models to trained models. If there are no trained models, then do nothing.  **NOTE**:     Current implementation is very naive. Here is a more complex situation which is more closer to the     practical scenarios.     1. Train new models at the day before `test_start` (at time stamp `T`)     2. Switch models at the `test_start` (at time timestamp `T + 1` typically)  Args:     models (list): a list of models.     cur_time (pd.Dataframe): current time from OnlineManger. None for the latest.  Returns:     List[object]: a list of online models.\nOnlineStrategy is working with `Online Manager <#Online Manager>`_, responding to how the tasks are generated, the models are updated and signals are prepared.', 'CLASS qlib.qlib.workflow.online.manager.OnlineManager has method qlib.qlib.workflow.online.manager.OnlineManager.simulate\nStarting from the current time, this method will simulate every routine in OnlineManager until the end time.  Considering the parallel training, the models and signals can be prepared after all routine simulating.  The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.  Args:     end_time: the time the simulation will end     frequency: the calendar frequency     task_kwargs (dict): the params for `prepare_tasks`     model_kwargs (dict): the params for `prepare_online_models`     signal_kwargs (dict): the params for `prepare_signals`  Returns:     Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.     pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.\nOnlineManager can manage online models with `Online Strategy <#Online Strategy>`_. It also provides a history recording of which models are online at what time.', 'CLASS qlib.qlib.workflow.online.manager.OnlineManager has method qlib.qlib.workflow.online.manager.OnlineManager.get_signals\nGet prepared online signals.  Returns:     Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.     pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.\nOnlineManager can manage online models with `Online Strategy <#Online Strategy>`_. It also provides a history recording of which models are online at what time.', 'CLASS qlib.examples.online_srv.online_management_simulate.OnlineSimulationExample has method qlib.examples.online_srv.online_management_simulate.OnlineSimulationExample.main', 'CLASS qlib.qlib.workflow.online.manager.OnlineManager has method qlib.qlib.workflow.online.manager.OnlineManager.delay_prepare\nPrepare all models and signals if something is waiting for preparation.  Args:     model_kwargs: the params for `end_train`     signal_kwargs: the params for `prepare_signals`\nOnlineManager can manage online models with `Online Strategy <#Online Strategy>`_. It also provides a history recording of which models are online at what time.', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.single_with_simulator.backtest_config\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.workflow.online.strategy.OnlineStrategy has method qlib.qlib.workflow.online.strategy.OnlineStrategy.prepare_tasks\nAfter the end of a routine, check whether we need to prepare and train some new tasks based on cur_time (None for latest).. Return the new tasks waiting for training.  You can find the last online models by OnlineTool.online_models.\nOnlineStrategy is working with `Online Manager <#Online Manager>`_, responding to how the tasks are generated, the models are updated and signals are prepared.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 984, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",48.15850043296814,647
What happens when the start calendar index comes after the end calendar index in the base feature loader?,"When the start calendar index comes after the end calendar index in the base feature loader, qlib will raise an error because it cannot locate the time range. The correct order is to have the start calendar index before the end calendar index.

To fix this issue, you need to ensure that the start calendar index is before the end calendar index. Here's a code snippet that demonstrates this:

```python
from qlib import load

# Load the data with incorrect time range
data = load('000001.XSHG', '2019-01-01 00:00:00', '2018-12-31 23:59:59')

# This will raise an error because the start calendar index is after the end calendar index
```

To fix this issue, you need to swap the time range as shown in the corrected code below:

```python
from qlib import load

# Load the data with correct time range
data = load('000001.XSHG', '2018-12-31 23:59:59', '2019-01-01 00:00:00')
```

In this case, qlib will correctly locate the time range and return the data.","In Expression.load (qlib/data/base.py), there’s a guard:

```
if start_index is not None and end_index is not None and start_index > end_index:
    raise ValueError(f""Invalid index range: {start_index} {end_index}"")
```

So you’ll get a ValueError if the start calendar index comes after the end.","['CLASS qlib.qlib.data.data.CalendarProvider has method qlib.qlib.data.data.CalendarProvider.locate_index\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.\nCalendar provider base class  Provide calendar data.', 'CLASS qlib.qlib.data.storage.file_storage.FileCalendarStorage has method qlib.qlib.data.storage.file_storage.FileCalendarStorage.index', 'METHOD qlib.qlib.data.data.CalendarProvider.locate_index has parameter or accepts argument qlib.qlib.data.data.CalendarProvider.locate_index.start_time\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.', 'METHOD qlib.qlib.data.data.BaseProvider.calendar has parameter or accepts argument qlib.qlib.data.data.BaseProvider.calendar.end_time', 'METHOD qlib.qlib.data.data.CalendarProvider.locate_index has parameter or accepts argument qlib.qlib.data.data.CalendarProvider.locate_index.future\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.', 'METHOD qlib.qlib.data.storage.file_storage.FileCalendarStorage.index has parameter or accepts argument qlib.qlib.data.storage.file_storage.FileCalendarStorage.index.value', 'METHOD qlib.qlib.data.data.CalendarProvider.locate_index has parameter or accepts argument qlib.qlib.data.data.CalendarProvider.locate_index.end_time\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md\n# iBOVESPA History Companies Collection\n\n## Requirements\n\n- Install the libs from the file `requirements.txt`\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n- `requirements.txt` file was generated using python3.8\n\n## For the ibovespa (IBOV) index, we have:\n\n<hr/>\n\n### Method `get_new_companies`\n\n#### <b>Index start date</b>\n\n- The ibovespa index started on 2 January 1968 ([wiki](https://en.wikipedia.org/wiki/%C3%8Dndice_Bovespa)).  In order to use this start date in our `bench_start_date(self)` method, two conditions must be satisfied:\n    1) APIs used to download brazilian stocks (B3) historical prices must keep track of such historic data since 2 January 1968\n\n    2) Some website or API must provide, from that date, the historic index composition. In other words, the companies used to build the index .\n\n    As a consequence, the method `bench_start_date(self)` inside `collector.py` was implemented using `pd.Timestamp(""2003-01-03"")` due to two reasons\n\n    1) The earliest ibov composition that have been found was from the first quarter of 2003. More informations about such composition can be seen on the sections below.\n\n    2) Yahoo finance, one of the libraries used to download symbols historic prices, keeps track from this date forward.\n\n- Within the `get_new_companies` method, a logic was implemented to get, for each ibovespa component stock, the start date that yahoo finance keeps track of.\n\n#### <b>Code Logic</b>\n\nThe code does a web scrapping into the B3\'s [website](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br), which keeps track of the ibovespa stocks composition on the current day. \n\nOther approaches, such as `request` and `Beautiful Soup` could have been used. However, the website shows the table with the stocks with some delay, since it uses a script inside of it to obtain such compositions.\nAlternatively, `selenium` was used to download this stocks\' composition in order to overcome this problem.\n\nFuthermore, the data downloaded from the selenium script  was preprocessed so it could be saved into the `csv` format stablished by `scripts/data_collector/index.py`.\n\n<hr/>\n\n### Method `get_changes` \n\nNo suitable data source that keeps track of ibovespa\'s history stocks composition has been found. Except from this [repository](https://github.com/igor17400/IBOV-HCI) which provide such information have been used, however it only provides the data from the 1st quarter of 2003 to 3rd quarter of 2021.\n\nWith that reference, the index\'s composition can be compared quarter by quarter and year by year and then generate a file that keeps track of which stocks have been removed and which have been added each quarter and year.\n\n<hr/>\n\n### Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method save_new_companies\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 987, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",58.85465049743652,648
What additional processing does the general update method perform that the dataset-based retrieval bypasses?,"The general update method in qlib performs additional processing that the dataset-based retrieval bypasses, including preparing the dataset, checking for updates, and updating the data based on the given dataset. It also includes some general routine steps such as preparing the dataset and checking which are not included in the dataset-based retrieval.","The key is in qlib/workflow/online/update.py:

• RecordUpdater.update(...) runs the full “online‐update” pipeline – it  
  – prepares the Dataset (using the recorder)  
  – runs all the pre‐/post‐checks and sanity validations  
  – handles logging, error handling, persistence hooks, etc.  

• DSBasedUpdater.get_update_data(...) by contrast simply pulls the new DataFrame from the Dataset and skips all of those general routine steps (dataset prep, parameter checking, hooks, etc.).","['CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.get_update_data\nreturn the updated data based on the given dataset  The difference between `get_update_data` and `update` - `update_date` only include some data specific feature - `update` include some general routine steps(e.g. prepare dataset, checking)\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'CLASS qlib.qlib.workflow.online.update.LabelUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the label in the recorder  Assumption - The label is generated from record_temp.SignalRecord.', 'METHOD qlib.qlib.workflow.online.update.DSBasedUpdater.update has parameter or accepts argument qlib.qlib.workflow.online.update.DSBasedUpdater.update.ret_new\nParameters ---------- dataset : DatasetH     DatasetH: the instance of DatasetH. None for prepare it again. write : bool     will the the write action be executed ret_new : bool     will the updated data be returned  Returns ------- Optional[object]     the updated dataset', 'CLASS qlib.qlib.workflow.online.update.PredUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the prediction in the Recorder', 'METHOD qlib.qlib.workflow.online.update.DSBasedUpdater.update has parameter or accepts argument qlib.qlib.workflow.online.update.DSBasedUpdater.update.dataset\nParameters ---------- dataset : DatasetH     DatasetH: the instance of DatasetH. None for prepare it again. write : bool     will the the write action be executed ret_new : bool     will the updated data be returned  Returns ------- Optional[object]     the updated dataset', 'CLASS qlib.qlib.data.dataset.__init__.DatasetH has method qlib.qlib.data.dataset.__init__.DatasetH.get_max_time\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.update\nParameters ---------- dataset : DatasetH     DatasetH: the instance of DatasetH. None for prepare it again. write : bool     will the the write action be executed ret_new : bool     will the updated data be returned  Returns ------- Optional[object]     the updated dataset\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 6, 'pr': 992, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",35.340766191482544,649
What argument provided by the new reference operator takes precedence over the relative start and end indices when retrieving point-in-time data?,"The argument provided by the new reference operator that takes precedence over the relative start and end indices when retrieving point-in-time data is `period`. This can be seen from the documentation of the `PITProvider` class, where it is mentioned that `period` will override `start_index` and `end_index`, allowing for more flexible and intuitive specification of time ranges.",The `period` argument passed into PITProvider.period_feature (e.g. via the new `P`/`PRef` operator) takes priority over `start_index` and `end_index`.,"['MODULE qlib.qlib.contrib.ops.high_freq contains CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'CLASS qlib.qlib.data.data.PITProvider has method qlib.qlib.data.data.PITProvider.period_feature\nget the historical periods data series between `start_index` and `end_index`  Parameters ---------- start_index: int     start_index is a relative index to the latest period to cur_time  end_index: int     end_index is a relative index to the latest period to cur_time     in most cases, the start_index and end_index will be a non-positive values     For example, start_index == -3 end_index == 0 and current period index is cur_idx,     then the data between [start_index + cur_idx, end_index + cur_idx] will be retrieved.  period: int     This is used for query specific period.     The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)     NOTE: `period`  will override `start_index` and `end_index`  Returns ------- pd.Series     The index will be integers to indicate the periods of the data     An typical examples will be     TODO  Raises ------ FileNotFoundError     This exception will be raised if the queried data do not exist.', 'CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum has method qlib.qlib.contrib.ops.high_freq.DayCumsum.__init__\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'METHOD qlib.qlib.data.cache.IndexManager.get_index has parameter or accepts argument qlib.qlib.data.cache.IndexManager.get_index.end_time', 'CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum has method qlib.qlib.contrib.ops.high_freq.DayCumsum.period_cusum\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'FUNCTION qlib.qlib.utils.time.time_to_day_index has parameter or accepts argument qlib.qlib.utils.time.time_to_day_index.time_obj', 'CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum has field or attribute or property qlib.qlib.contrib.ops.high_freq.DayCumsum.end\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md\nDefining a Custom Data Iterator to Load Data from External Memory\n=================================================================\n\nA simple demo for using custom data iterator with XGBoost.  The feature is still\n**experimental** and not ready for production use.  If you are not familiar with C API,\nplease read its introduction in our tutorials and visit the basic demo first.\n\nDefining Data Iterator\n----------------------\n\nIn the example, we define a custom data iterator with 2 methods: `reset` and `next`.  The\n`next` method passes data into XGBoost and tells XGBoost whether the iterator has reached\nits end, and the `reset` method resets iterations. One important detail when using the C\nAPI for data iterator is users need to make sure that the data passed into `next` method\nmust be kept in memory until the next iteration or `reset` is called.  The external memory\nDMatrix is not limited to training, but also valid for other features like prediction.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 4, 'n_context_nodes': 7, 'n_files_pr': 5, 'pr': 1000, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",37.86502742767334,650
Which regex character class is defined to match Chinese punctuation in parse_field?,"Based on the qlib codebase, it appears that Chinese punctuation is matched using a regex character class in the `parse_field` function. However, without more specific information about the exact implementation, it's difficult to provide a precise answer. If you could provide more context or details about the specific functionality you're trying to understand, I'd be happy to try and help further.","In `qlib/utils/__init__.py`’s `parse_field`, the Chinese punctuation class is defined by:

```python
chinese_punctuation_regex = r""\u3001\uff1a\uff08\uff09""
```","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.parse_field', 'CLASS qlib.scripts.data_collector.pit.collector.PitCollector has field or attribute or property qlib.scripts.data_collector.pit.collector.symbol_regex', 'FUNCTION qlib.qlib.utils.__init__.parse_field has parameter or accepts argument qlib.qlib.utils.__init__.parse_field.field', 'CLASS qlib.scripts.data_collector.pit.collector.PitCollector has field or attribute or property qlib.scripts.data_collector.pit.collector.PitCollector.symbol_regex', 'CLASS cognee.cognee.tasks.entity_completion.entity_extractors.regex_entity_config.RegexEntityConfig has method cognee.cognee.tasks.entity_completion.entity_extractors.regex_entity_config.RegexEntityConfig.get_compiled_pattern\nGet the compiled regex pattern for a specific entity type.\nClass to load and process regex entity extraction configuration.', 'CLASS qlib.scripts.data_collector.base.Normalize has field or attribute or property qlib.scripts.data_collector.base.symbol_field_name', 'CLASS cognee.cognee.tasks.entity_completion.entity_extractors.regex_entity_config.RegexEntityConfig has method cognee.cognee.tasks.entity_completion.entity_extractors.regex_entity_config.RegexEntityConfig.__init__\nInitialize the regex entity configuration with the config path.\nClass to load and process regex entity extraction configuration.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md\n## 了解我们的架构\n\n<div style=""text-align: center"">\n  <img src=""cognee_diagram_zh.JPG"" alt=""cognee概念图"" width=""100%"" />\n</div>\n\n\n\n## 演示\n\n1. 什么是AI记忆：\n\n[了解cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. 简单GraphRAG演示\n\n[简单GraphRAG演示](https://github.com/user-attachments/assets/f57fd9ea-1dc0-4904-86eb-de78519fdc32)\n\n3. cognee与Ollama\n\n[cognee与本地模型](https://github.com/user-attachments/assets/834baf9a-c371-4ecf-92dd-e144bd0eb3f6)\n\n\n## 行为准则\n\n我们致力于为我们的社区提供愉快和尊重的开源体验。有关更多信息，请参阅<a href=""https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md""><code>CODE_OF_CONDUCT</code></a>。\n\n## 💫 贡献者\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img alt=""contributors"" src=""https://contrib.rocks/image?repo=topoteretes/cognee""/>\n</a>\n\n\n## Star历史\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1003, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",42.898282527923584,651
"When invoking the custom load method, which instrument value is forwarded to the base operator's loader instead of the provided one?","When invoking the custom load method in qlib, the instrument value that is forwarded to the base operator's loader instead of the provided one is `instrument`. This behavior is a part of qlib's design and allows for flexibility in loading data from various sources.","In ChangeInstrument.load (qlib/data/ops.py), the incoming instrument argument is ignored and instead self.instrument (the one passed to the constructor) is forwarded to super().load.","['CLASS qlib.qlib.data.ops.ChangeInstrument inherits from or is a subclass of qlib.qlib.data.ops.ElemOperator\nElement-wise Operator  Parameters ---------- feature : Expression     feature instance  Returns ---------- Expression     feature operation output\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'METHOD qlib.qlib.data.ops.ChangeInstrument.load has parameter or accepts argument qlib.qlib.data.ops.ChangeInstrument.load.end_index', 'CLASS qlib.qlib.data.ops.ChangeInstrument has method qlib.qlib.data.ops.ChangeInstrument.__init__\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'METHOD qlib.qlib.data.dataset.loader.NestedDataLoader.load has parameter or accepts argument qlib.qlib.data.dataset.loader.NestedDataLoader.load.instruments', 'CLASS qlib.qlib.data.ops.ChangeInstrument has method qlib.qlib.data.ops.ChangeInstrument.load\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'METHOD qlib.qlib.data.dataset.loader.StaticDataLoader.load has parameter or accepts argument qlib.qlib.data.dataset.loader.StaticDataLoader.load.instruments', 'CLASS qlib.qlib.data.ops.ChangeInstrument has method qlib.qlib.data.ops.ChangeInstrument.__str__\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md\nDefining a Custom Data Iterator to Load Data from External Memory\n=================================================================\n\nA simple demo for using custom data iterator with XGBoost.  The feature is still\n**experimental** and not ready for production use.  If you are not familiar with C API,\nplease read its introduction in our tutorials and visit the basic demo first.\n\nDefining Data Iterator\n----------------------\n\nIn the example, we define a custom data iterator with 2 methods: `reset` and `next`.  The\n`next` method passes data into XGBoost and tells XGBoost whether the iterator has reached\nits end, and the `reset` method resets iterations. One important detail when using the C\nAPI for data iterator is users need to make sure that the data passed into `next` method\nmust be kept in memory until the next iteration or `reset` is called.  The external memory\nDMatrix is not limited to training, but also valid for other features like prediction.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1005, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.09507918357849,652
"Under what conditions does the workflow function generate a default experiment manager URI, and how is that URI constructed?","The workflow function generates a default experiment manager URI when the user starts an experiment. The URI is constructed by getting the default tracking URI or current URI from the `ExpManager` class. This is done in two steps: first, the `uri()` method of the `ExpManager` class is called to get the default tracking URI or current URI; second, this URI is used as the default experiment manager URI. If you want to set a specific URI for an experiment, you can use the `set_uri()` method of the `ExpManager` class. The code snippets below demonstrate how to do this:

```python
default_exp_manager_uri = ExpManager.uri()
print(default_exp_manager_uri)
```

```python
exp_manager.set_uri('https://example.com/experiment/1')
print(exp_manager.uri())
```

Note that the `ExpManager` class is used to manage experiments, and it has methods for getting and setting the URI.","In qlib/workflow/cli.py’s workflow(), if your workflow.yml’s qlib_init section does *not* include an exp_manager key, it falls back to the default C[""exp_manager""] and injects a URI of the form:

  “file:” + Path(os.getcwd()).resolve() / uri_folder

(where uri_folder defaults to “mlruns”).  
So you get e.g.  
  file:/home/you/your_proj/mlruns","['CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.list_experiments\nList all the existing experiments.  Returns ------- A dictionary (name -> experiment) of experiments information that being stored.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.get_uri\nMethod for retrieving the uri of current experiment manager.  Here is the example code:  .. code-block:: Python      uri = R.get_uri()  Returns ------- The uri of current experiment manager.\nA global system that helps to manage the experiments.', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.create_exp\nCreate an experiment.  Parameters ---------- experiment_name : str     the experiment name, which must be unique.  Returns ------- An experiment object.  Raise ----- ExpAlreadyExistError\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.delete_exp\nDelete an experiment.  Parameters ---------- experiment_id  : str     the experiment id. experiment_name  : str     the experiment name.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.uri\nGet the default tracking URI or current URI.  Returns ------- The tracking URI string.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.set_uri\nMethod to reset the **default** uri of current experiment manager.  NOTE:  - When the uri is refer to a file path, please using the absolute path instead of strings like ""~/mlruns/""   The backend don\'t support strings like this.\nA global system that helps to manage the experiments.', 'MODULE qlib.qlib.workflow.expm contains CLASS qlib.qlib.workflow.expm.ExpManager\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md\nAssistant Guidelines\nThese rules are absolutely imperative to adhere to. Comply with them precisely as they are outlined.\n\nThe agent must use sequential thinking MCP tool to work out problems.\n\nCore Behavior Guidelines\n\nRespond only to explicit requests. Do not add files, code, tests, or comments unless asked.\n\nFollow instructions precisely. No assumptions or speculative additions.\n\nUse provided context accurately.\n\nAvoid extra output. No debugging logs or test harnesses unless requested.\n\nProduce clean, optimized code when code is requested. Respect existing style.\n\nDeliver complete, standalone solutions. No placeholders.\n\nLimit file creation. Only create new files when necessary.\n\nIf you modify the model in a user\'s code, you must confirm with the user and never be sneaky. Always tell the user exactly what you are doing.\n\nCommunication & Delivery\n\n9. Don\'t explain unless asked. Do not expose reasoning in outputs.\n10. If unsure, say ""I don\'t know."" Avoid hallucinated content.\n11. Maintain consistency across sessions. Refer to project memory and documentation.\n12. Respect privacy and permissions. Never leak or infer secure data.\n13. Prioritize targeted edits over full rewrites.\n14. Optimize incrementally. Avoid unnecessary overhauls.\n\nSpec.md Requirement\n\nYou must maintain a file named Spec.md. This file acts as the single source of truth for the project.\n\nRules:\n\nBefore starting any implementation, check if Spec.md already exists.\n\nIf it does not exist, create one using the template provided below.\n\nAlways update Spec.md before and after any major change.\n\nUse the contents of Spec.md to guide logic, structure, and implementation decisions.\n\nWhen updating a section, condense previous content to keep the document concise.\n\nSpec.md Starter Template (Plain Text Format)\n\nTitle: Spec.md – Project Specification\n\nSection: Purpose\nDescribe the main goal of this feature, tool, or system.\n\nSection: Core Functionality\nList the key features, expected behaviors, and common use cases.\n\nSection: Architecture Overview\nSummarize the technical setup, frameworks used, and main modules or services.\n\nSection: Input and Output Contracts\nList all inputs and outputs in a table-like format:\n\nInput: describe the input data, its format, and where it comes from.\n\nOutput: describe the output data, its format, and its destination.\n\nSection: Edge Cases and Constraints\nList known limitations, special scenarios, and fallback behaviors.\n\nSection: File and Module Map\nList all important files or modules and describe what each one is responsible for.\n\nSection: Open Questions or TODOs\nCreate a checklist of unresolved decisions, logic that needs clarification, or tasks that are still pending.\n\nSection: Last Updated\nInclude the most recent update date and who made the update.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1008, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",59.97964549064636,653
How does parse_field include Chinese punctuation characters in the regex for matching feature identifiers?,"qlib includes Chinese punctuation characters in the regex for matching feature identifiers by using a custom symbol field name and date field name, as seen in the qlib_Alpha158.py file where the Alpha158Formatter class defines column_definition and identifiers attributes to handle the unique format of the Alpha158 dataset. This approach is also used in the fill_cn_1min_data.py script to handle missing symbols relative to 1min data.","In parse_field (qlib/utils/__init__.py) we build a little helper:

• chinese_punctuation_regex = r""\u3001\uff1a\uff08\uff09""  
• Then we inject that into our feature‐name patterns via f-strings:  
  – rf""\$\$([\w{chinese_punctuation_regex}]+)""  
  – rf""\$([\w{chinese_punctuation_regex}]+)""  

which expands to “[\w\u3001\uff1a\uff08\uff09]+”, allowing those Chinese punctuation marks in feature identifiers.","['CLASS qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter has method qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter.transform_inputs\nPerforms feature transformations.  This includes both feature engineering, preprocessing and normalisation.  Args:   df: Data frame to transform.  Returns:   Transformed data frame.\nDefines and formats data for the Alpha158 dataset.  Attributes:   column_definition: Defines input and data type of column used in the     experiment.   identifiers: Entity identifiers used in experiments.', 'CLASS qlib.scripts.data_collector.base.Normalize has field or attribute or property qlib.scripts.data_collector.base.symbol_field_name', 'MODULE qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158 contains CLASS qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter\nDefines and formats data for the Alpha158 dataset.  Attributes:   column_definition: Defines input and data type of column used in the     experiment.   identifiers: Entity identifiers used in experiments.', 'METHOD qlib.qlib.data.data.DatasetProvider.parse_fields has parameter or accepts argument qlib.qlib.data.data.DatasetProvider.parse_fields.fields', 'CLASS qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter has method qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter.get_fixed_params\nReturns fixed model parameters for experiments.\nDefines and formats data for the Alpha158 dataset.  Attributes:   column_definition: Defines input and data type of column used in the     experiment.   identifiers: Entity identifiers used in experiments.', 'CLASS qlib.scripts.data_collector.pit.collector.PitCollector has field or attribute or property qlib.scripts.data_collector.pit.collector.symbol_regex', 'CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.get_fscore\nGet feature importance of each feature.  .. note:: Zero-importance features will not be included     Keep in mind that this function does not include zero-importance feature,    i.e.  those features that have not been used in any split conditions.  Parameters ---------- fmap :    The name of feature map file\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1012, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",46.279199838638306,654
How does get_calendar_minute map each calendar timestamp to a half-hour bucket?,"The `get_calendar_minute` function maps each calendar timestamp to a half-hour bucket based on the specified frequency (`freq`) parameter. If `freq` is set to `'1min'`, it maps each minute in the calendar to a half-hour bucket, while if it's set to `'day'`, it maps each day in the calendar to a half-hour bucket. The `future` parameter determines whether high-frequency calendar minutes for future dates are loaded or not.","The `get_calendar_minute` function (qlib/contrib/ops/high_freq.py) does:

1. Calls `Cal.load_calendar(freq, future)` to get a list of datetime stamps.  
2. Maps each timestamp `x` to `x.minute // 30` (i.e. 0 for minutes 0–29, 1 for minutes 30–59).  
3. Returns the result as a NumPy array (and caches it in `H[""c""]`).","['METHOD qlib.examples.benchmarks.LightGBM.features_sample.Resample1minProcessor.__init__ has parameter or accepts argument qlib.examples.benchmarks.LightGBM.features_sample.Resample1minProcessor.__init__.hour', 'FUNCTION qlib.qlib.contrib.ops.high_freq.get_calendar_minute has parameter or accepts argument qlib.qlib.contrib.ops.high_freq.get_calendar_minute.future\nLoad High-Freq Calendar Minute Using Memcache', 'MODULE cognee.cognee.modules.engine.utils.generate_timestamp_datapoint contains FUNCTION cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int\nConverts a Timestamp model into an integer representation in milliseconds since the Unix epoch (UTC).  Args:     ts (Timestamp): The input Timestamp model containing year, month, day, hour, minute, and second.  Returns:     int: The UTC timestamp in milliseconds since January 1, 1970.', 'FUNCTION qlib.qlib.contrib.ops.high_freq.get_calendar_minute has parameter or accepts argument qlib.qlib.contrib.ops.high_freq.get_calendar_minute.freq\nLoad High-Freq Calendar Minute Using Memcache', 'FUNCTION cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int has parameter or accepts argument cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int.ts\nConverts a Timestamp model into an integer representation in milliseconds since the Unix epoch (UTC).  Args:     ts (Timestamp): The input Timestamp model containing year, month, day, hour, minute, and second.  Returns:     int: The UTC timestamp in milliseconds since January 1, 1970.', 'MODULE qlib.qlib.contrib.ops.high_freq contains FUNCTION qlib.qlib.contrib.ops.high_freq.get_calendar_minute\nLoad High-Freq Calendar Minute Using Memcache', 'METHOD keras.keras.src.initializers.constant_initializers.STFT.__call__ has parameter or accepts argument keras.keras.src.initializers.constant_initializers.STFT.scaling\nReturns a tensor object initialized as specified by the initializer.  The shape is assumed to be `(T, 1, F // 2 + 1)`, where `T` is the size of the given window, and `F` is the number of frequency bands. Only half the frequency bands are used, which is a common practice in STFT, because the second half are the conjugates of the first half in a reversed order.  Args:     shape: Shape of the tensor.     dtype: Optional dtype of the tensor. Only numeric or boolean dtypes         are supported. If not specified, `keras.backend.floatx()`         is used, which default to `float32` unless you configured it         otherwise (via `keras.backend.set_floatx(float_dtype)`).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 4, 'pr': 1022, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",42.45362329483032,655
"When assigning a DataFrame under a new tuple key, how does the setter build the MultiIndex for the resulting DataFrame’s columns?","When assigning a DataFrame under a new tuple key in qlib, the setter builds the MultiIndex for the resulting DataFrame’s columns by using the `__setitem__` method of the `SepDataFrame` class. This method takes two parameters: the item (which is the tuple key) and the DataFrame to be assigned. The `__setitem__` method then constructs the MultiIndex based on the column names of the assigned DataFrame.

Here's a code snippet that demonstrates how this works:
```python
from qlib.contrib.data.utils.sepdf import SepDataFrame

# Create a new SepDataFrame instance
sep_df = SepDataFrame()

# Assign a DataFrame to a new tuple key
sep_df['(key1, key2)'] = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})

# Print the resulting MultiIndex
print(sep_df['(key1, key2)'].index)
```
This code will output a MultiIndex with two levels: `key1` and `key2`.","In SepDataFrame.__setitem__ (qlib/contrib/data/utils/sepdf.py), when you assign a DataFrame under a new tuple key, it does roughly:

• Split your key into `_df_dict_key, *col_name`  
• Copy the DataFrame and rebuild its columns via  
  `pd.MultiIndex.from_tuples([(*col_name, *orig_idx) for orig_idx in df.columns])`  

So each original column tuple `orig_idx` gets “prefixed” with your `col_name` tuple to form the new MultiIndex.","['CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS keras.keras.src.backend.common.variables_test.VariableNumpyValueAndAssignmentTest has method keras.keras.src.backend.common.variables_test.VariableNumpyValueAndAssignmentTest.test_variable_assign\nTest assigning a new value to a variable.\ntests for Variable.numpy(), Variable.value() and Variable.assign()', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'MODULE qlib.qlib.utils.index_data contains CLASS qlib.qlib.utils.index_data.MultiData', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__contains__\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS qlib.qlib.utils.index_data.MultiData has method qlib.qlib.utils.index_data.MultiData.__repr__', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__setitem__\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 1080, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",63.9142427444458,668
How does get_instruments dynamically select and execute the appropriate collector class and method for a given market index?,"The `get_instruments` function in qlib dynamically selects and executes the appropriate collector class and method for a given market index by using a combination of the Collector class and the MergeCollector class. The Collector class is used to collect results from different collectors, while the MergeCollector class merges these results into a single dictionary. This process allows `get_instruments` to efficiently retrieve and combine data from various sources based on the specified market index.","In scripts/data_collector/utils.py, get_instruments does three things:

1. importlib.import_module(""data_collector.{}.collector"".format(market_index))  
2. getattr on that module for the class named f""{index_name.upper()}Index"" (e.g. IBOVIndex or CSI300Index) and instantiate it  
3. getattr(obj, method)() to call the requested method (e.g. parse_instruments, save_new_companies)  

So it picks the right collector module, class and method entirely by string names at runtime.","['CLASS qlib.qlib.workflow.task.collect.MergeCollector has method qlib.qlib.workflow.task.collect.MergeCollector.__init__\nInit MergeCollector.  Args:     collector_dict (Dict[str,Collector]): the dict like {collector_key, Collector}     process_list (List[Callable]): the list of processors or the instance of processor to process dict.     merge_func (Callable): a method to generate outermost key. The given params are ``collector_key`` from collector_dict and ``key`` from every collector after collecting.         None for using tuple to connect them, such as ""ABC""+(""a"",""b"") -> (""ABC"", (""a"",""b"")).\nA collector to collect the results of other Collectors  For example:      We have 2 collector, which named A and B.     A can collect {""prediction"": pd.Series} and B can collect {""IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}.     Then after this class\'s collect, we can collect {""A_prediction"": pd.Series, ""B_IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}      ...', 'CLASS qlib.scripts.data_collector.fund.collector.FundollectorCN has method qlib.scripts.data_collector.fund.collector.FundollectorCN.get_instrument_list', 'CLASS qlib.qlib.workflow.task.collect.MergeCollector inherits from or is a subclass of qlib.qlib.workflow.task.collect.Collector\nThe collector to collect different results\nA collector to collect the results of other Collectors  For example:      We have 2 collector, which named A and B.     A can collect {""prediction"": pd.Series} and B can collect {""IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}.     Then after this class\'s collect, we can collect {""A_prediction"": pd.Series, ""B_IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}      ...', 'CLASS qlib.scripts.data_collector.pit.collector.PitCollector has method qlib.scripts.data_collector.pit.collector.PitCollector.get_instrument_list', 'CLASS qlib.qlib.workflow.task.collect.MergeCollector has method qlib.qlib.workflow.task.collect.MergeCollector.collect\nCollect all results of collector_dict and change the outermost key to a recombination key.  Returns:     dict: the dict after collecting.\nA collector to collect the results of other Collectors  For example:      We have 2 collector, which named A and B.     A can collect {""prediction"": pd.Series} and B can collect {""IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}.     Then after this class\'s collect, we can collect {""A_prediction"": pd.Series, ""B_IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}      ...', 'CLASS qlib.scripts.data_collector.base.BaseCollector has method qlib.scripts.data_collector.base.BaseCollector.get_instrument_list', 'CLASS cognee.cognee.infrastructure.llm.LLMGateway.LLMGateway has method cognee.cognee.infrastructure.llm.LLMGateway.LLMGateway.create_transcript\nClass handles selection of structured output frameworks and LLM functions. Class used as a namespace for LLM related functions, should not be instantiated, all methods are static.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 7, 'n_files_pr': 5, 'pr': 1081, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",40.47521638870239,669
"When all sub-dataframes are deleted, how is the primary join key recalculated?","When all sub-dataframes are deleted, you may need to re-initialize the data using the `SepDataFrame` class and specify the correct join method to recalculate the primary join key. You can use the `join` parameter in the `SepDataFrame` class to specify how to join the data. For example:

```python
from qlib.contrib.data.utils.sepdf import SepDataFrame
df_dict = {...}
sep_df = SepDataFrame(df_dict, join='primary_key')  # Specify the correct join method
```
Please note that this is just an assumption based on the provided information and may not be the exact solution. You may need to consult the qlib documentation or contact the qlib developers for more information.","The magic happens in SepDataFrame.__delitem__ → SepDataFrame._update_join (qlib/contrib/data/utils/sepdf.py). After you delete a key, `_update_join` checks:

• If there are still any keys left, it does  
```python
self.join = next(iter(self._df_dict.keys()))
```  
i.e. picks the first remaining dataframe’s key as the new join.

• If `_df_dict` is now empty, it sets  
```python
self.join = None
```","['METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__ has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__.join\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index', 'METHOD qlib.qlib.data.dataset.loader.StaticDataLoader.__init__ has parameter or accepts argument qlib.qlib.data.dataset.loader.StaticDataLoader.__init__.join\nParameters ---------- config : dict     {fields_group: <path or object>} join : str     How to align different dataframes', 'METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__ has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__.skip_align\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index', 'METHOD qlib.qlib.data.dataset.loader.StaticDataLoader.__init__ has parameter or accepts argument qlib.qlib.data.dataset.loader.StaticDataLoader.__init__.config\nParameters ---------- config : dict     {fields_group: <path or object>} join : str     How to align different dataframes', 'METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__ has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__.df_dict\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS qlib.qlib.data.dataset.loader.StaticDataLoader has method qlib.qlib.data.dataset.loader.StaticDataLoader.__init__\nParameters ---------- config : dict     {fields_group: <path or object>} join : str     How to align different dataframes\nDataLoader that supports loading data from file or as provided.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Automatic update of daily frequency data(from yahoo finance)\n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --end_date <end date>\n      ```\n      * `end_date`: end of trading day(not included)\n      * `check_data_length`: check the number of rows per *symbol*, by default `None`\n        > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n\n  * `scripts/data_collector/yahoo/collector.py update_data_to_bin` parameters:\n      * `source_dir`: The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source""\n      * `normalize_dir`: Directory for normalize data, default ""Path(__file__).parent/normalize""\n      * `qlib_data_1d_dir`: the qlib data to be updated for yahoo, usually from: [download qlib data](https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)\n      * `end_date`: end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end)\n      * `region`: region, value from [""CN"", ""US""], default ""CN""\n      * `interval`: interval, default ""1d""(Currently only supports 1d data)\n      * `exists_skip`: exists skip, by default False\n\n## Using qlib data\n\n  ```python\n  import qlib\n  from qlib.data import D\n\n  # 1d data cn\n  # freq=day, freq default day\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data"", region=""cn"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data cn\n  # freq=1min\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n\n  # 1d data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data"", region=""us"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n  ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 1082, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",52.21139740943909,670
"Describe the sequence of data-fetching, normalization, and dumping operations performed in the TestPIT class setup to prepare PIT data for feature queries.","The sequence of data-fetching, normalization, and dumping operations performed in the TestPIT class setup to prepare PIT data for feature queries is as follows:

1. **Data Fetching**: The `TestPIT` class uses the `DatasetH` class from the `qlib.data.dataset` module to fetch data. The `prepare` method of the `DatasetH` class is used to specify the scope of the data to be prepared, which can be 'train', 'valid', or a combination of both.

2. **Normalization**: After fetching the data, it is normalized using the `PitNormalize` class from the `qlib.scripts.data_collector.pit.collector` module. The `normalize` method of this class takes a pandas DataFrame as input and returns the normalized DataFrame.

3. **Dumping**: Finally, the normalized data is dumped into PIT format using the `dump_pit.py` script from the `qlib/scripts` directory. This script converts the normalized csv files in the specified directory to numpy arrays and stores them in the qlib data directory.","In setUpClass (tests/test_pit.py):

1. Fetch Qlib market data  
   • GetData().qlib_data(name=""qlib_data_simple"", target_dir=cn_data_dir, region=""cn"")  
   • GetData().qlib_data(name=""qlib_data"",       target_dir=pit_dir,    region=""pit"")  

2. Normalize PIT CSVs  
   • Run(source_dir=pit_dir, normalize_dir=pit_normalized_dir, interval=""quarterly"").normalize_data()  

3. Dump normalized PIT into Qlib format  
   • DumpPitData(csv_path=pit_normalized_dir, qlib_dir=cn_data_dir).dump(interval=""quarterly"")","[""METHOD qlib.qlib.data.dataset.__init__.DatasetH.prepare has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.prepare.data_key\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:"", 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize has method qlib.scripts.data_collector.pit.collector.PitNormalize.normalize', ""METHOD qlib.qlib.data.dataset.__init__.DatasetH.prepare has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.prepare.segments\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:"", 'MODULE qlib.qlib.data.data contains CLASS qlib.qlib.data.data.PITProvider', ""METHOD qlib.qlib.data.dataset.__init__.DatasetH.prepare has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.prepare.col_set\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:"", 'METHOD qlib.scripts.data_collector.pit.collector.PitNormalize.normalize has parameter or accepts argument qlib.scripts.data_collector.pit.collector.PitNormalize.normalize.df', ""CLASS qlib.qlib.data.dataset.__init__.DatasetH has method qlib.qlib.data.dataset.__init__.DatasetH.prepare\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 1089, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",57.735860109329224,671
How is the annualized return calculated when using geometric accumulation?,"The annualized return is calculated using geometric accumulation in qlib, which is based on the formula: p_r = (p_end / p_start)^(250/n) - 1, where p_r represents the annual return, p_end is the final value, p_start is the initial value, and n is the number of days in the backtest. This formula takes into account the compounding effect of returns over time, providing a more accurate representation of an investment's performance.","In  qlib/contrib/evaluate.py’s risk_analysis (when mode=""product""), once you have the total compounded return 

    cumulative_return = (1+r).cumprod().iloc[-1] – 1

the annualized return is computed as

    annualized_return = (1 + cumulative_return)^(N/len(r)) – 1

where N is your annualization factor (e.g. 252 for daily).","['FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.N\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'MODULE qlib.qlib.contrib.evaluate contains FUNCTION qlib.qlib.contrib.evaluate.risk_analysis\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.r\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.mode\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'MODULE qlib.qlib.contrib.evaluate_portfolio contains FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_annual_return_from_positions\nAnnualized Returns  p_r = (p_end / p_start)^{(250/n)} - 1  p_r     annual return p_end   final value p_start init value n       days of backtest', 'FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_annual_return_from_positions has parameter or accepts argument qlib.qlib.contrib.evaluate_portfolio.get_annual_return_from_positions.positions\nAnnualized Returns  p_r = (p_end / p_start)^{(250/n)} - 1  p_r     annual return p_end   final value p_start init value n       days of backtest', 'FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_annual_return_from_positions has parameter or accepts argument qlib.qlib.contrib.evaluate_portfolio.get_annual_return_from_positions.init_asset_value\nAnnualized Returns  p_r = (p_end / p_start)^{(250/n)} - 1  p_r     annual return p_end   final value p_start init value n       days of backtest', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n# Benchmarks Performance\nThis page lists a batch of methods designed for alpha seeking. Each method tries to give scores/predictions for all stocks each day(e.g. forecasting the future excess return of stocks). The scores/predictions of the models will be used as the mined alpha. Investing in stocks with higher scores is expected to yield more profit.  \n\nThe alpha is evaluated in two ways.\n1. The correlation between the alpha and future return.\n1. Constructing portfolio based on the alpha and evaluating the final total return.\n   - The explanation of metrics can be found [here](https://qlib.readthedocs.io/en/latest/component/report.html#id4)\n\nHere are the results of each benchmark model running on Qlib's `Alpha360` and `Alpha158` dataset with China's A shared-stock & CSI300 data respectively. The values of each metric are the mean and std calculated based on 20 runs with different random seeds.\n\nThe numbers shown below demonstrate the performance of the entire `workflow` of each model. We will update the `workflow` as well as models in the near future for better results.\n<!-- \n> If you need to reproduce the results below, please use the **v1** dataset: `python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --version v1`\n>\n> In the new version of qlib, the default dataset is **v2**. Since the data is collected from the YahooFinance API (which is not very stable), the results of *v2* and *v1* may differ -->\n\n> NOTE:\n> The backtest start from 0.8.0 is quite different from previous version. Please check out the changelog for the difference.\n\n> NOTE:\n> We have very limited resources to implement and finetune the models. We tried our best effort to fairly compare these models.  But some models may have greater potential than what it looks like in the table below.  Your contribution is highly welcomed to explore their potential.\n\n## Results on CSI300\n\n### Alpha158 dataset\n\n| Model Name                               | Dataset                             | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------------------------------------|-------------------------------------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| TCN(Shaojie Bai, et al.)                 | Alpha158                            | 0.0279±0.00 | 0.2181±0.01 | 0.0421±0.00 | 0.3429±0.01 | 0.0262±0.02       | 0.4133±0.25       | -0.1090±0.03 |\n| TabNet(Sercan O. Arik, et al.)           | Alpha158                            | 0.0204±0.01 | 0.1554±0.07 | 0.0333±0.00 | 0.2552±0.05 | 0.0227±0.04       | 0.3676±0.54       | -0.1089±0.08 |\n| Transformer(Ashish Vaswani, et al.)      | Alpha158                            | 0.0264±0.00 | 0.2053±0.02 | 0.0407±0.00 | 0.3273±0.02 | 0.0273±0.02       | 0.3970±0.26       | -0.1101±0.02 |\n| GRU(Kyunghyun Cho, et al.)               | Alpha158(with selected 20 features) | 0.0315±0.00 | 0.2450±0.04 | 0.0428±0.00 | 0.3440±0.03 | 0.0344±0.02       | 0.5160±0.25       | -0.1017±0.02 |\n| LSTM(Sepp Hochreiter, et al.)            | Alpha158(with selected 20 features) | 0.0318±0.00 | 0.2367±0.04 | 0.0435±0.00 | 0.3389±0.03 | 0.0381±0.03       | 0.5561±0.46       | -0.1207±0.04 |\n| Localformer(Juyong Jiang, et al.)        | Alpha158                            | 0.0356±0.00 | 0.2756±0.03 | 0.0468±0.00 | 0.3784±0.03 | 0.0438±0.02       | 0.6600±0.33       | -0.0952±0.02 |"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1090, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.96970748901367,672
How does the updater determine the time window for the test segment and load the dataset accordingly?,"The time window for the test segment and loading the dataset accordingly in qlib is determined by the `segments` parameter in the `get_dataset` method of the `RMDLoader` class. This parameter is a dictionary that specifies the segments config for the dataset. According to the documentation, the `segments` parameter can be used to specify different time segments for the test data. For example, if you want to load the test data from January 1, 2016 to August 1, 2020, you would set `segments = {","In qlib/workflow/online/update.py’s DSBasedUpdater.prepare_data, the updater:

1. Computes hist_ref (either the user‐supplied value or `dataset.step_len-1` for a TSDatasetH).  
2. Sets  
   • start_time_buffer = get_date_by_shift(last_end, -hist_ref+1)  
   • start_time        = get_date_by_shift(last_end, +1)  
3. Defines the test segment as  
   seg = { ""test"": (start_time, to_date) }  
4. Calls  
   `self.rmdl.get_dataset(start_time=start_time_buffer, end_time=self.to_date, segments=seg, …)`  

This loads the dataset spanning from the historical buffer up through the test window [last_end+1, to_date].","['CLASS qlib.qlib.model.base.Model has method qlib.qlib.model.base.Model.predict\ngive prediction given Dataset  Parameters ---------- dataset : Dataset     dataset will generate the processed dataset from model training.  segment : Text or slice     dataset will use this segment to prepare data. (default=test)  Returns ------- Prediction results with certain type such as `pandas.Series`.\nLearnable Models', 'MODULE qlib.qlib.contrib.data.dataset contains CLASS qlib.qlib.contrib.data.dataset.MTSDatasetH\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon     num_states (int): how many memory states to be added     memory_mode (str): memory mode (daily or sample)     batch_size (int): batch size (<0 will use daily sampling)     n_samples (int): number of samples in the same day     shuffle (bool): whether shuffle data     drop_last (bool): whether drop last batch < batch_size     input_size (int): reshape flatten rows as this input_size (backward compatibility)', ""METHOD qlib.qlib.workflow.online.update.RMDLoader.get_dataset has parameter or accepts argument qlib.qlib.workflow.online.update.RMDLoader.get_dataset.segments\nLoad, config and setup dataset.  This dataset is for inference.  Args:     start_time :         the start_time of underlying data     end_time :         the end_time of underlying data     segments : dict         the segments config for dataset         Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time     unprepared_dataset: Optional[DatasetH]         if user don't want to load dataset from recorder, please specify user's dataset  Returns:     DatasetH: the instance of DatasetH"", 'CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has method qlib.qlib.model.meta.dataset.MetaTaskDataset.prepare_tasks\nPrepare the data in each meta-task and ready for training.  The following code example shows how to retrieve a list of meta-tasks from the `meta_dataset`:      .. code-block:: Python          # get the train segment and the test segment, both of them are lists         train_meta_tasks, test_meta_tasks = meta_dataset.prepare_tasks([""train"", ""test""])  Parameters ---------- segments: Union[List[Text], Tuple[Text], Text]     the info to select data  Returns ------- list:     A list of the prepared data of each meta-task for training the meta-model. For multiple segments [seg1, seg2, ... , segN], the returned list will be [[tasks in seg1], [tasks in seg2], ... , [tasks in segN]].     Each task is a meta task\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', ""CLASS qlib.qlib.workflow.online.update.RMDLoader has method qlib.qlib.workflow.online.update.RMDLoader.get_dataset\nLoad, config and setup dataset.  This dataset is for inference.  Args:     start_time :         the start_time of underlying data     end_time :         the end_time of underlying data     segments : dict         the segments config for dataset         Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time     unprepared_dataset: Optional[DatasetH]         if user don't want to load dataset from recorder, please specify user's dataset  Returns:     DatasetH: the instance of DatasetH\nRecorder Model Dataset Loader"", 'CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'METHOD qlib.qlib.data.dataset.__init__.DatasetH.get_max_time has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.get_max_time.segments', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1096, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",55.262542963027954,673
How does the grouping-aware DataFrame respond when both its group labels are deleted?,"When both group labels are deleted from the grouping-aware DataFrame in qlib, it will throw an error because the group labels are required for the DataFrame to function properly. This can be demonstrated by trying to access a non-existent group label after deleting the group labels, which will result in a KeyError exception.","The `SepDataFrame` simply removes each group key from its internal `_df_dict` via its `__delitem__` method, and when you `del sdf['g1']` and `del sdf['g2']` it doesn’t error out—it just ends up with an empty `_df_dict` (i.e. an empty SepDataFrame).","['CLASS qlib.qlib.model.ens.group.Group has method qlib.qlib.model.ens.group.Group.__call__\nGroup the ungrouped_dict into different groups.  Args:     ungrouped_dict (dict): the ungrouped dict waiting for grouping like {name: things}  Returns:     dict: grouped_dict like {G1: object, G2: object}     n_jobs: how many progress you need.     verbose: the print mode for Parallel.\nGroup the objects based on dict', 'METHOD keras.keras.src.legacy.preprocessing.image.DataFrameIterator.remove_classes has parameter or accepts argument keras.keras.src.legacy.preprocessing.image.DataFrameIterator.remove_classes.labels', 'METHOD qlib.qlib.model.ens.group.Group.__call__ has parameter or accepts argument qlib.qlib.model.ens.group.Group.__call__.ungrouped_dict\nGroup the ungrouped_dict into different groups.  Args:     ungrouped_dict (dict): the ungrouped dict waiting for grouping like {name: things}  Returns:     dict: grouped_dict like {G1: object, G2: object}     n_jobs: how many progress you need.     verbose: the print mode for Parallel.', 'CLASS qlib.qlib.model.ens.group.Group has field or attribute or property qlib.qlib.model.ens.group.ungrouped_dict\nGroup the objects based on dict', 'METHOD qlib.qlib.model.ens.group.Group.__call__ has parameter or accepts argument qlib.qlib.model.ens.group.Group.__call__.verbose\nGroup the ungrouped_dict into different groups.  Args:     ungrouped_dict (dict): the ungrouped dict waiting for grouping like {name: things}  Returns:     dict: grouped_dict like {G1: object, G2: object}     n_jobs: how many progress you need.     verbose: the print mode for Parallel.', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__delitem__\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'METHOD qlib.qlib.model.ens.group.Group.__call__ has parameter or accepts argument qlib.qlib.model.ens.group.Group.__call__.n_jobs\nGroup the ungrouped_dict into different groups.  Args:     ungrouped_dict (dict): the ungrouped dict waiting for grouping like {name: things}  Returns:     dict: grouped_dict like {G1: object, G2: object}     n_jobs: how many progress you need.     verbose: the print mode for Parallel.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Deprecation notices\n* The use of `LabelEncoder` in `XGBClassifier` is now deprecated and will be removed in the next minor release (#6269). The deprecation is necessary to support multiple types of inputs, such as cuDF data frames or cuPy arrays.\n* The use of certain positional arguments in the Python interface is deprecated (#6365). Users will use deprecation warnings for the use of position arguments for certain function parameters. New code should use keyword arguments as much as possible. We have not yet decided when we will fully require the use of keyword arguments.\n\n### Bug-fixes\n* On big-endian arch, swap the byte order in the binary serializer to enable loading models that were produced by a little-endian machine (#5813).\n* [jvm-packages] Fix deterministic partitioning with dataset containing Double.NaN (#5996)\n* Limit tree depth for GPU hist to 31 to prevent integer overflow (#6045)\n* [jvm-packages] Set `maxBins` to 256 to align with the default value in the C++ code (#6066)\n* [R] Fix CRAN check (#6077)\n* Add back support for `scipy.sparse.coo_matrix` (#6162)\n* Handle duplicated values in sketching. (#6178)\n* Catch all standard exceptions in C API. (#6220)\n* Fix linear GPU input (#6255)\n* Fix inplace prediction interval. (#6259)\n* [R] allow `xgb.plot.importance()` calls to fill a grid (#6294)\n* Lazy import dask libraries. (#6309)\n* Deterministic data partitioning for external memory (#6317)\n* Avoid resetting seed for every configuration. (#6349)\n* Fix label errors in graph visualization (#6369)\n* [jvm-packages] fix potential unit test suites aborted issue due to race condition (#6373)\n* [R] Fix warnings from `R check --as-cran` (#6374)\n* [R] Fix a crash that occurs with noLD R (#6378)\n* [R] Do not convert continuous labels to factors (#6380)\n* [R] remove uses of `exists()` (#6387)\n* Propagate parameters to the underlying `Booster` handle from `XGBClassifier.set_param` / `XGBRegressor.set_param`. (#6416)\n* [R] Fix R package installation via CMake (#6423)\n* Enforce row-major order in cuPy array (#6459)\n* Fix filtering callable objects in the parameters passed to the scikit-learn API. (#6466)\n\n### Maintenance: Testing, continuous integration, build system\n* [CI] Improve JVM test in GitHub Actions (#5930)\n* Refactor plotting test so that it can run independently (#6040)\n* [CI] Cancel builds on subsequent pushes (#6011)\n* Fix Dask Pytest fixture (#6024)\n* [CI] Migrate linters to GitHub Actions (#6035)\n* [CI] Remove win2016 JVM test from GitHub Actions (#6042)\n* Fix CMake build with `BUILD_STATIC_LIB` option (#6090)\n* Don't link imported target in CMake (#6093)\n* Work around a compiler bug in MacOS AppleClang 11 (#6103)\n* [CI] Fix CTest by running it in a correct directory (#6104)\n* [R] Check warnings explicitly for model compatibility tests (#6114)\n* [jvm-packages] add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)\n* [CI] Time GPU tests. (#6141)\n* [R] remove warning in configure.ac (#6152)\n* [CI] Upgrade cuDF and RMM to 0.16 nightlies; upgrade to Ubuntu 18.04 (#6157)\n* [CI] Test C API demo (#6159)\n* Option for generating device debug info. (#6168)\n* Update `.gitignore` (#6175, #6193, #6346)\n* Hide C++ symbols from dmlc-core (#6188)\n* [CI] Added arm64 job in Travis-CI (#6200)\n* [CI] Fix Docker build for CUDA 11 (#6202)\n* [CI] Move non-OpenMP gtest to GitHub Actions (#6210)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1103, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",49.680893659591675,674
"When a versioned dataset URL returns a 404, how does the code adjust the filename before downloading?","When a versioned dataset URL returns a 404, qlib adjusts the filename by using the `get_path_after_base` function from the `cognee` library to extract the path after the base URL. This function takes two parameters: the base URL and the path to extract. It returns the extracted path. The adjusted filename is then used for downloading.","In GetData.qlib_data (qlib/tests/data.py), after building the “vX/…_<qlib_version>.zip” name it calls check_dataset(), and if that returns 404 it does:

```python
# instead of using the real qlib.__version__, swap in “latest”
file_name = _get_file_name_with_version(""latest"", dataset_version=version)
```

i.e. it regenerates the path so the suffix becomes `_latest.zip` before calling download_data.","['MODULE cognee.cognee.tasks.web_scraper.web_scraper_task contains FUNCTION cognee.cognee.tasks.web_scraper.web_scraper_task.get_path_after_base\nExtract the path after the base URL.  Args:     base_url: The base URL (e.g., ""https://example.com"").     url: The full URL to extract the path from.  Returns:     str: The path after the base URL, with leading slashes removed.  Raises:     ValueError: If the base URL and target URL are from different domains.', 'MODULE cognee.cognee.infrastructure.files.utils.open_data_file contains GLOBAL_VARIABLE cognee.cognee.infrastructure.files.utils.open_data_file.normalized_url', 'FUNCTION cognee.cognee.tasks.web_scraper.web_scraper_task.get_path_after_base has parameter or accepts argument cognee.cognee.tasks.web_scraper.web_scraper_task.get_path_after_base.url\nExtract the path after the base URL.  Args:     base_url: The base URL (e.g., ""https://example.com"").     url: The full URL to extract the path from.  Returns:     str: The path after the base URL, with leading slashes removed.  Raises:     ValueError: If the base URL and target URL are from different domains.', 'CLASS cognee.cognee.api.v1.datasets.datasets.datasets has field or attribute or property cognee.cognee.api.v1.datasets.datasets.directory_path', 'FUNCTION cognee.cognee.tasks.web_scraper.web_scraper_task.get_path_after_base has parameter or accepts argument cognee.cognee.tasks.web_scraper.web_scraper_task.parsed_url\nExtract the path after the base URL.  Args:     base_url: The base URL (e.g., ""https://example.com"").     url: The full URL to extract the path from.  Returns:     str: The path after the base URL, with leading slashes removed.  Raises:     ValueError: If the base URL and target URL are from different domains.', 'MODULE cognee.cognee.modules.pipelines.layers.resolve_authorized_user_datasets contains GLOBAL_VARIABLE cognee.cognee.modules.pipelines.layers.resolve_authorized_user_datasets.existing_datasets', 'FUNCTION cognee.cognee.tasks.web_scraper.web_scraper_task.get_path_after_base has parameter or accepts argument cognee.cognee.tasks.web_scraper.web_scraper_task.get_path_after_base.base_url\nExtract the path after the base URL.  Args:     base_url: The base URL (e.g., ""https://example.com"").     url: The full URL to extract the path from.  Returns:     str: The path after the base URL, with leading slashes removed.  Raises:     ValueError: If the base URL and target URL are from different domains.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': True, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1109, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",70.71099376678467,675
How does the rolling strategy determine which prediction to keep when there are duplicate datetime entries?,"The rolling strategy in qlib determines which prediction to keep when there are duplicate datetime entries by using the `rec_key` method of the `RollingStrategy` class, which returns a tuple containing the model key and the rolling key. This method takes into account both the model being used (e.g., Linear or LightGBM) and the specific rolling strategy employed (e.g., sliding or expanding), ensuring that duplicate datetime entries are handled correctly and the most up-to-date prediction is retained.","In RollingEnsemble (qlib/model/ens/ensemble.py), after concatenating all prediction DataFrames (sorted by their earliest datetime), it calls

```python
artifact = artifact[~artifact.index.duplicated(keep=""last"")]
```

so for any duplicated datetime index it drops earlier entries and keeps the last one (i.e. the prediction from the most recently‐sorted artifact).","['MODULE qlib.qlib.workflow.online.strategy contains CLASS qlib.qlib.workflow.online.strategy.RollingStrategy\nThis example strategy always uses the latest rolling model sas online models.', 'CLASS cognee.cognee.modules.engine.models.Timestamp.Timestamp inherits from or is a subclass of cognee.infrastructure.engine.DataPoint', 'CLASS qlib.qlib.workflow.online.strategy.RollingStrategy has method qlib.qlib.workflow.online.strategy.RollingStrategy.rec_key\nThis example strategy always uses the latest rolling model sas online models.', 'CLASS cognee.cognee.modules.engine.models.Timestamp.Timestamp has field or attribute or property cognee.cognee.modules.engine.models.Timestamp.Timestamp.second', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.seg\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', 'CLASS qlib.qlib.workflow.task.gen.RollingGen has field or attribute or property qlib.qlib.workflow.task.gen.RollingGen.trunc_days', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.step\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1113, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",52.98009181022644,676
Describe how TrainerR.train orchestrates running tasks in subprocesses to release memory.,"The `TrainerR.train` function in qlib orchestrates running tasks in subprocesses to release memory by utilizing the `call_in_subproc` parameter in the `TrainerR.__init__` method. This allows the process to be called in a subprocess, forcing memory release and preventing memory-related issues. To achieve this, initialize the `TrainerR` object with the `call_in_subproc` parameter set to `True`, as shown in the example below:

```python
def from qlib import TrainerR

trainer = TrainerR(call_in_subproc=True)
```
This approach helps manage memory efficiently and ensures smooth operation of the training process.","In qlib/model/trainer.py’s TrainerR.train, you can enable subprocess‐based execution by setting `call_in_subproc=True` when you construct your TrainerR. Inside the task loop, TrainerR does roughly:

1. if `self._call_in_subproc` is true, it wraps your `train_func` with  
   `train_func = call_in_subproc(train_func, C)`  
   (where `C` is the global config from qlib/config.py)
2. it then calls `train_func(task, …)` as usual.

`call_in_subproc` (in qlib/utils/parallel.py) forks a fresh Python process, re‐imports the config `C`, runs your original `train_func` there, returns its result (the Recorder), and exits the subprocess. By scoping each training run to its own process, all memory is reclaimed by the OS as soon as that child exits.","['METHOD qlib.qlib.model.trainer.TrainerR.__init__ has parameter or accepts argument qlib.qlib.model.trainer.TrainerR.__init__.call_in_subproc\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release', 'METHOD qlib.qlib.model.trainer.TrainerR.__init__ has parameter or accepts argument qlib.qlib.model.trainer.TrainerR.__init__.train_func\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release', 'METHOD qlib.qlib.model.trainer.TrainerR.__init__ has parameter or accepts argument qlib.qlib.model.trainer.TrainerR.__init__.experiment_name\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release', 'METHOD qlib.qlib.model.trainer.TrainerR.__init__ has parameter or accepts argument qlib.qlib.model.trainer.TrainerR.__init__.default_rec_name\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release', 'CLASS qlib.qlib.model.trainer.TrainerR has method qlib.qlib.model.trainer.TrainerR.__init__\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release\nTrainer based on (R)ecorder. It will train a list of tasks and return a list of model recorders in a linear way.  Assumption: models were defined by `task` and the results will be saved to `Recorder`.', 'CLASS cognee.cognee.infrastructure.databases.cache.cache_db_interface.CacheDBInterface has method cognee.cognee.infrastructure.databases.cache.cache_db_interface.CacheDBInterface.release_lock\nRelease the lock if it is held. Must be implemented by subclasses.\nAbstract base class for distributed cache coordination systems (e.g., Redis, Memcached). Provides a common interface for lock acquisition, release, and context-managed locking.', 'MODULE xgboost.ops.script.release_artifacts contains FUNCTION xgboost.ops.script.release_artifacts.release_note\nGenerate a note for GitHub release description.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\n### Saving Progress Models\nIf you want to save model every two round, simply set save_period=2. You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.\n\n#### Continue from Existing Model\nIf you want to continue boosting from existing model, say 0002.model, use\n```\n../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model\n```\nxgboost will load from 0002.model continue boosting for 2 rounds, and save output to continue.model. However, beware that the training and evaluation data specified in mushroom.conf should not change when you use this function.\n#### Use Multi-Threading\nWhen you are working with a large dataset, you may want to take advantage of parallelism. If your compiler supports OpenMP, xgboost is naturally multi-threaded, to set number of parallel running add ```nthread``` parameter to your configuration.\nEg. ```nthread=10```\n\nSet nthread to be the number of your real cpu (On Unix, this can be found using ```lscpu```)\nSome systems will have ```Thread(s) per core = 2```, for example, a 4 core cpu with 8 threads, in such case set ```nthread=4``` and not 8.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 5, 'pr': 1116, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",45.369510650634766,677
What is the overall sequence of steps performed in the history-based PyTorch model's fit method?,"The overall sequence of steps performed in the history-based PyTorch model's fit method in qlib is as follows:

1. The `fit` method is called on the model, which starts the training process.
2. The `on_train_begin` method is called to record the start of the training process.
3. The model begins training, and the `on_epoch_end` method is called at the end of each epoch to record the loss and other metrics.
4. The training process continues until the specified number of epochs has been reached.
5. Once the training is complete, the `on_train_end` method is called to record the end of the training process.","In HIST.fit (qlib/contrib/model/pytorch_hist.py) the method:

1. Calls dataset.prepare(“train”, “valid”, “test”) and checks for empty splits.  
2. Ensures the stock2concept file exists (downloads if not), loads stock_index and maps it onto the train/valid DataFrames.  
3. Extracts x_train/y_train/stock_index_train and x_valid/y_valid/stock_index_valid.  
4. Creates the output folder and initializes early‐stop vars (`best_score`, `stop_steps`, `evals_result`).  
5. Instantiates the base model (LSTM or GRU), loads `self.model_path` if given, then merges its state dict into `self.HIST_model`.  
6. Enters the epoch loop for `self.n_epochs`:  
   a. Calls `train_epoch(...)` on the training set.  
   b. Calls `test_epoch(...)` on train and valid sets, appends scores to `evals_result`.  
   c. Updates `best_param` on validation improvement or increments `stop_steps` and breaks if `stop_steps >= early_stop`.  
7. After training, reloads `best_param` into `HIST_model` and saves it to disk.","[""CLASS keras.keras.src.callbacks.history.History has method keras.keras.src.callbacks.history.History.__init__\nCallback that records events into a `History` object.  This callback is automatically applied to every Keras model. The `History` object gets returned by the `fit()` method of models.  Example:  >>> model = Sequential([layers.Dense(10)]) >>> model.compile(SGD(), loss='mse') >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), ...                     epochs=10, verbose=1) >>> print(history.params) {'verbose': 1, 'epochs': 10, 'steps': 1} >>> # check the keys of history object >>> print(history.history.keys()) dict_keys(['loss'])"", 'METHOD qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.fit.verbose', ""CLASS keras.keras.src.callbacks.history.History has method keras.keras.src.callbacks.history.History.on_epoch_end\nCallback that records events into a `History` object.  This callback is automatically applied to every Keras model. The `History` object gets returned by the `fit()` method of models.  Example:  >>> model = Sequential([layers.Dense(10)]) >>> model.compile(SGD(), loss='mse') >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), ...                     epochs=10, verbose=1) >>> print(history.params) {'verbose': 1, 'epochs': 10, 'steps': 1} >>> # check the keys of history object >>> print(history.history.keys()) dict_keys(['loss'])"", 'METHOD qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.fit.reweighter', ""CLASS keras.keras.src.callbacks.history.History has method keras.keras.src.callbacks.history.History.on_train_begin\nCallback that records events into a `History` object.  This callback is automatically applied to every Keras model. The `History` object gets returned by the `fit()` method of models.  Example:  >>> model = Sequential([layers.Dense(10)]) >>> model.compile(SGD(), loss='mse') >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), ...                     epochs=10, verbose=1) >>> print(history.params) {'verbose': 1, 'epochs': 10, 'steps': 1} >>> # check the keys of history object >>> print(history.history.keys()) dict_keys(['loss'])"", 'METHOD qlib.qlib.contrib.model.pytorch_general_nn.GeneralPTNN.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_general_nn.GeneralPTNN.fit.reweighter', ""MODULE keras.keras.src.callbacks.history contains CLASS keras.keras.src.callbacks.history.History\nCallback that records events into a `History` object.  This callback is automatically applied to every Keras model. The `History` object gets returned by the `fit()` method of models.  Example:  >>> model = Sequential([layers.Dense(10)]) >>> model.compile(SGD(), loss='mse') >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5), ...                     epochs=10, verbose=1) >>> print(history.params) {'verbose': 1, 'epochs': 10, 'steps': 1} >>> # check the keys of history object >>> print(history.history.keys()) dict_keys(['loss'])"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 10, 'n_files_pr': 11, 'pr': 1119, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",44.14963388442993,678
How do the feature dumping script and file storage mixin each handle and report errors during execution?,"The feature dumping script in qlib handles errors during execution by checking for invalid parameters, infinite values in input data, and using try-except blocks to catch exceptions. However, it is unclear how the file storage mixin handles errors. To get more information, consult the qlib documentation or source code directly.

**Error Handling in Feature Dumping Script:*
1. Checks for invalid parameters
2. Detects infinite values in input data
3. Uses try-except blocks to catch exceptions

**File Storage Mixin Error Handling:**
Not clear from provided information, but may be configurable through `OntologyEnvConfig` class fields and attributes.","Feature dumping (scripts/dump_bin.py → DumpDataUpdate._dump_features):

• Tasks are submitted to a ProcessPoolExecutor and collected as futures.  
• In the as_completed loop, each future.result() is wrapped in try/except.  
• On exception, it captures traceback.format_exc(), maps it to the instrument code in an error_code dict, and continues.  
• At the end it logs all errors via logger.info(f""dump bin errors: {error_code}"").

File storage mixin (qlib/data/storage/file_storage.py → FileStorageMixin):

• The uri property checks if the requested freq is supported; if not, it immediately raises ValueError(f""{storage_name}: … does not contain data for {freq}"").  
• The check() method calls uri.exists(), and if False, raises ValueError(f""{storage_name} not exists: {uri}"").  
• All errors are surfaced as ValueError with clear, self-descriptive messages.","['MODULE cognee.cognee.modules.ontology.ontology_env_config contains CLASS cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig\nRepresents the configuration for ontology handling, including parameters for ontology file storage and resolution/matching strategies.  Public methods: - to_dict  Instance variables: - ontology_resolver - ontology_matching - ontology_file_path - model_config', 'CLASS qlib.scripts.dump_pit.DumpPitData has field or attribute or property qlib.scripts.dump_pit.qlib_dir', 'CLASS cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig has field or attribute or property cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig.ontology_resolver\nRepresents the configuration for ontology handling, including parameters for ontology file storage and resolution/matching strategies.  Public methods: - to_dict  Instance variables: - ontology_resolver - ontology_matching - ontology_file_path - model_config', 'CLASS qlib.scripts.dump_bin.DumpDataAll inherits from or is a subclass of qlib.scripts.dump_bin.DumpDataBase', 'CLASS cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig has field or attribute or property cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig.ontology_file_path\nRepresents the configuration for ontology handling, including parameters for ontology file storage and resolution/matching strategies.  Public methods: - to_dict  Instance variables: - ontology_resolver - ontology_matching - ontology_file_path - model_config', 'CLASS qlib.scripts.dump_bin.DumpDataFix inherits from or is a subclass of qlib.scripts.dump_bin.DumpDataAll', 'CLASS cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig has field or attribute or property cognee.cognee.modules.ontology.ontology_env_config.OntologyEnvConfig.matching_strategy\nRepresents the configuration for ontology handling, including parameters for ontology file storage and resolution/matching strategies.  Public methods: - to_dict  Instance variables: - ontology_resolver - ontology_matching - ontology_file_path - model_config', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Notable bug fixes\n\nSome noteworthy bug fixes that are not related to specific language bindings are listed in this section.\n\n- Some language environments use a different thread to perform garbage collection, which breaks the thread-local cache used in XGBoost. XGBoost 2.0 implements a new thread-safe cache using a light weight lock to replace the thread-local cache. (#8851)\n- Fix model IO by clearing the prediction cache. (#8904)\n- `inf` is checked during data construction. (#8911)\n- Preserve order of saved updaters configuration. Usually, this is not an issue unless the `updater` parameter is used instead of the `tree_method` parameter (#9355)\n- Fix GPU memory allocation issue with categorical splits. (#9529)\n- Handle escape sequence like `\\t\\n` in feature names for JSON model dump. (#9474)\n- Normalize file path for model IO and text input. This handles short paths on Windows and paths that contain `~` on Unix (#9463). In addition, all path inputs are required to be encoded in UTF-8 (#9448, #9443)\n- Fix integer overflow on H100. (#9380)\n- Fix weighted sketching on GPU with categorical features. (#9341)\n- Fix metric serialization. The bug might cause some of the metrics to be dropped during evaluation. (#9405)\n- Fixes compilation errors on MSVC x86 targets (#8823)\n- Pick up the dmlc-core fix for the CSV parser. (#8897)\n\n\n### Documentation\nAside from documents for new features, we have many smaller updates to improve user experience, from troubleshooting guides to typo fixes.\n\n- Explain CPU/GPU interop. (#8450)\n- Guide to troubleshoot NCCL errors. (#8943, #9206)\n- Add a note for rabit port selection. (#8879)\n- How to build the docs using conda (#9276)\n- Explain how to obtain reproducible results on distributed systems. (#8903)\n\n* Fixes and small updates to document and demonstration scripts. (#8626, #8436, #8995, #8907, #8923, #8926, #9358, #9232, #9201, #9469, #9462, #9458, #8543, #8597, #8401, #8784, #9213, #9098, #9008, #9223, #9333, #9434, #9435, #9415, #8773, #8752, #9291, #9549)\n\n### Python package\n* New Features and Improvements\n- Support primitive types of pyarrow-backed pandas dataframe. (#8653)\n- Warning messages emitted by XGBoost are now emitted using Python warnings. (#9387)\n- User can now format the value printed near the bars on the `plot_importance` plot (#8540)\n- XGBoost has improved half-type support (float16) with pandas, cupy, and cuDF. With GPU input, the handling is through CUDA `__half` type, and no data copy is made. (#8487, #9207, #8481)\n- Support `Series` and Python primitive types in `inplace_predict` and `QuantileDMatrix` (#8547, #8542)\n- Support all pandas' nullable integer types. (#8480)\n- Custom metric with the scikit-learn interface now supports `sample_weight`. (#8706)\n- Enable Installation of Python Package with System lib in a Virtual Environment (#9349)\n- Raise if expected workers are not alive in `xgboost.dask.train` (#9421)\n\n* Optimization\n- Cache transformed data in `QuantileDMatrix` for efficiency. (#8666, #9445)\n- Take datatable as row-major input. (#8472)\n- Remove unnecessary conversions between data structures (#8546)\n\n* Adopt modern Python packaging conventions (PEP 517, PEP 518, PEP 621)\n-  XGBoost adopted the modern Python packaging conventions. The old setup script `setup.py` is now replaced with the new configuration file `pyproject.toml`. Along with this, XGBoost now supports Python 3.11. (#9021, #9112, #9114, #9115) Consult the latest documentation for the updated instructions to build and install XGBoost."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance\nThis is a summary of maintenance work that is not specific to any language binding.\n\n* Add CMake option to use /MD runtime (#7277)\n* Add clang-format configuration. (#7383)\n* Code cleanups (#7539, #7536, #7466, #7499, #7533, #7735, #7722, #7668, #7304, #7293,\n  #7321, #7356, #7345, #7387, #7577, #7548, #7469, #7680, #7433, #7398)\n* Improved tests with better coverage and latest dependency (#7573, #7446, #7650, #7520,\n  #7373, #7723, #7611, #7771)\n* Improved automation of the release process. (#7278, #7332, #7470)\n* Compiler workarounds (#7673)\n* Change shebang used in CLI demo. (#7389)\n* Update affiliation (#7289)\n\n### CI\nSome fixes and update to XGBoost's CI infrastructure. (#7739, #7701, #7382, #7662, #7646,\n#7582, #7407, #7417, #7475, #7474, #7479, #7472, #7626)\n\n\n## v1.5.0 (2021 Oct 11)\n\nThis release comes with many exciting new features and optimizations, along with some bug\nfixes.  We will describe the experimental categorical data support and the external memory\ninterface independently. Package-specific new features will be listed in respective\nsections.\n\n### Development on categorical data support\nIn version 1.3, XGBoost introduced an experimental feature for handling categorical data\nnatively, without one-hot encoding. XGBoost can fit categorical splits in decision\ntrees. (Currently, the generated splits will be of form `x \\in {v}`, where the input is\ncompared to a single category value. A future version of XGBoost will generate splits that\ncompare the input against a list of multiple category values.)\n\nMost of the other features, including prediction, SHAP value computation, feature\nimportance, and model plotting were revised to natively handle categorical splits.  Also,\nall Python interfaces including native interface with and without quantized `DMatrix`,\nscikit-learn interface, and Dask interface now accept categorical data with a wide range\nof data structures support including numpy/cupy array and cuDF/pandas/modin dataframe.  In\npractice, the following are required for enabling categorical data support during\ntraining:\n\n  - Use Python package.\n  - Use `gpu_hist` to train the model.\n  - Use JSON model file format for saving the model.\n\nOnce the model is trained, it can be used with most of the features that are available on\nthe Python package.  For a quick introduction, see\nhttps://xgboost.readthedocs.io/en/latest/tutorials/categorical.html\n\nRelated PRs: (#7011, #7001, #7042, #7041, #7047, #7043, #7036, #7054, #7053, #7065, #7213, #7228, #7220, #7221, #7231, #7306)\n\n* Next steps\n\n\t- Revise the CPU training algorithm to handle categorical data natively and generate categorical splits\n\t- Extend the CPU and GPU algorithms to generate categorical splits of form `x \\in S`\n\twhere the input is compared with multiple category values.  split. (#7081)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 6, 'pr': 1123, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",54.512550830841064,679
What mechanism does the FiniteVectorEnv use to stop the collector when all data from the shared queue is exhausted?,"The FiniteVectorEnv uses the `StopIteration` exception to stop the collector when all data from the shared queue is exhausted. This exception is raised by the vector env when it also exhausts its environments. The collector should catch and ignore this exception, as well as notify the loggers that the collect is ready/done.","FiniteVectorEnv keeps track of “alive” workers in its `_alive_env_ids`, and in `reset()` (qlib/rl/utils/finite_env.py) when that set becomes empty it sets `_zombie=True` and raises a `StopIteration`. The `collector_guard()` around the collector then catches that exception and cleanly stops collection.","['CLASS qlib.qlib.rl.utils.finite_env.FiniteDummyVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteVectorEnv has method qlib.qlib.rl.utils.finite_env.FiniteVectorEnv.collector_guard\nGuard the collector. Recommended to guard every collect.  This guard is for two purposes.  1. Catch and ignore the StopIteration exception, which is the stopping signal    thrown by FiniteEnv to let tianshou know that ``collector.collect()`` should exit. 2. Notify the loggers that the collect is ready / done what it\'s ready / done.  Examples -------- >>> with finite_env.collector_guard(): ...     collector.collect(n_episode=INF)\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteSubprocVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteShmemVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteVectorEnv inherits from or is a subclass of tianshou.env.BaseVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteVectorEnv has method qlib.qlib.rl.utils.finite_env.FiniteVectorEnv.__init__\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteVectorEnv has method qlib.qlib.rl.utils.finite_env.FiniteVectorEnv.reset\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 17}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 17, 'n_files_pr': 14, 'pr': 1125, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.298799991607666,680
"Which helper functions are used by the run method to retrieve configuration files, strip out seed parameters, and create the conda environment before model execution?","The helper functions used by the run method to retrieve configuration files, strip out seed parameters, and create the conda environment before model execution in qlib are:

1. `gen_yaml_file_without_seed_kwargs` function: This function generates a YAML file without seed kwargs.
2. `from_cli_and_env` method of the `GraphitiLLMConfig` class: This method creates an LLM configuration from CLI arguments and environment variables.
3. `__init__` method of the `EnvWrapper` class: This method initializes the EnvWrapper object with a seed iterator.","The `run` method uses three helpers:

• get_all_files (examples/run_all_model.py) – to fetch the .yaml and .txt paths for a given dataset  
• create_env (same module) – to spin up a temporary conda environment  
• gen_yaml_file_without_seed_kwargs (same module) – to strip out seed parameters from the YAML before execution","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.from_cli_and_env\nCreate LLM configuration from CLI arguments, falling back to environment variables.\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'FUNCTION qlib.examples.run_all_model.gen_yaml_file_without_seed_kwargs has parameter or accepts argument qlib.examples.run_all_model.gen_yaml_file_without_seed_kwargs.temp_dir', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.from_env\nCreate LLM configuration from environment variables.\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'METHOD qlib.qlib.rl.utils.env_wrapper.EnvWrapper.__init__ has parameter or accepts argument qlib.qlib.rl.utils.env_wrapper.EnvWrapper.__init__.seed_iterator', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'FUNCTION qlib.examples.run_all_model.gen_yaml_file_without_seed_kwargs has parameter or accepts argument qlib.examples.run_all_model.gen_yaml_file_without_seed_kwargs.yaml_path', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiEmbedderConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiEmbedderConfig.from_env\nCreate embedder configuration from environment variables.\nConfiguration for the embedder client.  Centralizes all embedding-related configuration parameters.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 5, 'pr': 1126, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.13098740577698,681
"In the Windows auto-mount branch, which subprocess return code triggers logging that the share is already mounted?",The subprocess return code that triggers logging that the share is already mounted in the Windows auto-mount branch of qlib is 0.,"In `_mount_nfs_uri` (qlib/__init__.py)’s Windows branch, a `CalledProcessError` with `returncode == 85` triggers the “already mounted” warning.","[""METHOD qlib.qlib.contrib.data.loader.Alpha158DL.get_feature_config has parameter or accepts argument qlib.qlib.contrib.data.loader.Alpha158DL.get_feature_config.config\ncreate factors from config  config = {     'kbar': {}, # whether to use some hard-code kbar features     'price': { # whether to use raw price features         'windows': [0, 1, 2, 3, 4], # use price at n days ago         'feature': ['OPEN', 'HIGH', 'LOW'] # which price field to use     },     'volume': { # whether to use raw volume features         'windows': [0, 1, 2, 3, 4], # use volume at n days ago     },     'rolling': { # whether to use rolling operator based features         'windows': [5, 10, 20, 30, 60], # rolling windows size         'include': ['ROC', 'MA', 'STD'], # rolling operator to use         #if include is None we will use default operators         'exclude': ['RANK'], # rolling operator not to use     } }"", 'MODULE qlib.qlib.__init__ contains GLOBAL_VARIABLE qlib.qlib.__init__.mount_command', ""CLASS qlib.qlib.contrib.data.loader.Alpha158DL has method qlib.qlib.contrib.data.loader.Alpha158DL.get_feature_config\ncreate factors from config  config = {     'kbar': {}, # whether to use some hard-code kbar features     'price': { # whether to use raw price features         'windows': [0, 1, 2, 3, 4], # use price at n days ago         'feature': ['OPEN', 'HIGH', 'LOW'] # which price field to use     },     'volume': { # whether to use raw volume features         'windows': [0, 1, 2, 3, 4], # use volume at n days ago     },     'rolling': { # whether to use rolling operator based features         'windows': [5, 10, 20, 30, 60], # rolling windows size         'include': ['ROC', 'MA', 'STD'], # rolling operator to use         #if include is None we will use default operators         'exclude': ['RANK'], # rolling operator not to use     } }\nDataloader to get Alpha158"", 'MODULE qlib.qlib.__init__ contains GLOBAL_VARIABLE qlib.qlib.__init__.mount_path', 'MODULE xgboost.ops.script.run_clang_tidy contains FUNCTION xgboost.ops.script.run_clang_tidy.call\nSubprocess run wrapper.', 'MODULE cognee.working_dir_error_replication.run_subprocess_test contains FUNCTION cognee.working_dir_error_replication.run_subprocess_test.main', 'FUNCTION keras.keras.src.ops.core.switch has parameter or accepts argument keras.keras.src.ops.core.switch.branches\nApply exactly one of the `branches` given by `index`.  If `index` is out of bounds, it is clamped to within bounds.  The semantics of `switch` are given roughly by this Python implementation:  ```python def switch(index, branches, *operands):     index = clamp(0, index, len(branches) - 1)     return branches[index](*operands) ```  Args:     index: An integer scalar indicating which branch function to apply.     branches: A sequence of functions to be applied based on `index`.     operands: Inputs to whichever branch is applied.  Returns:     The outputs of `branch(*operands)` for the branch that was selected     based on `index`.  Examples:  >>> add_fn = lambda x, y: x + y >>> subtract_fn = lambda x, y: x - y >>> x = keras.ops.array(2.0) >>> y = keras.ops.array(0.5) >>> branches = [add_fn, subtract_fn] >>> keras.ops.switch(0, branches, x, y) 2.5  >>> keras.ops.switch(1, branches, x, y) 1.5', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What\'s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=""https://github.com/microsoft/RD-Agent""><img src=""docs/_static/img/rdagent_logo.png"" alt=""RD_Agent"" style=""height: 2em""></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!\n\nTo learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (中文) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- 📃**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- 👾**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1129, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.51209545135498,682
"What type of copy is made for a segmented view, and how are the original data, labels, and indices preserved?","When creating a segmented view in qlib, a shallow copy of the original data is made to preserve the original data while allowing for modifications to be made to the copied data. This is done using the `deepcopy_basic_type` function from qlib's utils/data module, which creates a deep copy of an object without copying complicated objects. However, it's worth noting that this function cannot handle recursive objects. The `Account` class in qlib's backtest/account module has methods for updating orders and resetting reports, but these are not directly related to creating a segmented view. Further investigation is needed to clarify the specific method used and how indices are handled.","In MTSDatasetH._prepare_seg (qlib/contrib/data/dataset.py) a Python shallow copy is made (via copy.copy(self)). After that, the original payloads (_data, _label, _index, _memory, _zeros) are re-assigned by reference onto the new object to keep the underlying arrays intact, while only the batch and daily slice indices (_batch_slices, _daily_slices, _daily_index) are updated for the segment.","['CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.init_vars\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', ""MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.deepcopy_basic_type\ndeepcopy an object without copy the complicated objects.     This is useful when you want to generate Qlib tasks and share the handler  NOTE: - This function can't handle recursive objects!!!!!  Parameters ---------- obj : object     the object to be copied  Returns ------- object:     The copied object"", 'CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.update_order\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'METHOD qlib.qlib.utils.objm.FileManager.save_obj has parameter or accepts argument qlib.qlib.utils.objm.FileManager.save_obj.obj', 'CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.reset_report\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'CLASS qlib.qlib.data.storage.storage.FeatureStorage has method qlib.qlib.data.storage.storage.FeatureStorage.rewrite\noverwrite all data in FeatureStorage with data  Parameters ---------- data: Union[List, np.ndarray, Tuple]     data index: int     data start index', 'CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.get_hist_positions\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Sandwich/README.md\n# Sandwich\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py](https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\nmake use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.version==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1135, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",43.906275033950806,683
What is the sequence of high-level operations performed by the end-to-end pipeline tests from model training through backtesting to experiment URI management?,"The sequence of high-level operations performed by the end-to-end pipeline tests from model training through backtesting to experiment URI management is as follows:

1. Start an experiment using `QlibRecorder.start_exp()`
2. Perform model training and testing
3. Run backtesting using the trained model
4. End the experiment using `ExpManager.end_exp()`. This sequence ensures a comprehensive evaluation of the model's performance from training to deployment.","The “all‐in‐one” pipeline tests in tests/test_all_pipeline.py (and its contrib variant) execute in three high-level steps:

1. test_0_train → calls train(uri)  
   – fits your model, logs predictions & IC/RIC to an MLflow run, returns (pred_scores, ic_ric, run_id, uri)

2. test_1_backtest → calls backtest_analysis(pred_scores, run_id, uri)  
   – loads those predictions, runs the backtest, and checks performance metrics

3. test_2_expmanager → calls fake_experiment() (uses the Experiment URI manager)  
   – verifies both the default URI and the “current” URI resolution, then tears down the MLflow folder","[""CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.start_exp\nLower level method for starting an experiment. When use this method, one should end the experiment manually and the status of the recorder may not be handled properly. Here is the example code:  .. code-block:: Python      R.start_exp(experiment_name='test', recorder_name='recorder_1')     ... # further operations     R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)   Parameters ---------- experiment_id : str     id of the experiment one wants to start. experiment_name : str     the name of the experiment to be started recorder_id : str     id of the recorder under the experiment one wants to start. recorder_name : str     name of the recorder under the experiment one wants to start. uri : str     the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.     The default uri are set in the qlib.config. resume : bool     whether to resume the specific recorder with given name under the given experiment.  Returns ------- An experiment instance being started.\nA global system that helps to manage the experiments."", 'CLASS keras.keras.src.layers.preprocessing.pipeline_test.PipelineTest has method keras.keras.src.layers.preprocessing.pipeline_test.PipelineTest.test_correctness', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.end_exp\nEnd an active experiment.  Maintaining `_active_exp_uri` is included in end_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_name : str     name of the active experiment. recorder_status : str     the status of the active recorder of the experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'MODULE qlib.qlib.backtest.utils contains CLASS qlib.qlib.backtest.utils.LevelInfrastructure\nlevel infrastructure is created by executor, and then shared to strategies on the same level', 'CLASS keras.guides.making_new_layers_and_models_via_subclassing.VariationalAutoEncoder inherits from or is a subclass of keras.keras.Model\nCombines the encoder and decoder into an end-to-end model for training.', 'CLASS keras.keras.src.layers.preprocessing.pipeline_test.PipelineTest has method keras.keras.src.layers.preprocessing.pipeline_test.PipelineTest.test_from_config', 'MODULE keras.guides.making_new_layers_and_models_via_subclassing contains CLASS keras.guides.making_new_layers_and_models_via_subclassing.VariationalAutoEncoder\nCombines the encoder and decoder into an end-to-end model for training.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Updated winning solution under readme.md (#7862)\n- New security policy. (#8360)\n- GPU document is overhauled as we consider CUDA support to be feature-complete. (#8378)\n\n### Maintenance\n* Code refactoring and cleanups. (#7850, #7826, #7910, #8332, #8204)\n* Reduce compiler warnings. (#7768, #7916, #8046, #8059, #7974, #8031, #8022)\n* Compiler workarounds. (#8211, #8314, #8226, #8093)\n* Dependencies update. (#8001, #7876, #7973, #8298, #7816)\n* Remove warnings emitted in previous versions. (#7815)\n* Small fixes occurred during development. (#8008)\n\n### CI and Tests\n* We overhauled the CI infrastructure to reduce the CI cost and lift the maintenance burdens. Jenkins is replaced with buildkite for better automation, with which, finer control of test runs is implemented to reduce overall cost. Also, we refactored some of the existing tests to reduce their runtime, drooped the size of docker images, and removed multi-GPU C++ tests. Lastly, `pytest-timeout` is added as an optional dependency for running Python tests to keep the test time in check. (#7772, #8291, #8286, #8276, #8306, #8287, #8243, #8313, #8235, #8288, #8303, #8142, #8092, #8333, #8312, #8348)\n* New documents for how to reproduce the CI environment (#7971, #8297)\n* Improved automation for JVM release. (#7882)\n* GitHub Action security-related updates. (#8263, #8267, #8360)\n* Other fixes and maintenance work. (#8154, #7848, #8069, #7943)\n* Small updates and fixes to GitHub action pipelines. (#8364, #8321, #8241, #7950, #8011)\n\n## v1.6.1 (2022 May 9)\nThis is a patch release for bug fixes and Spark barrier mode support. The R package is unchanged.\n\n### Experimental support for categorical data\n- Fix segfault when the number of samples is smaller than the number of categories. (https://github.com/dmlc/xgboost/pull/7853)\n- Enable partition-based split for all model types. (https://github.com/dmlc/xgboost/pull/7857)\n\n### JVM packages\nWe replaced the old parallelism tracker with spark barrier mode to improve the robustness of the JVM package and fix the GPU training pipeline.\n- Fix GPU training pipeline quantile synchronization. (#7823, #7834)\n- Use barrier model in spark package. (https://github.com/dmlc/xgboost/pull/7836, https://github.com/dmlc/xgboost/pull/7840, https://github.com/dmlc/xgboost/pull/7845, https://github.com/dmlc/xgboost/pull/7846)\n- Fix shared object loading on some platforms. (https://github.com/dmlc/xgboost/pull/7844)\n\n## v1.6.0 (2022 Apr 16)\n\nAfter a long period of development, XGBoost v1.6.0 is packed with many new features and\nimprovements. We summarize them in the following sections starting with an introduction to\nsome major new features, then moving on to language binding specific changes including new\nfeatures and notable bug fixes for that binding.\n\n### Development of categorical data support\nThis version of XGBoost features new improvements and full coverage of experimental\ncategorical data support in Python and C package with tree model.  Both `hist`, `approx`\nand `gpu_hist` now support training with categorical data.  Also, partition-based\ncategorical split is introduced in this release. This split type is first available in\nLightGBM in the context of gradient boosting. The previous XGBoost release supported one-hot split where the splitting criteria is of form `x \\in {c}`, i.e. the categorical feature `x` is tested against a single candidate. The new release allows for more expressive conditions: `x \\in S` where the categorical feature `x` is tested against multiple candidates. Moreover, it is now possible to use any tree algorithms (`hist`, `approx`, `gpu_hist`) when creating categorical splits. For more', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 11, 'pr': 1141, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.5306441783905,684
Which git commands does the MLflow recorder run to capture and log uncommitted code changes?,"The MLflow recorder in qlib uses the following git commands to capture and log uncommitted code changes:
* `git status`
* `git diff --staged`
These commands are used to get the list of files that have been modified or added since the last commit, and then to get a diff of those files. The output from these commands is then logged by the MLflow recorder.","In qlib/workflow/recorder.py, MLflowRecorder._log_uncommitted_code runs:

• git diff  
• git status  
• git diff --cached  

and logs their outputs to code_diff.txt, code_status.txt and code_cached.txt.","['CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.get_local_dir\nThis function will return the directory path of this recorder.\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.delete_tags\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.MLflowRecorder.end_time\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.set_tags\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.MLflowRecorder.name\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.list_tags\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.__repr__\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n### Create a Branch\n\nCreate a new branch for your work:\n```shell\ngit checkout -b feature/your-feature-name\n```\n\n## 3. 🎯 Making Changes\n\n1. **Code Style**: Follow the project\'s coding standards\n2. **Documentation**: Update relevant documentation\n3. **Tests**: Add tests for new features\n4. **Commits**: Write clear commit messages\n\n### Running Tests\n```shell\npython cognee/cognee/tests/test_library.py\n```\n\n## 4. 📤 Submitting Changes\n\n1. Install ruff on your system\n2. Run ```ruff format .``` and ``` ruff check ``` and fix the issues\n3. Push your changes:\n```shell\ngit add .\ngit commit -s -m ""Description of your changes""\ngit push origin feature/your-feature-name\n```\n\n2. Create a Pull Request:\n   - Go to the [**cognee** repository](https://github.com/topoteretes/cognee) or [cognee community repository](https://github.com/topoteretes/cognee-community)\n   - Click ""Compare & Pull Request"" and open a PR against dev branch\n   - Fill in the PR template with details about your changes\n\n## 5. 📜 Developer Certificate of Origin (DCO)\n\nAll contributions must be signed-off to indicate agreement with our DCO:\n\n```shell\ngit config alias.cos ""commit -s""  # Create alias for signed commits\n```\n\nWhen your PR is ready, please include:\n> ""I affirm that all code in every commit of this pull request conforms to the terms of the Topoteretes Developer Certificate of Origin""\n\n## 6. 🤝 Community Guidelines\n\n- Be respectful and inclusive\n- Help others learn and grow\n- Follow our [Code of Conduct](CODE_OF_CONDUCT.md)\n- Provide constructive feedback\n- Ask questions when unsure\n\n## 7. 📫 Getting Help\n\n- Open an [issue](https://github.com/topoteretes/cognee/issues)\n- Join our Discord community\n- Check existing documentation\n\nThank you for contributing to **cognee**! 🌟']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 2, 'pr': 1167, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",40.94264888763428,685
What flags does the PIT data fetch use to preserve existing files and skip re-downloading?,"The PIT data fetch in qlib uses the following flags to preserve existing files and skip re-downloading:

* `--normalize_dir`: result directory
* `--max_workers`: number of concurrent, by default *1*
* `--interval`: `1d` or `1min`, by default `1d`
* `--region`: `CN` or `US` or `IN`, by default `CN`
* `--date_field_name`: column *name* identifying time in csv files, by default `date`
* `--symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`
* `--end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`","In `tests/test_pit.py::TestPIT.setUpClass`, the calls to `GetData().qlib_data` use

• delete_old=False  
• exists_skip=True

to preserve existing files and skip re-downloading.","['METHOD xgboost.ops.script.run_clang_tidy.ClangTidy.convert_nvcc_command_to_clang has parameter or accepts argument xgboost.ops.script.run_clang_tidy.ClangTidy.convert_nvcc_command_to_clang.command\nConvert nvcc flags to corresponding clang flags.', 'MODULE cognee.cognee.tasks.ingestion.ingest_data contains GLOBAL_VARIABLE cognee.cognee.tasks.ingestion.ingest_data.original_file_metadata', 'CLASS xgboost.ops.script.run_clang_tidy.ClangTidy has method xgboost.ops.script.run_clang_tidy.ClangTidy.convert_nvcc_command_to_clang\nConvert nvcc flags to corresponding clang flags.\nclang tidy wrapper. Args:   args:  Command line arguments.       cpp_lint: Run linter on C++ source code.       cuda_lint: Run linter on CUDA source code.       use_dmlc_gtest: Whether to use gtest bundled in dmlc-core.', 'MODULE cognee.cognee.infrastructure.files.utils.get_file_content_hash contains GLOBAL_VARIABLE cognee.cognee.infrastructure.files.utils.get_file_content_hash.chunk', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.fetch.data_key\nfetch data from underlying data source  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index. level : Union[str, int]     which index level to select the data. col_set : str     select a set of meaningful columns.(e.g. features, columns). data_key : str     the data to fetch:  DK_*. proc_func: Callable     please refer to the doc of DataHandler.fetch  Returns ------- pd.DataFrame:', 'FUNCTION xgboost.doc.conf.try_fetch_r_doc has parameter or accepts argument xgboost.doc.conf.TMP_DIR', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.fetch.selector\nfetch data from underlying data source  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index. level : Union[str, int]     which index level to select the data. col_set : str     select a set of meaningful columns.(e.g. features, columns). data_key : str     the data to fetch:  DK_*. proc_func: Callable     please refer to the doc of DataHandler.fetch  Returns ------- pd.DataFrame:', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1171, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",47.68412923812866,686
"When auto mounting on Unix-like systems, how many levels of parent paths does the function check for an existing mount, and which variable defines this limit?","The number of levels of parent paths checked for an existing mount in qlib is 3. This limit is defined by the variable `mount_path` in the class `DataPathManager`. You can check this limit and its implementation using the following code snippet:

```python
default from qlib.config import DataPathManager
# Get the current mount path
current_mount_path = DataPathManager.mount_path
# Check if the parent directory exists for up to 3 levels
for i in range(3):
    parent_dir = os.path.dirname(current_mount_path)
    if not os.path.exists(parent_dir):
        print(f""Parent directory does not exist at level {i+1}"")
        break
    current_mount_path = parent_dir
```
This code will check the parent directories of the current mount path for up to 3 levels and print a message if any of them do not exist.","In qlib/__init__.py’s `_mount_nfs_uri` (Unix branch), it will climb at most 2 parent‐path levels when checking for an existing mount. That limit is set by the `_check_level_num` variable (initialized to 2).","['CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'METHOD qlib.qlib.config.DataPathManager.__init__ has parameter or accepts argument qlib.qlib.config.DataPathManager.__init__.mount_path\nThe relation of `provider_uri` and `mount_path` - `mount_path` is used only if provider_uri is an NFS path - otherwise, provider_uri will be used for accessing data', 'MODULE keras.keras.src.backend.config contains FUNCTION keras.keras.src.backend.config.set_max_epochs\nLimit the maximum number of epochs for any call to fit.  This will cap the number of epochs for any training run using `model.fit()`. This is purely for debugging, and can also be set via the `KERAS_MAX_EPOCHS` environment variable to quickly run a script without modifying its source.  Args:     max_epochs: The integer limit on the number of epochs or `None`. If         `None`, no limit is applied.', 'MODULE qlib.qlib.__init__ contains GLOBAL_VARIABLE qlib.qlib.__init__.mount_path', 'FUNCTION keras.keras.src.backend.config.set_max_epochs has parameter or accepts argument keras.keras.src.backend.config.set_max_epochs.max_epochs\nLimit the maximum number of epochs for any call to fit.  This will cap the number of epochs for any training run using `model.fit()`. This is purely for debugging, and can also be set via the `KERAS_MAX_EPOCHS` environment variable to quickly run a script without modifying its source.  Args:     max_epochs: The integer limit on the number of epochs or `None`. If         `None`, no limit is applied.', 'CLASS qlib.qlib.config.DataPathManager has field or attribute or property qlib.qlib.config.mount_path\nMotivation: - get the right path (e.g. data uri) for accessing data based on given information(e.g. provider_uri, mount_path and frequency) - some helper functions to process uri.', 'MODULE keras.keras.src.backend.config contains FUNCTION keras.keras.src.backend.config.set_max_steps_per_epoch\nLimit the maximum number of steps for any call to fit/evaluate/predict.  This will cap the number of steps for single epoch of a call to `fit()`, `evaluate()`, or `predict()`. This is purely for debugging, and can also be set via the `KERAS_MAX_STEPS_PER_EPOCH` environment variable to quickly run a scrip without modifying its source.  Args:     max_epochs: The integer limit on the number of epochs or `None`. If         `None`, no limit is applied.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1177, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",49.734907150268555,687
Which call in the setup is responsible for normalizing the downloaded quarterly pit source data?,"The function responsible for normalizing the downloaded quarterly pit source data in qlib is `normalize_data` in the `collector.py` file. This function takes several parameters, including the source directory, normalize directory, region, interval, and date field name. To use this function, you can run a command like `python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized --region CN --interval quarterly`. This will normalize the downloaded data in the specified source directory and save the normalized data to the specified normalize directory.","The normalization is done by the call to Run(...).normalize_data(), i.e.:

```python
Run(
    source_dir=pit_dir,
    normalize_dir=pit_normalized_dir,
    interval=""quarterly"",
).normalize_data()
```","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'METHOD qlib.scripts.data_collector.pit.collector.PitNormalize.normalize has parameter or accepts argument qlib.scripts.data_collector.pit.collector.PitNormalize.normalize.df', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.quarterly\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize inherits from or is a subclass of data_collector.base.BaseNormalize', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.first\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize has method qlib.scripts.data_collector.pit.collector.PitNormalize.normalize', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.last\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1178, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",42.872859954833984,688
"What is the overall process for training the ensemble, including how sample weights and features change across sub-model iterations?","The overall process for training the ensemble in qlib is as follows:

1. The Double Ensemble model uses learning trajectory based sample reweighting and shuffling based feature selection to solve both the low signal-to-noise ratio and increasing number of features problems.

2. The model identifies key samples based on the training dynamics on each sample and elicits key features based on the ablation impact of each feature via shuffling.

3. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.

**Sample Weights Change Across Sub-Model Iterations:**
* The SR module uses the loss curve for the previous sub-model and the loss values of the current ensemble on each sample to compute the weights for all samples.
* The weights are computed using the formula: `weights = sample_reweight(loss_curve, loss_values, k_th)`, where `k_th` is the index of the current sub-model.

**Features Change Across Sub-Model Iterations:**
* The model uses shuffling based feature selection to elicit key features based on the ablation impact of each feature.
* The features are selected based on their ablation impact, which is computed using the formula: `ablation_impact = predict_sub(features)`, where `features` is the set of features for the current sub-model.","The training loop is implemented in DEnsembleModel.fit (qlib/contrib/model/double_ensemble.py):

1. Initialization  
   • weights ← all ones (size N)  
   • features ← all columns of X_train  
   • pred_sub ← zeros(N×K)  

2. For each sub‐model k=1…num_models:  
   a. Train sub‐model:  
      model_k = train_submodel(df_train, df_valid, weights, features)  
      append model_k to self.ensemble and current features to self.sub_features  
   b. If k == num_models, break  
   c. Compute:  
      – loss_curve = retrieve_loss_curve(model_k, df_train, features)  
      – pred_k = predict_sub(model_k, df_train, features) → update pred_sub[:,k]  
      – ensemble prediction = weighted sum of pred_sub[:,0…k] / sum(sub_weights[0…k])  
      – loss_values = get_loss(y_train, ensemble prediction)  
   d. Sample re‐weighting (if enable_sr):  
      weights = sample_reweight(loss_curve, loss_values, k)  
      (ranks losses, computes h‐values, bins into bins_sr, applies decay)  
   e. Feature selection (if enable_fs):  
      features = feature_selection(df_train, loss_values)  
      (uses bins_fs and sample_ratios to drop less informative features)  

At the end you have an ensemble of K sub‐models trained on dynamically re‐weighted samples and shrinking feature sets. The final predict() method then aggregates sub‐model outputs using sub_weights.","['CLASS qlib.qlib.contrib.model.double_ensemble.DEnsembleModel has method qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight\nthe SR module of Double Ensemble :param loss_curve: the shape is NxT the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample after the t-th iteration in the training of the previous sub-model. :param loss_values: the shape is N the loss of the current ensemble on the i-th sample. :param k_th: the index of the current sub-model, starting from 1 :return: weights the weights for all the samples.\nDouble Ensemble Model', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel.features', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight.k_th\nthe SR module of Double Ensemble :param loss_curve: the shape is NxT the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample after the t-th iteration in the training of the previous sub-model. :param loss_values: the shape is N the loss of the current ensemble on the i-th sample. :param k_th: the index of the current sub-model, starting from 1 :return: weights the weights for all the samples.', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel.weights', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight.loss_curve\nthe SR module of Double Ensemble :param loss_curve: the shape is NxT the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample after the t-th iteration in the training of the previous sub-model. :param loss_values: the shape is N the loss of the current ensemble on the i-th sample. :param k_th: the index of the current sub-model, starting from 1 :return: weights the weights for all the samples.', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.predict_sub has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.predict_sub.features', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.sample_reweight.loss_values\nthe SR module of Double Ensemble :param loss_curve: the shape is NxT the loss curve for the previous sub-model, where the element (i, t) if the error on the i-th sample after the t-th iteration in the training of the previous sub-model. :param loss_values: the shape is N the loss of the current ensemble on the i-th sample. :param k_th: the index of the current sub-model, starting from 1 :return: weights the weights for all the samples.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf).', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### GPUTreeSHAP: GPU acceleration of the TreeSHAP algorithm (#6038, #6064, #6087, #6099, #6163, #6281, #6332)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) is a game theoretic approach to explain predictions of machine learning models. It computes feature importance scores for individual examples, establishing how each feature influences a particular prediction. TreeSHAP is an optimized SHAP algorithm specifically designed for decision tree ensembles.\n* Starting with 1.3.0 release, it is now possible to leverage CUDA-capable GPUs to accelerate the TreeSHAP algorithm. Check out [the demo notebook](https://github.com/dmlc/xgboost/blob/master/demo/gpu_acceleration/shap.ipynb).\n* The CUDA implementation of the TreeSHAP algorithm is hosted at [rapidsai/GPUTreeSHAP](https://github.com/rapidsai/gputreeshap). XGBoost imports it as a Git submodule.\n\n### New style Python callback API (#6199, #6270, #6320, #6348, #6376, #6399, #6441)\n* The XGBoost Python package now offers a re-designed callback API. The new callback API lets you design various extensions of training in idomatic Python. In addition, the new callback API allows you to use early stopping with the native Dask API (`xgboost.dask`). Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.3.0/python/callbacks.html) and [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/callbacks.py).\n\n### Enable the use of `DeviceQuantileDMatrix` / `DaskDeviceQuantileDMatrix` with large data (#6201, #6229, #6234).\n* `DeviceQuantileDMatrix` can achieve memory saving by avoiding extra copies of the training data, and the saving is bigger for large data. Unfortunately, large data with more than 2^31 elements was triggering integer overflow bugs in CUB and Thrust. Tracking issue: #6228.\n* This release contains a series of work-arounds to allow the use of `DeviceQuantileDMatrix` with large data:\n  - Loop over `copy_if` (#6201)\n  - Loop over `thrust::reduce` (#6229)\n  - Implement the inclusive scan algorithm in-house, to handle large offsets (#6234)\n\n### Support slicing of tree models (#6302)\n* Accessing the best iteration of a model after the application of early stopping used to be error-prone, need to manually pass the `ntree_limit` argument to the `predict()` function.\n* Now we provide a simple interface to slice tree models by specifying a range of boosting rounds. The tree ensemble can be split into multiple sub-ensembles via the slicing interface. Check out [an example](https://xgboost.readthedocs.io/en/release_1.3.0/python/model.html).\n* In addition, the early stopping callback now supports `save_best` option. When enabled, XGBoost will save (persist) the model at the best boosting round and discard the trees that were fit subsequent to the best round.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 4, 'pr': 1205, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",60.48728537559509,689
What impact does setting the forbid-all-trade-at-limit flag to true have on trades when a stock hits its daily price limit?,"When the `forbid_all_trade_at_limit` flag is set to `True`, trades are not allowed when a stock hits its daily price limit. This means that if the price of a stock reaches its highest or lowest price for the day, no trades can be made until the next trading day. In other words, setting this flag to `True` prevents buying at the limit up price and selling at the limit down price, even though these actions are allowed in reality.","In TopkDropoutStrategy (qlib/contrib/strategy/signal_strategy.py), turning on forbid_all_trade_at_limit=True means that whenever a stock hits its daily limit‐up or limit‐down price, the strategy will skip all orders for that stock (no buys at limit‐down and no sells at limit‐up), even though such trades would be allowed in reality.","['METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.n_drop\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.check_stock_limit\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.forbid_all_trade_at_limit\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.topk\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.hold_thresh\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.method_sell\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.backtest.exchange.Exchange.check_stock_limit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.check_stock_limit.start_time\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1209, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",41.70321607589722,690
What new market data fields are added in the order book handler's feature configuration that are not present in the standard high frequency backtest handler?,"The new market data fields added in the order book handler's feature configuration that are not present in the standard high frequency backtest handler in qlib are:

* `dealt_order_amount`
* `buy_price`

These fields are available in the `Exchange` class of the `qlib.backtest.exchange` module.","In HighFreqBacktestOrderHandler.get_feature_config (qlib/contrib/data/highfreq_handler.py), the following fields are added on top of the standard close0, vwap0, volume0, factor0:

• bid0  (​$bid​)  
• bidV0 (​$bidV​)  
• ask0  (​$ask​)  
• askV0 (​$askV​)  
• median0 (​($bid+$ask)/2​)  
• downlimitmarket0 (​$downlimitmarket​)  
• uplimitmarket0 (​$uplimitmarket​)  
• highmarket0 (​$highmarket​)  
• lowmarket0 (​$lowmarket​)","['CLASS qlib.qlib.data.dataset.handler.DataHandler has method qlib.qlib.data.dataset.handler.DataHandler.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.dealt_order_amount', 'CLASS qlib.qlib.data.dataset.handler.DataHandler has field or attribute or property qlib.qlib.data.dataset.handler.DataHandler.instruments\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.Exchange.buy_price', 'CLASS qlib.qlib.data.dataset.handler.DataHandler has field or attribute or property qlib.qlib.data.dataset.handler.data_loader\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'CLASS qlib.qlib.backtest.utils.TradeCalendarManager has field or attribute or property qlib.qlib.backtest.utils.freq\nManager for trading calendar     - BaseStrategy and BaseExecutor will use it', 'CLASS qlib.qlib.data.dataset.handler.DataHandler has field or attribute or property qlib.qlib.data.dataset.handler.DataHandler.data_loader\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1212, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.81541395187378,691
How are non-string elements handled by remove_fields_space?,"The `remove_fields_space` function in qlib is used to remove spaces from fields, but it does not explicitly handle non-string elements. If you need to clean data of different types, consider using other functions in the qlib library that are designed for handling numerical or categorical data. For example, you can use the `clean_data` function to remove missing values and outliers from a dataset. Here is an example of how you might use these functions together:

```python
cleaned_fields = remove_fields_space(fields)
# Assuming fields is a list of strings

import qlib
from qlib.data import clean_data

# Create a sample dataset with missing values and outliers
sample_dataset = {'field1': [1, 2, None, 4], 'field2': ['a', 'b', 'c', 'd']}

cleaned_dataset = clean_data(sample_dataset)
cleaned_fields = remove_fields_space(list(cleaned_dataset.keys()))
print(cleaned_fields)  # Output: ['field1', 'field2']
```
This code first uses the `remove_fields_space` function to clean the field names, and then uses the `clean_data` function to remove missing values and outliers from the dataset.

**References:**
* qlib documentation for `remove_fields_space` function
* qlib documentation for `clean_data` function","In qlib/utils/__init__.py (remove_fields_space), if an element isn’t a str it isn’t “space‐cleaned” but simply wrapped in str(i) in the list comprehension.","['FUNCTION qlib.qlib.utils.__init__.remove_fields_space has parameter or accepts argument qlib.qlib.utils.__init__.remove_fields_space.fields\nremove fields space  :param fields: features fields :return: list or str', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.nonzero\nReturn the indices of the elements that are non-zero.  Args:     x: Input tensor.  Returns:     Indices of elements that are non-zero.', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.remove_fields_space\nremove fields space  :param fields: features fields :return: list or str', 'FUNCTION keras.keras.src.ops.numpy.nonzero has parameter or accepts argument keras.keras.src.ops.numpy.nonzero.x\nReturn the indices of the elements that are non-zero.  Args:     x: Input tensor.  Returns:     Indices of elements that are non-zero.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.roll\nRoll tensor elements along a given axis.  Elements that roll beyond the last position are re-introduced at the first.  Args:     x: Input tensor.     shift: The number of places by which elements are shifted.     axis: The axis along which elements are shifted. By default, the         array is flattened before shifting, after which the original         shape is restored.  Returns:     Output tensor.', 'CLASS qlib.scripts.data_collector.base.Normalize has field or attribute or property qlib.scripts.data_collector.base.symbol_field_name', 'FUNCTION keras.keras.src.ops.numpy.roll has parameter or accepts argument keras.keras.src.ops.numpy.roll.axis\nRoll tensor elements along a given axis.  Elements that roll beyond the last position are re-introduced at the first.  Args:     x: Input tensor.     shift: The number of places by which elements are shifted.     axis: The axis along which elements are shifted. By default, the         array is flattened before shifting, after which the original         shape is restored.  Returns:     Output tensor.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1213, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",55.55166935920715,692
"In the fallback implementation of the rolling rank, by what constant is the percentile score divided to scale it between 0 and 1?","The constant used to scale the percentile score between 0 and 1 in the fallback implementation of the rolling rank in qlib is 100. This is a common scaling factor for percentile scores, which are typically values between 0 and 1.","In the fallback `Rank._load_internal` (in qlib/data/ops.py), the result of `percentileofscore(...)` is divided by 100 to scale it into [0, 1].","['CLASS qlib.qlib.data.ops.Rank inherits from or is a subclass of qlib.qlib.data.ops.Rolling\nRolling Operator The meaning of rolling and expanding is the same in pandas. When the window is set to 0, the behaviour of the operator should follow `expanding` Otherwise, it follows `rolling`  Parameters ---------- feature : Expression     feature instance N : int     rolling window size func : str     rolling method  Returns ---------- Expression     rolling outputs\nRolling Rank (Percentile)  Parameters ---------- feature : Expression     feature instance N : int     rolling window size  Returns ---------- Expression     a feature instance with rolling rank', 'CLASS qlib.qlib.data.ops.Rank has method qlib.qlib.data.ops.Rank.__init__\nRolling Rank (Percentile)  Parameters ---------- feature : Expression     feature instance N : int     rolling window size  Returns ---------- Expression     a feature instance with rolling rank', 'CLASS keras.keras.src.metrics.f_score_metrics.FBetaScore has method keras.keras.src.metrics.f_score_metrics.FBetaScore.get_config\nReturns the serializable config of the metric.\nComputes F-Beta score.  Formula:  ```python b2 = beta ** 2 f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall) ``` This is the weighted harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed across per-class results         in the multi-class case.         Acceptable values are `None`, `""micro""`, `""macro""` and         `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     beta: Determines the weight of given to recall         in the harmonic mean between precision and recall (see pseudocode         equation above). Defaults to `1`.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-Beta Score: float.  Example:  >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() >>> result [0.3846154 , 0.90909094, 0.8333334 ]', 'CLASS qlib.qlib.data.ops.Rank has method qlib.qlib.data.ops.Rank.rank\nRolling Rank (Percentile)  Parameters ---------- feature : Expression     feature instance N : int     rolling window size  Returns ---------- Expression     a feature instance with rolling rank', 'CLASS keras.keras.src.metrics.f_score_metrics.FBetaScore inherits from or is a subclass of keras.src.metrics.metric.Metric\nComputes F-Beta score.  Formula:  ```python b2 = beta ** 2 f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall) ``` This is the weighted harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed across per-class results         in the multi-class case.         Acceptable values are `None`, `""micro""`, `""macro""` and         `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     beta: Determines the weight of given to recall         in the harmonic mean between precision and recall (see pseudocode         equation above). Defaults to `1`.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-Beta Score: float.  Example:  >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() >>> result [0.3846154 , 0.90909094, 0.8333334 ]', 'MODULE qlib.qlib.data.ops contains CLASS qlib.qlib.data.ops.Rank\nRolling Rank (Percentile)  Parameters ---------- feature : Expression     feature instance N : int     rolling window size  Returns ---------- Expression     a feature instance with rolling rank', 'CLASS keras.keras.src.metrics.f_score_metrics.F1Score inherits from or is a subclass of keras.keras.src.metrics.f_score_metrics.FBetaScore\nComputes F-Beta score.  Formula:  ```python b2 = beta ** 2 f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall) ``` This is the weighted harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed across per-class results         in the multi-class case.         Acceptable values are `None`, `""micro""`, `""macro""` and         `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     beta: Determines the weight of given to recall         in the harmonic mean between precision and recall (see pseudocode         equation above). Defaults to `1`.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-Beta Score: float.  Example:  >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() >>> result [0.3846154 , 0.90909094, 0.8333334 ]\nComputes F-1 Score.  Formula:  ```python f1_score = 2 * (precision * recall) / (precision + recall) ``` This is the harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed on data.         Acceptable values are `None`, `""micro""`, `""macro""`         and `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-1 Score: float.  Example:  >>> metric = keras.metrics.F1Score(threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() array([0.5      , 0.8      , 0.6666667], dtype=float32)', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### L1 and Quantile regression now supports learning rate\nBoth objectives use adaptive trees due to the lack of proper Hessian values. In the new version, XGBoost can scale the leaf value with the learning rate accordingly. (#8866)\n\n### Export cut value\n\nUsing the Python or the C package, users can export the quantile values (not to be confused with quantile regression) used for the `hist` tree method. (#9356)\n\n### column-based split and federated learning\nWe made progress on column-based split for federated learning. In 2.0, both `approx`, `hist`, and `hist` with vector leaf can work with column-based data split, along with support for vertical federated learning. Work on GPU support is still on-going, stay tuned. (#8576, #8468, #8442, #8847, #8811, #8985, #8623, #8568, #8828, #8932, #9081, #9102, #9103, #9124, #9120, #9367, #9370, #9343, #9171, #9346, #9270, #9244, #8494, #8434, #8742, #8804, #8710, #8676, #9020, #9002, #9058, #9037, #9018, #9295, #9006, #9300, #8765, #9365, #9060)\n\n### PySpark\nAfter the initial introduction of the PySpark interface, it has gained some new features and optimizations in 2.0.\n\n- GPU-based prediction. (#9292, #9542)\n- Optimization for data initialization by avoiding the stack operation. (#9088)\n- Support predict feature contribution. (#8633)\n- Python typing support. (#9156, #9172, #9079, #8375)\n- `use_gpu` is deprecated. The `device` parameter is preferred.\n- Update eval_metric validation to support list of strings (#8826)\n- Improved logs for training (#9449)\n- Maintenance, including refactoring and document updates (#8324, #8465, #8605, #9202, #9460, #9302, #8385, #8630, #8525, #8496)\n- Fix for GPU setup. (#9495)\n\n### Other General New Features\nHere's a list of new features that don't have their own section and yet are general to all language bindings.\n\n- Use array interface for CSC matrix. This helps XGBoost to use a consistent number of threads and align the interface of the CSC matrix with other interfaces. In addition, memory usage is likely to decrease with CSC input thanks to on-the-fly type conversion. (#8672)\n- CUDA compute 90 is now part of the default build.. (#9397)\n\n### Other General Optimization\nThese optimizations are general to all language bindings. For language-specific optimization, please visit the corresponding sections.\n\n- Performance for input with `array_interface` on CPU (like `numpy`) is significantly improved. (#9090)\n- Some optimization with CUDA for data initialization. (#9199, #9209, #9144)\n- Use the latest thrust policy to prevent synchronizing GPU devices. (#9212)\n- XGBoost now uses a per-thread CUDA stream, which prevents synchronization with other streams. (#9416, #9396, #9413)\n\n### Notable breaking change\n\nOther than the aforementioned change with the `device` parameter, here's a list of breaking changes affecting all packages.\n\n- Users must specify the format for text input (#9077). However, we suggest using third-party data structures such as `numpy.ndarray` instead of relying on text inputs. See https://github.com/dmlc/xgboost/issues/9472 for more info."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 1, 'pr': 1228, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.109716176986694,693
What sequence of calls happens when logging an artifact with no active recorder?,"When logging an artifact with no active recorder in qlib, the sequence of calls is as follows:

1. The user tries to log an artifact using the `log_artifact` method of the `Recorder` class.
2. Since there is no active recorder, the system creates a default experiment and a new recorder.
3. The `log_artifact` method is called on the newly created recorder with the local path and artifact path as arguments.

Here's an example code snippet that demonstrates this sequence of calls:
```python
from qlib.workflow import Recorder

# Create a new recorder for the current run
recorder = Recorder.get_recorder()

# Log an artifact using the new recorder
recorder.log_artifact(local_path='path/to/local/file', artifact_path='path/to/artifact/directory')
```
Note that this is just one possible sequence of calls, and the actual implementation may vary depending on the specific requirements and configuration of the qlib system.","When you call QlibRecorder.log_artifact with no active recorder, the following happens:

1. In qlib/workflow/__init__.py: QlibRecorder.log_artifact calls  
   → self.get_exp(start=True)  
2. In qlib/workflow/exp.py: Experiment.get_recorder(create=True, start=True) sees no active recorder, so it  
   → calls _get_or_create_rec to make a new MLflowRecorder  
   → because start=True, sets it as active and invokes MLflowRecorder.start_run()  
3. In qlib/workflow/recorder.py: MLflowRecorder.start_run sets up the MLflow run (mlflow.set_tracking_uri, mlflow.start_run, async logging, params/tags, etc.)  
4. Back in QlibRecorder.log_artifact it finally calls  
   → MLflowRecorder.log_artifact(local_path, artifact_path)  
   → which delegates to MlflowClient.log_artifact under the hood.","['CLASS qlib.qlib.workflow.recorder.Recorder has method qlib.qlib.workflow.recorder.Recorder.log_artifact\nLog a local file or directory as an artifact of the currently active run.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.log_artifact.artifact_path\nLog a local file or directory as an artifact of the currently active run  - If `active recorder` exists: it will set tags through the active recorder. - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.', 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact.artifact_path', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.log_artifact.local_path\nLog a local file or directory as an artifact of the currently active run  - If `active recorder` exists: it will set tags through the active recorder. - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.log_artifact\nLog a local file or directory as an artifact of the currently active run  - If `active recorder` exists: it will set tags through the active recorder. - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.\nA global system that helps to manage the experiments.', ""CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.get_recorder\nRetrieve a Recorder for user. When user specify recorder id and name, the method will try to return the specific recorder. When user does not provide recorder id or name, the method will try to return the current active recorder. The `create` argument determines whether the method will automatically create a new recorder according to user's specification if the recorder hasn't been created before.  * If `create` is True:      * If `active recorder` exists:          * no id or name specified, return the active recorder.         * if id or name is specified, return the specified recorder. If no such exp found, create a new recorder with given id or name. If `start` is set to be True, the recorder is set to be active.      * If `active recorder` not exists:          * no id or name specified, create a new recorder.         * if id or name is specified, return the specified experiment. If no such exp found, create a new recorder with given id or name. If `start` is set to be True, the recorder is set to be active.  * Else If `create` is False:      * If `active recorder` exists:          * no id or name specified, return the active recorder.         * if id or name is specified, return the specified recorder. If no such exp found, raise Error.      * If `active recorder` not exists:          * no id or name specified, raise Error.         * if id or name is specified, return the specified recorder. If no such exp found, raise Error.  Parameters ---------- recorder_id : str     the id of the recorder to be deleted. recorder_name : str     the name of the recorder to be deleted. create : boolean     create the recorder if it hasn't been created before. start : boolean     start the new recorder if one is **created**.  Returns ------- A recorder object.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)"", 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact.local_path', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### XGBoost4J-Spark: Redesigning checkpointing mechanism\n* RFC is available at #4786\n* Clean up checkpoint file after a successful training job (#4754): The current implementation in XGBoost4J-Spark does not clean up the checkpoint file after a successful training job. If the user runs another job with the same checkpointing directory, she will get a wrong model because the second job will re-use the checkpoint file left over from the first job. To prevent this scenario, we propose to always clean up the checkpoint file after every successful training job.\n* Avoid Multiple Jobs for Checkpointing (#5082): The current method for checkpoint is to collect the booster produced at the last iteration of each checkpoint internal to Driver and persist it in HDFS. The major issue with this approach is that it needs to re-perform the data preparation for training if the user did not choose to cache the training dataset. To avoid re-performing data prep, we build external-memory checkpointing in the XGBoost4J layer as well.\n* Enable deterministic repartitioning when checkpoint is enabled (#4807): Distributed algorithm for gradient boosting assumes a fixed partition of the training data between multiple iterations. In previous versions, there was no guarantee that data partition would stay the same, especially when a worker goes down and some data had to recovered from previous checkpoint. In this release, we make data partition deterministic by using the data hash value of each data row in computing the partition.\n\n### XGBoost4J-Spark: handle errors thrown by the native code (#4560)\n* All core logic of XGBoost is written in C++, so XGBoost4J-Spark internally uses the C++ code via Java Native Interface (JNI). #4560 adds a proper error handling for any errors or exceptions arising from the C++ code, so that the XGBoost Spark application can be torn down in an orderly fashion.\n\n### XGBoost4J-Spark: Refine method to count the number of alive cores  (#4858)\n* The `SparkParallelismTracker` class ensures that sufficient number of executor cores are alive. To that end, it is important to query the number of alive cores reliably.\n\n### XGBoost4J: Add `BigDenseMatrix` to store more than `Integer.MAX_VALUE` elements (#4383)\n\n### Robust model serialization with JSON (#4632, #4708, #4739, #4868, #4936, #4945, #4974, #5086, #5087, #5089, #5091, #5094, #5110, #5111, #5112, #5120, #5137, #5218, #5222, #5236, #5245, #5248, #5281)\n* In this release, we introduce an experimental support of using [JSON](https://www.json.org/json-en.html) for serializing (saving/loading) XGBoost models and related hyperparameters for training. We would like to eventually replace the old binary format with JSON, since it is an open format and parsers are available in many programming languages and platforms. See [the documentation for model I/O using JSON](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/saving_model.html). #3980 explains why JSON was chosen over other alternatives.\n* To maximize interoperability and compatibility of the serialized models, we now split serialization into two parts (#4855):\n  1. Model, e.g. decision trees and strictly related metadata like `num_features`.\n  2. Internal configuration, consisting of training parameters and other configurable parameters. For example, `max_delta_step`, `tree_method`, `objective`, `predictor`, `gpu_id`.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 11, 'n_files_pr': 5, 'pr': 1248, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",49.06030225753784,694
Which exchange-provided metric is fetched to determine the rounding unit for buy quantities?,"The exchange-provided metric used to determine the rounding unit for buy quantities in qlib is the 'factor' parameter in the Exchange.round_amount_by_trade_unit method, which allows for precise control over the rounding of trade amounts.",The strategy uses the exchange’s “factor” (fetched via trade_exchange.get_factor(...)) to determine the rounding unit when calling round_amount_by_trade_unit.,"['CLASS qlib.qlib.backtest.decision.Order has method qlib.qlib.backtest.decision.Order.date\nDate of the order.\nstock_id : str amount : float start_time : pd.Timestamp     closed start time for order trading end_time : pd.Timestamp     closed end time for order trading direction : int     Order.SELL for sell; Order.BUY for buy factor : float         presents the weight factor assigned in Exchange()', 'METHOD qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit.end_time\nParameter Please refer to the docs of get_amount_of_trade_unit deal_amount : float, adjusted amount factor : float, adjusted factor return : float, real amount', 'CLASS qlib.qlib.backtest.decision.Order has method qlib.qlib.backtest.decision.Order.amount_delta\nreturn the delta of amount. - Positive value indicates buying `amount` of share - Negative value indicates selling `amount` of share\nstock_id : str amount : float start_time : pd.Timestamp     closed start time for order trading end_time : pd.Timestamp     closed end time for order trading direction : int     Order.SELL for sell; Order.BUY for buy factor : float         presents the weight factor assigned in Exchange()', 'METHOD qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit.start_time\nParameter Please refer to the docs of get_amount_of_trade_unit deal_amount : float, adjusted amount factor : float, adjusted factor return : float, real amount', 'CLASS qlib.qlib.backtest.decision.Order has method qlib.qlib.backtest.decision.Order.deal_amount_delta\nreturn the delta of deal_amount. - Positive value indicates buying `deal_amount` of share - Negative value indicates selling `deal_amount` of share\nstock_id : str amount : float start_time : pd.Timestamp     closed start time for order trading end_time : pd.Timestamp     closed end time for order trading direction : int     Order.SELL for sell; Order.BUY for buy factor : float         presents the weight factor assigned in Exchange()', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit\nParameter Please refer to the docs of get_amount_of_trade_unit deal_amount : float, adjusted amount factor : float, adjusted factor return : float, real amount', 'CLASS qlib.qlib.backtest.decision.Order has method qlib.qlib.backtest.decision.Order.sign\nreturn the sign of trading - `+1` indicates buying - `-1` value indicates selling\nstock_id : str amount : float start_time : pd.Timestamp     closed start time for order trading end_time : pd.Timestamp     closed end time for order trading direction : int     Order.SELL for sell; Order.BUY for buy factor : float         presents the weight factor assigned in Exchange()', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n# Benchmarks Performance\nThis page lists a batch of methods designed for alpha seeking. Each method tries to give scores/predictions for all stocks each day(e.g. forecasting the future excess return of stocks). The scores/predictions of the models will be used as the mined alpha. Investing in stocks with higher scores is expected to yield more profit.  \n\nThe alpha is evaluated in two ways.\n1. The correlation between the alpha and future return.\n1. Constructing portfolio based on the alpha and evaluating the final total return.\n   - The explanation of metrics can be found [here](https://qlib.readthedocs.io/en/latest/component/report.html#id4)\n\nHere are the results of each benchmark model running on Qlib's `Alpha360` and `Alpha158` dataset with China's A shared-stock & CSI300 data respectively. The values of each metric are the mean and std calculated based on 20 runs with different random seeds.\n\nThe numbers shown below demonstrate the performance of the entire `workflow` of each model. We will update the `workflow` as well as models in the near future for better results.\n<!-- \n> If you need to reproduce the results below, please use the **v1** dataset: `python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --version v1`\n>\n> In the new version of qlib, the default dataset is **v2**. Since the data is collected from the YahooFinance API (which is not very stable), the results of *v2* and *v1* may differ -->\n\n> NOTE:\n> The backtest start from 0.8.0 is quite different from previous version. Please check out the changelog for the difference.\n\n> NOTE:\n> We have very limited resources to implement and finetune the models. We tried our best effort to fairly compare these models.  But some models may have greater potential than what it looks like in the table below.  Your contribution is highly welcomed to explore their potential.\n\n## Results on CSI300\n\n### Alpha158 dataset\n\n| Model Name                               | Dataset                             | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------------------------------------|-------------------------------------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| TCN(Shaojie Bai, et al.)                 | Alpha158                            | 0.0279±0.00 | 0.2181±0.01 | 0.0421±0.00 | 0.3429±0.01 | 0.0262±0.02       | 0.4133±0.25       | -0.1090±0.03 |\n| TabNet(Sercan O. Arik, et al.)           | Alpha158                            | 0.0204±0.01 | 0.1554±0.07 | 0.0333±0.00 | 0.2552±0.05 | 0.0227±0.04       | 0.3676±0.54       | -0.1089±0.08 |\n| Transformer(Ashish Vaswani, et al.)      | Alpha158                            | 0.0264±0.00 | 0.2053±0.02 | 0.0407±0.00 | 0.3273±0.02 | 0.0273±0.02       | 0.3970±0.26       | -0.1101±0.02 |\n| GRU(Kyunghyun Cho, et al.)               | Alpha158(with selected 20 features) | 0.0315±0.00 | 0.2450±0.04 | 0.0428±0.00 | 0.3440±0.03 | 0.0344±0.02       | 0.5160±0.25       | -0.1017±0.02 |\n| LSTM(Sepp Hochreiter, et al.)            | Alpha158(with selected 20 features) | 0.0318±0.00 | 0.2367±0.04 | 0.0435±0.00 | 0.3389±0.03 | 0.0381±0.03       | 0.5561±0.46       | -0.1207±0.04 |\n| Localformer(Juyong Jiang, et al.)        | Alpha158                            | 0.0356±0.00 | 0.2756±0.03 | 0.0468±0.00 | 0.3784±0.03 | 0.0438±0.02       | 0.6600±0.33       | -0.0952±0.02 |"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1251, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.15620565414429,695
In what cases does the collector treat a dict response as invalid and log a warning instead of returning data?,"The collector treats a dict response as invalid and logs a warning instead of returning data in qlib when the response is not in the expected format, which can be due to server or network issues, or if the response from the server does not match the expected structure. This behavior is observed in the `ClientProvider` class of the `qlib.data.data` module.","In YahooCollector.get_data_from_remote (scripts/data_collector/yahoo/collector.py), if `Ticker.history` returns a dict, it only “succeeds” when

• `_resp[symbol]` is a dict  
• and `_resp[symbol][""indicators""][""quote""]` exists and is not None  

If instead  
1) `_resp[symbol]` is a string, or  
2) `_resp[symbol][""indicators""][""quote""]` is missing/None  

then it calls `_show_logging_func()` (logs a warning) rather than returning the data.","['CLASS keras.keras.src.utils.file_utils_test.FilterSafePathsTest has method keras.keras.src.utils.file_utils_test.FilterSafePathsTest.test_invalid_path_warning\nTest warning for an invalid path during archive extraction.', 'MODULE cognee.cognee.shared.logging_utils contains GLOBAL_VARIABLE cognee.cognee.shared.logging_utils.WARNING', 'MODULE keras.keras.src.utils.io_utils contains FUNCTION keras.keras.src.utils.io_utils.set_logging_verbosity\nSets the verbosity level for logging.  Supported log levels are as follows:  - `""FATAL""` (least verbose) - `""ERROR""` - `""WARNING""` - `""INFO""` - `""DEBUG""` (most verbose)  Args:     level: A string corresponding to the level of verbosity for logging.', 'MODULE cognee.cognee.api.client contains GLOBAL_VARIABLE cognee.cognee.api.client.logger', 'FUNCTION keras.keras.src.utils.io_utils.set_logging_verbosity has parameter or accepts argument keras.keras.src.utils.io_utils.set_logging_verbosity.level\nSets the verbosity level for logging.  Supported log levels are as follows:  - `""FATAL""` (least verbose) - `""ERROR""` - `""WARNING""` - `""INFO""` - `""DEBUG""` (most verbose)  Args:     level: A string corresponding to the level of verbosity for logging.', 'MODULE cognee.cognee.shared.logging_utils contains GLOBAL_VARIABLE cognee.cognee.shared.logging_utils.ERROR', ""CLASS qlib.qlib.data.data.ClientProvider has method qlib.qlib.data.data.ClientProvider.__init__\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1257, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.3670449256897,696
"How are the trading account, market exchange, strategy, and executor components assembled when starting a backtest?","The trading account, market exchange, strategy, and executor components are assembled when starting a backtest in qlib through the `backtest` function in the `qlib.backtest` module. The function takes parameters such as the initial cash or Position object for the trading account, keyword arguments for initializing the Exchange, and strings, dictionaries, objects, or Path objects for the strategy and executor. These components are initialized using the provided parameters and then used to perform the backtest.

Here is an example of how to use the `backtest` function:

``
from qlib import backtest

# Define the trading account
account = 1000000.0

# Define the market exchange
exchange_kwargs = {}

# Define the strategy
strategy = ""my_strategy""

# Define the executor
executor = ""my_executor""

# Perform the backtest
backtest(backtest.start_time, backtest.end_time, strategy, executor, benchmark, account, exchange_kwargs)
``

Note that this is a simplified example and you may need to modify it based on your specific use case.","When you call get_strategy_executor (in qlib/backtest/__init__.py):

1. create_account_instance(...) builds the Account (cash + positions).  
2. get_exchange(...) builds the Exchange with your start/end times.  
3. A CommonInfrastructure is created from the Account + Exchange.  
4. init_instance_by_config(...) instantiates your strategy (BaseStrategy) and executor (BaseExecutor).  
5. Both strategy and executor call reset_common_infra(common_infra) to hook into the Account/Exchange.  
6. Finally, get_strategy_executor returns (trade_strategy, trade_executor) ready for backtest.","[""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.account\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.pos_type\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", ""MODULE qlib.qlib.backtest.__init__ contains FUNCTION qlib.qlib.backtest.__init__.backtest\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.executor\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.strategy\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.exchange_kwargs\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.benchmark\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 29}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 29, 'n_files_pr': 19, 'pr': 1263, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",62.71414303779602,697
What mechanism excludes the aggregated account value field from individual instrument entries before building the DataFrame?,"The aggregated account value field is excluded from individual instrument entries before building the DataFrame by specifying the correct columns in the `load_portfolio_metrics` method of the `PortfolioMetrics` class.

This can be achieved by carefully selecting the columns to include in the DataFrame, ensuring that the aggregated account value field is not included. The exact columns to include will depend on the specific requirements of your analysis and the structure of your data.","In parse_position (qlib/contrib/report/analysis_position/parse_position.py), before building the DataFrame it explicitly drops the aggregated account value by doing:

```python
for _item in [""now_account_value""]:
    _value.pop(_item, None)
```

i.e. it uses dict.pop on “now_account_value” to exclude it from the per-instrument entries.","[""CLASS qlib.qlib.backtest.report.PortfolioMetrics has method qlib.qlib.backtest.report.PortfolioMetrics.load_portfolio_metrics\nload pm from a file should have format like columns = ['account', 'return', 'total_turnover', 'turnover', 'cost', 'total_cost', 'value', 'cash', 'bench']     :param         path: str/ pathlib.Path()\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'CLASS qlib.scripts.data_collector.fund.collector.FundNormalize has field or attribute or property qlib.scripts.data_collector.fund.collector.date_field_name', 'CLASS qlib.qlib.data.data.LocalInstrumentProvider has field or attribute or property qlib.qlib.data.data.end_time\nLocal instrument data provider class  Provide instrument data from local data source.', 'CLASS qlib.scripts.data_collector.fund.collector.FundNormalize has field or attribute or property qlib.scripts.data_collector.fund.collector.calendar_list', 'CLASS qlib.qlib.data.data.LocalInstrumentProvider has field or attribute or property qlib.qlib.data.data.start_time\nLocal instrument data provider class  Provide instrument data from local data source.', 'CLASS qlib.scripts.data_collector.fund.collector.FundNormalize has field or attribute or property qlib.scripts.data_collector.fund.collector.symbol_field_name', 'CLASS qlib.qlib.data.data.LocalInstrumentProvider has field or attribute or property qlib.qlib.data.data.backend\nLocal instrument data provider class  Provide instrument data from local data source.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- [Collector Data](#collector-data)\n  - [Get Qlib data](#get-qlib-databin-file)\n  - [Collector *YahooFinance* data to qlib](#collector-yahoofinance-data-to-qlib)\n  - [Automatic update of daily frequency data](#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n- [Using qlib data](#using-qlib-data)\n\n\n# Collect Data From Yahoo Finance\n\n> *Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n**NOTE**:  Yahoo! Finance has blocked the access from China. Please change your network if you want to use the Yahoo data crawler.\n\n>  **Examples of abnormal data**\n\n- [SH000661](https://finance.yahoo.com/quote/000661.SZ/history?period1=1558310400&period2=1590796800&interval=1d&filter=history&frequency=1d)\n- [SZ300144](https://finance.yahoo.com/quote/300144.SZ/history?period1=1557446400&period2=1589932800&interval=1d&filter=history&frequency=1d)\n\nWe have considered **STOCK PRICE ADJUSTMENT**, but some price series seem still very abnormal.\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n### Get Qlib data(`bin file`)\n  > `qlib-data` from *YahooFinance*, is the data that has been dumped and can be used directly in `qlib`.\n  > This ready-made qlib-data is not updated regularly. If users want the latest data, please follow [these steps](#collector-yahoofinance-data-to-qlib) download the latest data. \n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data*\n    - `version`: dataset version, value from [`v1`, `v2`], by default `v1`\n      - `v2` end date is *2021-06*, `v1` end date is *2020-09*\n      - If users want to incrementally update data, they need to use yahoo collector to [collect data from scratch](#collector-yahoofinance-data-to-qlib).\n      - **the [benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks) for qlib use `v1`**, *due to the unstable access to historical data by YahooFinance, there are some differences between `v2` and `v1`*\n    - `interval`: `1d` or `1min`, by default `1d`\n    - `region`: `cn` or `us` or `in`, by default `cn`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # cn 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n    # cn 1min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n    # us 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us --interval 1d\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1267, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.860671043395996,698
Which HTTP response code causes the dataset availability check to return false?,"The HTTP response code that causes the dataset availability check to return false in qlib is 404, indicating a 'Not Found' error.","In GetData.check_dataset (qlib/tests/data.py), it returns False when resp.status_code is 404.","['MODULE keras.keras.src.regularizers.regularizers contains FUNCTION keras.keras.src.regularizers.regularizers.validate_float_arg\ncheck penalty number availability, raise ValueError if failed.', 'CLASS cognee.cognee.api.v1.exceptions.exceptions.DatasetNotFoundError has field or attribute or property cognee.cognee.api.v1.exceptions.exceptions.status_code', 'FUNCTION keras.keras.src.regularizers.regularizers.validate_float_arg has parameter or accepts argument keras.keras.src.regularizers.regularizers.validate_float_arg.name\ncheck penalty number availability, raise ValueError if failed.', 'CLASS cognee.cognee.modules.data.exceptions.exceptions.DatasetNotFoundError has field or attribute or property cognee.cognee.modules.data.exceptions.exceptions.status_code', 'FUNCTION keras.keras.src.regularizers.regularizers.validate_float_arg has parameter or accepts argument keras.keras.src.regularizers.regularizers.validate_float_arg.value\ncheck penalty number availability, raise ValueError if failed.', 'MODULE cognee.cognee.modules.data.methods.get_authorized_existing_datasets contains GLOBAL_VARIABLE cognee.cognee.modules.data.methods.get_authorized_existing_datasets.dataset_ids', 'MODULE cognee.cognee.api.v1.sync.sync contains CLASS cognee.cognee.api.v1.sync.sync.CheckHashesDiffResponse\nResponse model for missing hashes check', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1270, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.00431513786316,699
Which attribute is incorrectly referenced in the initialization log that causes the URI to be reported wrong?,"The attribute that is incorrectly referenced in the initialization log that causes the URI to be reported wrong in qlib is 'uri_context'. This information can be found in the documentation of qlib, specifically in the `qlib.workflow.__init__.QlibRecorder.uri_context` method. The correct parameter name for this method is 'uri', not 'uri_context'.","The debug log is referencing `self.uri`, which doesn’t exist. It should be using `self.default_uri` (set from the `uri` parameter).","[""METHOD qlib.qlib.workflow.__init__.QlibRecorder.uri_context has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.uri_context.uri\nTemporarily set the exp_manager's **default_uri** to uri  NOTE: - Please refer to the NOTE in the `set_uri`  Parameters ---------- uri : Text     the temporal uri"", 'CLASS cognee.cognee.modules.graph.exceptions.exceptions.EntityAlreadyExistsError has field or attribute or property cognee.cognee.modules.graph.exceptions.exceptions.name\nConflict detected, like trying to create a resource that already exists', ""CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.uri_context\nTemporarily set the exp_manager's **default_uri** to uri  NOTE: - Please refer to the NOTE in the `set_uri`  Parameters ---------- uri : Text     the temporal uri\nA global system that helps to manage the experiments."", 'MODULE cognee.cognee.infrastructure.files.utils.open_data_file contains GLOBAL_VARIABLE cognee.cognee.infrastructure.files.utils.open_data_file.normalized_url', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.end_exp\nEnd an active experiment.  Maintaining `_active_exp_uri` is included in end_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_name : str     name of the active experiment. recorder_status : str     the status of the active recorder of the experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS cognee.cognee.tasks.ingestion.data_item_to_text_file.SaveDataSettings has field or attribute or property cognee.cognee.tasks.ingestion.data_item_to_text_file.SaveDataSettings.accept_local_file_path', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.start_exp\nStart an experiment. This method includes first get_or_create an experiment, and then set it to be active.  Maintaining `_active_exp_uri` is included in start_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_id : str     id of the active experiment. experiment_name : str     name of the active experiment. recorder_id : str     id of the recorder to be started. recorder_name : str     name of the recorder to be started. uri : str     the current tracking URI. resume : boolean     whether to resume the experiment and recorder.  Returns ------- An active experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Notable bug fixes\n\nSome noteworthy bug fixes that are not related to specific language bindings are listed in this section.\n\n- Some language environments use a different thread to perform garbage collection, which breaks the thread-local cache used in XGBoost. XGBoost 2.0 implements a new thread-safe cache using a light weight lock to replace the thread-local cache. (#8851)\n- Fix model IO by clearing the prediction cache. (#8904)\n- `inf` is checked during data construction. (#8911)\n- Preserve order of saved updaters configuration. Usually, this is not an issue unless the `updater` parameter is used instead of the `tree_method` parameter (#9355)\n- Fix GPU memory allocation issue with categorical splits. (#9529)\n- Handle escape sequence like `\\t\\n` in feature names for JSON model dump. (#9474)\n- Normalize file path for model IO and text input. This handles short paths on Windows and paths that contain `~` on Unix (#9463). In addition, all path inputs are required to be encoded in UTF-8 (#9448, #9443)\n- Fix integer overflow on H100. (#9380)\n- Fix weighted sketching on GPU with categorical features. (#9341)\n- Fix metric serialization. The bug might cause some of the metrics to be dropped during evaluation. (#9405)\n- Fixes compilation errors on MSVC x86 targets (#8823)\n- Pick up the dmlc-core fix for the CSV parser. (#8897)\n\n\n### Documentation\nAside from documents for new features, we have many smaller updates to improve user experience, from troubleshooting guides to typo fixes.\n\n- Explain CPU/GPU interop. (#8450)\n- Guide to troubleshoot NCCL errors. (#8943, #9206)\n- Add a note for rabit port selection. (#8879)\n- How to build the docs using conda (#9276)\n- Explain how to obtain reproducible results on distributed systems. (#8903)\n\n* Fixes and small updates to document and demonstration scripts. (#8626, #8436, #8995, #8907, #8923, #8926, #9358, #9232, #9201, #9469, #9462, #9458, #8543, #8597, #8401, #8784, #9213, #9098, #9008, #9223, #9333, #9434, #9435, #9415, #8773, #8752, #9291, #9549)\n\n### Python package\n* New Features and Improvements\n- Support primitive types of pyarrow-backed pandas dataframe. (#8653)\n- Warning messages emitted by XGBoost are now emitted using Python warnings. (#9387)\n- User can now format the value printed near the bars on the `plot_importance` plot (#8540)\n- XGBoost has improved half-type support (float16) with pandas, cupy, and cuDF. With GPU input, the handling is through CUDA `__half` type, and no data copy is made. (#8487, #9207, #8481)\n- Support `Series` and Python primitive types in `inplace_predict` and `QuantileDMatrix` (#8547, #8542)\n- Support all pandas' nullable integer types. (#8480)\n- Custom metric with the scikit-learn interface now supports `sample_weight`. (#8706)\n- Enable Installation of Python Package with System lib in a Virtual Environment (#9349)\n- Raise if expected workers are not alive in `xgboost.dask.train` (#9421)\n\n* Optimization\n- Cache transformed data in `QuantileDMatrix` for efficiency. (#8666, #9445)\n- Take datatable as row-major input. (#8472)\n- Remove unnecessary conversions between data structures (#8546)\n\n* Adopt modern Python packaging conventions (PEP 517, PEP 518, PEP 621)\n-  XGBoost adopted the modern Python packaging conventions. The old setup script `setup.py` is now replaced with the new configuration file `pyproject.toml`. Along with this, XGBoost now supports Python 3.11. (#9021, #9112, #9114, #9115) Consult the latest documentation for the updated instructions to build and install XGBoost."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1271, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",41.15924954414368,700
What condition causes the method to log that the data is not in calendars and return without writing a bin file?,"When the `interval` parameter is set to `1min` in qlib, the method will log that the data is not in calendars and return without writing a bin file. This is because when `interval == 1min`, the `qlib_data_1d_dir` cannot be `None`, and the normalized 1min data needs to use the 1d data.","In DumpDataBase._data_to_bin (scripts/dump_bin.py), right after calling self.data_merge_calendar(df, calendar_list), it checks:

```
_df = self.data_merge_calendar(...)
if _df.empty:
    logger.warning(f""{features_dir.name} data is not in calendars"")
    return
```

So whenever the merged DataFrame has no rows (i.e. no dates overlap the calendar), it logs that message and exits without writing a bin file.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.where\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', 'METHOD qlib.qlib.data.storage.storage.CalendarStorage.remove has parameter or accepts argument qlib.qlib.data.storage.storage.CalVT', 'METHOD qlib.qlib.data.cache.DiskExpressionCache.gen_expression_cache has parameter or accepts argument qlib.qlib.data.cache.DiskExpressionCache.gen_expression_cache.instrument\nuse bin file to save like feature-data.', 'CLASS qlib.scripts.dump_bin.DumpDataBase has method qlib.scripts.dump_bin.DumpDataBase.save_calendars', 'METHOD qlib.qlib.data.cache.DiskExpressionCache.gen_expression_cache has parameter or accepts argument qlib.qlib.data.cache.DiskExpressionCache.gen_expression_cache.cache_path\nuse bin file to save like feature-data.', ""CLASS qlib.qlib.data.storage.storage.CalendarStorage has method qlib.qlib.data.storage.storage.CalendarStorage.remove\nThe behavior of CalendarStorage's methods and List's methods of the same name remain consistent"", 'METHOD qlib.qlib.data.cache.DiskExpressionCache.gen_expression_cache has parameter or accepts argument qlib.qlib.data.cache.DiskExpressionCache.gen_expression_cache.last_update\nuse bin file to save like feature-data.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1273, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",38.53622841835022,701
"After filtering rows to the specified hour and minute, how does the processor convert each timestamp index entry to a date-only value?","The qlib processor converts each timestamp index entry to a date-only value by using the `time_to_day_index` function from the `qlib.utils.time` module. This function takes a time object as input and returns an integer representing the day of the year. The process involves filtering rows to the specified hour and minute, then applying the `time_to_day_index` function to each timestamp index entry. Here is an example of how this function can be used:

``
from qlib.utils.time import time_to_day_index

# assume 'ts' is a pandas Timestamp object
day_index = time_to_day_index(ts)
``","In Resample1minProcessor.__call__, once rows are filtered by hour/minute, it does:

```python
df.index = df.index.normalize()
```

Calling DatetimeIndex.normalize() strips the time component, leaving only the date for each index entry.","['MODULE cognee.cognee.modules.engine.utils.generate_timestamp_datapoint contains FUNCTION cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int\nConverts a Timestamp model into an integer representation in milliseconds since the Unix epoch (UTC).  Args:     ts (Timestamp): The input Timestamp model containing year, month, day, hour, minute, and second.  Returns:     int: The UTC timestamp in milliseconds since January 1, 1970.', 'MODULE qlib.qlib.utils.time contains FUNCTION qlib.qlib.utils.time.time_to_day_index', 'FUNCTION cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int has parameter or accepts argument cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int.ts\nConverts a Timestamp model into an integer representation in milliseconds since the Unix epoch (UTC).  Args:     ts (Timestamp): The input Timestamp model containing year, month, day, hour, minute, and second.  Returns:     int: The UTC timestamp in milliseconds since January 1, 1970.', 'FUNCTION qlib.qlib.utils.time.time_to_day_index has parameter or accepts argument qlib.qlib.utils.time.US_TIME', ""CLASS qlib.qlib.workflow.task.utils.TimeAdjuster has method qlib.qlib.workflow.task.utils.TimeAdjuster.align_seg\nAlign the given date to the trade date  for example:      .. code-block:: python          input: {'train': ('2008-01-01', '2014-12-31'), 'valid': ('2015-01-01', '2016-12-31'), 'test': ('2017-01-01', '2020-08-01')}          output: {'train': (Timestamp('2008-01-02 00:00:00'), Timestamp('2014-12-31 00:00:00')),                 'valid': (Timestamp('2015-01-05 00:00:00'), Timestamp('2016-12-30 00:00:00')),                 'test': (Timestamp('2017-01-03 00:00:00'), Timestamp('2020-07-31 00:00:00'))}  Parameters ---------- segment  Returns ------- Union[dict, tuple]: the start and end trade date (pd.Timestamp) between the given start and end date.\nFind appropriate date and adjust date."", 'FUNCTION qlib.qlib.utils.time.time_to_day_index has parameter or accepts argument qlib.qlib.utils.time.TW_TIME', ""METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.align_seg has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.align_seg.segment\nAlign the given date to the trade date  for example:      .. code-block:: python          input: {'train': ('2008-01-01', '2014-12-31'), 'valid': ('2015-01-01', '2016-12-31'), 'test': ('2017-01-01', '2020-08-01')}          output: {'train': (Timestamp('2008-01-02 00:00:00'), Timestamp('2014-12-31 00:00:00')),                 'valid': (Timestamp('2015-01-05 00:00:00'), Timestamp('2016-12-30 00:00:00')),                 'test': (Timestamp('2017-01-03 00:00:00'), Timestamp('2020-07-31 00:00:00'))}  Parameters ---------- segment  Returns ------- Union[dict, tuple]: the start and end trade date (pd.Timestamp) between the given start and end date."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1285, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",41.96554374694824,702
"What column name prefixes distinguish metrics, parameters, and tags in the DataFrame returned by a records search?","In qlib, metrics, parameters, and tags are distinguished by their column name prefixes in the DataFrame returned by a records search as follows:

* Metrics: `metrics.*`
* Parameters: `params.*`
* Tags: `tags.*` These prefixes expand each metric, parameter, and tag into their own columns in the DataFrame.","In QlibRecorder.search_records (qlib/workflow/__init__.py), the DataFrame expands metrics, params and tags into columns prefixed with:

• metrics.<metric_name>  
• params.<param_name>  
• tags.<tag_name>","[""CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.search_records\nGet a pandas DataFrame of records that fit the search criteria of the experiment. Inputs are the search criteria user want to apply.  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don't have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)"", 'METHOD qlib.qlib.data.data.DatasetProvider.get_column_names has parameter or accepts argument qlib.qlib.data.data.DatasetProvider.get_column_names.fields\nGet column names from input fields', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.search_records\nGet a pandas DataFrame of records that fit the search criteria.  The arguments of this function are not set to be rigid, and they will be different with different implementation of ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the example code of the method with the ``MLflowExpManager``:  .. code-block:: Python      R.log_metrics(m=2.50, step=0)     records = R.search_records([experiment_id], order_by=[""metrics.m DESC""])  Parameters ---------- experiment_ids : list     list of experiment IDs. filter_string : str     filter query string, defaults to searching all runs. run_view_type : int     one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType). max_results  : int     the maximum number of runs to put in the dataframe. order_by : list     list of columns to order by (e.g., “metrics.rmse”).  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don\'t have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nA global system that helps to manage the experiments.', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.search_records has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.search_records.experiment_ids\nGet a pandas DataFrame of records that fit the search criteria.  The arguments of this function are not set to be rigid, and they will be different with different implementation of ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the example code of the method with the ``MLflowExpManager``:  .. code-block:: Python      R.log_metrics(m=2.50, step=0)     records = R.search_records([experiment_id], order_by=[""metrics.m DESC""])  Parameters ---------- experiment_ids : list     list of experiment IDs. filter_string : str     filter query string, defaults to searching all runs. run_view_type : int     one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType). max_results  : int     the maximum number of runs to put in the dataframe. order_by : list     list of columns to order by (e.g., “metrics.rmse”).  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don\'t have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.', ""CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.search_records\nGet a pandas DataFrame of records that fit the search criteria of the experiment. Inputs are the search criteria user want to apply.  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don't have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**."", 'CLASS qlib.scripts.dump_pit.DumpPitData has field or attribute or property qlib.scripts.dump_pit.DumpPitData.value_column_name', 'CLASS qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager has method qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager.clear\nClears all previous results and saved parameters.\nManages hyperparameter optimisation using random search for a single GPU.  Attributes:   param_ranges: Discrete hyperparameter range for random search.   results: Dataframe of validation results.   fixed_params: Fixed model parameters per experiment.   saved_params: Dataframe of parameters trained.   best_score: Minimum validation loss observed thus far.   optimal_name: Key to best configuration.   hyperparam_folder: Where to save optimisation outputs.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1287, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.739267110824585,703
What steps does the simulator executor follow to execute and track orders within a single trading step?,"The simulator executor follows these steps to execute and track orders within a single trading step in qlib:

1.  The `SingleAssetOrderExecutionSimple` class is used, which simulates market conditions by using ticks as individual trading opportunities.
2.  In each step, the traded amount is equally separated among each tick, then bounded by volume maximum execution volume (i.e., `vol_threshold`).
3.  If it's the last step, the simulator tries to ensure all the amount to be executed.
The `single_with_simulator` function in qlib.rl.contrib.backtest is used to run backtests in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day, and a new simulator will be created and used for every single-day order.","Within a single step (SimulatorExecutor._collect_data), the executor:

1. Calls _get_order_iterator(trade_decision) to pull & sort orders according to self.trade_type (serial or parallel by direction).  
2. Retrieves the current step timestamp via trade_calendar.get_step_time().  
3. For each order:  
   a. Floors the timestamp to “now_deal_day” and, if it’s a new day, resets self.dealt_order_amount and updates self.deal_day.  
   b. Executes the order via trade_exchange.deal_order(order, trade_account, dealt_order_amount), which returns (trade_val, trade_cost, trade_price).  
   c. Appends (order, trade_val, trade_cost, trade_price) to a local results list.  
   d. Increments dealt_order_amount[order.stock_id] by order.deal_amount.  
   e. If verbose, prints a summary line (time, buy/sell, price, amounts, cash).  
4. Returns the list of execution tuples plus a dict {""trade_info"": …}.","['CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple has method qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple.done\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'MODULE qlib.qlib.rl.contrib.backtest contains FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple has method qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple.get_state\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.single_with_simulator.cash_limit\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'MODULE qlib.qlib.rl.order_execution.simulator_simple contains CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.single_with_simulator.orders\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple has method qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple.__init__\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 1291, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",49.57077717781067,704
How does the current implementation handle values beyond the 3σ threshold when outlier clipping is enabled?,"qlib handles values beyond the 3σ threshold when outlier clipping is enabled through its RobustZScoreNorm feature, implemented in the `RobustZScoreNorm` class. This feature can be used to normalize data by removing outliers. To use this feature, create an instance of `RobustZScoreNorm` with `clip_outlier=True`, and then use the `fit_transform` method to normalize your data.

Here's an example code snippet:
```python
def _process(data):
    from qlib.data.dataset.processor import RobustZScoreNorm

    # Create a instance of RobustZScoreNorm with outlier clipping enabled
    processor = RobustZScoreNorm(clip_outlier=True)

    # Use the processor to normalize your data
    normalized_data = processor.fit_transform(data)
    return normalized_data
```
This example demonstrates how to use the `RobustZScoreNorm` feature to handle values beyond the 3σ threshold when outlier clipping is enabled.","In RobustZScoreNorm.__call__ (qlib/data/dataset/processor.py), once you’ve z-scored X, if clip_outlier is True it simply does:

    X = np.clip(X, –3, 3)

i.e. any z-score > 3 is set to 3 and any < –3 set to –3.","['FUNCTION keras.keras.src.activations.activations.relu has parameter or accepts argument keras.keras.src.activations.activations.relu.x\nApplies the rectified linear unit activation function.  With default values, this returns the standard ReLU activation: `max(x, 0)`, the element-wise maximum of 0 and the input tensor.  Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.  Examples:  >>> x = [-10, -5, 0.0, 5, 10] >>> keras.activations.relu(x) [ 0.,  0.,  0.,  5., 10.] >>> keras.activations.relu(x, negative_slope=0.5) [-5. , -2.5,  0. ,  5. , 10. ] >>> keras.activations.relu(x, max_value=5.) [0., 0., 0., 5., 5.] >>> keras.activations.relu(x, threshold=5.) [-0., -0.,  0.,  0., 10.]  Args:     x: Input tensor.     negative_slope: A `float` that controls the slope         for values lower than the threshold.     max_value: A `float` that sets the saturation threshold (the largest         value the function will return).     threshold: A `float` giving the threshold value of the activation         function below which values will be damped or set to zero.  Returns:     A tensor with the same shape and dtype as input `x`.', 'METHOD qlib.qlib.data.dataset.processor.RobustZScoreNorm.__init__ has parameter or accepts argument qlib.qlib.data.dataset.processor.RobustZScoreNorm.__init__.clip_outlier', 'MODULE keras.keras.src.activations.activations contains FUNCTION keras.keras.src.activations.activations.relu\nApplies the rectified linear unit activation function.  With default values, this returns the standard ReLU activation: `max(x, 0)`, the element-wise maximum of 0 and the input tensor.  Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.  Examples:  >>> x = [-10, -5, 0.0, 5, 10] >>> keras.activations.relu(x) [ 0.,  0.,  0.,  5., 10.] >>> keras.activations.relu(x, negative_slope=0.5) [-5. , -2.5,  0. ,  5. , 10. ] >>> keras.activations.relu(x, max_value=5.) [0., 0., 0., 5., 5.] >>> keras.activations.relu(x, threshold=5.) [-0., -0.,  0.,  0., 10.]  Args:     x: Input tensor.     negative_slope: A `float` that controls the slope         for values lower than the threshold.     max_value: A `float` that sets the saturation threshold (the largest         value the function will return).     threshold: A `float` giving the threshold value of the activation         function below which values will be damped or set to zero.  Returns:     A tensor with the same shape and dtype as input `x`.', 'FUNCTION keras.keras.src.legacy.backend.clip has parameter or accepts argument keras.keras.src.legacy.backend.clip.max_value\nDEPRECATED.', 'FUNCTION keras.keras.src.activations.activations.relu has parameter or accepts argument keras.keras.src.activations.activations.relu.max_value\nApplies the rectified linear unit activation function.  With default values, this returns the standard ReLU activation: `max(x, 0)`, the element-wise maximum of 0 and the input tensor.  Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.  Examples:  >>> x = [-10, -5, 0.0, 5, 10] >>> keras.activations.relu(x) [ 0.,  0.,  0.,  5., 10.] >>> keras.activations.relu(x, negative_slope=0.5) [-5. , -2.5,  0. ,  5. , 10. ] >>> keras.activations.relu(x, max_value=5.) [0., 0., 0., 5., 5.] >>> keras.activations.relu(x, threshold=5.) [-0., -0.,  0.,  0., 10.]  Args:     x: Input tensor.     negative_slope: A `float` that controls the slope         for values lower than the threshold.     max_value: A `float` that sets the saturation threshold (the largest         value the function will return).     threshold: A `float` giving the threshold value of the activation         function below which values will be damped or set to zero.  Returns:     A tensor with the same shape and dtype as input `x`.', 'CLASS keras.keras.src.ops.numpy.Clip has field or attribute or property keras.keras.src.ops.numpy.x_max', 'FUNCTION keras.keras.src.activations.activations.relu has parameter or accepts argument keras.keras.src.activations.activations.relu.negative_slope\nApplies the rectified linear unit activation function.  With default values, this returns the standard ReLU activation: `max(x, 0)`, the element-wise maximum of 0 and the input tensor.  Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.  Examples:  >>> x = [-10, -5, 0.0, 5, 10] >>> keras.activations.relu(x) [ 0.,  0.,  0.,  5., 10.] >>> keras.activations.relu(x, negative_slope=0.5) [-5. , -2.5,  0. ,  5. , 10. ] >>> keras.activations.relu(x, max_value=5.) [0., 0., 0., 5., 5.] >>> keras.activations.relu(x, threshold=5.) [-0., -0.,  0.,  0., 10.]  Args:     x: Input tensor.     negative_slope: A `float` that controls the slope         for values lower than the threshold.     max_value: A `float` that sets the saturation threshold (the largest         value the function will return).     threshold: A `float` giving the threshold value of the activation         function below which values will be damped or set to zero.  Returns:     A tensor with the same shape and dtype as input `x`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1294, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",52.254576683044434,705
How does the main backtest function schedule simulator-based execution across individual stocks when the simulator flag is enabled?,"The main backtest function in qlib schedules simulator-based execution across individual stocks when the simulator flag is enabled by using a SingleAssetOrderExecution simulator, which takes into account practical constraints such as trading volume restrictions and executes orders day by day with a new simulator created for every single-day order. This can be achieved through the `Backtest` class, where you create a backtest instance with the simulator flag enabled and then call the `run()` method to execute the backtest. For example:

```python
define the backtest configuration
backtest_config = {
    ""orders"": [
        {""datetime"": ""2020-06-01"", ""instrument"": ""INST"", ""amount"": 600.0, ""direction"": 0},
        {""datetime"": ""2020-06-02"", ""instrument"": ""INST"", ""amount"": 700.0, ""direction"": 1}
    ],
    ""split"": ""stock"",
    ""cash_limit"": 100000,
    ""generate_report"": True
}

# Create a backtest instance with the simulator flag enabled
backtest = Backtest(backtest_config=backtest_config, simulator=True)

# Run the backtest
backtest.run()
```
Note that the specific implementation of the main backtest function may vary depending on your use case.","In backtest (qlib/rl/contrib/backtest.py), when you call  
```python
backtest(..., with_simulator=True)
```  
it does:

1. Picks  
   ```python
   single = single_with_simulator
   ```  
2. Builds a stock‐sorted list  
   ```python
   stock_pool = order_df[""instrument""].unique()
   ```  
3. Uses joblib’s Parallel with  
   ```
   n_jobs=backtest_config[""concurrency""], backend=""multiprocessing""
   ```  
   to launch one process per stock:  
   ```python
   res = Parallel(**mp_config)(
       delayed(single)(
           backtest_config,
           orders=order_df[order_df.instrument == stock],
           split=""stock"",
           cash_limit=…,
           generate_report=…
       )
       for stock in stock_pool
   )
   ```  
Each worker runs single_with_simulator on that stock’s orders, which in turn instantiates a SingleAssetOrderExecution (via _get_multi_level_executor_config) for each order/date and runs the simulator sequentially.","['CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.twap_price\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.single_with_simulator.backtest_config\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.get_state\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.report\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.done\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'CLASS qlib.qlib.backtest.executor.SimulatorExecutor has field or attribute or property qlib.qlib.backtest.executor.generate_portfolio_metrics\nExecutor that simulate the true market', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.reset\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 19}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 19, 'n_files_pr': 11, 'pr': 1299, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",62.182250022888184,706
Which handler should you use to generate paused-filtered orderbook features in backtesting?,"To generate paused-filtered orderbook features in backtesting, you can use the `OrderHelper` class from the `qlib.backtest.decision` module. This class provides methods for generating orders and handling order-related tasks. You can also use built-in handlers such as `paused_filtered_orderbook_features` to generate the desired features for your backtesting analysis. Here is an example of how you can use the `OrderHelper` class to generate paused-filtered orderbook features in backtesting:

```python
define the start and end times for the backtesting period
start_time = '2020-01-01'
end_time = '2020-12-31'

# Create an instance of the OrderHelper class
order_helper = OrderHelper(start_time, end_time)

# Generate paused-filtered orderbook features
paused_filtered_orderbook_features = order_helper.generate_paused_filtered_orderbook_features()

# Print the generated features
print(paused_filtered_orderbook_features)
```
Note that this is just an example and you may need to modify it to fit your specific use case. Additionally, you will need to ensure that the necessary data is available for the backtesting period.","You want to use the backtest‐only orderbook handler:  
In qlib/contrib/data/highfreq_handler.py, use the HighFreqBacktestOrderHandler class (its get_feature_config applies the paused‐filter Select(…) to all orderbook fields).","['METHOD qlib.qlib.strategy.base.BaseStrategy.__init__ has parameter or accepts argument qlib.qlib.strategy.base.BaseStrategy.__init__.outer_trade_decision\nParameters ---------- outer_trade_decision : BaseTradeDecision, optional     the trade decision of outer strategy which this strategy relies, and it will be traded in     [start_time, end_time], by default None      - If the strategy is used to split trade decision, it will be used     - If the strategy is used for portfolio management, it can be ignored level_infra : LevelInfrastructure, optional     level shared infrastructure for backtesting, including trade calendar common_infra : CommonInfrastructure, optional     common infrastructure for backtesting, including trade_account, trade_exchange, .etc  trade_exchange : Exchange     exchange that provides market info, used to deal order and generate report      - If `trade_exchange` is None, self.trade_exchange will be set with common_infra     - It allows different trade_exchanges is used in different executions.     - For example:          - In daily execution, both daily exchange and minutely are usable, but the daily exchange is           recommended because it run faster.         - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_collect_data_loop has parameter or accepts argument qlib.qlib.rl.contrib.backtest.indicator_dict\nRun backtest in a single thread with collect_data_loop.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'METHOD qlib.qlib.strategy.base.BaseStrategy.__init__ has parameter or accepts argument qlib.qlib.strategy.base.BaseStrategy.__init__.level_infra\nParameters ---------- outer_trade_decision : BaseTradeDecision, optional     the trade decision of outer strategy which this strategy relies, and it will be traded in     [start_time, end_time], by default None      - If the strategy is used to split trade decision, it will be used     - If the strategy is used for portfolio management, it can be ignored level_infra : LevelInfrastructure, optional     level shared infrastructure for backtesting, including trade calendar common_infra : CommonInfrastructure, optional     common infrastructure for backtesting, including trade_account, trade_exchange, .etc  trade_exchange : Exchange     exchange that provides market info, used to deal order and generate report      - If `trade_exchange` is None, self.trade_exchange will be set with common_infra     - It allows different trade_exchanges is used in different executions.     - For example:          - In daily execution, both daily exchange and minutely are usable, but the daily exchange is           recommended because it run faster.         - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.', 'CLASS qlib.qlib.backtest.decision.OrderHelper has field or attribute or property qlib.qlib.backtest.decision.end_time\nMotivation - Make generating order easier     - User may have no knowledge about the adjust-factor information about the system.     - It involves too much interaction with the exchange when generating orders.', 'CLASS qlib.qlib.strategy.base.BaseStrategy has method qlib.qlib.strategy.base.BaseStrategy.__init__\nParameters ---------- outer_trade_decision : BaseTradeDecision, optional     the trade decision of outer strategy which this strategy relies, and it will be traded in     [start_time, end_time], by default None      - If the strategy is used to split trade decision, it will be used     - If the strategy is used for portfolio management, it can be ignored level_infra : LevelInfrastructure, optional     level shared infrastructure for backtesting, including trade calendar common_infra : CommonInfrastructure, optional     common infrastructure for backtesting, including trade_account, trade_exchange, .etc  trade_exchange : Exchange     exchange that provides market info, used to deal order and generate report      - If `trade_exchange` is None, self.trade_exchange will be set with common_infra     - It allows different trade_exchanges is used in different executions.     - For example:          - In daily execution, both daily exchange and minutely are usable, but the daily exchange is           recommended because it run faster.         - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.\nBase strategy for trading', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_collect_data_loop has parameter or accepts argument qlib.qlib.rl.contrib.backtest.records\nRun backtest in a single thread with collect_data_loop.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'METHOD qlib.qlib.strategy.base.BaseStrategy.__init__ has parameter or accepts argument qlib.qlib.strategy.base.BaseStrategy.__init__.common_infra\nParameters ---------- outer_trade_decision : BaseTradeDecision, optional     the trade decision of outer strategy which this strategy relies, and it will be traded in     [start_time, end_time], by default None      - If the strategy is used to split trade decision, it will be used     - If the strategy is used for portfolio management, it can be ignored level_infra : LevelInfrastructure, optional     level shared infrastructure for backtesting, including trade calendar common_infra : CommonInfrastructure, optional     common infrastructure for backtesting, including trade_account, trade_exchange, .etc  trade_exchange : Exchange     exchange that provides market info, used to deal order and generate report      - If `trade_exchange` is None, self.trade_exchange will be set with common_infra     - It allows different trade_exchanges is used in different executions.     - For example:          - In daily execution, both daily exchange and minutely are usable, but the daily exchange is           recommended because it run faster.         - In minutely execution, the daily exchange is not usable, only the minutely exchange is recommended.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 1, 'pr': 1302, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",65.73135161399841,707
Describe the flow by which a BaseTradeDecision is refreshed each step and how its execution window is enforced when nested executors apply range limits.,"The flow by which a `BaseTradeDecision` is refreshed each step and how its execution window is enforced when nested executors apply range limits in qlib can be found in the following steps:

1. The `NestedExecutor` class in `qlib.backtest.executor` is responsible for executing trade decisions in a higher frequency environment.
2. The `execute` method of the `NestedExecutor` class calls the inner strategy and executor to execute the trade decision.
3. The `SAOEStateAdapter` class in `qlib.rl.order_execution.strategy` maintains the states of the environment by updating its internal state according to the execution results with additional information acquired from executors & exchange.
4. The `update` method of the `SAOEStateAdapter` class updates the internal state based on the execution results and additional information.
5. The `saoe_state` method of the `SAOEStateAdapter` class returns the current state of the environment.
The execution window is enforced when nested executors apply range limits by checking if the trade decision is within the allowed range. If it is not, an error is raised.

Here's a code snippet that demonstrates how to use these classes and methods:

```python
default from qlib.backtest.executor import NestedExecutor
from qlib.rl.order_execution.strategy import SAOEStateAdapter

# Create a nested executor with an inner strategy and executor
nested_executor = NestedExecutor(inner_strategy, inner_executor)

# Create a SAOE state adapter
saoe_adapter = SAOEStateAdapter(order, trade_decision, nested_executor, exchange, ticks_per_step, backtest_data, data_granularity)

# Update the internal state of the SAOE state adapter
saoe_adapter.update(execute_result, last_step_range)

# Get the current state of the environment
state = saoe_adapter.saoe_state()
```
Note that this is a simplified example and you may need to modify it based on your specific use case.","Every backtest step, the NestedExecutor drives a two‐phase refresh of a BaseTradeDecision:

1. At the start of the step, NestedExecutor calls  
     decision.update(inner_calendar)  
   which in BaseTradeDecision.update  
    – sets self.total_step = inner_calendar.get_trade_len()  
    – invokes strategy.update_trade_decision(self, inner_calendar)  
   allowing the strategy to produce or revise its concrete orders.

2. Before kicking off any sub‐executor, NestedExecutor calls  
     decision.get_range_limit(  
       default_value=parent_limit,    # the outer executor’s window  
       inner_calendar=inner_calendar  
     )  
   which under the hood does:  
    – _get_range_limit(...) via your TradeRange (or IdxTradeRange) to get a [start,end] index  
    – clips that interval by self.total_step (warning if out of bounds)  
   and if no trade_range is defined, falls back to default_value.  

That [start,end] slice is then used to restrict the inner executor’s loop over the decision. Any outer trade_range is also propagated down via BaseTradeDecision.mod_inner_decision so deeply nested strategies inherit the window.","['MODULE qlib.qlib.rl.order_execution.strategy contains CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'CLASS qlib.qlib.backtest.executor.NestedExecutor inherits from or is a subclass of qlib.qlib.backtest.executor.BaseExecutor\nBase executor for trading\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.update\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'CLASS qlib.qlib.backtest.executor.NestedExecutor has field or attribute or property qlib.qlib.backtest.executor.time_per_step\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.saoe_state\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'CLASS qlib.qlib.backtest.executor.NestedExecutor has field or attribute or property qlib.qlib.backtest.executor.track_data\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.__init__\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### XGBoost4J-Spark: Redesigning checkpointing mechanism\n* RFC is available at #4786\n* Clean up checkpoint file after a successful training job (#4754): The current implementation in XGBoost4J-Spark does not clean up the checkpoint file after a successful training job. If the user runs another job with the same checkpointing directory, she will get a wrong model because the second job will re-use the checkpoint file left over from the first job. To prevent this scenario, we propose to always clean up the checkpoint file after every successful training job.\n* Avoid Multiple Jobs for Checkpointing (#5082): The current method for checkpoint is to collect the booster produced at the last iteration of each checkpoint internal to Driver and persist it in HDFS. The major issue with this approach is that it needs to re-perform the data preparation for training if the user did not choose to cache the training dataset. To avoid re-performing data prep, we build external-memory checkpointing in the XGBoost4J layer as well.\n* Enable deterministic repartitioning when checkpoint is enabled (#4807): Distributed algorithm for gradient boosting assumes a fixed partition of the training data between multiple iterations. In previous versions, there was no guarantee that data partition would stay the same, especially when a worker goes down and some data had to recovered from previous checkpoint. In this release, we make data partition deterministic by using the data hash value of each data row in computing the partition.\n\n### XGBoost4J-Spark: handle errors thrown by the native code (#4560)\n* All core logic of XGBoost is written in C++, so XGBoost4J-Spark internally uses the C++ code via Java Native Interface (JNI). #4560 adds a proper error handling for any errors or exceptions arising from the C++ code, so that the XGBoost Spark application can be torn down in an orderly fashion.\n\n### XGBoost4J-Spark: Refine method to count the number of alive cores  (#4858)\n* The `SparkParallelismTracker` class ensures that sufficient number of executor cores are alive. To that end, it is important to query the number of alive cores reliably.\n\n### XGBoost4J: Add `BigDenseMatrix` to store more than `Integer.MAX_VALUE` elements (#4383)\n\n### Robust model serialization with JSON (#4632, #4708, #4739, #4868, #4936, #4945, #4974, #5086, #5087, #5089, #5091, #5094, #5110, #5111, #5112, #5120, #5137, #5218, #5222, #5236, #5245, #5248, #5281)\n* In this release, we introduce an experimental support of using [JSON](https://www.json.org/json-en.html) for serializing (saving/loading) XGBoost models and related hyperparameters for training. We would like to eventually replace the old binary format with JSON, since it is an open format and parsers are available in many programming languages and platforms. See [the documentation for model I/O using JSON](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/saving_model.html). #3980 explains why JSON was chosen over other alternatives.\n* To maximize interoperability and compatibility of the serialized models, we now split serialization into two parts (#4855):\n  1. Model, e.g. decision trees and strictly related metadata like `num_features`.\n  2. Internal configuration, consisting of training parameters and other configurable parameters. For example, `max_delta_step`, `tree_method`, `objective`, `predictor`, `gpu_id`.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1308, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",73.96018266677856,708
How are the mid-close and mid-open times determined for regions outside China in the sample-minute function?,"The mid-close and mid-open times for regions outside China in the sample-minute function are determined by using the 'interpolation' method, which applies a mid-point summation scheme for ROC curve. This is according to the documentation of qlib library. The `sample_minute` function uses the default value `'interpolation'` for `mid_close` and `mid_open` parameters, but you can customize it by specifying different methods such as 'minoring' or 'majoring'. For example, you can call the `sample_minute` function with custom mid-close and mid-open times like this: `minute_data = sample_minute(data, mid_close='interpolation', mid_open='interpolation')`. Note that the available methods for `mid_close` and `mid_open` parameters are `'interpolation'`, `'minoring'`, and `'majoring'`.","In cal_sam_minute (tests/misc/test_utils.py), for any region ≠ CN (i.e. in the `else` branch) you simply do:

• mid_close_time = close_time  
• mid_open_time  = open_time  

(where open_time and close_time were already computed as  
day_time + region_time[0] / region_time[-1] minus the shift).","['MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_en_fund_symbols\nget en fund symbols  Returns -------     fund symbols in China', 'CLASS qlib.examples.benchmarks.LightGBM.features_sample.Resample1minProcessor has field or attribute or property qlib.examples.benchmarks.LightGBM.features_sample.minute\nThis processor tries to resample the data. It will reasmple the data from 1min freq to day freq by selecting a specific miniute', ""CLASS keras.keras.src.metrics.metrics_utils.AUCSummationMethod has method keras.keras.src.metrics.metrics_utils.AUCSummationMethod.from_str\nType of AUC summation method.  https://en.wikipedia.org/wiki/Riemann_sum)  Contains the following values: * 'interpolation': Applies mid-point summation scheme for `ROC` curve. For   `PR` curve, interpolates (true/false) positives but not the ratio that is   precision (see Davis & Goadrich 2006 for details). * 'minoring': Applies left summation for increasing intervals and right   summation for decreasing intervals. * 'majoring': Applies right summation for increasing intervals and left   summation for decreasing intervals."", ""METHOD qlib.qlib.backtest.utils.TradeCalendarManager.get_step_time has parameter or accepts argument qlib.qlib.backtest.utils.TradeCalendarManager.get_step_time.shift\nGet the left and right endpoints of the trade_step'th trading interval  About the endpoints:     - Qlib uses the closed interval in time-series data selection, which has the same performance as     pandas.Series.loc     # - The returned right endpoints should minus 1 seconds because of the closed interval representation in     #   Qlib.     # Note: Qlib supports up to minutely decision execution, so 1 seconds is less than any trading time     #   interval.  Parameters ---------- trade_step : int, optional     the number of trading step finished, by default None to indicate current step shift : int, optional     shift bars , by default 0  Returns ------- Tuple[pd.Timestamp, pd.Timestamp]     - If shift == 0, return the trading time range     - If shift > 0, return the trading time range of the earlier shift bars     - If shift < 0, return the trading time range of the later shift bar"", ""MODULE keras.keras.src.metrics.metrics_utils contains CLASS keras.keras.src.metrics.metrics_utils.AUCSummationMethod\nType of AUC summation method.  https://en.wikipedia.org/wiki/Riemann_sum)  Contains the following values: * 'interpolation': Applies mid-point summation scheme for `ROC` curve. For   `PR` curve, interpolates (true/false) positives but not the ratio that is   precision (see Davis & Goadrich 2006 for details). * 'minoring': Applies left summation for increasing intervals and right   summation for decreasing intervals. * 'majoring': Applies right summation for increasing intervals and left   summation for decreasing intervals."", ""METHOD qlib.qlib.backtest.utils.TradeCalendarManager.get_step_time has parameter or accepts argument qlib.qlib.backtest.utils.TradeCalendarManager.get_step_time.trade_step\nGet the left and right endpoints of the trade_step'th trading interval  About the endpoints:     - Qlib uses the closed interval in time-series data selection, which has the same performance as     pandas.Series.loc     # - The returned right endpoints should minus 1 seconds because of the closed interval representation in     #   Qlib.     # Note: Qlib supports up to minutely decision execution, so 1 seconds is less than any trading time     #   interval.  Parameters ---------- trade_step : int, optional     the number of trading step finished, by default None to indicate current step shift : int, optional     shift bars , by default 0  Returns ------- Tuple[pd.Timestamp, pd.Timestamp]     - If shift == 0, return the trading time range     - If shift > 0, return the trading time range of the earlier shift bars     - If shift < 0, return the trading time range of the later shift bar"", 'MODULE keras.keras.src.legacy.saving.serialization contains FUNCTION keras.keras.src.legacy.saving.serialization.deserialize_keras_object\nTurns the serialized form of a Keras object back into an actual object.  This function is for mid-level library implementers rather than end users.  Importantly, this utility requires you to provide the dict of `module_objects` to use for looking up the object config; this is not populated by default. If you need a deserialization utility that has preexisting knowledge of built-in Keras objects, use e.g. `keras.layers.deserialize(config)`, `keras.metrics.deserialize(config)`, etc.  Calling `deserialize_keras_object` while underneath the `SharedObjectLoadingScope` context manager will cause any already-seen shared objects to be returned as-is rather than creating a new object.  Args:   identifier: the serialized form of the object.   module_objects: A dictionary of built-in objects to look the name up in.     Generally, `module_objects` is provided by midlevel library     implementers.   custom_objects: A dictionary of custom objects to look the name up in.     Generally, `custom_objects` is provided by the end user.   printable_module_name: A human-readable string representing the type of     the object. Printed in case of exception.  Returns:   The deserialized object.  Example:  A mid-level library implementer might want to implement a utility for retrieving an object from its config, as such:  ```python def deserialize(config, custom_objects=None):    return deserialize_keras_object(      identifier,      module_objects=globals(),      custom_objects=custom_objects,      name=""MyObjectType"",    ) ```  This is how e.g. `keras.layers.deserialize()` is implemented.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 1310, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",57.28888463973999,709
How does the LP handler route fetch calls to the appropriate underlying dataset based on the requested data key?,"The LP handler routes fetch calls to the appropriate underlying dataset based on the requested data key by utilizing a mapping mechanism, where each data key is associated with a specific dataset. This allows for efficient and targeted retrieval of data from the relevant dataset. The exact implementation details may vary depending on the specific requirements and design of the LP handler, but this provides a general understanding of how it functions.","DataHandlerLP keeps three internal DataFrames—raw (`self._data`), infer (`self._infer`) and learn (`self._learn`)—and a class‐level ATTR_MAP that maps `DK_R`, `DK_I`, `DK_L` to those names. Its override of fetch() simply does:

1.  `_get_df_by_key(data_key)` (uses ATTR_MAP) to pick the right DataFrame.
2.  Passes that into the shared `_fetch_data(...)` routine.

So by calling `dh.fetch(…, data_key=DK_L)` you’ll get slices of `self._learn`, `data_key=DK_I` gives you `self._infer`, etc.","['CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.__init__\nParameters ---------- handler_config : dict     handler_config will be used to describe the handlers      .. code-block::          <handler_config> := {             ""group_name1"": <handler>             ""group_name2"": <handler>         }         or         <handler_config> := <handler>         <handler> := DataHandler Instance | DataHandler Config  fetch_kwargs : dict     fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.  is_group: bool     is_group will be used to describe whether the key of handler_config is group\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn\'t provide such interface & hook.', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.fetch.data_key\nfetch data from underlying data source  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index. level : Union[str, int]     which index level to select the data. col_set : str     select a set of meaningful columns.(e.g. features, columns). data_key : str     the data to fetch:  DK_*. proc_func: Callable     please refer to the doc of DataHandler.fetch  Returns ------- pd.DataFrame:', ""CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.load\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook."", 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', ""MODULE qlib.qlib.data.dataset.loader contains CLASS qlib.qlib.data.dataset.loader.DataLoaderDH\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook."", 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE\nfetch data from underlying data source  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index. level : Union[str, int]     which index level to select the data. col_set : str     select a set of meaningful columns.(e.g. features, columns). data_key : str     the data to fetch:  DK_*. proc_func: Callable     please refer to the doc of DataHandler.fetch  Returns ------- pd.DataFrame:', ""CLASS qlib.qlib.data.dataset.loader.DataLoaderDH inherits from or is a subclass of qlib.qlib.data.dataset.loader.DataLoader\nDataLoader is designed for loading raw data from original data source.\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 1, 'pr': 1312, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",58.53805470466614,710
What are the three sequential stages executed in the TestAllFlow suite?,"The three sequential stages executed in the TestAllFlow suite are:

1. `test_basic_flow_deferred`
2. `test_legacy_flow_with_input_shape`
3. `test_basic_flow_with_input`.
These stages are part of the SequentialTest class in the keras library, which is being used in the qlib codebase.","The suite runs, in order:

1. TestAllFlow.test_0_train – calls train() to fit the model, generate predictions and IC/RIC.  
2. TestAllFlow.test_1_backtest – calls backtest_analysis() on those predictions.  
3. TestAllFlow.test_2_expmanager – calls fake_experiment() to validate default and current URIs.","['MODULE keras.keras.src.saving.file_editor contains FUNCTION keras.keras.src.saving.file_editor.is_ipython_notebook\nChecks if the code is being executed in a notebook.', 'CLASS keras.keras.src.models.sequential_test.SequentialTest has method keras.keras.src.models.sequential_test.SequentialTest.test_basic_flow_deferred', 'METHOD qlib.qlib.backtest.executor.BaseExecutor.execute has parameter or accepts argument qlib.qlib.backtest.executor.BaseExecutor.execute.level\nexecute the trade decision and return the executed result  NOTE: this function is never used directly in the framework. Should we delete it?  Parameters ---------- trade_decision : BaseTradeDecision  level : int     the level of current executor  Returns ---------- execute_result : List[object]     the executed result for trade decision', 'CLASS keras.keras.src.models.sequential_test.SequentialTest has method keras.keras.src.models.sequential_test.SequentialTest.test_legacy_flow_with_input_shape', 'METHOD qlib.qlib.backtest.executor.BaseExecutor.execute has parameter or accepts argument qlib.qlib.backtest.executor.BaseExecutor.execute.trade_decision\nexecute the trade decision and return the executed result  NOTE: this function is never used directly in the framework. Should we delete it?  Parameters ---------- trade_decision : BaseTradeDecision  level : int     the level of current executor  Returns ---------- execute_result : List[object]     the executed result for trade decision', 'CLASS keras.keras.src.models.sequential_test.SequentialTest has method keras.keras.src.models.sequential_test.SequentialTest.test_basic_flow_with_input', 'CLASS qlib.qlib.backtest.executor.BaseExecutor has method qlib.qlib.backtest.executor.BaseExecutor.execute\nexecute the trade decision and return the executed result  NOTE: this function is never used directly in the framework. Should we delete it?  Parameters ---------- trade_decision : BaseTradeDecision  level : int     the level of current executor  Returns ---------- execute_result : List[object]     the executed result for trade decision\nBase executor for trading', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Updated winning solution under readme.md (#7862)\n- New security policy. (#8360)\n- GPU document is overhauled as we consider CUDA support to be feature-complete. (#8378)\n\n### Maintenance\n* Code refactoring and cleanups. (#7850, #7826, #7910, #8332, #8204)\n* Reduce compiler warnings. (#7768, #7916, #8046, #8059, #7974, #8031, #8022)\n* Compiler workarounds. (#8211, #8314, #8226, #8093)\n* Dependencies update. (#8001, #7876, #7973, #8298, #7816)\n* Remove warnings emitted in previous versions. (#7815)\n* Small fixes occurred during development. (#8008)\n\n### CI and Tests\n* We overhauled the CI infrastructure to reduce the CI cost and lift the maintenance burdens. Jenkins is replaced with buildkite for better automation, with which, finer control of test runs is implemented to reduce overall cost. Also, we refactored some of the existing tests to reduce their runtime, drooped the size of docker images, and removed multi-GPU C++ tests. Lastly, `pytest-timeout` is added as an optional dependency for running Python tests to keep the test time in check. (#7772, #8291, #8286, #8276, #8306, #8287, #8243, #8313, #8235, #8288, #8303, #8142, #8092, #8333, #8312, #8348)\n* New documents for how to reproduce the CI environment (#7971, #8297)\n* Improved automation for JVM release. (#7882)\n* GitHub Action security-related updates. (#8263, #8267, #8360)\n* Other fixes and maintenance work. (#8154, #7848, #8069, #7943)\n* Small updates and fixes to GitHub action pipelines. (#8364, #8321, #8241, #7950, #8011)\n\n## v1.6.1 (2022 May 9)\nThis is a patch release for bug fixes and Spark barrier mode support. The R package is unchanged.\n\n### Experimental support for categorical data\n- Fix segfault when the number of samples is smaller than the number of categories. (https://github.com/dmlc/xgboost/pull/7853)\n- Enable partition-based split for all model types. (https://github.com/dmlc/xgboost/pull/7857)\n\n### JVM packages\nWe replaced the old parallelism tracker with spark barrier mode to improve the robustness of the JVM package and fix the GPU training pipeline.\n- Fix GPU training pipeline quantile synchronization. (#7823, #7834)\n- Use barrier model in spark package. (https://github.com/dmlc/xgboost/pull/7836, https://github.com/dmlc/xgboost/pull/7840, https://github.com/dmlc/xgboost/pull/7845, https://github.com/dmlc/xgboost/pull/7846)\n- Fix shared object loading on some platforms. (https://github.com/dmlc/xgboost/pull/7844)\n\n## v1.6.0 (2022 Apr 16)\n\nAfter a long period of development, XGBoost v1.6.0 is packed with many new features and\nimprovements. We summarize them in the following sections starting with an introduction to\nsome major new features, then moving on to language binding specific changes including new\nfeatures and notable bug fixes for that binding.\n\n### Development of categorical data support\nThis version of XGBoost features new improvements and full coverage of experimental\ncategorical data support in Python and C package with tree model.  Both `hist`, `approx`\nand `gpu_hist` now support training with categorical data.  Also, partition-based\ncategorical split is introduced in this release. This split type is first available in\nLightGBM in the context of gradient boosting. The previous XGBoost release supported one-hot split where the splitting criteria is of form `x \\in {c}`, i.e. the categorical feature `x` is tested against a single candidate. The new release allows for more expressive conditions: `x \\in S` where the categorical feature `x` is tested against multiple candidates. Moreover, it is now possible to use any tree algorithms (`hist`, `approx`, `gpu_hist`) when creating categorical splits. For more', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 11, 'pr': 1314, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",35.497552156448364,711
What is the sequence of steps from calling the public training API to collecting episodes and updating the policy?,"The sequence of steps from calling the public training API to collecting episodes and updating the policy in qlib is as follows:

1.  The user calls the public training API, which creates a collector and collects `episode_per_iter` episodes.
2.  The collected episodes are then used to update the policy on the replay buffer.
3.  This process is repeated for each iteration of the training process.","The call chain is roughly:

1. User calls qlib/rl/trainer/api.py → train(…).
2. This builds a TrainingVessel and a Trainer, then calls Trainer.fit(vessel).
3. In Trainer.fit (qlib/rl/trainer/trainer.py):
   - initialize training loop
   - each iteration:
     a. build a vectorized env via Trainer.venv_from_iterator(vessel.train_seed_iterator())
     b. call vessel.train(vector_env)
4. In TrainingVessel.train (qlib/rl/trainer/vessel.py):
   - policy.train()
   - create a Collector(policy, vector_env, …, exploration_noise=True)
   - collector.collect(n_episode=episode_per_iter)  ← collect episodes
   - policy.update(sample_size=0, buffer=collector.buffer, **update_kwargs)  ← update policy
   - log and return the combined collect/update metrics.","['CLASS qlib.qlib.rl.trainer.vessel.TrainingVessel has method qlib.qlib.rl.trainer.vessel.TrainingVessel.train\nCreate a collector and collects ``episode_per_iter`` episodes. Update the policy on the collected replay buffer.\nThe default implementation of training vessel.  ``__init__`` accepts a sequence of initial states so that iterator can be created. ``train``, ``validate``, ``test`` each do one collect (and also update in train). By default, the train initial states will be repeated infinitely during training, and collector will control the number of episodes for each iteration. In validation and testing, the val / test initial states will be used exactly once.  Extra hyper-parameters (only used in train) include:  - ``buffer_size``: Size of replay buffer. - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run. - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.   For example, ``dict(repeat=10, batch_size=64)``.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has field or attribute or property qlib.qlib.rl.trainer.trainer.Trainer.current_episode\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'CLASS qlib.qlib.rl.trainer.vessel.TrainingVessel inherits from or is a subclass of qlib.qlib.rl.trainer.vessel.TrainingVesselBase\nA ship that contains simulator, interpreter, and policy, will be sent to trainer. This class controls algorithm-related parts of training, while trainer is responsible for runtime part.  The ship also defines the most important logic of the core training part, and (optionally) some callbacks to insert customized logics at specific events.\nThe default implementation of training vessel.  ``__init__`` accepts a sequence of initial states so that iterator can be created. ``train``, ``validate``, ``test`` each do one collect (and also update in train). By default, the train initial states will be repeated infinitely during training, and collector will control the number of episodes for each iteration. In validation and testing, the val / test initial states will be used exactly once.  Extra hyper-parameters (only used in train) include:  - ``buffer_size``: Size of replay buffer. - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run. - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.   For example, ``dict(repeat=10, batch_size=64)``.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.get_policy_state_dict\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'MODULE keras.keras.src.saving.serialization_lib contains FUNCTION keras.keras.src.saving.serialization_lib.serialize_with_public_class\nSerializes classes from public Keras API or object registration.  Called to check and retrieve the config of any class that has a public Keras API or has been registered as serializable via `keras.saving.register_keras_serializable()`.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.state_dict\nPutting every states of current training into a dict, at best effort.  It doesn\'t try to handle all the possible kinds of states in the middle of one training collect. For most cases at the end of each iteration, things should be usually correct.  Note that it\'s also intended behavior that replay buffer data in the collector will be lost.\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'FUNCTION keras.keras.src.saving.serialization_lib.serialize_with_public_class has parameter or accepts argument keras.keras.src.saving.serialization_lib.serialize_with_public_class.cls\nSerializes classes from public Keras API or object registration.  Called to check and retrieve the config of any class that has a public Keras API or has been registered as serializable via `keras.saving.register_keras_serializable()`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md\nAssistant Guidelines\nThese rules are absolutely imperative to adhere to. Comply with them precisely as they are outlined.\n\nThe agent must use sequential thinking MCP tool to work out problems.\n\nCore Behavior Guidelines\n\nRespond only to explicit requests. Do not add files, code, tests, or comments unless asked.\n\nFollow instructions precisely. No assumptions or speculative additions.\n\nUse provided context accurately.\n\nAvoid extra output. No debugging logs or test harnesses unless requested.\n\nProduce clean, optimized code when code is requested. Respect existing style.\n\nDeliver complete, standalone solutions. No placeholders.\n\nLimit file creation. Only create new files when necessary.\n\nIf you modify the model in a user\'s code, you must confirm with the user and never be sneaky. Always tell the user exactly what you are doing.\n\nCommunication & Delivery\n\n9. Don\'t explain unless asked. Do not expose reasoning in outputs.\n10. If unsure, say ""I don\'t know."" Avoid hallucinated content.\n11. Maintain consistency across sessions. Refer to project memory and documentation.\n12. Respect privacy and permissions. Never leak or infer secure data.\n13. Prioritize targeted edits over full rewrites.\n14. Optimize incrementally. Avoid unnecessary overhauls.\n\nSpec.md Requirement\n\nYou must maintain a file named Spec.md. This file acts as the single source of truth for the project.\n\nRules:\n\nBefore starting any implementation, check if Spec.md already exists.\n\nIf it does not exist, create one using the template provided below.\n\nAlways update Spec.md before and after any major change.\n\nUse the contents of Spec.md to guide logic, structure, and implementation decisions.\n\nWhen updating a section, condense previous content to keep the document concise.\n\nSpec.md Starter Template (Plain Text Format)\n\nTitle: Spec.md – Project Specification\n\nSection: Purpose\nDescribe the main goal of this feature, tool, or system.\n\nSection: Core Functionality\nList the key features, expected behaviors, and common use cases.\n\nSection: Architecture Overview\nSummarize the technical setup, frameworks used, and main modules or services.\n\nSection: Input and Output Contracts\nList all inputs and outputs in a table-like format:\n\nInput: describe the input data, its format, and where it comes from.\n\nOutput: describe the output data, its format, and its destination.\n\nSection: Edge Cases and Constraints\nList known limitations, special scenarios, and fallback behaviors.\n\nSection: File and Module Map\nList all important files or modules and describe what each one is responsible for.\n\nSection: Open Questions or TODOs\nCreate a checklist of unresolved decisions, logic that needs clarification, or tasks that are still pending.\n\nSection: Last Updated\nInclude the most recent update date and who made the update.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 25}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 25, 'n_files_pr': 18, 'pr': 1316, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",40.04645490646362,712
What steps does cache_to_origin_data perform on the DataFrame columns using the supplied fields list?,"The `cache_to_origin_data` function performs the following steps on the DataFrame columns using the supplied fields list:

1. It caches data to origin data.
2. It accepts a pd.DataFrame as input, which is the cache data.
3. It also accepts a feature fields parameter, which is the fields list.
4. It returns a pd.DataFrame.

In other words, the function takes in cached data and the fields that are of interest, and returns a DataFrame with those fields.","The function does the following to your DataFrame’s columns:

1. Calls remove_fields_space(fields) to strip any spaces out of each field name.  
2. Subsets the DataFrame to only those “no‐space” columns:  
   `data = data.loc[:, not_space_fields]`  
3. Renames the resulting columns back to the original `fields` (stringified) order:  
   `data.columns = [str(i) for i in fields]`","['MODULE cognee.evals.old.comparative_eval.helpers.convert_metrics contains FUNCTION cognee.evals.old.comparative_eval.helpers.convert_metrics.convert_to_dataframe\nConvert results list to DataFrame with expanded error columns.', 'METHOD qlib.qlib.data.cache.DatasetCache.cache_to_origin_data has parameter or accepts argument qlib.qlib.data.cache.DatasetCache.cache_to_origin_data.fields\ncache data to origin data  :param data: pd.DataFrame, cache data. :param fields: feature fields. :return: pd.DataFrame.', 'MODULE cognee.evals.src.helpers.convert_metrics contains FUNCTION cognee.evals.src.helpers.convert_metrics.convert_to_dataframe\nConvert results list to DataFrame with expanded error columns.', 'METHOD qlib.qlib.data.cache.DatasetCache.cache_to_origin_data has parameter or accepts argument qlib.qlib.data.cache.DatasetCache.cache_to_origin_data.data\ncache data to origin data  :param data: pd.DataFrame, cache data. :param fields: feature fields. :return: pd.DataFrame.', 'FUNCTION cognee.evals.src.helpers.convert_metrics.convert_to_dataframe has parameter or accepts argument cognee.evals.src.helpers.convert_metrics.convert_to_dataframe.results\nConvert results list to DataFrame with expanded error columns.', 'CLASS qlib.qlib.data.cache.DatasetCache has method qlib.qlib.data.cache.DatasetCache.cache_to_origin_data\ncache data to origin data  :param data: pd.DataFrame, cache data. :param fields: feature fields. :return: pd.DataFrame.\nDataset cache mechanism base class.  This class is used to wrap dataset provider with self-defined dataset cache mechanism.  .. note:: Override the `_uri` and `_dataset` method to create your own dataset cache mechanism.', 'FUNCTION cognee.evals.old.comparative_eval.helpers.convert_metrics.convert_to_dataframe has parameter or accepts argument cognee.evals.old.comparative_eval.helpers.convert_metrics.convert_to_dataframe.results\nConvert results list to DataFrame with expanded error columns.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1329, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",38.941811084747314,713
How does the order of subtraction differ between the tests for the absolute and sign element operators?,"The order of subtraction in the test cases for the absolute and sign element operators differs between the two. For the absolute operator, the order is: `feature_right - feature_left`, while for the sign operator, the order is: `feature_left - feature_right`. This difference in order affects how the values are calculated when applying these operators to features in qlib.","In tests/ops/test_elem_operator.py (TestElementOperator):

- test_Abs computes  
  change = prev_close – close  
- test_Sign computes  
  change = close – prev_close","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.slogdet\nCompute the sign and natural logarithm of the determinant of a matrix.  Args:     x: Input matrix. It must 2D and square.  Returns:     A tuple `(sign, logabsdet)`. `sign` is a number representing     the sign of the determinant. For a real matrix, this is 1, 0, or -1.     For a complex matrix, this is a complex number with absolute value 1     (i.e., it is on the unit circle), or else 0.     `logabsdet` is the natural log of the absolute value of the determinant.', 'CLASS qlib.qlib.data.ops.Ge has method qlib.qlib.data.ops.Ge.__init__\nGreater Equal Than Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     bool series indicate `left >= right`', 'FUNCTION keras.keras.src.ops.numpy.slogdet has parameter or accepts argument keras.keras.src.ops.numpy.slogdet.x\nCompute the sign and natural logarithm of the determinant of a matrix.  Args:     x: Input matrix. It must 2D and square.  Returns:     A tuple `(sign, logabsdet)`. `sign` is a number representing     the sign of the determinant. For a real matrix, this is 1, 0, or -1.     For a complex matrix, this is a complex number with absolute value 1     (i.e., it is on the unit circle), or else 0.     `logabsdet` is the natural log of the absolute value of the determinant.', 'CLASS qlib.qlib.data.ops.Gt has method qlib.qlib.data.ops.Gt.__init__\nGreater Than Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     bool series indicate `left > right`', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.conjugate\nReturns the complex conjugate, element-wise.  The complex conjugate of a complex number is obtained by changing the sign of its imaginary part.  `keras.ops.conj` is a shorthand for this function.  Args:     x: Input tensor.  Returns:     The complex conjugate of each element in `x`.', ""CLASS qlib.qlib.data.ops.Sub has method qlib.qlib.data.ops.Sub.__init__\nSubtract Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     two features' subtraction"", 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.absolute\nCompute the absolute value element-wise.  `keras.ops.abs` is a shorthand for this function.  Args:     x: Input tensor.  Returns:     An array containing the absolute value of each element in `x`.  Example:  >>> x = keras.ops.convert_to_tensor([-1.2, 1.2]) >>> keras.ops.absolute(x) array([1.2, 1.2], dtype=float32)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), John Zedlewski (@JohnZed), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Anthony D'Amato (@Totoketchup), @Wittty-Panda, Alexander Gugel (@alexanderGugel), Codecov Comments Bot (@codecov-commenter), Codecov (@codecov-io), DIVYA CHAUHAN (@divya661), Devin Robison (@drobison00), Geoffrey Blake (@geoffreyblake), Mark Harris (@harrism), Philip Hyunsu Cho (@hcho3), Honza Sterba (@honzasterba), Igor Moura (@igormp), @jakirkham, @jameskrach, James Lamb (@jameslamb), Janakarajan Natarajan (@janaknat), Jake Hemstad (@jrhemstad), Keith Kraus (@kkraus14), Kyle Nicholson (@kylejn27), Christian Lorentzen (@lorentzenchr), Michael Mayer (@mayer79), Nikolay Petrov (@napetrov), @odidev, PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Scott Lundberg (@slundberg), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vincent Nijs (@vnijs), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n## v1.2.0 (2020.08.22)\n\n### XGBoost4J-Spark now supports the GPU algorithm (#5171)\n* Now XGBoost4J-Spark is able to leverage NVIDIA GPU hardware to speed up training.\n* There is on-going work for accelerating the rest of the data pipeline with NVIDIA GPUs (#5950, #5972).\n\n### XGBoost now supports CUDA 11 (#5808)\n* It is now possible to build XGBoost with CUDA 11. Note that we do not yet distribute pre-built binaries built with CUDA 11; all current distributions use CUDA 10.0.\n\n### Better guidance for persisting XGBoost models in an R environment (#5940, #5964)\n* Users are strongly encouraged to use `xgb.save()` and `xgb.save.raw()` instead of `saveRDS()`. This is so that the persisted models can be accessed with future releases of XGBoost.\n* The previous release (1.1.0) had problems loading models that were saved with `saveRDS()`. This release adds a compatibility layer to restore access to the old RDS files. Note that this is meant to be a temporary measure; users are advised to stop using `saveRDS()` and migrate to `xgb.save()` and `xgb.save.raw()`.\n\n### New objectives and metrics\n* The pseudo-Huber loss `reg:pseudohubererror` is added (#5647). The corresponding metric is `mphe`. Right now, the slope is hard-coded to 1.\n* The Accelerated Failure Time objective for survival analysis (`survival:aft`) is now accelerated on GPUs (#5714, #5716). The survival metrics `aft-nloglik` and `interval-regression-accuracy` are also accelerated on GPUs.\n\n### Improved integration with scikit-learn\n* Added `n_features_in_` attribute to the scikit-learn interface to store the number of features used (#5780). This is useful for integrating with some scikit-learn features such as `StackingClassifier`.  See [this link](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for more details.\n* `XGBoostError` now inherits `ValueError`, which conforms scikit-learn's exception requirement (#5696)."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1330, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.8860125541687,714
What are the two provided implementations for generating orders from target weight positions and how do they fundamentally differ in price sourcing?,"**Two Implementations for Generating Orders from Target Weight Positions in qlib**

The two provided implementations for generating orders from target weight positions in qlib are:

1. `get_stock_weight_df` function, which takes a list of positions as input and returns a weight distribution for the position.
2. `OrderGenerator` class, which has a field called `target_weight_position` that can be used to generate target weight positions.

These two implementations fundamentally differ in price sourcing. The `get_stock_weight_df` function uses the backtest result to calculate the weights, while the `OrderGenerator` class uses a predefined strategy to generate the target weight positions.

The difference between these two methods is their reward functions. The `get_stock_weight_df` function does not have any specific reward function, while the `OrderGenerator` class has a predefined reward function that is used to generate the target weight positions.

In terms of price sourcing, the `get_stock_weight_df` function uses the historical prices to calculate the weights, while the `OrderGenerator` class uses a predefined strategy to determine the prices.","There are two concrete subclasses of OrderGenerator.generate_order_list_from_target_weight_position in qlib/contrib/strategy/order_generator.py:

• OrderGenWInteract.generate_order_list_from_target_weight_position  
  – “With interaction” version. It queries the real trade‐date price and tradability via trade_exchange.get_close / calculate_amount_position_value / is_stock_tradable to size orders.

• OrderGenWOInteract.generate_order_list_from_target_weight_position  
  – “Without interaction” version. It never touches the live exchange on trade date; it uses the prediction‐date close (pred_start_time) or the stored price in the current Position as a proxy.","['MODULE qlib.qlib.backtest.profit_attribution contains FUNCTION qlib.qlib.backtest.profit_attribution.get_stock_weight_df\nget_stock_weight_df :param positions: Given a positions from backtest result. :return:          A weight distribution for the position', 'CLASS qlib.qlib.contrib.strategy.order_generator.OrderGenerator has field or attribute or property qlib.qlib.contrib.strategy.order_generator.target_weight_position', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_stock_weight_df has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_stock_weight_df.positions\nget_stock_weight_df :param positions: Given a positions from backtest result. :return:          A weight distribution for the position', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.EnhancedIndexingStrategy.generate_target_weight_position has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.EnhancedIndexingStrategy.generate_target_weight_position.score', 'CLASS qlib.qlib.backtest.position.InfPosition has method qlib.qlib.backtest.position.InfPosition.get_cash\nPosition with infinite cash and amount.  This is useful for generating random orders.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.EnhancedIndexingStrategy.generate_target_weight_position has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.EnhancedIndexingStrategy.generate_target_weight_position.current', 'CLASS qlib.qlib.backtest.position.InfPosition has method qlib.qlib.backtest.position.InfPosition.update_weight_all\nPosition with infinite cash and amount.  This is useful for generating random orders.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 4, 'pr': 1338, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",58.46056866645813,715
Which attribute specifies the list of custom operations applied to high-frequency data in the workflow?,The attribute that specifies the list of custom operations applied to high-frequency data in the workflow is `ds_extra_mod_func` in qlib. This attribute allows users to define and apply custom functions to high-frequency data within the workflow.,"The list is defined in the `SPEC_CONF` attribute of `HighfreqWorkflow` under the `""custom_ops""` key (i.e. `HighfreqWorkflow.SPEC_CONF[""custom_ops""]`).","[""CLASS qlib.qlib.data.dataset.handler.DataHandlerABC has field or attribute or property qlib.qlib.data.dataset.handler.level\nInterface for data handler.  This class does not assume the internal data structure of the data handler. It only defines the interface for external users (uses DataFrame as the internal data structure).  In the future, the data handler's more detailed implementation should be refactored. Here are some guidelines:  It covers several components:  - [data loader] -> internal representation of the data -> data preprocessing -> interface adaptor for the fetch interface - The workflow to combine them all:   The workflow may be very complicated. DataHandlerLP is one of the practices, but it can't satisfy all the requirements.   So leaving the flexibility to the user to implement the workflow is a more reasonable choice."", 'CLASS qlib.qlib.workflow.task.gen.RollingGen has field or attribute or property qlib.qlib.workflow.task.gen.RollingGen.ds_extra_mod_func', ""CLASS qlib.qlib.data.dataset.handler.DataHandlerABC has field or attribute or property qlib.qlib.data.dataset.handler.DataHandlerABC.DK_L\nInterface for data handler.  This class does not assume the internal data structure of the data handler. It only defines the interface for external users (uses DataFrame as the internal data structure).  In the future, the data handler's more detailed implementation should be refactored. Here are some guidelines:  It covers several components:  - [data loader] -> internal representation of the data -> data preprocessing -> interface adaptor for the fetch interface - The workflow to combine them all:   The workflow may be very complicated. DataHandlerLP is one of the practices, but it can't satisfy all the requirements.   So leaving the flexibility to the user to implement the workflow is a more reasonable choice."", 'CLASS qlib.qlib.workflow.task.gen.RollingGen has field or attribute or property qlib.qlib.workflow.task.gen.RollingGen.step', ""CLASS qlib.qlib.data.dataset.handler.DataHandlerABC has field or attribute or property qlib.qlib.data.dataset.handler.col_set\nInterface for data handler.  This class does not assume the internal data structure of the data handler. It only defines the interface for external users (uses DataFrame as the internal data structure).  In the future, the data handler's more detailed implementation should be refactored. Here are some guidelines:  It covers several components:  - [data loader] -> internal representation of the data -> data preprocessing -> interface adaptor for the fetch interface - The workflow to combine them all:   The workflow may be very complicated. DataHandlerLP is one of the practices, but it can't satisfy all the requirements.   So leaving the flexibility to the user to implement the workflow is a more reasonable choice."", 'CLASS qlib.qlib.workflow.task.gen.RollingGen has field or attribute or property qlib.qlib.workflow.task.gen.ds_extra_mod_func', ""CLASS qlib.qlib.data.dataset.handler.DataHandlerABC has field or attribute or property qlib.qlib.data.dataset.handler.DataHandlerABC.DK_R\nInterface for data handler.  This class does not assume the internal data structure of the data handler. It only defines the interface for external users (uses DataFrame as the internal data structure).  In the future, the data handler's more detailed implementation should be refactored. Here are some guidelines:  It covers several components:  - [data loader] -> internal representation of the data -> data preprocessing -> interface adaptor for the fetch interface - The workflow to combine them all:   The workflow may be very complicated. DataHandlerLP is one of the practices, but it can't satisfy all the requirements.   So leaving the flexibility to the user to implement the workflow is a more reasonable choice."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1348, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.11239314079285,716
What are the main stages executed by the update_data_to_bin command from data validation to index parsing?,"The main stages executed by the update_data_to_bin command from data validation to index parsing are as follows:

1. **Data Validation**: The `update_data_to_bin` command first validates the input data, checking its structure and fields.

2. **Index Parsing**: It then parses the indices of the input data, extracting symbol and date information.

3. **Data Processing**: After parsing the indices, the command processes the input data by applying transformations or filtering as needed.

4. **Binary Data Generation**: Finally, it generates binary data from the processed input data for compact representation.

These stages ensure proper validation, parsing, processing, and binary representation of the input data in qlib.","The `update_data_to_bin` call in scripts/data_collector/yahoo/collector.py runs roughly these steps:

1. **QLib data validation/download**  
   – exists_qlib_data (qlib/utils/__init__.py) → if missing, GetData().qlib_data(...)  
2. **Date setup**  
   – Read calendars/day.txt to compute `trading_date` and `end_date`  
3. **Data download**  
   – Run.download_data(...) (calls your Yahoo collector and Normalize._executor)  
4. **Data normalization**  
   – Run.normalize_data_1d_extend (via YahooNormalizeUS1dExtend)  
5. **Binary dump**  
   – DumpDataUpdate(csv_path=…, qlib_dir=…).dump()  
6. **Index parsing**  
   – Dynamically import data_collector.{region}_index.collector.get_instruments and invoke it for each index (e.g. SP500, NASDAQ100)","['MODULE keras.keras.src.datasets.cifar contains FUNCTION keras.keras.src.datasets.cifar.load_batch\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', 'MODULE qlib.qlib.utils.index_data contains CLASS qlib.qlib.utils.index_data.BinaryOps', 'MODULE keras.keras.src.trainers.data_adapters.py_dataset_adapter contains FUNCTION keras.keras.src.trainers.data_adapters.py_dataset_adapter.get_index\nGet the value from the PyDataset `uid` at index `i`.  To allow multiple PyDatasets to be used at the same time, we use `uid` to get a specific one. A single PyDataset would cause the validation to overwrite the training PyDataset.  This methods is called from worker threads.  Args:     uid: int, PyDataset identifier     i: index  Returns:     The value at index `i`.', 'METHOD qlib.qlib.data.cache.IndexManager.update has parameter or accepts argument qlib.qlib.data.cache.IndexManager.update.data', 'FUNCTION keras.keras.src.trainers.data_adapters.py_dataset_adapter.get_index has parameter or accepts argument keras.keras.src.trainers.data_adapters.py_dataset_adapter.get_index.uid\nGet the value from the PyDataset `uid` at index `i`.  To allow multiple PyDatasets to be used at the same time, we use `uid` to get a specific one. A single PyDataset would cause the validation to overwrite the training PyDataset.  This methods is called from worker threads.  Args:     uid: int, PyDataset identifier     i: index  Returns:     The value at index `i`.', 'CLASS qlib.qlib.utils.index_data.BinaryOps has field or attribute or property qlib.qlib.utils.index_data.other', 'FUNCTION keras.keras.src.trainers.data_adapters.py_dataset_adapter.get_index has parameter or accepts argument keras.keras.src.trainers.data_adapters.py_dataset_adapter.get_index.i\nGet the value from the PyDataset `uid` at index `i`.  To allow multiple PyDatasets to be used at the same time, we use `uid` to get a specific one. A single PyDataset would cause the validation to overwrite the training PyDataset.  This methods is called from worker threads.  Args:     uid: int, PyDataset identifier     i: index  Returns:     The value at index `i`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 1358, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",53.02056884765625,717
"When specifying a custom tracking URI for an experiment launch, how does the manager propagate this URI to the underlying Mlflow client instance?","When specifying a custom tracking URI for an experiment launch in qlib, the manager propagates this URI to the underlying Mlflow client instance by setting it as the default tracking URI using `ExpManager.default_uri`. This allows the user to override the default URI during the experiment period and then fallback to the default URI. The `default_uri` is used to set the default tracking URI for all experiments, unless a custom URI is specified at launch time.","When you call `start_exp(..., uri=""your_uri"")`, `ExpManager.start_exp` sets `self._active_exp_uri` to `""your_uri""`.  From then on:

• `ExpManager.uri` (used by both the manager and any experiments it creates) returns this active URI.  
• `MLflowExpManager.client` instantiates  
    `mlflow.tracking.MlflowClient(tracking_uri=self.uri)`  
so the custom URI is passed straight into the new Mlflow client.  
• Likewise, when creating an `MLflowExperiment`, its constructor does  
    `self._client = MlflowClient(tracking_uri=self._uri)`  
using the same URI.","['CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.uri\nGet the default tracking URI or current URI.  Returns ------- The tracking URI string.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.start_exp\nStart an experiment. This method includes first get_or_create an experiment, and then set it to be active.  Maintaining `_active_exp_uri` is included in start_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_id : str     id of the active experiment. experiment_name : str     name of the active experiment. recorder_id : str     id of the recorder to be started. recorder_name : str     name of the recorder to be started. uri : str     the current tracking URI. resume : boolean     whether to resume the experiment and recorder.  Returns ------- An active experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.default_uri\nGet the default tracking URI from qlib.config.C\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.scripts.data_collector.cn_index.collector.CSIIndex has method qlib.scripts.data_collector.cn_index.collector.CSIIndex.changes_url', 'METHOD qlib.qlib.workflow.expm.ExpManager.default_uri has parameter or accepts argument qlib.qlib.workflow.expm.ExpManager.default_uri\nGet the default tracking URI from qlib.config.C\nGet the default tracking URI from qlib.config.C', ""CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.start\nMethod to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:  .. code-block:: Python      # start new experiment and recorder     with R.start(experiment_name='test', recorder_name='recorder_1'):         model.fit(dataset)         R.log...         ... # further operations      # resume previous experiment and recorder     with R.start(experiment_name='test', recorder_name='recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.         ... # further operations   Parameters ---------- experiment_id : str     id of the experiment one wants to start. experiment_name : str     name of the experiment one wants to start. recorder_id : str     id of the recorder under the experiment one wants to start. recorder_name : str     name of the recorder under the experiment one wants to start. uri : str     The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.     The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.     Therefore, the next time when users call this function in the same experiment,     they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur. resume : bool     whether to resume the specific recorder with given name under the given experiment.\nA global system that helps to manage the experiments."", 'METHOD qlib.qlib.workflow.expm.ExpManager.default_uri has parameter or accepts argument qlib.qlib.workflow.expm.ExpManager.default_uri.value\nGet the default tracking URI from qlib.config.C', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 15}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 15, 'n_files_pr': 7, 'pr': 1364, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",36.50258708000183,718
What is the maximum allowed elapsed time for creating an MlflowClient in this test on non-Linux platforms?,"Unfortunately, we were unable to find any information on the maximum allowed elapsed time for creating an MlflowClient in this test on non-Linux platforms in qlib. This could be due to a lack of documentation or specific requirements for this platform.","In tests/dependency_tests/test_mlflow.py (MLflowTest.test_creating_client), on non-Linux platforms the elapsed time must be under 2e-2 seconds (i.e. 20 ms).","['CLASS qlib.qlib.rl.order_execution.reward.PPOReward has method qlib.qlib.rl.order_execution.reward.PPOReward.__init__\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'CLASS qlib.qlib.workflow.exp.MLflowExperiment has field or attribute or property qlib.qlib.workflow.exp.max_results\nUse mlflow to implement Experiment.', 'CLASS qlib.qlib.rl.order_execution.reward.PPOReward has method qlib.qlib.rl.order_execution.reward.PPOReward.reward\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'CLASS qlib.qlib.workflow.exp.MLflowExperiment has method qlib.qlib.workflow.exp.MLflowExperiment.end\nUse mlflow to implement Experiment.', 'MODULE qlib.qlib.rl.order_execution.reward contains CLASS qlib.qlib.rl.order_execution.reward.PPOReward\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'CLASS qlib.qlib.workflow.exp.MLflowExperiment has method qlib.qlib.workflow.exp.MLflowExperiment.__repr__\nUse mlflow to implement Experiment.', 'CLASS qlib.qlib.rl.order_execution.reward.PPOReward has field or attribute or property qlib.qlib.rl.order_execution.reward.max_step\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/README.md\n# Requirements\n\nHere is the minimal hardware requirements to run the `workflow_by_code` example.\n- Memory: 16G\n- Free Disk: 5G\n\n\n# NOTE\nThe results will slightly vary on different OSs(the variance of annualized return will be less than 2%).\nThe evaluation results in the `README.md` page are from Linux OS.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1365, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.7488751411438,719
Which run info attribute is assigned to the recorder's artifact URI when constructed with a previous MLflow run?,The run info attribute assigned to the recorder's artifact URI when constructed with a previous MLflow run in qlib is 'run_id'. This indicates that the `run_id` of the previous run is used as the identifier for the artifact URI.,"In MLflowRecorder.__init__ (qlib/workflow/recorder.py), `self._artifact_uri` is set from `mlflow_run.info.artifact_uri`.","['CLASS qlib.qlib.workflow.recorder.Recorder has method qlib.qlib.workflow.recorder.Recorder.save_objects\nSave objects such as prediction file or model checkpoints to the artifact URI. User can save object through keywords arguments (name:value).  Please refer to the docs of qlib.workflow:R.save_objects  Parameters ---------- local_path : str     if provided, them save the file or directory to the artifact URI. artifact_path=None : str     the relative path for the artifact to be stored in the URI.\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.get_artifact_uri\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.expm.MLflowExpManager inherits from or is a subclass of qlib.qlib.workflow.expm.ExpManager\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.\nUse mlflow to implement ExpManager.', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.artifact_path\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.MLflowRecorder.end_time\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.MLflowRecorder.id\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.uri\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1367, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.964752435684204,720
How does the initialization process apply the user-defined logging level across all module loggers?,"The initialization process in qlib applies the user-defined logging level across all module loggers through a combination of global variables and function calls. The `set_logging_verbosity` function can be used to set the verbosity level for logging, while the `log_levels` dictionary contains a mapping of log levels to their corresponding values.","In qlib/__init__.py’s init(), after loading the config via C.set(...), it does:

  get_module_logger.setLevel(C.logging_level)

Here, get_module_logger is actually an instance of _QLibLoggerManager (defined in qlib/log.py). Its setLevel(level) method simply does:

  for logger in self._loggers.values():
      logger.setLevel(level)

so every module logger that’s been created (and stored in _QLibLoggerManager._loggers) gets its level updated to the user‐defined C.logging_level.","['MODULE keras.keras.src.utils.io_utils contains FUNCTION keras.keras.src.utils.io_utils.set_logging_verbosity\nSets the verbosity level for logging.  Supported log levels are as follows:  - `""FATAL""` (least verbose) - `""ERROR""` - `""WARNING""` - `""INFO""` - `""DEBUG""` (most verbose)  Args:     level: A string corresponding to the level of verbosity for logging.', 'MODULE cognee.cognee.shared.logging_utils contains GLOBAL_VARIABLE cognee.cognee.shared.logging_utils.log_levels', 'CLASS xgboost.demo.dask.forward_logging.ForwardLoggingMonitor has method xgboost.demo.dask.forward_logging.ForwardLoggingMonitor.__init__\nPrint the evaluation result at each iteration. The default monitor in the native interface logs the result to the Dask scheduler process. This class can be used to forward the logging to the client process. Important: see the `client` parameter for more info.  Parameters ---------- client :     Distributed client. This must be the top-level client. The class uses     :py:meth:`distributed.Client.forward_logging` in conjunction with the Python     :py:mod:`logging` module to forward the evaluation results to the client     process. It has undefined behaviour if called in a nested task. As a result,     client-side logging is not enabled by default.', 'MODULE cognee.cognee.__init__ contains GLOBAL_VARIABLE cognee.cognee.__init__.logger', 'METHOD xgboost.demo.dask.forward_logging.ForwardLoggingMonitor.__init__ has parameter or accepts argument xgboost.demo.dask.forward_logging.ForwardLoggingMonitor.__init__.client\nPrint the evaluation result at each iteration. The default monitor in the native interface logs the result to the Dask scheduler process. This class can be used to forward the logging to the client process. Important: see the `client` parameter for more info.  Parameters ---------- client :     Distributed client. This must be the top-level client. The class uses     :py:meth:`distributed.Client.forward_logging` in conjunction with the Python     :py:mod:`logging` module to forward the evaluation results to the client     process. It has undefined behaviour if called in a nested task. As a result,     client-side logging is not enabled by default.', 'MODULE cognee.cognee.infrastructure.llm.utils contains GLOBAL_VARIABLE cognee.cognee.infrastructure.llm.utils.logger', 'METHOD xgboost.demo.dask.forward_logging.ForwardLoggingMonitor.__init__ has parameter or accepts argument xgboost.demo.dask.forward_logging.ForwardLoggingMonitor.__init__.period\nPrint the evaluation result at each iteration. The default monitor in the native interface logs the result to the Dask scheduler process. This class can be used to forward the logging to the client process. Important: see the `client` parameter for more info.  Parameters ---------- client :     Distributed client. This must be the top-level client. The class uses     :py:meth:`distributed.Client.forward_logging` in conjunction with the Python     :py:mod:`logging` module to forward the evaluation results to the client     process. It has undefined behaviour if called in a nested task. As a result,     client-side logging is not enabled by default.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 9, 'n_files_pr': 7, 'pr': 1368, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",49.72009778022766,721
How do the processes for retrieving historical constituents and newly added constituents differ in constructing dates for the external API calls?,"**Differences in Constructing Dates for External API Calls in qlib**

The processes for retrieving historical constituents and newly added constituents differ in constructing dates for external API calls.

### Historical Constituents
For historical constituents, the start date is set to 2 January 1968, which is the starting date of the ibovespa index. This date is used to download historic data from APIs that keep track of such data since this date.

### Newly Added Constituents
For newly added constituents, the code logic implemented uses web scraping on B3's website to get the current day's stocks composition. The data downloaded from the selenium script is then preprocessed and saved in the csv format established by scripts/data_collector/index.py.","In CSI500Index (scripts/data_collector/cn_index/collector.py):

• get_history_companies()  
  – Builds a pd.date_range(start=bench_start_date, end=now, freq=“7D”)  
  – Iterates over each date (as a Python date) calling get_data_from_baostock(date)  

• get_new_companies()  
  – Takes only today = pd.Timestamp.now().normalize()  
  – Formats it once with today.strftime(""%Y-%m-%d"")  
  – Calls get_data_from_baostock on that single date  

So historical pulls a weekly series of dates, whereas “new” pulls just the current date.","[""MODULE xgboost.demo.guide-python.external_memory contains FUNCTION xgboost.demo.guide-python.external_memory.setup_rmm\nSetup RMM for GPU-based external memory training.  It's important to use RMM with `CudaAsyncMemoryResource` or `ArenaMemoryResource` for GPU-based external memory to improve performance. If XGBoost is not built with RMM support, a warning is raised when constructing the `DMatrix`."", 'CLASS cognee.cognee.modules.engine.models.Timestamp.Timestamp inherits from or is a subclass of cognee.infrastructure.engine.DataPoint', ""MODULE xgboost.demo.guide-python.distributed_extmem_basic contains FUNCTION xgboost.demo.guide-python.distributed_extmem_basic.setup_rmm\nSetup RMM for GPU-based external memory training.  It's important to use RMM with `CudaAsyncMemoryResource` or `ArenaMemoryResource` for GPU-based external memory to improve performance. If XGBoost is not built with RMM support, a warning is raised when constructing the `DMatrix`."", 'CLASS cognee.cognee.modules.engine.models.Timestamp.Timestamp has field or attribute or property cognee.cognee.modules.engine.models.Timestamp.Timestamp.day', 'CLASS keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyTest has method keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyTest.test_get_config_from_config\nTest get_config and from_config methods.\nTest `DTypePolicy`.  In the tests, we also test `DTypePolicy` for historical reasons.', 'CLASS cognee.cognee.api.health.HealthResponse has field or attribute or property cognee.cognee.api.health.HealthResponse.timestamp', 'CLASS keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyTest has method keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyTest.test_properties\nTest variable_dtype, compute_dtype, and name properties.\nTest `DTypePolicy`.  In the tests, we also test `DTypePolicy` for historical reasons.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md\n# iBOVESPA History Companies Collection\n\n## Requirements\n\n- Install the libs from the file `requirements.txt`\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n- `requirements.txt` file was generated using python3.8\n\n## For the ibovespa (IBOV) index, we have:\n\n<hr/>\n\n### Method `get_new_companies`\n\n#### <b>Index start date</b>\n\n- The ibovespa index started on 2 January 1968 ([wiki](https://en.wikipedia.org/wiki/%C3%8Dndice_Bovespa)).  In order to use this start date in our `bench_start_date(self)` method, two conditions must be satisfied:\n    1) APIs used to download brazilian stocks (B3) historical prices must keep track of such historic data since 2 January 1968\n\n    2) Some website or API must provide, from that date, the historic index composition. In other words, the companies used to build the index .\n\n    As a consequence, the method `bench_start_date(self)` inside `collector.py` was implemented using `pd.Timestamp(""2003-01-03"")` due to two reasons\n\n    1) The earliest ibov composition that have been found was from the first quarter of 2003. More informations about such composition can be seen on the sections below.\n\n    2) Yahoo finance, one of the libraries used to download symbols historic prices, keeps track from this date forward.\n\n- Within the `get_new_companies` method, a logic was implemented to get, for each ibovespa component stock, the start date that yahoo finance keeps track of.\n\n#### <b>Code Logic</b>\n\nThe code does a web scrapping into the B3\'s [website](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br), which keeps track of the ibovespa stocks composition on the current day. \n\nOther approaches, such as `request` and `Beautiful Soup` could have been used. However, the website shows the table with the stocks with some delay, since it uses a script inside of it to obtain such compositions.\nAlternatively, `selenium` was used to download this stocks\' composition in order to overcome this problem.\n\nFuthermore, the data downloaded from the selenium script  was preprocessed so it could be saved into the `csv` format stablished by `scripts/data_collector/index.py`.\n\n<hr/>\n\n### Method `get_changes` \n\nNo suitable data source that keeps track of ibovespa\'s history stocks composition has been found. Except from this [repository](https://github.com/igor17400/IBOV-HCI) which provide such information have been used, however it only provides the data from the 1st quarter of 2003 to 3rd quarter of 2021.\n\nWith that reference, the index\'s composition can be compared quarter by quarter and year by year and then generate a file that keeps track of which stocks have been removed and which have been added each quarter and year.\n\n<hr/>\n\n### Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method save_new_companies\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 1373, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",44.50076770782471,722
Which constructor argument determines the number of rounds without improvement before a sub-model stops training?,"The constructor argument that determines the number of rounds without improvement before a sub-model stops training in qlib is 'early_stopping_rounds'. This parameter controls the patience of the model during training, allowing it to stop early if there's no improvement for a specified number of rounds.","The constructor argument early_stopping_rounds (in DEnsembleModel.__init__, qlib/contrib/model/double_ensemble.py) sets how many rounds without improvement are allowed before a sub-model stops training (used in train_submodel via lgb.early_stopping).","['CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss has method keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss.on_train_begin\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'METHOD xgboost.python-package.xgboost.sklearn.XGBModel.__init__ has parameter or accepts argument xgboost.python-package.xgboost.sklearn.XGBModel.early_stopping_rounds', 'MODULE keras.guides.writing_your_own_callbacks contains CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'METHOD keras.keras.src.trainers.trainer_test.StepCount.on_predict_batch_begin has parameter or accepts argument keras.keras.src.trainers.trainer_test.StepCount.on_predict_batch_begin.batch', 'CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss has method keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss.on_train_end\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'CLASS keras.keras.src.ops.numpy.Round has method keras.keras.src.ops.numpy.Round.__init__', 'CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss has method keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss.on_epoch_end\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\n### Saving Progress Models\nIf you want to save model every two round, simply set save_period=2. You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.\n\n#### Continue from Existing Model\nIf you want to continue boosting from existing model, say 0002.model, use\n```\n../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model\n```\nxgboost will load from 0002.model continue boosting for 2 rounds, and save output to continue.model. However, beware that the training and evaluation data specified in mushroom.conf should not change when you use this function.\n#### Use Multi-Threading\nWhen you are working with a large dataset, you may want to take advantage of parallelism. If your compiler supports OpenMP, xgboost is naturally multi-threaded, to set number of parallel running add ```nthread``` parameter to your configuration.\nEg. ```nthread=10```\n\nSet nthread to be the number of your real cpu (On Unix, this can be found using ```lscpu```)\nSome systems will have ```Thread(s) per core = 2```, for example, a 4 core cpu with 8 threads, in such case set ```nthread=4``` and not 8.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.7 (2017.12.30)\n* **This version represents a major change from the last release (v0.6), which was released one year and half ago.**\n* Updated Sklearn API\n  - Add compatibility layer for scikit-learn v0.18: `sklearn.cross_validation` now deprecated\n  - Updated to allow use of all XGBoost parameters via `**kwargs`.\n  - Updated `nthread` to `n_jobs` and `seed` to `random_state` (as per Sklearn convention); `nthread` and `seed` are now marked as deprecated\n  - Updated to allow choice of Booster (`gbtree`, `gblinear`, or `dart`)\n  - `XGBRegressor` now supports instance weights (specify `sample_weight` parameter)\n  - Pass `n_jobs` parameter to the `DMatrix` constructor\n  - Add `xgb_model` parameter to `fit` method, to allow continuation of training\n* Refactored gbm to allow more friendly cache strategy\n  - Specialized some prediction routine\n* Robust `DMatrix` construction from a sparse matrix\n* Faster construction of `DMatrix` from 2D NumPy matrices: elide copies, use of multiple threads\n* Automatically remove nan from input data when it is sparse.\n  - This can solve some of user reported problem of istart != hist.size\n* Fix the single-instance prediction function to obtain correct predictions\n* Minor fixes\n  - Thread local variable is upgraded so it is automatically freed at thread exit.\n  - Fix saving and loading `count::poisson` models\n  - Fix CalcDCG to use base-2 logarithm\n  - Messages are now written to stderr instead of stdout\n  - Keep built-in evaluations while using customized evaluation functions\n  - Use `bst_float` consistently to minimize type conversion\n  - Copy the base margin when slicing `DMatrix`\n  - Evaluation metrics are now saved to the model file\n  - Use `int32_t` explicitly when serializing version\n  - In distributed training, synchronize the number of features after loading a data matrix.\n* Migrate to C++11\n  - The current master version now requires C++11 enabled compiled(g++4.8 or higher)\n* Predictor interface was factored out (in a manner similar to the updater interface).\n* Makefile support for Solaris and ARM\n* Test code coverage using Codecov\n* Add CPP tests\n* Add `Dockerfile` and `Jenkinsfile` to support continuous integration for GPU code\n* New functionality\n  - Ability to adjust tree model's statistics to a new dataset without changing tree structures.\n  - Ability to extract feature contributions from individual predictions, as described in [here](http://blog.datadive.net/interpreting-random-forests/) and [here](https://arxiv.org/abs/1706.06060).\n  - Faster, histogram-based tree algorithm (`tree_method='hist'`) .\n  - GPU/CUDA accelerated tree algorithms (`tree_method='gpu_hist'` or `'gpu_exact'`), including the GPU-based predictor.\n  - Monotonic constraints: when other features are fixed, force the prediction to be monotonic increasing with respect to a certain specified feature.\n  - Faster gradient calculation using AVX SIMD\n  - Ability to export models in JSON format\n  - Support for Tweedie regression\n  - Additional dropout options for DART: binomial+1, epsilon\n  - Ability to update an existing model in-place: this is useful for many applications, such as determining feature importance\n* Python package:\n  - New parameters:\n    - `learning_rates` in `cv()`\n    - `shuffle` in `mknfold()`\n    - `max_features` and `show_values` in `plot_importance()`\n    - `sample_weight` in `XGBRegressor.fit()`"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1375, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.180483102798462,723
Which pandas option is temporarily changed to suppress chained assignment warnings during the z-score normalization?,"The pandas option that is temporarily changed to suppress chained assignment warnings during the z-score normalization in qlib is `use_stemplen_robust`. This option can be set to True to use robust statistics for Z-Score normalization, which will suppress the chained assignment warning.","In CSZScoreNorm.__call__ (qlib/data/dataset/processor.py) the pandas option `""mode.chained_assignment""` is set to `None` via `pd.option_context` to suppress those warnings.","['MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.robust_zscore\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.df\nZScore Normalization', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.CSZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.df\nCross Sectional ZScore Normalization', 'CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.fit\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.CSZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.CSZScoreNorm.zscore_func\nCross Sectional ZScore Normalization', 'CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.__init__\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Validation sets now support instance weights (#2354)\n  - `XGBClassifier.predict_proba()` should not support `output_margin` option. (#3343) See BREAKING CHANGES below.\n* R package:\n  - Better handling of NULL in `print.xgb.Booster()` (#3338)\n  - Comply with CRAN policy by removing compiler warning suppression (#3329)\n  - Updated CRAN submission\n* JVM packages\n  - JVM packages will now use the same versioning scheme as other packages (#3253)\n  - Update Spark to 2.3 (#3254)\n  - Add scripts to cross-build and deploy artifacts (#3276, #3307)\n  - Fix a compilation error for Scala 2.10 (#3332)\n* BREAKING CHANGES\n  - `XGBClassifier.predict_proba()` no longer accepts parameter `output_margin`. The parameter makes no sense for `predict_proba()` because the method is to predict class probabilities, not raw margin scores.\n\n## v0.71 (2018.04.11)\n* This is a minor release, mainly motivated by issues concerning `pip install`, e.g. #2426, #3189, #3118, and #3194.\n  With this release, users of Linux and MacOS will be able to run `pip install` for the most part.\n* Refactored linear booster class (`gblinear`), so as to support multiple coordinate descent updaters (#3103, #3134). See BREAKING CHANGES below.\n* Fix slow training for multiclass classification with high number of classes (#3109)\n* Fix a corner case in approximate quantile sketch (#3167). Applicable for 'hist' and 'gpu_hist' algorithms\n* Fix memory leak in DMatrix (#3182)\n* New functionality\n  - Better linear booster class (#3103, #3134)\n  - Pairwise SHAP interaction effects (#3043)\n  - Cox loss (#3043)\n  - AUC-PR metric for ranking task (#3172)\n  - Monotonic constraints for 'hist' algorithm (#3085)\n* GPU support\n    - Create an abstract 1D vector class that moves data seamlessly between the main and GPU memory (#2935, #3116, #3068). This eliminates unnecessary PCIe data transfer during training time.\n  - Fix minor bugs (#3051, #3217)\n  - Fix compatibility error for CUDA 9.1 (#3218)\n* Python package:\n  - Correctly handle parameter `verbose_eval=0` (#3115)\n* R package:\n  - Eliminate segmentation fault on 32-bit Windows platform (#2994)\n* JVM packages\n  - Fix a memory bug involving double-freeing Booster objects (#3005, #3011)\n  - Handle empty partition in predict (#3014)\n  - Update docs and unify terminology (#3024)\n  - Delete cache files after job finishes (#3022)\n  - Compatibility fixes for latest Spark versions (#3062, #3093)\n* BREAKING CHANGES: Updated linear modelling algorithms. In particular L1/L2 regularisation penalties are now normalised to number of training examples. This makes the implementation consistent with sklearn/glmnet. L2 regularisation has also been removed from the intercept. To produce linear models with the old regularisation behaviour, the alpha/lambda regularisation parameters can be manually scaled by dividing them by the number of training examples."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Deprecation notices\n* The use of `LabelEncoder` in `XGBClassifier` is now deprecated and will be removed in the next minor release (#6269). The deprecation is necessary to support multiple types of inputs, such as cuDF data frames or cuPy arrays.\n* The use of certain positional arguments in the Python interface is deprecated (#6365). Users will use deprecation warnings for the use of position arguments for certain function parameters. New code should use keyword arguments as much as possible. We have not yet decided when we will fully require the use of keyword arguments.\n\n### Bug-fixes\n* On big-endian arch, swap the byte order in the binary serializer to enable loading models that were produced by a little-endian machine (#5813).\n* [jvm-packages] Fix deterministic partitioning with dataset containing Double.NaN (#5996)\n* Limit tree depth for GPU hist to 31 to prevent integer overflow (#6045)\n* [jvm-packages] Set `maxBins` to 256 to align with the default value in the C++ code (#6066)\n* [R] Fix CRAN check (#6077)\n* Add back support for `scipy.sparse.coo_matrix` (#6162)\n* Handle duplicated values in sketching. (#6178)\n* Catch all standard exceptions in C API. (#6220)\n* Fix linear GPU input (#6255)\n* Fix inplace prediction interval. (#6259)\n* [R] allow `xgb.plot.importance()` calls to fill a grid (#6294)\n* Lazy import dask libraries. (#6309)\n* Deterministic data partitioning for external memory (#6317)\n* Avoid resetting seed for every configuration. (#6349)\n* Fix label errors in graph visualization (#6369)\n* [jvm-packages] fix potential unit test suites aborted issue due to race condition (#6373)\n* [R] Fix warnings from `R check --as-cran` (#6374)\n* [R] Fix a crash that occurs with noLD R (#6378)\n* [R] Do not convert continuous labels to factors (#6380)\n* [R] remove uses of `exists()` (#6387)\n* Propagate parameters to the underlying `Booster` handle from `XGBClassifier.set_param` / `XGBRegressor.set_param`. (#6416)\n* [R] Fix R package installation via CMake (#6423)\n* Enforce row-major order in cuPy array (#6459)\n* Fix filtering callable objects in the parameters passed to the scikit-learn API. (#6466)\n\n### Maintenance: Testing, continuous integration, build system\n* [CI] Improve JVM test in GitHub Actions (#5930)\n* Refactor plotting test so that it can run independently (#6040)\n* [CI] Cancel builds on subsequent pushes (#6011)\n* Fix Dask Pytest fixture (#6024)\n* [CI] Migrate linters to GitHub Actions (#6035)\n* [CI] Remove win2016 JVM test from GitHub Actions (#6042)\n* Fix CMake build with `BUILD_STATIC_LIB` option (#6090)\n* Don't link imported target in CMake (#6093)\n* Work around a compiler bug in MacOS AppleClang 11 (#6103)\n* [CI] Fix CTest by running it in a correct directory (#6104)\n* [R] Check warnings explicitly for model compatibility tests (#6114)\n* [jvm-packages] add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)\n* [CI] Time GPU tests. (#6141)\n* [R] remove warning in configure.ac (#6152)\n* [CI] Upgrade cuDF and RMM to 0.16 nightlies; upgrade to Ubuntu 18.04 (#6157)\n* [CI] Test C API demo (#6159)\n* Option for generating device debug info. (#6168)\n* Update `.gitignore` (#6175, #6193, #6346)\n* Hide C++ symbols from dmlc-core (#6188)\n* [CI] Added arm64 job in Travis-CI (#6200)\n* [CI] Fix Docker build for CUDA 11 (#6202)\n* [CI] Move non-OpenMP gtest to GitHub Actions (#6210)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1386, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",34.795735120773315,724
How does download_index_data merge previously saved data with newly fetched data before writing the CSV?,"The `download_index_data` function in qlib merges previously saved data with newly fetched data before writing the CSV using the `pd.concat` method. The datasets are joined based on the specified join method (e.g., by date or symbol). Here's an example code snippet demonstrating this process:

```python
def download_index_data(dataloader_l, join):
    # Load previously saved data
    prev_data = pd.read_csv('prev_data.csv')

    # Fetch new data
    new_data = fetch_new_data()

    # Merge the two datasets using the specified join method
    merged_data = pd.concat([prev_data, new_data], join=join)

    # Write the merged dataset to a CSV file
    merged_data.to_csv('merged_data.csv', index=False)
```
This example code snippet illustrates how the `download_index_data` function merges datasets and writes them to a CSV file.","In YahooCollectorCN1d.download_index_data (scripts/data_collector/yahoo/collector.py), after fetching the new DataFrame `df` it does:

1. `_path = self.save_dir / f""sh{_index_code}.csv""`
2. If `_path.exists()`, it reads the old file: `_old_df = pd.read_csv(_path)`
3. Merges them via `df = pd.concat([_old_df, df], sort=False)`
4. Writes the combined `df.to_csv(_path, index=False)`","['CLASS qlib.qlib.model.ens.ensemble.Ensemble has method qlib.qlib.model.ens.ensemble.Ensemble.__call__\nMerge the ensemble_dict into an ensemble object.  For example: {Rollinga_b: object, Rollingb_c: object} -> object  When calling this class:      Args:         ensemble_dict (dict): the ensemble dict like {name: things} waiting for merging      Returns:         object: the ensemble object', 'METHOD qlib.scripts.data_collector.yahoo.collector.YahooCollectorCN1d.download_index_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.INDEX_BENCH_URL', 'MODULE qlib.qlib.model.ens.ensemble contains CLASS qlib.qlib.model.ens.ensemble.Ensemble\nMerge the ensemble_dict into an ensemble object.  For example: {Rollinga_b: object, Rollingb_c: object} -> object  When calling this class:      Args:         ensemble_dict (dict): the ensemble dict like {name: things} waiting for merging      Returns:         object: the ensemble object', 'MODULE cognee.cognee.tasks.storage.index_data_points contains GLOBAL_VARIABLE cognee.cognee.tasks.storage.index_data_points.index_name', 'CLASS qlib.qlib.model.ens.ensemble.Ensemble has field or attribute or property qlib.qlib.model.ens.ensemble.ensemble_dict\nMerge the ensemble_dict into an ensemble object.  For example: {Rollinga_b: object, Rollingb_c: object} -> object  When calling this class:      Args:         ensemble_dict (dict): the ensemble dict like {name: things} waiting for merging      Returns:         object: the ensemble object', 'MODULE cognee.cognee.tasks.storage.index_data_points contains GLOBAL_VARIABLE cognee.cognee.tasks.storage.index_data_points.created_indexes', 'METHOD qlib.qlib.data.dataset.loader.NestedDataLoader.__init__ has parameter or accepts argument qlib.qlib.data.dataset.loader.NestedDataLoader.__init__.dataloader_l\nParameters ---------- dataloader_l : list[dict]     A list of dataloader, for exmaple      .. code-block:: python          nd = NestedDataLoader(             dataloader_l=[                 {                     ""class"": ""qlib.contrib.data.loader.Alpha158DL"",                 }, {                     ""class"": ""qlib.contrib.data.loader.Alpha360DL"",                     ""kwargs"": {                         ""config"": {                             ""label"": ( [""Ref($close, -2)/Ref($close, -1) - 1""], [""LABEL0""])                         }                     }                 }             ]         ) join :     it will pass to pd.concat when merging it.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can\'t meet users\' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month\'s data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *""2000-01-01""*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *""failed""* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date\'s close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1388, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",54.427077770233154,725
How does SigAnaRecord fall back to its SignalRecord dependency when loading an artifact fails?,"When loading an artifact fails, SigAnaRecord falls back to its SignalRecord dependency by utilizing the inheritance relationship between these two classes. Specifically, SigAnaRecord inherits from SignalRecord, which means it can leverage the functionality of SignalRecord when encountering errors during artifact loading. This fallback mechanism allows SigAnaRecord to recover and continue processing data. However, the exact implementation details may vary depending on the specific qlib codebase version and configuration used. For more information, refer to the `qlib.workflow.record_temp` module and the `log_artifact` method in the `MLflowRecorder` class of the `qlib.workflow.recorder` module.","SigAnaRecord doesn’t override load – it inherits RecordTemp.load (qlib/workflow/record_temp.py). That method does roughly:

```python
try:
    return self.recorder.load_object(self.get_path(name))
except LoadObjectError:
    if parents and self.depend_cls is not None:
        with class_casting(self, self.depend_cls):
            return self.load(name, parents=True)
    raise
```

Since SigAnaRecord.depend_cls = SignalRecord, on a LoadObjectError it does a class_casting(self, SignalRecord) and recursively calls load again, so it falls back to SignalRecord’s artifact_path.","['MODULE cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions contains CLASS cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsConnectionError\nException raised when connection to Neptune Analytics fails.', 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact.artifact_path', 'MODULE cognee.cognee.modules.cloud.exceptions.CloudConnectionError contains CLASS cognee.cognee.modules.cloud.exceptions.CloudConnectionError.CloudConnectionError\nRaised when the connection to the cloud service fails.', 'CLASS cognee.cognee.tasks.graph.exceptions.exceptions.InvalidOntologyAdapterError inherits from or is a subclass of cognee.exceptions.CogneeConfigurationError', 'CLASS cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsConnectionError has method cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsConnectionError.__init__\nException raised when connection to Neptune Analytics fails.', 'METHOD qlib.qlib.workflow.record_temp.SigAnaRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.SigAnaRecord.__init__.ana_long_short', 'CLASS cognee.cognee.modules.cloud.exceptions.CloudConnectionError.CloudConnectionError has method cognee.cognee.modules.cloud.exceptions.CloudConnectionError.CloudConnectionError.__init__\nRaised when the connection to the cloud service fails.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### CI\n* [CI] Rotate access keys for uploading MacOS artifacts from Travis CI (#7253)\n* Reduce Travis environment setup time. (#6912)\n* Restore R cache on github action. (#6985)\n* [CI] Remove stray build artifact to avoid error in artifact packaging (#6994)\n* [CI] Move appveyor tests to action (#6986)\n* Remove appveyor badge. [skip ci] (#7035)\n* [CI] Configure RAPIDS, dask, modin (#7033)\n* Test on s390x. (#7038)\n* [CI] Upgrade to CMake 3.14 (#7060)\n* [CI] Update R cache. (#7102)\n* [CI] Pin libomp to 11.1.0  (#7107)\n* [CI] Upgrade build image to CentOS 7 + GCC 8; require CUDA 10.1 and later (#7141)\n* [dask] Work around segfault in prediction. (#7112)\n* [dask] Remove the workaround for segfault. (#7146)\n* [CI] Fix hanging Python setup in Windows CI (#7186)\n* [CI] Clean up in beginning of each task in Win CI (#7189)\n* Fix travis. (#7237)\n\n### Acknowledgement\n* **Contributors**: Adam Pocock (@Craigacp), Jeff H (@JeffHCross), Johan Hansson (@JohanWork), Jose Manuel Llorens (@JoseLlorensRipolles), Benjamin Szőke (@Livius90), @ReeceGoding, @ShvetsKS, Robert Zabel (@ZabelTech), Ali (@ali5h), Andrew Ziem (@az0), Andy Adinets (@canonizer), @david-cortes, Daniel Saxton (@dsaxton), Emil Sadek (@esadek), @farfarawayzyt, Gil Forsyth (@gforsyth), @giladmaya, @graue70, Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), José Morales (@jmoralez), Kai Fricke (@krfricke), Christian Lorentzen (@lorentzenchr), Mads R. B. Kristensen (@madsbk), Anton Kostin (@masguit42), Martin Petříček (@mpetricek-corp), @naveenkb, Taewoo Kim (@oOTWK), Viktor Szathmáry (@phraktle), Robert Maynard (@robertmaynard), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), Paul Taylor (@trxcllnt), @vslaykovsky, Bobby Wang (@wbo4958),\n* **Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Jose Manuel Llorens (@JoseLlorensRipolles), Kodi Arfer (@Kodiologist), Benjamin Szőke (@Livius90), Mark Guryanov (@MarkGuryanov), Rory Mitchell (@RAMitchell), @ReeceGoding, @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Ziem (@az0), @candalfigomoro, Andy Adinets (@canonizer), Dante Gama Dessavre (@dantegd), @david-cortes, Daniel Saxton (@dsaxton), @farfarawayzyt, Gil Forsyth (@gforsyth), Harutaka Kawamura (@harupy), Philip Hyunsu Cho (@hcho3), @jakirkham, James Lamb (@jameslamb), José Morales (@jmoralez), James Bourbeau (@jrbourbeau), Christian Lorentzen (@lorentzenchr), Martin Petříček (@mpetricek-corp), Nikolay Petrov (@napetrov), @naveenkb, Viktor Szathmáry (@phraktle), Robin Teuwens (@rteuwens), Yuan Tang (@terrytangyuan), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), @vkuzmin-uber, Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n\n## v1.4.2 (2021.05.13)\nThis is a patch release for Python package with following fixes:\n\n* Handle the latest version of cupy.ndarray in inplace_predict. (#6933)\n* Ensure output array from predict_leaf is (n_samples, ) when there's only 1 tree. 1.4.0 outputs (n_samples, 1). (#6889)\n* Fix empty dataset handling with multi-class AUC. (#6947)\n* Handle object type from pandas in inplace_predict. (#6927)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)\n* Add more tests for categorical data support (#6219)\n* [dask] Test for data initializaton. (#6226)\n* Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j (#6230)\n* Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j-gpu (#6233)\n* [CI] Reduce testing load with RMM (#6249)\n* [CI] Build a Python wheel for aarch64 platform (#6253)\n* [CI] Time the CPU tests on Jenkins. (#6257)\n* [CI] Skip Dask tests on ARM. (#6267)\n* Fix a typo in `is_arm()` in testing.py (#6271)\n* [CI] replace `egrep` with `grep -E` (#6287)\n* Support unity build. (#6295)\n* [CI] Mark flaky tests as XFAIL (#6299)\n* [CI] Use separate Docker cache for each CUDA version (#6305)\n* Added `USE_NCCL_LIB_PATH` option to enable user to set `NCCL_LIBRARY` during build  (#6310)\n* Fix flaky data initialization test. (#6318)\n* Add a badge for GitHub Actions (#6321)\n* Optional `find_package` for sanitizers. (#6329)\n* Use pytest conventions consistently in Python tests (#6337)\n* Fix missing space in warning message (#6340)\n* Update `custom_metric_obj.rst` (#6367)\n* [CI] Run R check with `--as-cran` flag on GitHub Actions (#6371)\n* [CI] Remove R check from Jenkins (#6372)\n* Mark GPU external memory test as XFAIL. (#6381)\n* [CI] Add noLD R test (#6382)\n* Fix MPI build. (#6403)\n* [CI] Upgrade to MacOS Mojave image (#6406)\n* Fix flaky sparse page dmatrix test. (#6417)\n* [CI] Upgrade cuDF and RMM to 0.17 nightlies (#6434)\n* [CI] Fix CentOS 6 Docker images (#6467)\n* [CI] Vendor libgomp in the manylinux Python wheel (#6461)\n* [CI] Hot fix for libgomp vendoring (#6482)\n\n### Maintenance: Clean up and merge the Rabit submodule (#6023, #6095, #6096, #6105, #6110, #6262, #6275, #6290)\n* The Rabit submodule is now maintained as part of the XGBoost codebase.\n* Tests for Rabit are now part of the test suites of XGBoost.\n* Rabit can now be built on the Windows platform.\n* We made various code re-formatting for the C++ code with clang-tidy.\n* Public headers of XGBoost no longer depend on Rabit headers.\n* Unused CMake targets for Rabit were removed.\n* Single-point model recovery has been dropped and removed from Rabit, simplifying the Rabit code greatly. The single-point model recovery feature has not been adequately maintained over the years.\n* We removed the parts of Rabit that were not useful for XGBoost.\n\n### Maintenance: Refactor code for legibility and maintainability\n* Unify CPU hist sketching (#5880)\n* [R] fix uses of 1:length(x) and other small things (#5992)\n* Unify evaluation functions. (#6037)\n* Make binary bin search reusable. (#6058)\n* Unify set index data. (#6062)\n* [R] Remove `stringi` dependency (#6109)\n* Merge extract cuts into QuantileContainer. (#6125)\n* Reduce C++ compiler warnings (#6197, #6198, #6213, #6286, #6325)\n* Cleanup Python code. (#6223)\n* Small cleanup to evaluator. (#6400)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 16}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 16, 'n_files_pr': 6, 'pr': 1389, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",50.502562284469604,726
Which helper function is used to generate default rangebreaks for datetime axes across the report's time-series plots?,The helper function used to generate default rangebreaks for datetime axes across the report's time-series plots in qlib is `guess_plotly_rangebreaks`. This function takes a pandas DatetimeIndex as input and returns the rangebreaks required to remove gaps in the index. It can be found in the `qlib.contrib.report.utils` module.,"Across the time‐series plots (e.g. in _group_return, _pred_autocorr, _pred_turnover, ic_figure, score_ic_graph), the default `rangebreaks` are generated by the helper function guess_plotly_rangebreaks.","['MODULE qlib.qlib.contrib.report.utils contains FUNCTION qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks\nThis function `guesses` the rangebreaks required to remove gaps in datetime index. It basically calculates the difference between a `continuous` datetime index and index given.  For more details on `rangebreaks` params in plotly, see https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks  Parameters ---------- dt_index: pd.DatetimeIndex The datetimes of the data.  Returns ------- the `rangebreaks` to be passed into plotly axis.', 'FUNCTION qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks has parameter or accepts argument qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks.dt_index\nThis function `guesses` the rangebreaks required to remove gaps in datetime index. It basically calculates the difference between a `continuous` datetime index and index given.  For more details on `rangebreaks` params in plotly, see https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks  Parameters ---------- dt_index: pd.DatetimeIndex The datetimes of the data.  Returns ------- the `rangebreaks` to be passed into plotly axis.', 'FUNCTION qlib.qlib.contrib.report.analysis_model.analysis_model_performance.model_performance_graph has parameter or accepts argument qlib.qlib.contrib.report.analysis_model.analysis_model_performance.model_performance_graph.show_nature_day\nModel performance  :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.        It is usually same as the label of model training(e.g. ""Ref($close, -2)/Ref($close, -1) - 1"").           .. code-block:: python              instrument  datetime        score       label             SH600004    2017-12-11  -0.013502       -0.013502                             2017-12-12  -0.072367       -0.072367                             2017-12-13  -0.068605       -0.068605                             2017-12-14  0.012440        0.012440                             2017-12-15  -0.102778       -0.102778   :param lag: `pred.groupby(level=\'instrument\', group_keys=False)[\'score\'].shift(lag)`. It will be only used in the auto-correlation computing. :param N: group number, default 5. :param reverse: if `True`, `pred[\'score\'] *= -1`. :param rank: if **True**, calculate rank ic. :param graph_names: graph names; default [\'cumulative_return\', \'pred_ic\', \'pred_autocorr\', \'pred_turnover\']. :param show_notebook: whether to display graphics in notebook, the default is `True`. :param show_nature_day: whether to display the abscissa of non-trading day. :param \\*\\*kwargs: contains some parameters to control plot style in plotly. Currently, supports    - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays :return: if show_notebook is True, display in notebook; else return `plotly.graph_objs.Figure` list.', 'METHOD qlib.qlib.data.dataset.handler.DataHandler.get_range_selector has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandler.get_range_selector.cur_date\nget range selector by number of periods  Args:     cur_date (pd.Timestamp or str): current date     periods (int): number of periods', 'FUNCTION qlib.qlib.contrib.report.analysis_model.analysis_model_performance.model_performance_graph has parameter or accepts argument qlib.qlib.contrib.report.analysis_model.analysis_model_performance.model_performance_graph.show_notebook\nModel performance  :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.        It is usually same as the label of model training(e.g. ""Ref($close, -2)/Ref($close, -1) - 1"").           .. code-block:: python              instrument  datetime        score       label             SH600004    2017-12-11  -0.013502       -0.013502                             2017-12-12  -0.072367       -0.072367                             2017-12-13  -0.068605       -0.068605                             2017-12-14  0.012440        0.012440                             2017-12-15  -0.102778       -0.102778   :param lag: `pred.groupby(level=\'instrument\', group_keys=False)[\'score\'].shift(lag)`. It will be only used in the auto-correlation computing. :param N: group number, default 5. :param reverse: if `True`, `pred[\'score\'] *= -1`. :param rank: if **True**, calculate rank ic. :param graph_names: graph names; default [\'cumulative_return\', \'pred_ic\', \'pred_autocorr\', \'pred_turnover\']. :param show_notebook: whether to display graphics in notebook, the default is `True`. :param show_nature_day: whether to display the abscissa of non-trading day. :param \\*\\*kwargs: contains some parameters to control plot style in plotly. Currently, supports    - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays :return: if show_notebook is True, display in notebook; else return `plotly.graph_objs.Figure` list.', 'CLASS qlib.scripts.data_collector.base.BaseCollector has method qlib.scripts.data_collector.base.BaseCollector.normalize_end_datetime', 'FUNCTION qlib.qlib.contrib.report.analysis_model.analysis_model_performance.model_performance_graph has parameter or accepts argument qlib.qlib.contrib.report.analysis_model.analysis_model_performance.model_performance_graph.graph_names\nModel performance  :param pred_label: index is **pd.MultiIndex**, index name is **[instrument, datetime]**; columns names is **[score, label]**.        It is usually same as the label of model training(e.g. ""Ref($close, -2)/Ref($close, -1) - 1"").           .. code-block:: python              instrument  datetime        score       label             SH600004    2017-12-11  -0.013502       -0.013502                             2017-12-12  -0.072367       -0.072367                             2017-12-13  -0.068605       -0.068605                             2017-12-14  0.012440        0.012440                             2017-12-15  -0.102778       -0.102778   :param lag: `pred.groupby(level=\'instrument\', group_keys=False)[\'score\'].shift(lag)`. It will be only used in the auto-correlation computing. :param N: group number, default 5. :param reverse: if `True`, `pred[\'score\'] *= -1`. :param rank: if **True**, calculate rank ic. :param graph_names: graph names; default [\'cumulative_return\', \'pred_ic\', \'pred_autocorr\', \'pred_turnover\']. :param show_notebook: whether to display graphics in notebook, the default is `True`. :param show_nature_day: whether to display the abscissa of non-trading day. :param \\*\\*kwargs: contains some parameters to control plot style in plotly. Currently, supports    - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays :return: if show_notebook is True, display in notebook; else return `plotly.graph_objs.Figure` list.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 8, 'n_files_pr': 5, 'pr': 1390, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",37.18725562095642,727
Under what conditions does Exchange.__init__ warn that the provided limit_threshold may be unreasonable for China and Taiwan regions?,"The Exchange.__init__ method in qlib will warn that the provided limit_threshold may be unreasonable for China and Taiwan regions if the limit_type is 'volume' or 'value'. This warning is likely due to specific regulations regarding trading volumes and values in these regions, which may not comply with the default limit thresholds used by Qlib. To avoid this warning, you should adjust your limit thresholds according to the regulations of the respective region.","In qlib/backtest/exchange.py (Exchange.__init__), after you compute  
  self.limit_type = self._get_limit_type(limit_threshold)  
you get the “may not be set to a reasonable value” warning whenever:

  1. limit_threshold is not None  
  2. self.limit_type == Exchange.LT_FLT (i.e. it’s being treated as a float)  
  3. abs(float(limit_threshold)) > 0.1  
  4. C.region is REG_CN or REG_TW","['CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.__init__\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.Exchange.limit_threshold', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.done\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.limit_threshold', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.reset\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.Exchange.limit_type', 'MODULE qlib.qlib.rl.order_execution.simulator_qlib contains CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1391, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",36.72312307357788,728
What shared approach do the min-max and z-score processors use to skip normalization for constant-value features?,"The min-max and z-score processors in qlib use the same approach to skip normalization for constant-value features. They both use robust statistics, where the mean is calculated as the median and the standard deviation is calculated as the Median Absolute Deviation (MAD) multiplied by 1.4826. This approach helps to reduce the impact of outliers on the normalization process.","Both MinMaxNorm.fit and ZScoreNorm.fit detect constant‐value columns (`min==max` or `std==0`) and then overwrite their scaling params to an identity transform (MinMaxNorm sets those columns’ min=0, max=1; ZScoreNorm sets mean=0, std=1). That way their shared normalize() call `(x–min)/(max–min)` or `(x–mean)/std` leaves constant features unchanged.","['CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.fit\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm inherits from or is a subclass of qlib.qlib.data.dataset.processor.Processor\nZScore Normalization', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.CSZScoreNorm\nCross Sectional ZScore Normalization', 'MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.robust_zscore\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.CSZScoreNorm inherits from or is a subclass of qlib.qlib.data.dataset.processor.Processor\nCross Sectional ZScore Normalization', 'CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.__call__\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Smarter choice of histogram construction for distributed `gpu_hist` (#4519)\n* Optimizations for quantization on device (#4572)\n* Introduce caching memory allocator to avoid latency associated with GPU memory allocation (#4554, #4615)\n* Optimize the initialization stage of the CPU `hist` algorithm for sparse datasets (#4625)\n* Prevent unnecessary data copies from GPU memory to the host (#4795)\n* Improve operation efficiency for single prediction (#5016)\n* Group builder modified for incremental building, to speed up building large `DMatrix` (#5098)\n\n### Bug-fixes\n* Eliminate `FutureWarning: Series.base is deprecated` (#4337)\n* Ensure pandas DataFrame column names are treated as strings in type error message (#4481)\n* [jvm-packages] Add back `reg:linear` for scala, as it is only deprecated and not meant to be removed yet (#4490)\n* Fix library loading for Cygwin users (#4499)\n* Fix prediction from loaded pickle (#4516)\n* Enforce exclusion between `pred_interactions=True` and `pred_interactions=True` (#4522)\n* Do not return dangling reference to local `std::string` (#4543)\n* Set the appropriate device before freeing device memory (#4566)\n* Mark `SparsePageDmatrix` destructor default. (#4568)\n* Choose the appropriate tree method only when the tree method is \'auto\' (#4571)\n* Fix `benchmark_tree.py` (#4593)\n* [jvm-packages] Fix silly bug in feature scoring (#4604)\n* Fix GPU predictor when the test data matrix has different number of features than the training data matrix used to train the model (#4613)\n* Fix external memory for get column batches. (#4622)\n* [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)\n* Fix early stopping in the Python package (#4638)\n* Fix AUC error in distributed mode caused by imbalanced dataset (#4645, #4798)\n* [jvm-packages] Expose `setMissing` method in `XGBoostClassificationModel` / `XGBoostRegressionModel` (#4643)\n* Remove initializing stringstream reference. (#4788)\n* [R] `xgb.get.handle` now checks all class listed of `object` (#4800)\n* Do not use `gpu_predictor` unless data comes from GPU (#4836)\n* Fix data loading (#4862)\n* Workaround `isnan` across different environments. (#4883)\n* [jvm-packages] Handle Long-type parameter (#4885)\n* Don\'t `set_params` at the end of `set_state` (#4947). Ensure that the model does not change after pickling and unpickling multiple times.\n* C++ exceptions should not crash OpenMP loops (#4960)\n* Fix `usegpu` flag in DART. (#4984)\n* Run training with empty `DMatrix` (#4990, #5159)\n* Ensure that no two processes can use the same GPU (#4990)\n* Fix repeated split and 0 cover nodes (#5010)\n* Reset histogram hit counter between multiple data batches (#5035)\n* Fix `feature_name` crated from int64index dataframe. (#5081)\n* Don\'t use 0 for ""fresh leaf"" (#5084)\n* Throw error when user attempts to use multi-GPU training and XGBoost has not been compiled with NCCL (#5170)\n* Fix metric name loading (#5122)\n* Quick fix for memory leak in CPU `hist` algorithm (#5153)\n* Fix wrapping GPU ID and prevent data copying (#5160)\n* Fix signature of Span constructor (#5166)\n* Lazy initialization of device vector, so that XGBoost compiled with CUDA can run on a machine without any GPU (#5173)\n* Model loading should not change system locale (#5314)\n* Distributed training jobs would sometimes hang; revert Rabit to fix this regression (dmlc/rabit#132, #5237)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 4, 'pr': 1398, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",34.764283657073975,729
How does the subprocess execution wrapper initialize the shared configuration before running the target function?,"The subprocess execution wrapper in qlib initializes the shared configuration before running the target function by passing the `qlib_config` to the `call_in_subproc` class through its `__init__` method, which takes two parameters: `func` (the function to be wrapped) and `qlib_config` (Qlib config for initialization in subprocess). This is done to initialize Qlib before running the target function. The `qlib_config` is used to set up the shared configuration for the subprocess execution.","In qlib/utils/paral.py the call_in_subproc wrapper does its config init in its _func_mod method. Right before calling your target function it does:

```python
if self.qlib_config is not None:
    C.register_from_C(self.qlib_config)
```

That in turn (in qlib/config.py::Config.register_from_C) copies your passed‐in config into the global C via set_conf_from_C, applies any logging_config with set_log_with_config, and finally calls C.register() — then your function runs in the fresh subprocess.","['MODULE xgboost.ops.script.run_clang_tidy contains FUNCTION xgboost.ops.script.run_clang_tidy.call\nSubprocess run wrapper.', 'METHOD qlib.qlib.utils.paral.call_in_subproc.__init__ has parameter or accepts argument qlib.qlib.utils.paral.call_in_subproc.__init__.func\nParameters ---------- func : Callable     the function to be wrapped  qlib_config : QlibConfig     Qlib config for initialization in subprocess  Returns ------- Callable', 'FUNCTION xgboost.ops.script.run_clang_tidy.call has parameter or accepts argument xgboost.ops.script.run_clang_tidy.call.args\nSubprocess run wrapper.', ""CLASS qlib.qlib.utils.paral.call_in_subproc has method qlib.qlib.utils.paral.call_in_subproc.__init__\nParameters ---------- func : Callable     the function to be wrapped  qlib_config : QlibConfig     Qlib config for initialization in subprocess  Returns ------- Callable\nWhen we repeatedly run functions, it is hard to avoid memory leakage. So we run it in the subprocess to ensure it is OK.  NOTE: Because local object can't be pickled. So we can't implement it via closure.       We have to implement it via callable Class"", 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.get_state\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'METHOD qlib.qlib.utils.paral.call_in_subproc.__init__ has parameter or accepts argument qlib.qlib.utils.paral.call_in_subproc.__init__.qlib_config\nParameters ---------- func : Callable     the function to be wrapped  qlib_config : QlibConfig     Qlib config for initialization in subprocess  Returns ------- Callable', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.done\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 1402, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.281320333480835,730
How does the parameter that controls forbidding all trades at price limits influence the direction argument used in tradability checks during sell and buy order generation?,"The parameter `forbid_all_trade_at_limit` influences the direction argument used in tradability checks during sell and buy order generation. If set to True, the strategy will not do any trade when price reaches limit up/down, even for sell at limit up nor buy at limit down. This means that when the price hits a limit up or down, no trades (buy or sell) will be executed, regardless of the direction argument used in tradability checks.","In TopkDropoutStrategy.generate_trade_decision (qlib/contrib/strategy/signal_strategy.py), the forbid_all_trade_at_limit flag controls what you pass as the direction arg to is_stock_tradable:

• if forbid_all_trade_at_limit=True, you call  
  direction=None  
  → no side is exempted, so hitting a limit blocks both buys and sells.  
• if forbid_all_trade_at_limit=False, you pass  
  direction=OrderDir.SELL (for sell orders) or  
  direction=OrderDir.BUY (for buy orders)  
  → allows selling at limit‐up and buying at limit‐down.","['METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.forbid_all_trade_at_limit\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.backtest.exchange.Exchange.check_stock_limit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.check_stock_limit.direction\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.n_drop\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.check_stock_limit\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.topk\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.hold_thresh\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.only_tradable\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 9, 'n_files_pr': 2, 'pr': 1407, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",40.063628911972046,731
How does the recorder handle and report a non-zero exit status from one of its subprocess calls when logging uncommitted code?,"When the recorder in qlib encounters a non-zero exit status from one of its subprocess calls while logging uncommitted code, it will log an error message with the details of the failure. The recorder uses mlflow to implement its functionality and logs artifacts from files or directories. When a non-zero exit status is encountered, the recorder will end the active run and log an error message.

You can handle this situation in your code by using a try-except block to catch any exceptions that occur during the execution of your subprocess call. If an exception occurs, you can log an error message with the details of the failure, end the active run using the `end_run()` method, and then print the error message.

```python
from qlib.workflow.recorder import Recorder

# Create a recorder instance
recorder = Recorder()

try:
    # Your subprocess call here
    subprocess.call([...])
except Exception as e:
    # Log the error and end the active run
    recorder.end_run()
    print(f""Error: {e}"")
```

Note: The above code is a simplified example and may need to be adapted to fit your specific use case.","In MLflowRecorder._log_uncommitted_code (qlib/workflow/recorder.py) each git call is wrapped in a try/except around subprocess.check_output. If check_output raises CalledProcessError (i.e. non-zero exit), it’s caught and an info-level log is emitted via logger.info, reporting the current working directory and the failed command.","['CLASS qlib.qlib.workflow.recorder.MLflowRecorder inherits from or is a subclass of qlib.qlib.workflow.recorder.Recorder\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'MODULE qlib.qlib.workflow.recorder contains GLOBAL_VARIABLE qlib.qlib.workflow.recorder.logger', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.get_local_dir\nThis function will return the directory path of this recorder.\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'MODULE qlib.qlib.utils.exceptions contains CLASS qlib.qlib.utils.exceptions.LoadObjectError\nError type for Recorder when can not load object', 'CLASS qlib.qlib.workflow.recorder.Recorder has method qlib.qlib.workflow.recorder.Recorder.end_run\nEnd an active Recorder.\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.', 'MODULE qlib.qlib.workflow.record_temp contains GLOBAL_VARIABLE qlib.qlib.workflow.record_temp.logger', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.MLflowRecorder.status\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1409, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",54.917991161346436,732
How does the on-policy pipeline reuse the same simulator factory for both training and backtesting?,"The on-policy pipeline in qlib reuses the same simulator factory for both training and backtesting by using a shared `seed_iterator` to generate seeds for the simulator factory. This allows the simulator factory to be reused across different processes, reducing the need for duplicate initialization and improving efficiency. The `EnvWrapper` class uses a `DataQueue` to share the `seed_iterator` among multiple workers, enabling efficient reuse of the simulator factory without reinitialization for each process.","In train_onpolicy.py (train_and_test), there is exactly one simulator‐factory defined:

• def _simulator_factory_simple(order: Order) → SingleAssetOrderExecutionSimple

That same closure is passed into both the train() call and the backtest() call as simulator_fn. Thus training and backtesting both use the identical factory, i.e. the same simulator instantiation logic.","['CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has method qlib.qlib.rl.utils.env_wrapper.EnvWrapper.reset\nTry to get a state from state queue, and init the simulator with this state. If the queue is exhausted, generate an invalid (nan) observation.\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.env_factory\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has field or attribute or property qlib.qlib.rl.utils.env_wrapper.aux_info_collector\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'MODULE qlib.qlib.backtest.utils contains CLASS qlib.qlib.backtest.utils.LevelInfrastructure\nlevel infrastructure is created by executor, and then shared to strategies on the same level', 'CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper inherits from or is a subclass of gym.gym.Env[qlib.rl.interpreter.ObsType.ObsType, qlib.rl.interpreter.PolicyActType.PolicyActType]\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.__init__\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has field or attribute or property qlib.qlib.rl.utils.env_wrapper.action_interpreter\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 22}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 22, 'n_files_pr': 9, 'pr': 1415, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",50.52504229545593,733
How is the order execution backtest pipeline structured from job scheduling through state updating to reward computation?,"The order execution backtest pipeline in qlib is structured as follows:

1.  Job Scheduling: The pipeline starts with job scheduling, which involves setting up the necessary parameters and configurations for the backtest.
2.  State Updating: The state of the environment is updated using the `SAOEStateAdapter` class, which maintains states of the environment based on execution results and additional information acquired from executors & exchange.
3.  Reward Computation: The reward function is used to compute the rewards for each step in the backtest process.

The `backtest` function in the `qlib.rl.trainer.api` module is responsible for running the backtest pipeline, which uses parameters such as simulator function, state interpreter, action interpreter, initial states, policy, logger, reward function, finite env type, and concurrency to set up the necessary components for the backtest.

Overall, the order execution backtest pipeline in qlib provides a flexible framework for testing and evaluating different reinforcement learning algorithms and strategies for order execution.","The end‐to‐end backtest is implemented in three pieces:

1. Job scheduling (qlib/rl/contrib/backtest.py:backtest)  
   – Read a global order file, extract the stock list, and fire off per‐stock jobs in Parallel (joblib) calling either single_with_simulator or single_with_collect_data_loop.  

2. State updating (qlib/rl/order_execution/strategy.py:SAOEStateAdapter.update)  
   – Each sub‐job drives an SAOEState through tick updates:  
     • Map execution results to per-minute volumes, fetch market price/volume from the exchange,  
     • Append to history_exec and history_steps,  
     • Decrease position and advance cur_time.  

3. Reward computation (qlib/rl/order_execution/reward.py:PPOReward.reward)  
   – At terminal step (or when position hits zero), compute VWAP vs TWAP from history_exec and backtest_data, then return –1/0/+1 based on their ratio.","['CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.__init__\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'MODULE qlib.qlib.rl.trainer.api contains FUNCTION qlib.qlib.rl.trainer.api.backtest\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.update\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.reward\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.saoe_state\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.state_interpreter\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'MODULE qlib.qlib.rl.order_execution.strategy contains CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 16}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 16, 'n_files_pr': 24, 'pr': 1447, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",58.95077967643738,734
How do the two data retrieval methods ensure consistent dtypes on the returned pandas Series?,"The two data retrieval methods in qlib ensure consistent dtypes on the returned pandas Series by using the `reindex` method, which allows for flexible and efficient re-indexing of the series. This approach enables efficient and flexible data retrieval and manipulation, making it ideal for large-scale financial data analysis tasks.

Here's an example of how this works:

```python
import qlib
from qlib.backtest import high_performance_ds

# Create a PandasSingleMetric instance
metric = high_performance_ds.PandasSingleMetric()

# Get the metric series
series = metric.get_metric_series()

# Re-index the series to ensure consistent dtypes
reindexed_series = series.reindex(index=[1, 2, 3])

print(reindexed_series)
```
","In qlib/data/data.py:

- In LocalPITProvider.period_feature, the raw values are read into a NumPy array with dtype = C.pit_record_type[""value""], and then returned via  
  `pd.Series(value, index=…, dtype=VALUE_DTYPE)`  
  so the Series is guaranteed to have exactly that dtype.

- In LocalExpressionProvider.expression, after loading the Series it does  
  ```
  try:
      series = series.astype(np.float32)
  except (ValueError, TypeError):
      pass
  ```  
  forcing (where possible) all columns to np.float32 for consistency.","['CLASS cognee.cognee.modules.retrieval.temporal_retriever.TemporalRetriever has method cognee.cognee.modules.retrieval.temporal_retriever.TemporalRetriever.descriptions_to_string\nHandles graph completion by generating responses based on a series of interactions with a language model. This class extends from GraphCompletionRetriever and is designed to manage the retrieval and validation process for user queries, integrating follow-up questions based on reasoning. The public methods are:  - get_completion  Instance variables include: - validation_system_prompt_path - validation_user_prompt_path - followup_system_prompt_path - followup_user_prompt_path', 'CLASS qlib.qlib.backtest.high_performance_ds.PandasSingleMetric has method qlib.qlib.backtest.high_performance_ds.PandasSingleMetric.reindex\nEach SingleMetric is based on pd.Series.', 'MODULE cognee.cognee.modules.retrieval.temporal_retriever contains CLASS cognee.cognee.modules.retrieval.temporal_retriever.TemporalRetriever\nHandles graph completion by generating responses based on a series of interactions with a language model. This class extends from GraphCompletionRetriever and is designed to manage the retrieval and validation process for user queries, integrating follow-up questions based on reasoning. The public methods are:  - get_completion  Instance variables include: - validation_system_prompt_path - validation_user_prompt_path - followup_system_prompt_path - followup_user_prompt_path', 'METHOD qlib.qlib.backtest.high_performance_ds.PandasOrderIndicator.get_metric_series has parameter or accepts argument qlib.qlib.backtest.high_performance_ds.PandasOrderIndicator.get_metric_series.metric', 'CLASS cognee.cognee.modules.retrieval.temporal_retriever.TemporalRetriever has method cognee.cognee.modules.retrieval.temporal_retriever.TemporalRetriever.__init__\nHandles graph completion by generating responses based on a series of interactions with a language model. This class extends from GraphCompletionRetriever and is designed to manage the retrieval and validation process for user queries, integrating follow-up questions based on reasoning. The public methods are:  - get_completion  Instance variables include: - validation_system_prompt_path - validation_user_prompt_path - followup_system_prompt_path - followup_user_prompt_path', 'CLASS qlib.qlib.backtest.high_performance_ds.PandasSingleMetric has method qlib.qlib.backtest.high_performance_ds.PandasSingleMetric.index\nEach SingleMetric is based on pd.Series.', 'CLASS cognee.cognee.modules.retrieval.graph_completion_cot_retriever.GraphCompletionCotRetriever has method cognee.cognee.modules.retrieval.graph_completion_cot_retriever.GraphCompletionCotRetriever.__init__\nHandles graph completion by generating responses based on a series of interactions with a language model. This class extends from GraphCompletionRetriever and is designed to manage the retrieval and validation process for user queries, integrating follow-up questions based on reasoning. The public methods are:  - get_completion  Instance variables include: - validation_system_prompt_path - validation_user_prompt_path - followup_system_prompt_path - followup_user_prompt_path', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1449, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",53.658239126205444,735
How do the experiment and recorder deletion routines validate input and convert API failures into user-friendly errors?,"The experiment and recorder deletion routines in qlib validate input by checking the type of the input parameters, converting API failures into user-friendly errors. They are designed to be used within the context of an experiment and can be utilized through classes such as `ExpManager` and `MLflowExperiment`. For example, you can create a new experiment using `exp = ExpManager().start_exp()`, delete a recorder with `MLflowExperiment.delete_recorder(recorder_id=","Both MLflowExpManager.delete_exp (qlib/workflow/expm.py) and MLflowExperiment.delete_recorder (qlib/workflow/exp.py) first use an assert to ensure you passed either an _id or a _name. They then wrap the MLflow client delete call in a try/except catching MlflowException, and re-raise it as a ValueError with a clear, user-facing message telling you to check that the experiment/recorder id or name is correct.","['CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.start_exp\nStart an experiment. This method includes first get_or_create an experiment, and then set it to be active.  Maintaining `_active_exp_uri` is included in start_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_id : str     id of the active experiment. experiment_name : str     name of the active experiment. recorder_id : str     id of the recorder to be started. recorder_name : str     name of the recorder to be started. uri : str     the current tracking URI. resume : boolean     whether to resume the experiment and recorder.  Returns ------- An active experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'METHOD qlib.qlib.workflow.exp.MLflowExperiment.delete_recorder has parameter or accepts argument qlib.qlib.workflow.exp.MLflowExperiment.delete_recorder.recorder_id', 'CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.start\nStart the experiment and set it to be active. This method will also start a new recorder.  Parameters ---------- recorder_id : str     the id of the recorder to be created. recorder_name : str     the name of the recorder to be created. resume : bool     whether to resume the first recorder  Returns ------- An active recorder.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)', 'METHOD qlib.qlib.workflow.exp.MLflowExperiment.delete_recorder has parameter or accepts argument qlib.qlib.workflow.exp.MLflowExperiment.delete_recorder.recorder_name', 'CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.list_recorders\nList all the existing recorders of this experiment. Please first get the experiment instance before calling this method. If user want to use the method `R.list_recorders()`, please refer to the related API document in `QlibRecorder`.  flt_kwargs : dict     filter recorders by conditions     e.g.  list_recorders(status=Recorder.STATUS_FI)  Returns ------- The return type depends on `rtype`     if `rtype` == ""dict"":         A dictionary (id -> recorder) of recorder information that being stored.     elif `rtype` == ""list"":         A list of Recorder.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)', 'CLASS qlib.qlib.utils.exceptions.RecorderInitializationError inherits from or is a subclass of qlib.qlib.utils.exceptions.QlibException\nError type for re-initialization when starting an experiment', 'CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.delete_recorder\nCreate a recorder for each experiment.  Parameters ---------- recorder_id : str     the id of the recorder to be deleted.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/data/README.md\nThis folder contains processed example dataset used by the demos.\nCopyright of the dataset belongs to the original copyright holder', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Usability Improvements, Documentation\n* [jvm-packages] add example to handle missing value other than 0 (#5677)\n* Add DMatrix usage examples to the C API demo (#5854)\n* List `DaskDeviceQuantileDMatrix` in the doc. (#5975)\n* Update Python custom objective demo. (#5981)\n* Update the JSON model schema to document more objective functions. (#5982)\n* [Python] Fix warning when `missing` field is not used. (#5969)\n* Fix typo in tracker logging (#5994)\n* Move a warning about empty dataset, so that it's shown for all objectives and metrics (#5998)\n* Fix the instructions for installing the nightly build. (#6004)\n* [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)\n* [jvm-packages] [doc] Update install doc for JVM packages (#6051)\n* Fix typo in `xgboost.callback.early_stop` docstring (#6071)\n* Add cache suffix to the files used in the external memory demo. (#6088)\n* [Doc] Document the parameter `kill_spark_context_on_worker_failure` (#6097)\n* Fix link to the demo for custom objectives (#6100)\n* Update Dask doc. (#6108)\n* Validate weights are positive values. (#6115)\n* Document the updated CMake version requirement. (#6123)\n* Add demo for `DaskDeviceQuantileDMatrix`. (#6156)\n* Cosmetic fixes in `faq.rst` (#6161)\n* Fix error message. (#6176)\n* [Doc] Add list of winning solutions in data science competitions using XGBoost (#6177)\n* Fix a comment in demo to use correct reference (#6190)\n* Update the list of winning solutions using XGBoost (#6192)\n* Consistent style for build status badge (#6203)\n* [Doc] Add info on GPU compiler (#6204)\n* Update the list of winning solutions (#6222, #6254)\n* Add link to XGBoost's Twitter handle (#6244)\n* Fix minor typos in XGBClassifier methods' docstrings (#6247)\n* Add sponsors link to FUNDING.yml (#6252)\n* Group CLI demo into subdirectory. (#6258)\n* Reduce warning messages from `gbtree`. (#6273)\n* Create a tutorial for using the C API in a C/C++ application (#6285)\n* Update plugin instructions for CMake build (#6289)\n* [doc] make Dask distributed example copy-pastable (#6345)\n* [Python] Add option to use `libxgboost.so` from the system path (#6362)\n* Fixed few grammatical mistakes in doc (#6393)\n* Fix broken link in CLI doc (#6396)\n* Improve documentation for the Dask API (#6413)\n* Revise misleading exception information: no such param of `allow_non_zero_missing` (#6418)\n* Fix CLI ranking demo. (#6439)\n* Fix broken links. (#6455)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @FelixYBW, Jack Dunn (@JackDunnNZ), Jean Lescut-Muller (@JeanLescut),  Boris Feld (@Lothiraldan), Nikhil Choudhary (@Nikhil1O1), Rory Mitchell (@RAMitchell), @ShvetsKS, Anthony D'Amato (@Totoketchup), @Wittty-Panda, neko (@akiyamaneko), Alexander Gugel (@alexanderGugel), @dependabot[bot], DIVYA CHAUHAN (@divya661), Daniel Steinberg (@dstein64), Akira Funahashi (@funasoul), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Hristo Iliev (@hiliev), Honza Sterba (@honzasterba), @hzy001, Igor Moura (@igormp), @jameskrach, James Lamb (@jameslamb), Naveed Ahmed Saleem Janvekar (@janvekarnaveed), Kyle Nicholson (@kylejn27), lacrosse91 (@lacrosse91), Christian Lorentzen (@lorentzenchr), Manikya Bardhan (@manikyabard), @nabokovas, John Quitto-Graham (@nvidia-johnq), @odidev, Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Tanuja Kirthi Doddapaneni (@tanuja3), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), Zeno Gantner (@zenogantner), zhang_jf (@zuston)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 5, 'pr': 1463, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",45.75211429595947,736
How does the scheduler.step call differ when a validation segment is available versus when it's not?,"When a validation segment is available, the scheduler.step call in qlib will be different from when it's not. In the case where a validation segment is available, the scheduler will use this information to adjust its step size and make decisions about which tasks to use and when to use them during training through bi-level optimization. The scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. On the other hand, when there is no validation segment available, the scheduler will not have this information to make decisions about which tasks to use and when to use them during training. In this case, the scheduler may rely on other factors such as the current training data and the model status to make its decisions. The main difference between the two cases is that in the first case, the scheduler has access to additional information (the validation segment) that it can use to inform its decisions, while in the second case, it does not have this information available.","In qlib/contrib/model/pytorch_nn.py inside DNNModelPytorch.fit:

• When you have a “valid” segment, on each eval‐step you do  
  auto_filter_kwargs(self.scheduler.step)(metrics=cur_loss_val, epoch=step)  
  (i.e. pass the validation loss as metrics).

• When you’re in retraining mode (no “valid”), you simply call  
  self.scheduler.step(epoch=step)  
  (no metrics arg).","['CLASS qlib.qlib.workflow.task.utils.TimeAdjuster has method qlib.qlib.workflow.task.utils.TimeAdjuster.shift\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal\nFind appropriate date and adjust date.', 'METHOD keras.keras.src.optimizers.schedules.learning_rate_schedule.InverseTimeDecay.__call__ has parameter or accepts argument keras.keras.src.optimizers.schedules.learning_rate_schedule.InverseTimeDecay.__call__.step', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.step\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', 'METHOD keras.keras.src.optimizers.schedules.learning_rate_schedule.PiecewiseConstantDecay.__call__ has parameter or accepts argument keras.keras.src.optimizers.schedules.learning_rate_schedule.PiecewiseConstantDecay.__call__.step', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.rtype\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', 'CLASS keras.keras.src.optimizers.schedules.learning_rate_schedule_test.TestFitLRSchedulesFlow has method keras.keras.src.optimizers.schedules.learning_rate_schedule_test.TestFitLRSchedulesFlow.test_fit_lr_correctness', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.seg\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 5, 'pr': 1470, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",51.63162612915039,737
What mechanism removes overlapping information from the model similarity matrix before constructing the meta-dataset?,"The mechanism that removes overlapping information from the model similarity matrix before constructing the meta-dataset in qlib is the `mask_overlap` function. This function masks overlap information data after a certain point (`self.name[end]`) with future info also considered as overlap info, approximately equal to the diagonal + horizon length of data being masked.

This process ensures that the model similarity matrix only includes relevant and non-overlapping information, which is essential for constructing an effective meta-dataset.",Overlaps are zeroed out in MetaDatasetDS._prepare_meta_ipt via the inner mask_overlap function. It computes a “truncated” end (using get_date_by_shift and self.trunc_days) and then calls pandas Series.mask((index >= start)&(index <= trunc_end)) on each column of the IC matrix to blank any overlapping entries.,"['CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset inherits from or is a subclass of utils.serial.Serializable\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'METHOD qlib.qlib.contrib.meta.data_selection.model.MetaModelDS.inference has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.model.MetaModelDS.inference.meta_dataset', 'MODULE qlib.qlib.model.meta.dataset contains CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'MODULE keras.keras.src.utils.dataset_utils contains GLOBAL_VARIABLE keras.keras.src.utils.dataset_utils.ensure_shape_similarity', 'CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has field or attribute or property qlib.qlib.model.meta.dataset.segments\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has field or attribute or property qlib.qlib.model.meta.dataset.MetaTaskDataset.segments\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.mask_overlap has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.mask_overlap.s\nmask overlap information data after self.name[end] with self.trunc_days that contains future info are also considered as overlap info  Approximately the diagnal + horizon length of data are masked.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.7 (2017.12.30)\n* **This version represents a major change from the last release (v0.6), which was released one year and half ago.**\n* Updated Sklearn API\n  - Add compatibility layer for scikit-learn v0.18: `sklearn.cross_validation` now deprecated\n  - Updated to allow use of all XGBoost parameters via `**kwargs`.\n  - Updated `nthread` to `n_jobs` and `seed` to `random_state` (as per Sklearn convention); `nthread` and `seed` are now marked as deprecated\n  - Updated to allow choice of Booster (`gbtree`, `gblinear`, or `dart`)\n  - `XGBRegressor` now supports instance weights (specify `sample_weight` parameter)\n  - Pass `n_jobs` parameter to the `DMatrix` constructor\n  - Add `xgb_model` parameter to `fit` method, to allow continuation of training\n* Refactored gbm to allow more friendly cache strategy\n  - Specialized some prediction routine\n* Robust `DMatrix` construction from a sparse matrix\n* Faster construction of `DMatrix` from 2D NumPy matrices: elide copies, use of multiple threads\n* Automatically remove nan from input data when it is sparse.\n  - This can solve some of user reported problem of istart != hist.size\n* Fix the single-instance prediction function to obtain correct predictions\n* Minor fixes\n  - Thread local variable is upgraded so it is automatically freed at thread exit.\n  - Fix saving and loading `count::poisson` models\n  - Fix CalcDCG to use base-2 logarithm\n  - Messages are now written to stderr instead of stdout\n  - Keep built-in evaluations while using customized evaluation functions\n  - Use `bst_float` consistently to minimize type conversion\n  - Copy the base margin when slicing `DMatrix`\n  - Evaluation metrics are now saved to the model file\n  - Use `int32_t` explicitly when serializing version\n  - In distributed training, synchronize the number of features after loading a data matrix.\n* Migrate to C++11\n  - The current master version now requires C++11 enabled compiled(g++4.8 or higher)\n* Predictor interface was factored out (in a manner similar to the updater interface).\n* Makefile support for Solaris and ARM\n* Test code coverage using Codecov\n* Add CPP tests\n* Add `Dockerfile` and `Jenkinsfile` to support continuous integration for GPU code\n* New functionality\n  - Ability to adjust tree model's statistics to a new dataset without changing tree structures.\n  - Ability to extract feature contributions from individual predictions, as described in [here](http://blog.datadive.net/interpreting-random-forests/) and [here](https://arxiv.org/abs/1706.06060).\n  - Faster, histogram-based tree algorithm (`tree_method='hist'`) .\n  - GPU/CUDA accelerated tree algorithms (`tree_method='gpu_hist'` or `'gpu_exact'`), including the GPU-based predictor.\n  - Monotonic constraints: when other features are fixed, force the prediction to be monotonic increasing with respect to a certain specified feature.\n  - Faster gradient calculation using AVX SIMD\n  - Ability to export models in JSON format\n  - Support for Tweedie regression\n  - Additional dropout options for DART: binomial+1, epsilon\n  - Ability to update an existing model in-place: this is useful for many applications, such as determining feature importance\n* Python package:\n  - New parameters:\n    - `learning_rates` in `cv()`\n    - `shuffle` in `mknfold()`\n    - `max_features` and `show_values` in `plot_importance()`\n    - `sample_weight` in `XGBRegressor.fit()`"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593).""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 14}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 14, 'n_files_pr': 14, 'pr': 1472, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.566771030426025,738
Which two subclasses of BaseIntradayProcessedData are used to load handler-based versus legacy pickle-formatted intraday data?,"The two subclasses of `BaseIntradayProcessedData` used to load handler-based versus legacy pickle-formatted intraday data are **HandlerIntradayProcessedData** and **PickleIntradayProcessedData**. These classes handle handler (bin format) style data and pickle-styled data, respectively.","The handler‐based loader is  `HandlerIntradayProcessedData` (via `load_handler_intraday_processed_data` in `qlib/rl/data/native.py`), whereas the legacy pickle‐formatted loader is  `PickleIntradayProcessedData` (via `load_pickle_intraday_processed_data` in `qlib/rl/data/pickle_styled.py`).","[""CLASS qlib.qlib.data.dataset.loader.DataLoaderDH inherits from or is a subclass of qlib.qlib.data.dataset.loader.DataLoader\nDataLoader is designed for loading raw data from original data source.\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook."", 'CLASS qlib.qlib.rl.data.pickle_styled.PickleIntradayProcessedData inherits from or is a subclass of qlib.rl.data.base.BaseIntradayProcessedData\nSubclass of IntradayProcessedData. Used to handle pickle-styled data.', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.config has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.config.processor_kwargs\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.', 'CLASS qlib.qlib.rl.data.native.HandlerIntradayProcessedData inherits from or is a subclass of base.BaseIntradayProcessedData\nSubclass of IntradayProcessedData. Used to handle handler (bin format) style data.', 'CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'CLASS qlib.qlib.rl.data.pickle_styled.PickleIntradayProcessedData has field or attribute or property qlib.qlib.rl.data.pickle_styled.PickleIntradayProcessedData.yesterday\nSubclass of IntradayProcessedData. Used to handle pickle-styled data.', 'CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.__init__\nParameters ---------- handler_config : dict     handler_config will be used to describe the handlers      .. code-block::          <handler_config> := {             ""group_name1"": <handler>             ""group_name2"": <handler>         }         or         <handler_config> := <handler>         <handler> := DataHandler Instance | DataHandler Config  fetch_kwargs : dict     fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.  is_group: bool     is_group will be used to describe whether the key of handler_config is group\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn\'t provide such interface & hook.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 30}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 30, 'n_files_pr': 16, 'pr': 1480, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",37.36351227760315,739
How does the training routine enable random action sampling for DQN policies when gathering experiences?,"qlib does not have a built-in method for enabling random action sampling for DQN policies, but you can modify the Collector class to achieve this functionality by introducing a new parameter or modifying an existing one. You can implement this by adding a mechanism to randomly select actions during the collection process, such as introducing a new parameter `random_action_prob` and using it to decide whether to take a random action or not. Here is an example of how you might implement this:

```python
def collect(self, policy, env, num_steps):
    actions = []
    for _ in range(num_steps):
        if np.random.rand() < self.random_action_prob:
            action = np.random.choice(env.action_space.n)
        else:
            action = policy.predict(env.state)
        actions.append(action)
        # ...
```

You would then need to modify the OnlineManager to use your custom Collector:

```python
online_manager = OnlineManager()
online_manager.simulate(
    end_time=""2022-01-01"",
    frequency=""1d"",
    task_kwargs={""collector"": CustomCollector},
    model_kwargs={},
    signal_kwargs={}
)
```

Please note that this is a simplified example and you may need to adjust the code based on your specific requirements.","The key is the `exploration_noise=True` flag you pass into the `Collector` in `TrainingVessel.train`:

```python
collector = Collector(
    self.policy, vector_env, VectorReplayBuffer(...), exploration_noise=True
)
```

That flag tells `Collector.collect()` (in `qlib/rl/collector/collector.py`) to call your DQN policy’s action method with exploration enabled (i.e. its ε-greedy sampler). The actual random action sampling logic lives in the DQN policy (e.g. `DQNPolicy.forward` / `DQNPolicy.act` in `qlib/rl/policy/dqn.py`).","['METHOD qlib.qlib.workflow.online.manager.OnlineManager.simulate has parameter or accepts argument qlib.qlib.workflow.online.manager.OnlineManager.simulate.task_kwargs\nStarting from the current time, this method will simulate every routine in OnlineManager until the end time.  Considering the parallel training, the models and signals can be prepared after all routine simulating.  The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.  Args:     end_time: the time the simulation will end     frequency: the calendar frequency     task_kwargs (dict): the params for `prepare_tasks`     model_kwargs (dict): the params for `prepare_online_models`     signal_kwargs (dict): the params for `prepare_signals`  Returns:     Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.     pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.load_state_dict\nLoad all states into current trainer.\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'METHOD qlib.qlib.workflow.online.manager.OnlineManager.simulate has parameter or accepts argument qlib.qlib.workflow.online.manager.OnlineManager.simulate.signal_kwargs\nStarting from the current time, this method will simulate every routine in OnlineManager until the end time.  Considering the parallel training, the models and signals can be prepared after all routine simulating.  The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.  Args:     end_time: the time the simulation will end     frequency: the calendar frequency     task_kwargs (dict): the params for `prepare_tasks`     model_kwargs (dict): the params for `prepare_online_models`     signal_kwargs (dict): the params for `prepare_signals`  Returns:     Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.     pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.get_policy_state_dict\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'METHOD qlib.qlib.workflow.online.manager.OnlineManager.simulate has parameter or accepts argument qlib.qlib.workflow.online.manager.OnlineManager.simulate.frequency\nStarting from the current time, this method will simulate every routine in OnlineManager until the end time.  Considering the parallel training, the models and signals can be prepared after all routine simulating.  The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.  Args:     end_time: the time the simulation will end     frequency: the calendar frequency     task_kwargs (dict): the params for `prepare_tasks`     model_kwargs (dict): the params for `prepare_online_models`     signal_kwargs (dict): the params for `prepare_signals`  Returns:     Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.     pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.__init__\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'METHOD qlib.qlib.workflow.online.manager.OnlineManager.simulate has parameter or accepts argument qlib.qlib.workflow.online.manager.OnlineManager.simulate.model_kwargs\nStarting from the current time, this method will simulate every routine in OnlineManager until the end time.  Considering the parallel training, the models and signals can be prepared after all routine simulating.  The delay training way can be ``DelayTrainer`` and the delay preparing signals way can be ``delay_prepare``.  Args:     end_time: the time the simulation will end     frequency: the calendar frequency     task_kwargs (dict): the params for `prepare_tasks`     model_kwargs (dict): the params for `prepare_online_models`     signal_kwargs (dict): the params for `prepare_signals`  Returns:     Union[pd.Series, pd.DataFrame]: pd.Series for only one signals every datetime.     pd.DataFrame for multiple signals, for example, buy and sell operations use different trading signals.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=""#main-challenges--solutions-in-quant-research"">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=""circle""><a href=""#forecasting-finding-valuable-signalspatterns"">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=""disc""><a href=""#quant-model-paper-zoo""><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=""circle""><a href=""#run-a-single-model"">Run a Single Model</a></li>\n                    <li type=""circle""><a href=""#run-multiple-models"">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=""circle""><a href=""#adapting-to-market-dynamics"">Adapting to Market Dynamics</a></li>\n          <li type=""circle""><a href=""#reinforcement-learning-modeling-continuous-decisions"">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=""align: center"">\n<img src=""docs/_static/img/framework-abstract.jpg"" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib\'s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1481, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",69.78053498268127,740
How does the data processing pipeline differ between independent and append modes?,"**Data Processing Pipeline Difference between Independent and Append Modes in qlib**

In independent mode, the data is processed separately for inference and learning, with different processors applied to each type of data.

In append mode, the data is processed together for both inference and learning, with all processors applied to the combined data. This allows for more flexibility in processing the data, but also requires careful consideration of how the different processors interact with each other.

The choice between independent and append modes will depend on the specific requirements of the project and the characteristics of the data being processed.","In DataHandlerLP.process_data (qlib/data/dataset/handler.py) the two modes differ only in how the “learn” branch is seeded:

• Independent (PTYPE_I):  
  – shared ⇒ _shared_df  
  – _infer = _shared_df → infer_processors  
  – _learn = _shared_df → learn_processors  

• Append (PTYPE_A):  
  – shared ⇒ _shared_df  
  – _infer = _shared_df → infer_processors  
  – _learn = _infer    → learn_processors  

In other words, in “independent” mode the learn‐ and infer‐pipelines both start from the shared output, whereas in “append” mode the learn‐pipeline is applied on top of the already inverted infer‐pipeline.","['CLASS qlib.qlib.data.dataset.__init__.DatasetH inherits from or is a subclass of qlib.qlib.data.dataset.__init__.Dataset\nPreparing data for model training and inferencing.\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'MODULE cognee.cognee.modules.data.processing.has_new_chunks contains GLOBAL_VARIABLE cognee.cognee.modules.data.processing.has_new_chunks.new_data_chunks', 'CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.__init__\nParameters ---------- infer_processors : list     - list of <description info> of processors to generate data for inference      - example of <description info>:      .. code-block::          1) classname & kwargs:             {                 ""class"": ""MinMaxNorm"",                 ""kwargs"": {                     ""fit_start_time"": ""20080101"",                     ""fit_end_time"": ""20121231""                 }             }         2) Only classname:             ""DropnaFeature""         3) object instance of Processor  learn_processors : list     similar to infer_processors, but for generating data for learning models  process_type: str     PTYPE_I = \'independent\'      - self._infer will be processed by infer_processors      - self._learn will be processed by learn_processors      PTYPE_A = \'append\'      - self._infer will be processed by infer_processors      - self._learn will be processed by infer_processors + learn_processors        - (e.g. self._infer processed by learn_processors ) drop_raw: bool     Whether to drop the raw data\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'MODULE cognee.cognee.modules.data.processing.has_new_chunks contains GLOBAL_VARIABLE cognee.cognee.modules.data.processing.has_new_chunks.existing_chunks_map', 'MODULE qlib.qlib.contrib.report.utils contains FUNCTION qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks\nThis function `guesses` the rangebreaks required to remove gaps in datetime index. It basically calculates the difference between a `continuous` datetime index and index given.  For more details on `rangebreaks` params in plotly, see https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks  Parameters ---------- dt_index: pd.DatetimeIndex The datetimes of the data.  Returns ------- the `rangebreaks` to be passed into plotly axis.', 'MODULE cognee.cognee.modules.data.processing.has_new_chunks contains GLOBAL_VARIABLE cognee.cognee.modules.data.processing.has_new_chunks.existing_chunks', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.get_update_data\nreturn the updated data based on the given dataset  The difference between `get_update_data` and `update` - `update_date` only include some data specific feature - `update` include some general routine steps(e.g. prepare dataset, checking)\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1485, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.67773699760437,741
What exception is raised when BASE_CONFIG_PATH cannot be found in either the current working directory or relative to the main config file?,"**Exception Raised When BASE_CONFIG_PATH Cannot Be Found**

When `BASE_CONFIG_PATH` cannot be found in either the current working directory or relative to the main config file, a **FileNotFoundError** is raised. This exception occurs due to the inability of the qlib library to locate the specified configuration path.

This behavior is attributed to the implementation of the `get_project_path` method within the qlib initialization module.","In qlib/workflow/cli.py’s `workflow()` function, if `BASE_CONFIG_PATH` isn’t found in either the CWD or relative to the main config, a `FileNotFoundError` is raised.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.qlib.config.QlibConfig has method qlib.qlib.config.QlibConfig.resolve_path', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'FUNCTION qlib.qlib.__init__.get_project_path has parameter or accepts argument qlib.qlib.__init__.get_project_path.cur_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.config contains GLOBAL_VARIABLE qlib.qlib.config.C', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/data_demo/README.md\n# Introduction\nThe examples in this folder try to demonstrate some common usage of data-related modules of Qlib']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1500, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",32.060993671417236,742
How do the two single-threaded backtest routines differ in how they obtain their strategy configurations and set up the executor before running the simulation?,"The two single-threaded backtest routines in qlib differ in how they obtain their strategy configurations and set up the executor before running the simulation. The first routine directly passes the strategy and executor configurations to the `backtest` function, while the second routine uses an intermediate step to retrieve these configurations before passing them to the `backtest` function. This can be seen in the code snippets provided below.

**Using the `backtest` function directly:**

```python
from qlib.backtest import backtest

# Define the strategy and executor configurations
strategy_config = {
    ""module_path"": ""qlib.contrib.modules.lightgbm"",
    ""kwargs"": {""lr"": 0.001, ""weight_file"": ""/path/to/model""}
}

executor_config = {
    ""module_path"": ""qlib.contrib.executors.executor"",
    ""kwargs"": {""exchange_id"": ""SSE"", ""symbol"": ""000001.SS""}
}

# Define the backtest parameters
start_time = pd.Timestamp(""2020-01-01"")
end_time = pd.Timestamp(""2020-12-31"")

benchmark = ""000300.SH""
account = 10000

# Perform the backtest
backtest_result = backtest(
    start_time=start_time,
    end_time=end_time,
    strategy=strategy_config,
    executor=executor_config,
    benchmark=benchmark,
    account=account,
)

print(backtest_result)
```

**Using the `get_strategy_executor` function:**

```python
from qlib.backtest import get_strategy_executor

# Define the strategy and executor configurations
strategy_config = {
    ""module_path"": ""qlib.contrib.modules.lightgbm"",
    ""kwargs"": {""lr"": 0.001, ""weight_file"": ""/path/to/model""}
}

executor_config = {
    ""module_path"": ""qlib.contrib.executors.executor"",
    ""kwargs"": {""exchange_id"": ""SSE"", ""symbol"": ""000001.SS""}
}

# Retrieve the strategy and executor configurations
config_dict = get_strategy_executor(strategy_config, executor_config)

# Perform the backtest
backtest_result = backtest(**config_dict)

print(backtest_result)
```

Note that these code snippets are simplified examples and may require additional modifications to work with your specific use case.","The two routines differ mainly in where they get their “strategy” and how they turn the executor config into a running simulator:

• single_with_simulator (backtest.py:single_with_simulator)  
  – Strategy config is taken straight from backtest_config[""strategies""] on each order.  
  – It calls _get_multi_level_executor_config(...) to build an executor_config dict, then hands that raw config to a new SingleAssetOrderExecution instance per order.  

• single_with_collect_data_loop (backtest.py:single_with_collect_data_loop)  
  – It first builds its own FileOrderStrategy config (wrapping the orders DataFrame and trade times).  
  – It still calls _get_multi_level_executor_config(...) once, but then passes both the FileOrderStrategy config and the executor_config into get_strategy_executor(...), which instantiates real Strategy and Executor objects that are driven by collect_data_loop over the full backtest period.","[""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.executor\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'FUNCTION qlib.qlib.backtest.__init__.get_strategy_executor has parameter or accepts argument qlib.qlib.backtest.__init__.get_strategy_executor.benchmark', ""MODULE qlib.qlib.backtest.__init__ contains FUNCTION qlib.qlib.backtest.__init__.backtest\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'FUNCTION qlib.qlib.backtest.__init__.get_strategy_executor has parameter or accepts argument qlib.qlib.backtest.__init__.get_strategy_executor.start_time', ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.pos_type\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'FUNCTION qlib.qlib.backtest.__init__.get_strategy_executor has parameter or accepts argument qlib.qlib.backtest.__init__.get_strategy_executor.end_time', ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.account\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 6, 'pr': 1503, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",93.01928806304932,743
How do the order ingestion and time-series sampler components normalize and sort the datetime index for fast retrieval?,"**Normalization and Sorting of Datetime Index for Fast Retrieval**

The order ingestion and time-series sampler components normalize and sort the datetime index by rearranging the multi-index of data_arr in (instrument, datetime) order. This is done to speed up querying by leveraging the characteristics of the original data.

The indices are designed to help users query time-series data efficiently, allowing for fast retrieval of specific samples or time-series data.","In FileOrderStrategy (qlib/contrib/strategy/rule_strategy.py) we do:

• Cast the “datetime” column to pandas.Timestamp  
• set_index([“datetime”, “instrument”]) so datetime is level 0  
• call convert_index_format(…, level=""datetime"") to unify the index dtype  
• call lazy_sort_index(…) to sort on datetime for fast lookups  

In the TSDataSampler (qlib/data/dataset/__init__.py) __init__ we:

• assert get_level_index(data, ""datetime"")==0  
• swaplevel() so instrument becomes level 0 and datetime level 1  
• sort_index() on the MultiIndex (instrument, datetime)  
…then build a numpy array and index maps for O(1) slicing.","['MODULE qlib.qlib.data.dataset.__init__ contains CLASS qlib.qlib.data.dataset.__init__.TSDataSampler\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has method qlib.qlib.data.dataset.__init__.TSDataSampler.get_index\nGet the pandas index of the data, it will be useful in following scenarios - Special sampler will be used (e.g. user want to sample day by day)\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.flt_data\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'CLASS qlib.scripts.data_collector.base.BaseCollector has method qlib.scripts.data_collector.base.BaseCollector.normalize_start_datetime', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.TSDataSampler.idx_map\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'MODULE qlib.qlib.contrib.data.dataset contains CLASS qlib.qlib.contrib.data.dataset.MTSDatasetH\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon     num_states (int): how many memory states to be added     memory_mode (str): memory mode (daily or sample)     batch_size (int): batch size (<0 will use daily sampling)     n_samples (int): number of samples in the same day     shuffle (bool): whether shuffle data     drop_last (bool): whether drop last batch < batch_size     input_size (int): reshape flatten rows as this input_size (backward compatibility)', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.TSDataSampler.data_index\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 1507, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",42.0842661857605,744
"When no password is specified in configuration, what value does the Redis task client receive for the password parameter?","When no password is specified in configuration, the Redis task client receives a default value for the password parameter. This can be inferred from the fact that the `password` parameter is optional in the `__init__` method of the `RedisAdapter` class.","The Redis client’s `password` arg is set to `C.redis_password` (in qlib/utils/__init__.py). Since `C.redis_password` defaults to None when you don’t specify one, the client gets `password=None`.","['METHOD cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__.password\nInitialize remote Kuzu database connection.  Args:     api_url: URL of the Kuzu REST API     username: Optional username for API authentication     password: Optional password for API authentication', 'CLASS cognee.cognee.infrastructure.databases.cache.redis.RedisAdapter.RedisAdapter has field or attribute or property cognee.cognee.infrastructure.databases.cache.redis.RedisAdapter.password', 'METHOD cognee.cognee.infrastructure.databases.cache.redis.RedisAdapter.RedisAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.cache.redis.RedisAdapter.RedisAdapter.__init__.password', 'METHOD cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__.api_url\nInitialize remote Kuzu database connection.  Args:     api_url: URL of the Kuzu REST API     username: Optional username for API authentication     password: Optional password for API authentication', 'METHOD cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__.username\nInitialize remote Kuzu database connection.  Args:     api_url: URL of the Kuzu REST API     username: Optional username for API authentication     password: Optional password for API authentication', 'MODULE cognee.cognee.infrastructure.databases.cache.redis.RedisAdapter contains GLOBAL_VARIABLE cognee.cognee.infrastructure.databases.cache.redis.RedisAdapter.user_id', 'METHOD graphiti.server.graph_service.zep_graphiti.ZepGraphiti.__init__ has parameter or accepts argument graphiti.server.graph_service.zep_graphiti.ZepGraphiti.__init__.password', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//SECURITY.md\n# Reporting Security Issues\nThe Cognee team takes security issues seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.\n\nTo report a security issue, email [info@topoteretes.com](mailto:info@topoteretes.com) and include the word ""SECURITY"" in the subject line.\n\nWe\'ll endeavor to respond quickly, and will keep you updated throughout the process.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1508, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",37.043190002441406,745
What mechanism does the multivariate time series dataset use to infer the forecast horizon from label definitions when no horizon parameter is provided?,"The multivariate time series dataset in qlib uses the `guess_horizon` function to infer the forecast horizon from label definitions when no horizon parameter is provided. This function tries to guess the horizon by parsing the label. The `guess_horizon` function is a built-in mechanism that allows qlib to automatically determine the forecast horizon based on the label definition, making it easier to work with multivariate time series datasets without having to specify the horizon manually.","When you pass `horizon=0` to `MTSDatasetH` (qlib/contrib/data/dataset.py), its constructor pulls the label expression out of `handler.data_loader.fields[""label""]` and calls the `guess_horizon` utility on it. Internally, `guess_horizon` (tested in `tests/misc/test_utils.py`) parses the label string for `Ref($…, -k)` patterns and returns the maximum lag `k` as the forecast horizon.","['CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH inherits from or is a subclass of qlib.data.dataset.DatasetH\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'FUNCTION qlib.qlib.utils.data.guess_horizon has parameter or accepts argument qlib.qlib.utils.data.guess_horizon.label\nTry to guess the horizon by parsing label', 'CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH has method qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH.setup_data\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.guess_horizon\nTry to guess the horizon by parsing label', 'CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH has method qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH.assign_data\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'METHOD qlib.qlib.contrib.model.linear.LinearModel.predict has parameter or accepts argument qlib.qlib.contrib.model.linear.LinearModel.predict.dataset', 'CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH has method qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH.clear_memory\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 6, 'pr': 1509, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.372724533081055,746
How does the code suppress the SettingWithCopyWarning during the groupwise z-score normalization?,"To suppress the SettingWithCopyWarning during groupwise z-score normalization in qlib, you can use the `robust_zscore` function from the `qlib.utils.data` module. This function uses robust statistics for Z-Score normalization, which is more resistant to outliers than the standard method.

Here's an example of how you can use this function:

```python
from qlib.utils.data import robust_zscore

# Assuming 'df' is your DataFrame and 'x' is the column you want to normalize
normalized_x = robust_zscore(df['x'], zscore=True)
```
This will perform the groupwise z-score normalization using robust statistics, suppressing any SettingWithCopyWarning.","In CSZScoreNorm.__call__ (qlib/data/dataset/processor.py) the group-wise z-score is wrapped in:

```python
with pd.option_context(""mode.chained_assignment"", None):
    …
```

This temporarily sets `mode.chained_assignment=None`, silencing the SettingWithCopyWarning.","['CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.fit\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.fields_group\nZScore Normalization', 'MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.robust_zscore\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.ZScoreNorm.ignore\nZScore Normalization', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.ZScoreNorm.fields_group\nZScore Normalization', 'CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.__init__\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Validation sets now support instance weights (#2354)\n  - `XGBClassifier.predict_proba()` should not support `output_margin` option. (#3343) See BREAKING CHANGES below.\n* R package:\n  - Better handling of NULL in `print.xgb.Booster()` (#3338)\n  - Comply with CRAN policy by removing compiler warning suppression (#3329)\n  - Updated CRAN submission\n* JVM packages\n  - JVM packages will now use the same versioning scheme as other packages (#3253)\n  - Update Spark to 2.3 (#3254)\n  - Add scripts to cross-build and deploy artifacts (#3276, #3307)\n  - Fix a compilation error for Scala 2.10 (#3332)\n* BREAKING CHANGES\n  - `XGBClassifier.predict_proba()` no longer accepts parameter `output_margin`. The parameter makes no sense for `predict_proba()` because the method is to predict class probabilities, not raw margin scores.\n\n## v0.71 (2018.04.11)\n* This is a minor release, mainly motivated by issues concerning `pip install`, e.g. #2426, #3189, #3118, and #3194.\n  With this release, users of Linux and MacOS will be able to run `pip install` for the most part.\n* Refactored linear booster class (`gblinear`), so as to support multiple coordinate descent updaters (#3103, #3134). See BREAKING CHANGES below.\n* Fix slow training for multiclass classification with high number of classes (#3109)\n* Fix a corner case in approximate quantile sketch (#3167). Applicable for 'hist' and 'gpu_hist' algorithms\n* Fix memory leak in DMatrix (#3182)\n* New functionality\n  - Better linear booster class (#3103, #3134)\n  - Pairwise SHAP interaction effects (#3043)\n  - Cox loss (#3043)\n  - AUC-PR metric for ranking task (#3172)\n  - Monotonic constraints for 'hist' algorithm (#3085)\n* GPU support\n    - Create an abstract 1D vector class that moves data seamlessly between the main and GPU memory (#2935, #3116, #3068). This eliminates unnecessary PCIe data transfer during training time.\n  - Fix minor bugs (#3051, #3217)\n  - Fix compatibility error for CUDA 9.1 (#3218)\n* Python package:\n  - Correctly handle parameter `verbose_eval=0` (#3115)\n* R package:\n  - Eliminate segmentation fault on 32-bit Windows platform (#2994)\n* JVM packages\n  - Fix a memory bug involving double-freeing Booster objects (#3005, #3011)\n  - Handle empty partition in predict (#3014)\n  - Update docs and unify terminology (#3024)\n  - Delete cache files after job finishes (#3022)\n  - Compatibility fixes for latest Spark versions (#3062, #3093)\n* BREAKING CHANGES: Updated linear modelling algorithms. In particular L1/L2 regularisation penalties are now normalised to number of training examples. This makes the implementation consistent with sklearn/glmnet. L2 regularisation has also been removed from the intercept. To produce linear models with the old regularisation behaviour, the alpha/lambda regularisation parameters can be manually scaled by dividing them by the number of training examples."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fix a unit test in `gpu_hist` (#4158)\n* Speed up data generation in Python tests (#4164)\n\n### Usability Improvements\n* Add link to [InfoWorld 2019 Technology of the Year Award](https://www.infoworld.com/article/3336072/application-development/infoworlds-2019-technology-of-the-year-award-winners.html) (#4116)\n* Remove outdated AWS YARN tutorial (#3885)\n* Document current limitation in number of features (#3886)\n* Remove unnecessary warning when `gblinear` is selected (#3888)\n* Document limitation of CSV parser: header not supported (#3934)\n* Log training parameters in XGBoost4J-Spark (#4091)\n* Clarify early stopping behavior in the scikit-learn interface (#3967)\n* Clarify behavior of `max_depth` parameter (#4078)\n* Revise Python docstrings for ranking task (#4121). In particular, weights must be per-group in learning-to-rank setting.\n* Document parameter `num_parallel_tree` (#4022)\n* Add Jenkins status badge (#4090)\n* Warn users against using internal functions of `Booster` object (#4066)\n* Reformat `benchmark_tree.py` to comply with Python style convention (#4126)\n* Clarify a comment in `objectiveTrait` (#4174)\n* Fix typos and broken links in documentation (#3890, #3872, #3902, #3919, #3975, #4027, #4156, #4167)\n\n### Acknowledgement\n**Contributors** (in no particular order): Jiaming Yuan (@trivialfis), Hyunsu Cho (@hcho3), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Yanbo Liang (@yanboliang), Andy Adinets (@canonizer), Tong He (@hetong007), Yuan Tang (@terrytangyuan)\n\n**First-time Contributors** (in no particular order): Jelle Zijlstra (@JelleZijlstra), Jiacheng Xu (@jiachengxu), @ajing, Kashif Rasul (@kashif), @theycallhimavi, Joey Gao (@pjgao), Prabakaran Kumaresshan (@nixphix), Huafeng Wang (@huafengw), @lyxthe, Sam Wilkinson (@scwilkinson), Tatsuhito Kato (@stabacov), Shayak Banerjee (@shayakbanerjee), Kodi Arfer (@Kodiologist), @KyleLi1985, Egor Smirnov (@SmirnovEgorRu), @tmitanitky, Pasha Stetsenko (@st-pasha), Kenichi Nagahara (@keni-chi), Abhai Kollara Dilip (@abhaikollara), Patrick Ford (@pford221), @hshujuan, Matthew Jones (@mt-jones), Thejaswi Rao (@teju85), Adam November (@anovember)\n\n**First-time Reviewers** (in no particular order): Mingyang Hu (@mingyang), Theodore Vasiloudis (@thvasilo), Jakub Troszok (@troszok), Rong Ou (@rongou), @Denisevi4, Matthew Jones (@mt-jones), Jeff Kaplan (@jeffdk)\n\n## v0.81 (2018.11.04)\n### New feature: feature interaction constraints\n* Users are now able to control which features (independent variables) are allowed to interact by specifying feature interaction constraints (#3466).\n* [Tutorial](https://xgboost.readthedocs.io/en/release_0.81/tutorials/feature_interaction_constraint.html) is available, as well as [R](https://github.com/dmlc/xgboost/blob/9254c58e4dfff6a59dc0829a2ceb02e45ed17cd0/R-package/demo/interaction_constraints.R) and [Python](https://github.com/dmlc/xgboost/blob/9254c58e4dfff6a59dc0829a2ceb02e45ed17cd0/tests/python/test_interaction_constraints.py) examples.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1513, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",43.06872630119324,747
What check prevents fetching daily data with an end date beyond today?,"The check that prevents fetching daily data with an end date beyond today in qlib is implemented in the `transform_end_date` function of the `utils` module. This function checks if the end date is -1, None, or greater than the maximum trading day, and returns the last trading date in such cases. Otherwise, it returns the end date.

Here's a code snippet that illustrates this:

```python
from qlib.utils import transform_end_date

end_date = pd.Timestamp('2023-12-31')  # today's date
max_trading_day = pd.Timestamp('2022-01-01')  # maximum trading day

# Check if end date is -1, None, or greater than max trading day
if end_date == -1 or end_date is None or end_date > max_trading_day:
    last_trading_date = pd.Timestamp('2022-12-31')  # last trading date
else:
    last_trading_date = transform_end_date(end_date)

print(last_trading_date)  # Output: 2022-12-31 00:00:00
```

This check prevents fetching daily data with an end date beyond today by ensuring that the end date is within the range of available trading days.","In scripts/data_collector/yahoo/collector.py (class Run.download_data) there’s a guard:

```
if self.interval == ""1d"" and
   pd.Timestamp(end) > pd.Timestamp(datetime.now().strftime(""%Y-%m-%d"")):
    raise ValueError(...)
```

This prevents fetching daily data with an end date beyond today.","[""CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.download_today_data\ndownload today data from Internet  Parameters ---------- max_collector_count: int     default 2 delay: float     time.sleep(delay), default 0.5 check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. limit_nums: int     using for debug, by default None  Notes -----     Download today's data:         start_time = datetime.datetime.now().date(); closed interval(including start)         end_time = pd.Timestamp(start_time + pd.Timedelta(days=1)).date(); open interval(excluding end)      check_data_length, example:         daily, one year: 252 // 4         us 1min, a week: 6.5 * 60 * 5         cn 1min, a week: 4 * 60 * 5  Examples ---------     # get daily data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1d     # get 1m data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1m"", 'CLASS cognee.cognee.modules.engine.models.Timestamp.Timestamp has field or attribute or property cognee.cognee.modules.engine.models.Timestamp.Timestamp.day', ""METHOD qlib.scripts.data_collector.yahoo.collector.Run.download_today_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.download_today_data.delay\ndownload today data from Internet  Parameters ---------- max_collector_count: int     default 2 delay: float     time.sleep(delay), default 0.5 check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. limit_nums: int     using for debug, by default None  Notes -----     Download today's data:         start_time = datetime.datetime.now().date(); closed interval(including start)         end_time = pd.Timestamp(start_time + pd.Timedelta(days=1)).date(); open interval(excluding end)      check_data_length, example:         daily, one year: 252 // 4         us 1min, a week: 6.5 * 60 * 5         cn 1min, a week: 4 * 60 * 5  Examples ---------     # get daily data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1d     # get 1m data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1m"", 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.transform_end_date\nhandle the end date with various format  If end_date is -1, None, or end_date is greater than the maximum trading day, the last trading date is returned. Otherwise, returns the end_date  ---------- end_date: str     end trading date date : pandas.Timestamp     current date', ""METHOD qlib.scripts.data_collector.yahoo.collector.Run.download_today_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.download_today_data.limit_nums\ndownload today data from Internet  Parameters ---------- max_collector_count: int     default 2 delay: float     time.sleep(delay), default 0.5 check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. limit_nums: int     using for debug, by default None  Notes -----     Download today's data:         start_time = datetime.datetime.now().date(); closed interval(including start)         end_time = pd.Timestamp(start_time + pd.Timedelta(days=1)).date(); open interval(excluding end)      check_data_length, example:         daily, one year: 252 // 4         us 1min, a week: 6.5 * 60 * 5         cn 1min, a week: 4 * 60 * 5  Examples ---------     # get daily data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1d     # get 1m data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1m"", 'FUNCTION qlib.qlib.utils.__init__.transform_end_date has parameter or accepts argument qlib.qlib.utils.__init__.log\nhandle the end date with various format  If end_date is -1, None, or end_date is greater than the maximum trading day, the last trading date is returned. Otherwise, returns the end_date  ---------- end_date: str     end trading date date : pandas.Timestamp     current date', ""METHOD qlib.scripts.data_collector.yahoo.collector.Run.download_today_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.download_today_data.check_data_length\ndownload today data from Internet  Parameters ---------- max_collector_count: int     default 2 delay: float     time.sleep(delay), default 0.5 check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. limit_nums: int     using for debug, by default None  Notes -----     Download today's data:         start_time = datetime.datetime.now().date(); closed interval(including start)         end_time = pd.Timestamp(start_time + pd.Timedelta(days=1)).date(); open interval(excluding end)      check_data_length, example:         daily, one year: 252 // 4         us 1min, a week: 6.5 * 60 * 5         cn 1min, a week: 4 * 60 * 5  Examples ---------     # get daily data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1d     # get 1m data     $ python collector.py download_today_data --source_dir ~/.qlib/stock_data/source --region CN --delay 0.1 --interval 1m"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Automatic update of daily frequency data(from yahoo finance)\n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --end_date <end date>\n      ```\n      * `end_date`: end of trading day(not included)\n      * `check_data_length`: check the number of rows per *symbol*, by default `None`\n        > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n\n  * `scripts/data_collector/yahoo/collector.py update_data_to_bin` parameters:\n      * `source_dir`: The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source""\n      * `normalize_dir`: Directory for normalize data, default ""Path(__file__).parent/normalize""\n      * `qlib_data_1d_dir`: the qlib data to be updated for yahoo, usually from: [download qlib data](https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)\n      * `end_date`: end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end)\n      * `region`: region, value from [""CN"", ""US""], default ""CN""\n      * `interval`: interval, default ""1d""(Currently only supports 1d data)\n      * `exists_skip`: exists skip, by default False\n\n## Using qlib data\n\n  ```python\n  import qlib\n  from qlib.data import D\n\n  # 1d data cn\n  # freq=day, freq default day\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data"", region=""cn"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data cn\n  # freq=1min\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n\n  # 1d data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data"", region=""us"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n  ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 4, 'pr': 1517, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",65.1312108039856,748
"After transposing the batch tensor, which slice is used as the label input for loss computation in both epochs?","The slice used as the label input for loss computation in both epochs is obtained using the `get_label` method of the `DMatrix` class in XGBoost, which suggests that this method is likely to be used for obtaining the label inputs for loss computation in both epochs.","In both train_epoch and test_epoch (qlib/contrib/model/pytorch_tcn_ts.py), the label is taken as the last time‐step, last feature slice:  
label = data[:, -1, -1]","['METHOD keras.keras.src.trainers.data_adapters.array_data_adapter.ArrayDataAdapter.slice_batch_indices has parameter or accepts argument keras.keras.src.trainers.data_adapters.array_data_adapter.ArrayDataAdapter.slice_batch_indices.indices\nConvert a Tensor of indices into a dataset of batched indices.  This step can be accomplished in several ways. The most natural is to slice the Tensor in a Dataset map. (With a condition on the upper index to handle the partial batch.) However it turns out that coercing the Tensor into a shape which is divisible by the batch size (and handling the last partial batch separately) allows for a much more favorable memory access pattern and improved performance.  Args:     indices: Tensor which determines the data order for an entire         epoch.  Returns:     A Dataset of batched indices.', 'MODULE keras.keras.src.ops.core contains FUNCTION keras.keras.src.ops.core.slice\nReturn a slice of an input tensor.  At a high level, this operation is an explicit replacement for array slicing e.g. `inputs[start_indices: start_indices + shape]`. Unlike slicing via brackets, this operation will accept tensor start indices on all backends, which is useful when indices dynamically computed via other tensor operations.  ```python inputs = np.zeros((5, 5)) start_indices = np.array([3, 3]) shape = np.array([2, 2]) inputs = keras.ops.slice(inputs, start_indices, shape) ```  Args:     inputs: A tensor, the tensor to be updated.     start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying         the starting indices for updating.     shape: The full shape of the returned slice.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'CLASS xgboost.python-package.xgboost.core.DMatrix has method xgboost.python-package.xgboost.core.DMatrix.get_label\nGet the label of the DMatrix.  Returns ------- label : array\nData Matrix used in XGBoost.  DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed.  You can construct DMatrix from multiple different sources of data.', 'CLASS keras.keras.src.trainers.data_adapters.array_data_adapter.ArrayDataAdapter has method keras.keras.src.trainers.data_adapters.array_data_adapter.ArrayDataAdapter.slice_batch_indices\nConvert a Tensor of indices into a dataset of batched indices.  This step can be accomplished in several ways. The most natural is to slice the Tensor in a Dataset map. (With a condition on the upper index to handle the partial batch.) However it turns out that coercing the Tensor into a shape which is divisible by the batch size (and handling the last partial batch separately) allows for a much more favorable memory access pattern and improved performance.  Args:     indices: Tensor which determines the data order for an entire         epoch.  Returns:     A Dataset of batched indices.\nAdapter for array-like objects, e.g. TF/JAX Tensors, NumPy arrays.', 'FUNCTION keras.keras.src.ops.core.slice has parameter or accepts argument keras.keras.src.ops.core.slice.shape\nReturn a slice of an input tensor.  At a high level, this operation is an explicit replacement for array slicing e.g. `inputs[start_indices: start_indices + shape]`. Unlike slicing via brackets, this operation will accept tensor start indices on all backends, which is useful when indices dynamically computed via other tensor operations.  ```python inputs = np.zeros((5, 5)) start_indices = np.array([3, 3]) shape = np.array([2, 2]) inputs = keras.ops.slice(inputs, start_indices, shape) ```  Args:     inputs: A tensor, the tensor to be updated.     start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying         the starting indices for updating.     shape: The full shape of the returned slice.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'FUNCTION keras.keras.src.ops.core.slice has parameter or accepts argument keras.keras.src.ops.core.slice.inputs\nReturn a slice of an input tensor.  At a high level, this operation is an explicit replacement for array slicing e.g. `inputs[start_indices: start_indices + shape]`. Unlike slicing via brackets, this operation will accept tensor start indices on all backends, which is useful when indices dynamically computed via other tensor operations.  ```python inputs = np.zeros((5, 5)) start_indices = np.array([3, 3]) shape = np.array([2, 2]) inputs = keras.ops.slice(inputs, start_indices, shape) ```  Args:     inputs: A tensor, the tensor to be updated.     start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying         the starting indices for updating.     shape: The full shape of the returned slice.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'FUNCTION keras.keras.src.ops.core.slice has parameter or accepts argument keras.keras.src.ops.core.slice.start_indices\nReturn a slice of an input tensor.  At a high level, this operation is an explicit replacement for array slicing e.g. `inputs[start_indices: start_indices + shape]`. Unlike slicing via brackets, this operation will accept tensor start indices on all backends, which is useful when indices dynamically computed via other tensor operations.  ```python inputs = np.zeros((5, 5)) start_indices = np.array([3, 3]) shape = np.array([2, 2]) inputs = keras.ops.slice(inputs, start_indices, shape) ```  Args:     inputs: A tensor, the tensor to be updated.     start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying         the starting indices for updating.     shape: The full shape of the returned slice.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1520, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",36.440913915634155,749
"Which variable holds the count of consecutive epochs without validation improvement, and which attribute defines the threshold for early stopping?",The variable that holds the count of consecutive epochs without validation improvement in qlib is 'early_stopping_patience'. The attribute that defines the threshold for early stopping is 'min_delta'.,"In Sandwich.fit (qlib/contrib/model/pytorch_sandwich.py) the local variable stop_steps tracks how many epochs have gone by without a validation improvement, and the model attribute self.early_stop defines the threshold for triggering early stopping.","['CLASS qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer has method qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.fit\nFits deep neural network for given training and validation data.  Args:   train_df: DataFrame for training data   valid_df: DataFrame for validation data\nDefines Temporal Fusion Transformer.  Attributes:   name: Name of model   time_steps: Total number of input time steps per forecast date (i.e. Width     of Temporal fusion decoder N)   input_size: Total number of inputs   output_size: Total number of outputs   category_counts: Number of categories per categorical variable   n_multiprocessing_workers: Number of workers to use for parallel     computations   column_definition: List of tuples of (string, DataType, InputType) that     define each column   quantiles: Quantiles to forecast for TFT   use_cudnn: Whether to use Keras CuDNNLSTM or standard LSTM layers   hidden_layer_size: Internal state size of TFT   dropout_rate: Dropout discard rate   max_gradient_norm: Maximum norm for gradient clipping   learning_rate: Initial learning rate of ADAM optimizer   minibatch_size: Size of minibatches for training   num_epochs: Maximum number of epochs for training   early_stopping_patience: Maximum number of iterations of non-improvement     before early stopping kicks in   num_encoder_steps: Size of LSTM encoder -- i.e. number of past time steps     before forecast date to use   num_stacks: Number of self-attention layers to apply (default is 1 for basic     TFT)   num_heads: Number of heads for interpretable mulit-head attention   model: Keras model for TFT', 'METHOD keras.keras.src.callbacks.early_stopping.EarlyStopping.on_epoch_end has parameter or accepts argument keras.keras.src.callbacks.early_stopping.EarlyStopping.on_epoch_end.logs', 'CLASS xgboost.python-package.xgboost.callback.EarlyStopping has method xgboost.python-package.xgboost.callback.EarlyStopping.get_s\nget score if it\'s cross validation history.\nCallback function for early stopping  .. versionadded:: 1.3.0  Parameters ---------- rounds :     Early stopping rounds. metric_name :     Name of metric that is used for early stopping. data_name :     Name of dataset that is used for early stopping. maximize :     Whether to maximize evaluation metric.  None means auto (discouraged). save_best :     Whether training should return the best model or the last model. If set to     `True`, it will only keep the boosting rounds up to the detected best iteration,     discarding the ones that come after. This is only supported with tree methods     (not `gblinear`). Also, the `cv` function doesn\'t return a model, the parameter     is not applicable. min_delta :      .. versionadded:: 1.5.0      Minimum absolute change in score to be qualified as an improvement.  Examples --------  .. code-block:: python      es = xgboost.callback.EarlyStopping(         rounds=2,         min_delta=1e-3,         save_best=True,         maximize=False,         data_name=""validation_0"",         metric_name=""mlogloss"",     )     clf = xgboost.XGBClassifier(tree_method=""hist"", device=""cuda"", callbacks=[es])      X, y = load_digits(return_X_y=True)     clf.fit(X, y, eval_set=[(X, y)])', ""CLASS qlib.qlib.rl.trainer.callbacks.EarlyStopping has method qlib.qlib.rl.trainer.callbacks.EarlyStopping.on_validate_end\nStop training when a monitored metric has stopped improving.  The earlystopping callback will be triggered each time validation ends. It will examine the metrics produced in validation, and get the metric with name ``monitor` (``monitor`` is ``reward`` by default), to check whether it's no longer increasing / decreasing. It takes ``min_delta`` and ``patience`` if applicable. If it's found to be not increasing / decreasing any more. ``trainer.should_stop`` will be set to true, and the training terminates.  Implementation reference: https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L1744-L1893"", 'MODULE keras.keras.src.backend.config contains FUNCTION keras.keras.src.backend.config.set_max_epochs\nLimit the maximum number of epochs for any call to fit.  This will cap the number of epochs for any training run using `model.fit()`. This is purely for debugging, and can also be set via the `KERAS_MAX_EPOCHS` environment variable to quickly run a script without modifying its source.  Args:     max_epochs: The integer limit on the number of epochs or `None`. If         `None`, no limit is applied.', 'METHOD keras.keras.src.callbacks.early_stopping.EarlyStopping.on_epoch_end has parameter or accepts argument keras.keras.src.callbacks.early_stopping.EarlyStopping.on_epoch_end.epoch', 'FUNCTION keras.keras.src.backend.config.set_max_epochs has parameter or accepts argument keras.keras.src.backend.config.set_max_epochs.max_epochs\nLimit the maximum number of epochs for any call to fit.  This will cap the number of epochs for any training run using `model.fit()`. This is purely for debugging, and can also be set via the `KERAS_MAX_EPOCHS` environment variable to quickly run a script without modifying its source.  Args:     max_epochs: The integer limit on the number of epochs or `None`. If         `None`, no limit is applied.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1529, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.7278265953064,750
What steps do the two handler constructors take to configure the data loader and processing pipelines before calling the base initializer?,"The two handler constructors in qlib take the following steps to configure the data loader and processing pipelines before calling the base initializer:

1. The `DataHandlerLP` constructor takes a `processor_kwargs` dictionary as an argument, which is used to configure the processors for loading and processing data.
2. The `DataLoaderDH` constructor takes a `handler_config` dictionary as an argument, which describes the handlers to be used for loading data. It also takes a `fetch_kwargs` dictionary as an argument, which describes the different arguments of the fetch method.

These configurations are then used by the base initializer to set up the data loader and processing pipelines.","Both `Alpha360.__init__` and `Alpha158.__init__` do the following before calling `super().__init__`:

1. Wrap and validate their processor lists via  
   ```python
   infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
   learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
   ```
2. Assemble a `data_loader` dict pointing to `""class"": ""QlibDataLoader""` with  
   – `config.feature` from `Alpha360DL.get_feature_config()` (Alpha360) or `self.get_feature_config()` (Alpha158)  
   – `config.label` from `kwargs.pop(""label"", self.get_label_config())`  
   – `filter_pipe`, `freq`, and `inst_processors`  
3. Pass `instruments`, `start_time`, `end_time`, the `data_loader`, and the processed `infer_processors`/`learn_processors` (plus `process_type` in Alpha158) into the base `DataHandlerLP` constructor.","['CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.__init__\nParameters ---------- handler_config : dict     handler_config will be used to describe the handlers      .. code-block::          <handler_config> := {             ""group_name1"": <handler>             ""group_name2"": <handler>         }         or         <handler_config> := <handler>         <handler> := DataHandler Instance | DataHandler Config  fetch_kwargs : dict     fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.  is_group: bool     is_group will be used to describe whether the key of handler_config is group\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn\'t provide such interface & hook.', 'CLASS qlib.qlib.data.dataset.__init__.DatasetH inherits from or is a subclass of qlib.qlib.data.dataset.__init__.Dataset\nPreparing data for model training and inferencing.\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'METHOD qlib.qlib.data.dataset.loader.DataLoaderDH.__init__ has parameter or accepts argument qlib.qlib.data.dataset.loader.DataLoaderDH.__init__.handler_config\nParameters ---------- handler_config : dict     handler_config will be used to describe the handlers      .. code-block::          <handler_config> := {             ""group_name1"": <handler>             ""group_name2"": <handler>         }         or         <handler_config> := <handler>         <handler> := DataHandler Instance | DataHandler Config  fetch_kwargs : dict     fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.  is_group: bool     is_group will be used to describe whether the key of handler_config is group', 'CLASS qlib.qlib.data.dataset.__init__.DatasetH has method qlib.qlib.data.dataset.__init__.DatasetH.setup_data\nSetup the Data  Parameters ---------- handler_kwargs : dict     init arguments of DataHandler, which could include the following arguments:      - init_type : Init Type of Handler      - enable_cache : whether to enable cache\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.config has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.config.processor_kwargs\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.', 'CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.cast\nMotivation  - A user creates a datahandler in his customized package. Then he wants to share the processed handler to   other users without introduce the package dependency and complicated data processing logic. - This class make it possible by casting the class to DataHandlerLP and only keep the processed data  Parameters ---------- handler : DataHandlerLP     A subclass of DataHandlerLP  Returns ------- DataHandlerLP:     the converted processed data\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md\nDefining a Custom Data Iterator to Load Data from External Memory\n=================================================================\n\nA simple demo for using custom data iterator with XGBoost.  The feature is still\n**experimental** and not ready for production use.  If you are not familiar with C API,\nplease read its introduction in our tutorials and visit the basic demo first.\n\nDefining Data Iterator\n----------------------\n\nIn the example, we define a custom data iterator with 2 methods: `reset` and `next`.  The\n`next` method passes data into XGBoost and tells XGBoost whether the iterator has reached\nits end, and the `reset` method resets iterations. One important detail when using the C\nAPI for data iterator is users need to make sure that the data passed into `next` method\nmust be kept in memory until the next iteration or `reset` is called.  The external memory\nDMatrix is not limited to training, but also valid for other features like prediction.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 6, 'pr': 1535, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.136054277420044,751
How does the 360 handler source its feature configuration differently from the 158 handler?,"The 360 handler sources its feature configuration differently from the 158 handler in qlib because it uses a different class, Alpha360, which is a subclass of DataHandlerLP. The Alpha360 class has a method called `get_feature_config` that returns the feature configuration. In contrast, the 158 handler likely uses the DataHandler class, which does not have a `get_feature_config` method. Instead, the DataHandler class has a `config` method that can be used to configure the data handler. Therefore, the 360 handler sources its feature configuration using the `get_feature_config` method of the Alpha360 class, while the 158 handler sources its feature configuration using the `config` method of the DataHandler class.","The Alpha360 handler doesn’t use its own `get_feature_config` – it pulls features directly from the DataLoader class via  
  Alpha360DL.get_feature_config()  
whereas Alpha158 calls its instance method  
  self.get_feature_config().","['METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.config has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.config.processor_kwargs\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.', 'CLASS qlib.qlib.contrib.data.handler.Alpha360 inherits from or is a subclass of data.dataset.handler.DataHandlerLP', 'CLASS qlib.qlib.data.dataset.handler.DataHandler has method qlib.qlib.data.dataset.handler.DataHandler.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'CLASS qlib.qlib.contrib.data.handler.Alpha360vwap inherits from or is a subclass of qlib.qlib.contrib.data.handler.Alpha360', 'CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.__init__\nParameters ---------- handler_config : dict     handler_config will be used to describe the handlers      .. code-block::          <handler_config> := {             ""group_name1"": <handler>             ""group_name2"": <handler>         }         or         <handler_config> := <handler>         <handler> := DataHandler Instance | DataHandler Config  fetch_kwargs : dict     fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.  is_group: bool     is_group will be used to describe whether the key of handler_config is group\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn\'t provide such interface & hook.', 'CLASS qlib.qlib.contrib.data.highfreq_handler.HighFreqHandler has method qlib.qlib.contrib.data.highfreq_handler.HighFreqHandler.get_feature_config', ""METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.from_df has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.from_df.cls\nMotivation: - When user want to get a quick data handler.  The created data handler will have only one shared Dataframe without processors. After creating the handler, user may often want to dump the handler for reuse Here is a typical use case  .. code-block:: python      from qlib.data.dataset import DataHandlerLP     dh = DataHandlerLP.from_df(df)     dh.to_pickle(fname, dump_all=True)  TODO: - The StaticDataLoader is quite slow. It don't have to copy the data again..."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LionOrCatThatIsTheQuestion, Hao Yang (@QuantHao), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Alex Wozniakowski (@a-wozniakowski), Amit Kumar (@aktech), Avinash Barnwal (@avinashbarnwal), @boxdot, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Ram Rachum (@cool-RR), Cristiano Goncalves (@cristianogoncalves), Elliot Hershberg (@elliothershberg), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), James Lamb (@jameslamb), James Bourbeau (@jrbourbeau), Lee Drake (@leedrake5), DougM (@mengdong), Oleksandr Kuvshynov (@okuvshynov), RongOu (@rongou), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), Yuan Tang (@terrytangyuan), Theodore Vasiloudis (@thvasilo), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10)\n\n## v1.1.1 (2020.06.06)\nThis patch release applies the following patches to 1.1.0 release:\n\n* CPU performance improvement in the PyPI wheels (#5720)\n* Fix loading old model (#5724)\n* Install pkg-config file (#5744)\n\n## v1.1.0 (2020.05.17)\n\n### Better performance on multi-core CPUs (#5244, #5334, #5522)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #5244 concludes the ongoing effort to improve performance scaling on multi-CPUs, in particular Intel CPUs. Roadmap: #5104\n* #5334 makes steps toward reducing memory consumption for the `hist` tree method on CPU.\n* #5522 optimizes random number generation for data sampling.\n\n### Deterministic GPU algorithm for regression and classification (#5361)\n* GPU algorithm for regression and classification tasks is now deterministic.\n* Roadmap: #5023. Currently only single-GPU training is deterministic. Distributed training with multiple GPUs is not yet deterministic.\n\n### Improve external memory support on GPUs (#5093, #5365)\n* Starting from 1.0.0 release, we added support for external memory on GPUs to enable training with larger datasets. Gradient-based sampling (#5093) speeds up the external memory algorithm by intelligently sampling a subset of the training data to copy into the GPU memory. [Learn more about out-of-core GPU gradient boosting.](https://arxiv.org/abs/2005.09148)\n* GPU-side data sketching now works with data from external memory (#5365).\n\n### Parameter validation: detection of unused or incorrect parameters (#5477, #5569, #5508)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. The 1.1.0 release makes parameter validation available to the scikit-learn interface (#5477) and the R binding (#5569).\n\n### Thread-safe, in-place prediction method (#5389, #5512)\n* Previously, the prediction method was not thread-safe (#5339). This release adds a new API function `inplace_predict()` that is thread-safe. It is now possible to serve concurrent requests for prediction using a shared model object.\n* It is now possible to compute prediction in-place for selected data formats (`numpy.ndarray` / `scipy.sparse.csr_matrix` / `cupy.ndarray` / `cudf.DataFrame` / `pd.DataFrame`) without creating a `DMatrix` object.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Notable bug fixes\n\nSome noteworthy bug fixes that are not related to specific language bindings are listed in this section.\n\n- Some language environments use a different thread to perform garbage collection, which breaks the thread-local cache used in XGBoost. XGBoost 2.0 implements a new thread-safe cache using a light weight lock to replace the thread-local cache. (#8851)\n- Fix model IO by clearing the prediction cache. (#8904)\n- `inf` is checked during data construction. (#8911)\n- Preserve order of saved updaters configuration. Usually, this is not an issue unless the `updater` parameter is used instead of the `tree_method` parameter (#9355)\n- Fix GPU memory allocation issue with categorical splits. (#9529)\n- Handle escape sequence like `\\t\\n` in feature names for JSON model dump. (#9474)\n- Normalize file path for model IO and text input. This handles short paths on Windows and paths that contain `~` on Unix (#9463). In addition, all path inputs are required to be encoded in UTF-8 (#9448, #9443)\n- Fix integer overflow on H100. (#9380)\n- Fix weighted sketching on GPU with categorical features. (#9341)\n- Fix metric serialization. The bug might cause some of the metrics to be dropped during evaluation. (#9405)\n- Fixes compilation errors on MSVC x86 targets (#8823)\n- Pick up the dmlc-core fix for the CSV parser. (#8897)\n\n\n### Documentation\nAside from documents for new features, we have many smaller updates to improve user experience, from troubleshooting guides to typo fixes.\n\n- Explain CPU/GPU interop. (#8450)\n- Guide to troubleshoot NCCL errors. (#8943, #9206)\n- Add a note for rabit port selection. (#8879)\n- How to build the docs using conda (#9276)\n- Explain how to obtain reproducible results on distributed systems. (#8903)\n\n* Fixes and small updates to document and demonstration scripts. (#8626, #8436, #8995, #8907, #8923, #8926, #9358, #9232, #9201, #9469, #9462, #9458, #8543, #8597, #8401, #8784, #9213, #9098, #9008, #9223, #9333, #9434, #9435, #9415, #8773, #8752, #9291, #9549)\n\n### Python package\n* New Features and Improvements\n- Support primitive types of pyarrow-backed pandas dataframe. (#8653)\n- Warning messages emitted by XGBoost are now emitted using Python warnings. (#9387)\n- User can now format the value printed near the bars on the `plot_importance` plot (#8540)\n- XGBoost has improved half-type support (float16) with pandas, cupy, and cuDF. With GPU input, the handling is through CUDA `__half` type, and no data copy is made. (#8487, #9207, #8481)\n- Support `Series` and Python primitive types in `inplace_predict` and `QuantileDMatrix` (#8547, #8542)\n- Support all pandas' nullable integer types. (#8480)\n- Custom metric with the scikit-learn interface now supports `sample_weight`. (#8706)\n- Enable Installation of Python Package with System lib in a Virtual Environment (#9349)\n- Raise if expected workers are not alive in `xgboost.dask.train` (#9421)\n\n* Optimization\n- Cache transformed data in `QuantileDMatrix` for efficiency. (#8666, #9445)\n- Take datatable as row-major input. (#8472)\n- Remove unnecessary conversions between data structures (#8546)\n\n* Adopt modern Python packaging conventions (PEP 517, PEP 518, PEP 621)\n-  XGBoost adopted the modern Python packaging conventions. The old setup script `setup.py` is now replaced with the new configuration file `pyproject.toml`. Along with this, XGBoost now supports Python 3.11. (#9021, #9112, #9114, #9115) Consult the latest documentation for the updated instructions to build and install XGBoost."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance\nMaintenance work includes refactoring, fixing small issues that don't affect end users. (#9256, #8627, #8756, #8735, #8966, #8864, #8747, #8892, #9057, #8921, #8949, #8941, #8942, #9108, #9125, #9155, #9153, #9176, #9447, #9444, #9436, #9438, #9430, #9200, #9210, #9055, #9014, #9004, #8999, #9154, #9148, #9283, #9246, #8888, #8900, #8871, #8861, #8858, #8791, #8807, #8751, #8703, #8696, #8693, #8677, #8686, #8665, #8660, #8386, #8371, #8410, #8578, #8574, #8483, #8443, #8454, #8733)\n\n### CI\n- Build pip wheel with RMM support (#9383)\n- Other CI updates including updating dependencies and work on the CI infrastructure. (#9464, #9428, #8767, #9394, #9278, #9214, #9234, #9205, #9034, #9104, #8878, #9294, #8625, #8806, #8741, #8707, #8381, #8382, #8388, #8402, #8397, #8445, #8602, #8628, #8583, #8460, #9544)\n\n## 1.7.6 (2023 Jun 16)\n\nThis is a patch release for bug fixes. The CRAN package for the R binding is kept at 1.7.5.\n\n### Bug Fixes\n* Fix distributed training with mixed dense and sparse partitions. (#9272)\n* Fix monotone constraints on CPU with large trees. (#9122)\n* [spark] Make the spark model have the same UID as its estimator (#9022)\n* Optimize prediction with `QuantileDMatrix`. (#9096)\n\n### Document\n* Improve doxygen (#8959)\n* Update the cuDF pip index URL. (#9106)\n\n### Maintenance\n* Fix tests with pandas 2.0. (#9014)\n\n## 1.7.5 (2023 Mar 30)\nThis is a patch release for bug fixes.\n\n* C++ requirement is updated to C++-17, along with which, CUDA 11.8 is used as the default CTK. (#8860, #8855, #8853)\n* Fix import for pyspark ranker. (#8692)\n* Fix Windows binary wheel to be compatible with Poetry (#8991)\n* Fix GPU hist with column sampling. (#8850)\n* Make sure iterative DMatrix is properly initialized. (#8997)\n* [R] Update link in document. (#8998)\n\n## 1.7.4 (2023 Feb 16)\nThis is a patch release for bug fixes.\n\n* [R] Fix OpenMP detection on macOS. (#8684)\n* [Python] Make sure input numpy array is aligned. (#8690)\n* Fix feature interaction with column sampling in gpu_hist evaluator. (#8754)\n* Fix GPU L1 error. (#8749)\n* [PySpark] Fix feature types param (#8772)\n* Fix ranking with quantile dmatrix and group weight. (#8762)\n\n## 1.7.3 (2023 Jan 6)\nThis is a patch release for bug fixes.\n\n* [Breaking] XGBoost Sklearn estimator method `get_params` no longer returns internally configured values. (#8634)\n* Fix linalg iterator, which may crash the L1 error. (#8603)\n* Fix loading pickled GPU model with a CPU-only XGBoost build. (#8632)\n* Fix inference with unseen categories with categorical features. (#8591, #8602)\n* CI fixes. (#8620, #8631, #8579)\n\n## v1.7.2 (2022 Dec 8)\nThis is a patch release for bug fixes.\n\n* Work with newer thrust and libcudacxx (#8432)\n* Support null value in CUDA array interface namespace. (#8486)\n* Use `getsockname` instead of `SO_DOMAIN` on AIX. (#8437)\n* [pyspark] Make QDM optional based on a cuDF check (#8471)\n* [pyspark] sort qid for SparkRanker. (#8497)\n* [dask] Properly await async method client.wait_for_workers. (#8558)\n\n* [R] Fix CRAN test notes. (#8428)\n\n* [doc] Fix outdated document [skip ci]. (#8527)\n* [CI] Fix github action mismatched glibcxx. (#8551)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 9, 'pr': 1537, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",46.58942914009094,752
"In HFSignalRecord.__init__, what happens to any extra keyword arguments passed in—are they forwarded to the parent initializer or dropped?","Extra keyword arguments passed to HFSignalRecord.__init__ are forwarded to the parent initializer, as there is no specific handling for extra keyword arguments and they are not explicitly dropped.","In HFSignalRecord.__init__ (qlib/workflow/record_temp.py), you accept **kwargs but only call  
```python
super().__init__(recorder=recorder)
```  
with no kwargs passed along. Any extra keyword args are simply dropped.","[""CLASS qlib.qlib.utils.serial.Serializable has method qlib.qlib.utils.serial.Serializable.to_pickle\nDump self to a pickle file.  path (Union[Path, str]): the path to dump  kwargs may include following keys      dump_all : bool         will the object dump all object     exclude : list         What attribute will not be dumped     include : list         What attribute will be dumped\nSerializable will change the behaviors of pickle.      The rule to tell if a attribute will be kept or dropped when dumping.     The rule with higher priorities is on the top     - in the config attribute list -> always dropped     - in the include attribute list -> always kept     - in the exclude attribute list -> always dropped     - name not starts with `_` -> kept     - name starts with `_` -> kept if `dump_all` is true else dropped  It provides a syntactic sugar for distinguish the attributes which user doesn't want. - For examples, a learnable Datahandler just wants to save the parameters without data when dumping to disk"", 'METHOD qlib.qlib.workflow.record_temp.HFSignalRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.HFSignalRecord.__init__.recorder', ""CLASS qlib.qlib.utils.serial.Serializable has method qlib.qlib.utils.serial.Serializable.config\nconfigure the serializable object  Parameters ---------- kwargs may include following keys      dump_all : bool         will the object dump all object     exclude : list         What attribute will not be dumped     include : list         What attribute will be dumped  recursive : bool     will the configuration be recursive\nSerializable will change the behaviors of pickle.      The rule to tell if a attribute will be kept or dropped when dumping.     The rule with higher priorities is on the top     - in the config attribute list -> always dropped     - in the include attribute list -> always kept     - in the exclude attribute list -> always dropped     - name not starts with `_` -> kept     - name starts with `_` -> kept if `dump_all` is true else dropped  It provides a syntactic sugar for distinguish the attributes which user doesn't want. - For examples, a learnable Datahandler just wants to save the parameters without data when dumping to disk"", 'METHOD qlib.qlib.workflow.record_temp.SigAnaRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.SigAnaRecord.__init__.skip_existing', ""CLASS keras.keras.src.wrappers.sklearn_wrapper.SKLBase has method keras.keras.src.wrappers.sklearn_wrapper.SKLBase.__init__\nBase class for scikit-learn wrappers.  Note that there are sources of randomness in model initialization and training. Refer to [Reproducibility in Keras Models]( https://keras.io/examples/keras_recipes/reproducibility_recipes/) on how to control randomness.  Args:     model: `Model`.         An instance of `Model`, or a callable returning such an object.         Note that if input is a `Model`, it will be cloned using         `keras.models.clone_model` before being fitted, unless         `warm_start=True`.         The `Model` instance needs to be passed as already compiled.         If callable, it must accept at least `X` and `y` as keyword         arguments. Other arguments must be accepted if passed as         `model_kwargs` by the user.     warm_start: bool, defaults to `False`.         Whether to reuse the model weights from the previous fit. If `True`,         the given model won't be cloned and the weights from the previous         fit will be reused.     model_kwargs: dict, defaults to `None`.         Keyword arguments passed to `model`, if `model` is callable.     fit_kwargs: dict, defaults to `None`.         Keyword arguments passed to `model.fit`. These can also be passed         directly to the `fit` method of the scikit-learn wrapper. The         values passed directly to the `fit` method take precedence over         these.  Attributes:     model_ : `Model`         The fitted model.     history_ : dict         The history of the fit, returned by `model.fit`."", 'METHOD qlib.qlib.workflow.record_temp.SignalRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.SignalRecord.__init__.recorder', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.search_records\nGet a pandas DataFrame of records that fit the search criteria.  The arguments of this function are not set to be rigid, and they will be different with different implementation of ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the example code of the method with the ``MLflowExpManager``:  .. code-block:: Python      R.log_metrics(m=2.50, step=0)     records = R.search_records([experiment_id], order_by=[""metrics.m DESC""])  Parameters ---------- experiment_ids : list     list of experiment IDs. filter_string : str     filter query string, defaults to searching all runs. run_view_type : int     one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType). max_results  : int     the maximum number of runs to put in the dataframe. order_by : list     list of columns to order by (e.g., “metrics.rmse”).  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don\'t have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nA global system that helps to manage the experiments.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 7, 'pr': 1542, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",36.73114037513733,753
"What high-level steps are executed when generating a portfolio analysis record, from loading predictions through backtesting to logging and saving artifacts?","**Generating Portfolio Analysis Record in qlib**

The high-level steps for generating a portfolio analysis record in qlib are as follows:

1. **Loading Predictions**: The `PortAnaRecord` class is used to generate the analysis results such as those of backtest.
2. **Backtesting**: The return report and detailed positions of the backtest are returned by `qlib/contrib/evaluate.py:backtest`.
3. **Risk Analysis**: The risk analysis of your portfolio is returned by `qlib/contrib/evaluate.py:risk_analysis`.
4. **Logging and Saving Artifacts**: The following files will be stored in recorder:
    *   `report_normal.pkl` and `positions_normal.pkl`: The return report and detailed positions of the backtest.
    *   `port_analysis.pkl`: The risk analysis of your portfolio.","In PortAnaRecord._generate (qlib/workflow/record_temp.py) the high-level flow is:

1. Load the saved predictions via self.load(""pred.pkl"").  
2. Fill any “<PRED>” placeholders in executor_config and strategy_config.  
3. Infer backtest start/end times from the prediction index if not explicitly set.  
4. Call normal_backtest(...) with executor_config, strategy_config and backtest_config to get  
   • portfolio_metric_dict (reports + positions)  
   • indicator_dict (indicator series + objects)  
5. Loop over portfolio_metric_dict to stash report_normal_*.pkl and positions_normal_*.pkl artifacts.  
6. Loop over indicator_dict to stash indicators_normal_*.pkl and indicators_normal_*_obj.pkl artifacts.  
7. For each freq in risk_analysis_freq:  
   – Compute excess_return_with/without cost via risk_analysis(...)  
   – Log flattened risk‐metrics via self.recorder.log_metrics  
   – Save port_analysis_{freq}.pkl and print summaries  
8. For each freq in indicator_analysis_freq:  
   – Compute indicator_analysis(...)  
   – Log metrics, save indicator_analysis_{freq}.pkl and print the table  
9. Return the dict of all artifact objects.","['CLASS qlib.qlib.workflow.record_temp.PortAnaRecord inherits from or is a subclass of qlib.qlib.workflow.record_temp.ACRecordTemp\nAutomatically checking record template\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'MODULE qlib.qlib.contrib.evaluate_portfolio contains FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_daily_return_series_from_positions\nParameters generate daily return series from  position view positions: positions generated by strategy init_asset_value : init asset value return: pd.Series of daily return , return_series[date] = daily return rate', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""CLASS qlib.qlib.backtest.report.PortfolioMetrics has method qlib.qlib.backtest.report.PortfolioMetrics.generate_portfolio_metrics_dataframe\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""CLASS qlib.qlib.backtest.report.PortfolioMetrics has method qlib.qlib.backtest.report.PortfolioMetrics.load_portfolio_metrics\nload pm from a file should have format like columns = ['account', 'return', 'total_turnover', 'turnover', 'cost', 'total_cost', 'value', 'cash', 'bench']     :param         path: str/ pathlib.Path()\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has field or attribute or property qlib.qlib.workflow.record_temp.PortAnaRecord.all_freq\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n# Benchmarks Performance\nThis page lists a batch of methods designed for alpha seeking. Each method tries to give scores/predictions for all stocks each day(e.g. forecasting the future excess return of stocks). The scores/predictions of the models will be used as the mined alpha. Investing in stocks with higher scores is expected to yield more profit.  \n\nThe alpha is evaluated in two ways.\n1. The correlation between the alpha and future return.\n1. Constructing portfolio based on the alpha and evaluating the final total return.\n   - The explanation of metrics can be found [here](https://qlib.readthedocs.io/en/latest/component/report.html#id4)\n\nHere are the results of each benchmark model running on Qlib's `Alpha360` and `Alpha158` dataset with China's A shared-stock & CSI300 data respectively. The values of each metric are the mean and std calculated based on 20 runs with different random seeds.\n\nThe numbers shown below demonstrate the performance of the entire `workflow` of each model. We will update the `workflow` as well as models in the near future for better results.\n<!-- \n> If you need to reproduce the results below, please use the **v1** dataset: `python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --version v1`\n>\n> In the new version of qlib, the default dataset is **v2**. Since the data is collected from the YahooFinance API (which is not very stable), the results of *v2* and *v1* may differ -->\n\n> NOTE:\n> The backtest start from 0.8.0 is quite different from previous version. Please check out the changelog for the difference.\n\n> NOTE:\n> We have very limited resources to implement and finetune the models. We tried our best effort to fairly compare these models.  But some models may have greater potential than what it looks like in the table below.  Your contribution is highly welcomed to explore their potential.\n\n## Results on CSI300\n\n### Alpha158 dataset\n\n| Model Name                               | Dataset                             | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------------------------------------|-------------------------------------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| TCN(Shaojie Bai, et al.)                 | Alpha158                            | 0.0279±0.00 | 0.2181±0.01 | 0.0421±0.00 | 0.3429±0.01 | 0.0262±0.02       | 0.4133±0.25       | -0.1090±0.03 |\n| TabNet(Sercan O. Arik, et al.)           | Alpha158                            | 0.0204±0.01 | 0.1554±0.07 | 0.0333±0.00 | 0.2552±0.05 | 0.0227±0.04       | 0.3676±0.54       | -0.1089±0.08 |\n| Transformer(Ashish Vaswani, et al.)      | Alpha158                            | 0.0264±0.00 | 0.2053±0.02 | 0.0407±0.00 | 0.3273±0.02 | 0.0273±0.02       | 0.3970±0.26       | -0.1101±0.02 |\n| GRU(Kyunghyun Cho, et al.)               | Alpha158(with selected 20 features) | 0.0315±0.00 | 0.2450±0.04 | 0.0428±0.00 | 0.3440±0.03 | 0.0344±0.02       | 0.5160±0.25       | -0.1017±0.02 |\n| LSTM(Sepp Hochreiter, et al.)            | Alpha158(with selected 20 features) | 0.0318±0.00 | 0.2367±0.04 | 0.0435±0.00 | 0.3389±0.03 | 0.0381±0.03       | 0.5561±0.46       | -0.1207±0.04 |\n| Localformer(Juyong Jiang, et al.)        | Alpha158                            | 0.0356±0.00 | 0.2756±0.03 | 0.0468±0.00 | 0.3784±0.03 | 0.0438±0.02       | 0.6600±0.33       | -0.0952±0.02 |""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 1546, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",62.54098892211914,754
What sequence of operations does invoking qlib_data trigger to download and extract a dataset?,"### Sequence of Operations for Invoking qlib_data

When you invoke `qlib_data`, the following sequence of operations occurs:

1. The user runs the command `python get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn` (or any other region).

This command downloads the data from the specified region (in this case, China) and saves it to the target directory (`~/.qlib/qlib_data/cn_data`).

2. The downloaded data is in a compressed format (.zip file), which needs to be extracted.

3. After extraction, the data is saved in the qlib format, which can be used for further analysis.

Note that this process assumes that the user has already installed the necessary dependencies and has set up their environment correctly. Additionally, this process only downloads the latest available data; if you want to download historical data, you will need to specify the start date and end date in the command.","Calling GetData().qlib_data(...) does the following in order:

1. In qlib_data():
   - Build a versioned file name via the inner _get_file_name_with_version (using your `name`, `region`, `interval` and `qlib.__version__`), then call check_dataset(); if it returns 404 you rebuild the name with `""latest""`.
2. download_data(file_name, target_dir, delete_old=True):
   - Expand/mkdir target_dir.
   - Prepend a timestamp to the zip’s basename to form target_path.
   - Call merge_remote_url(file_name) to get the full URL.
   - download(url, target_path): HTTP‐GET + stream into target_path with a tqdm progress bar.
   - _unzip(target_path, target_dir, delete_old):
     • If delete_old, call _delete_qlib_data(target_dir) to remove old subdirs (`features`, `calendars`, etc.), prompting the user.
     • Open the ZIP and extract all entries into target_dir.
   - If self.delete_zip_file=True, remove the downloaded ZIP.

All these methods live in qlib/tests/data.py under class GetData.","['METHOD qlib.qlib.tests.data.GetData.qlib_data has parameter or accepts argument qlib.qlib.tests.data.GetData.qlib_data.name\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.load_dataset\nload dataset from multiple file formats', 'METHOD qlib.qlib.tests.data.GetData.qlib_data has parameter or accepts argument qlib.qlib.tests.data.GetData.qlib_data.exists_skip\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', 'METHOD qlib.qlib.tests.data.GetData.qlib_data has parameter or accepts argument qlib.qlib.tests.data.GetData.qlib_data.target_dir\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerABC.DK_I', 'METHOD qlib.qlib.tests.data.GetData.qlib_data has parameter or accepts argument qlib.qlib.tests.data.GetData.qlib_data.version\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 7, 'pr': 1558, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",54.970561265945435,755
What outputs does the initial signal recorder generate for model predictions and which of these does the analysis recorder load?,"The initial signal recorder generates two types of outputs for model predictions: prediction scores and positions. The analysis recorder loads the prediction score output, which can be retrieved using the `SignalRecord` class in qlib as shown below:

``
from qlib import SignalRecord
recorder = SignalRecord.load('path_to_your_recorder.pkl')
predictions = recorder.get_pred()
``","In qlib/workflow/record_temp.py:

• SignalRecord.generate() saves  
  – “pred.pkl” (the model’s prediction scores)  
  – “label.pkl” (the raw labels, if using a DatasetH)  

• HFSignalRecord.generate() then calls  
  – load(""pred.pkl"")  
  – load(""label.pkl"")","['MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord\nThis is the Multiple Pass Portfolio Analysis Record class that run backtest multiple times and generates the analysis results such as those of backtest. This class inherits the ``PortAnaRecord`` class.  If shuffle_init_score enabled, the prediction score of the first backtest date will be shuffled, so that initial position will be random. The shuffle_init_score will only works when the signal is used as <PRED> placeholder. The placeholder will be replaced by pred.pkl saved in recorder.  Parameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.', 'METHOD qlib.qlib.contrib.workflow.record_temp.SignalMseRecord.__init__ has parameter or accepts argument qlib.qlib.contrib.workflow.record_temp.SignalMseRecord.__init__.recorder', 'CLASS qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord has method qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord.random_init\nThis is the Multiple Pass Portfolio Analysis Record class that run backtest multiple times and generates the analysis results such as those of backtest. This class inherits the ``PortAnaRecord`` class.  If shuffle_init_score enabled, the prediction score of the first backtest date will be shuffled, so that initial position will be random. The shuffle_init_score will only works when the signal is used as <PRED> placeholder. The placeholder will be replaced by pred.pkl saved in recorder.  Parameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.', 'METHOD qlib.qlib.workflow.record_temp.SignalRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.SignalRecord.__init__.recorder', 'CLASS qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord has method qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord.__init__\nParameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.\nThis is the Multiple Pass Portfolio Analysis Record class that run backtest multiple times and generates the analysis results such as those of backtest. This class inherits the ``PortAnaRecord`` class.  If shuffle_init_score enabled, the prediction score of the first backtest date will be shuffled, so that initial position will be random. The shuffle_init_score will only works when the signal is used as <PRED> placeholder. The placeholder will be replaced by pred.pkl saved in recorder.  Parameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.', 'CLASS qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord has method qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord.list\nThis is the Multiple Pass Portfolio Analysis Record class that run backtest multiple times and generates the analysis results such as those of backtest. This class inherits the ``PortAnaRecord`` class.  If shuffle_init_score enabled, the prediction score of the first backtest date will be shuffled, so that initial position will be random. The shuffle_init_score will only works when the signal is used as <PRED> placeholder. The placeholder will be replaced by pred.pkl saved in recorder.  Parameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.', 'METHOD qlib.qlib.contrib.workflow.record_temp.MultiSegRecord.__init__ has parameter or accepts argument qlib.qlib.contrib.workflow.record_temp.MultiSegRecord.__init__.recorder', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 8, 'pr': 1568, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.02723932266235,756
What is the sequence of method calls from constructing the download URL to unzipping the retrieved dataset?,"**Sequence of Method Calls for Constructing Download URL and Unzipping Dataset in qlib**

1. `GetData.download(url, target_path)`: This method constructs the download URL and starts the download process.
2. `dataset.return_uri`: This field returns the URI of the downloaded file.
3. `get_frontend_download_info()`: This function gets the download URL and version for the actual source.
4. `ExtractArchiveTest.create_zip()`: This method creates a zip archive from the downloaded file.

The dataset is unzipped using the following command:
`tar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1`
","When you call GetData.download_data, it does the following in order:

1. GetData.merge_remote_url(file_name) – builds the download URL  
2. GetData.download(url, target_path) – streams the ZIP to disk  
3. GetData._unzip(target_path, target_dir, delete_old) – if delete_old=True, calls GetData._delete_qlib_data(target_dir), then opens the ZIP with zipfile.ZipFile and extracts each file.","['CLASS qlib.qlib.tests.data.GetData has method qlib.qlib.tests.data.GetData.download\nDownload a file from the specified url.  Parameters ---------- url: str     The url of the data. target_path: str     The location where the data is saved, including the file name.', 'CLASS keras.keras.src.utils.file_utils_test.ExtractArchiveTest has method keras.keras.src.utils.file_utils_test.ExtractArchiveTest.create_zip', 'METHOD qlib.qlib.tests.data.GetData.download has parameter or accepts argument qlib.qlib.tests.data.GetData.download.target_path\nDownload a file from the specified url.  Parameters ---------- url: str     The url of the data. target_path: str     The location where the data is saved, including the file name.', 'CLASS keras.keras.src.utils.file_utils_test.ExtractArchiveTest has method keras.keras.src.utils.file_utils_test.ExtractArchiveTest.test_extract_zip', 'METHOD qlib.qlib.tests.data.GetData.download has parameter or accepts argument qlib.qlib.tests.data.GetData.download.url\nDownload a file from the specified url.  Parameters ---------- url: str     The url of the data. target_path: str     The location where the data is saved, including the file name.', 'METHOD qlib.qlib.data.data.ClientDatasetProvider.dataset has parameter or accepts argument qlib.qlib.data.data.ClientDatasetProvider.dataset.return_uri', 'MODULE cognee.cognee.api.v1.ui.ui contains FUNCTION cognee.cognee.api.v1.ui.ui.get_frontend_download_info\nGet the download URL and version for the actual cognee-frontend source. Downloads the real frontend from GitHub releases, matching the installed version.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1577, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",48.39522361755371,757
What check ensures each subclass is only appended once in the result of find_all_classes?,"The check that ensures each subclass is only appended once in the result of `find_all_classes` in qlib involves maintaining a set or other data structure to keep track of unique class entries as they are encountered during the recursive search. This prevents duplicate classes from being added to the result list, which aligns with general practices in software development and the purpose of the function.

This implementation is likely used to avoid duplicate class entries and ensure that only unique subclasses are included in the result.","In the nested `_append_cls` in qlib/utils/mod.py, the `if` guard includes

```python
… and cls not in cls_list
```

so each subclass is only added once to `cls_list`.","['FUNCTION qlib.qlib.utils.mod.find_all_classes has parameter or accepts argument qlib.qlib.utils.mod.find_all_classes.cls\nFind all the classes recursively that inherit from `cls` in a given module. - `cls` itself is also included      >>> from qlib.data.dataset.handler import DataHandler     >>> find_all_classes(""qlib.contrib.data.handler"", DataHandler)     [<class \'qlib.contrib.data.handler.Alpha158\'>, <class \'qlib.contrib.data.handler.Alpha158vwap\'>, <class \'qlib.contrib.data.handler.Alpha360\'>, <class \'qlib.contrib.data.handler.Alpha360vwap\'>, <class \'qlib.data.dataset.handler.DataHandlerLP\'>]  TODO: - skip import error', 'CLASS qlib.qlib.contrib.model.pytorch_tabnet.AttentionTransformer inherits from or is a subclass of torch.nn.nn.Module\nArgs:     relax: relax coefficient. The greater it is, we can     use the same features more. When it is set to 1     we can use every feature only once', 'MODULE qlib.qlib.utils.mod contains FUNCTION qlib.qlib.utils.mod.find_all_classes\nFind all the classes recursively that inherit from `cls` in a given module. - `cls` itself is also included      >>> from qlib.data.dataset.handler import DataHandler     >>> find_all_classes(""qlib.contrib.data.handler"", DataHandler)     [<class \'qlib.contrib.data.handler.Alpha158\'>, <class \'qlib.contrib.data.handler.Alpha158vwap\'>, <class \'qlib.contrib.data.handler.Alpha360\'>, <class \'qlib.contrib.data.handler.Alpha360vwap\'>, <class \'qlib.data.dataset.handler.DataHandlerLP\'>]  TODO: - skip import error', 'MODULE cognee.cognee.tasks.repo_processor.get_non_code_files contains FUNCTION cognee.cognee.tasks.repo_processor.get_non_code_files.should_process\nDetermine if a file should be processed based on its extension and path patterns.  This function checks if the file extension is in the allowed list and ensures that none of the ignored patterns are present in the provided file path.  Parameters: -----------      - path: The file path to check for processing eligibility.  Returns: --------      Returns True if the file should be processed; otherwise, False.', 'FUNCTION cognee.cognee.tasks.repo_processor.get_non_code_files.should_process has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_non_code_files.ALLOWED_EXTENSIONS\nDetermine if a file should be processed based on its extension and path patterns.  This function checks if the file extension is in the allowed list and ensures that none of the ignored patterns are present in the provided file path.  Parameters: -----------      - path: The file path to check for processing eligibility.  Returns: --------      Returns True if the file should be processed; otherwise, False.', 'FUNCTION qlib.qlib.utils.mod.find_all_classes has parameter or accepts argument qlib.qlib.utils.mod.find_all_classes.module_path\nFind all the classes recursively that inherit from `cls` in a given module. - `cls` itself is also included      >>> from qlib.data.dataset.handler import DataHandler     >>> find_all_classes(""qlib.contrib.data.handler"", DataHandler)     [<class \'qlib.contrib.data.handler.Alpha158\'>, <class \'qlib.contrib.data.handler.Alpha158vwap\'>, <class \'qlib.contrib.data.handler.Alpha360\'>, <class \'qlib.contrib.data.handler.Alpha360vwap\'>, <class \'qlib.data.dataset.handler.DataHandlerLP\'>]  TODO: - skip import error', 'FUNCTION cognee.cognee.tasks.repo_processor.get_non_code_files.should_process has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_non_code_files.should_process.path\nDetermine if a file should be processed based on its extension and path patterns.  This function checks if the file extension is in the allowed list and ensures that none of the ignored patterns are present in the provided file path.  Parameters: -----------      - path: The file path to check for processing eligibility.  Returns: --------      Returns True if the file should be processed; otherwise, False.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1601, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",46.572327613830566,758
Which methods are used to retrieve new versus historical components of US market indices?,"qlib has several methods for retrieving new versus historical components of US market indices. The ChangeInstrument class allows you to change the instrument for which downstream operations are performed upon, while the IBOVIndex collector provides access to the index historic composition and allows you to compare it quarter by quarter and year by year. Additionally, the WIKIIndex and CSIIndex collectors get history trading date.

You can use these methods to retrieve new versus historical components of US market indices in qlib.","In scripts/data_collector/us_index/collector.py:

• New components are fetched via WIKIIndex._request_new_companies  
• Historical components are fetched via NASDAQ100Index._request_history_companies (POST to HISTORY_COMPANIES_URL)","['CLASS qlib.qlib.data.ops.ChangeInstrument has field or attribute or property qlib.qlib.data.ops.instrument\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'METHOD qlib.scripts.data_collector.br_index.collector.IBOVIndex.get_changes has parameter or accepts argument qlib.scripts.data_collector.br_index.collector.quarter_dict\nAccess the index historic composition and compare it quarter by quarter and year by year in order to generate a file that keeps track of which stocks have been removed and which have been added.  The Dataframe used as reference will provided the index composition for each year an quarter: pd.DataFrame:     symbol     SH600000     SH600001     .     .     .  Parameters ---------- self: is used to represent the instance of the class.  Returns ---------- pd.DataFrame:     symbol      date        type     SH600000  2019-11-11    add     SH600001  2020-11-10    remove     dtypes:         symbol: str         date: pd.Timestamp         type: str, value from [""add"", ""remove""]', 'CLASS qlib.qlib.data.ops.ChangeInstrument has method qlib.qlib.data.ops.ChangeInstrument.__init__\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'CLASS qlib.scripts.data_collector.us_index.collector.WIKIIndex has method qlib.scripts.data_collector.us_index.collector.WIKIIndex.calendar_list\nget history trading date  Returns -------     calendar list', 'CLASS qlib.qlib.data.ops.ChangeInstrument has field or attribute or property qlib.qlib.data.ops.start_index\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'CLASS qlib.scripts.data_collector.cn_index.collector.CSIIndex has method qlib.scripts.data_collector.cn_index.collector.CSIIndex.calendar_list\nget history trading date  Returns -------     calendar list', 'CLASS qlib.qlib.data.ops.ChangeInstrument has method qlib.qlib.data.ops.ChangeInstrument.__str__\nChange Instrument Operator In some case, one may want to change to another instrument when calculating, for example, to calculate beta of a stock with respect to a market index. This would require changing the calculation of features from the stock (original instrument) to the index (reference instrument) Parameters ---------- instrument: new instrument for which the downstream operations should be performed upon.             i.e., SH000300 (CSI300 index), or ^GPSC (SP500 index).  feature: the feature to be calculated for the new instrument. Returns ---------- Expression     feature operation output', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md\n# HIST\n* Code: [https://github.com/Wentao-Xu/HIST](https://github.com/Wentao-Xu/HIST)\n* Paper: [HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared InformationAdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/abs/2110.13716).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 41}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 41, 'n_files_pr': 17, 'pr': 1641, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",50.14579892158508,759
How does the calendar merge method restrict calendar dates to the range of the source DataFrame before reindexing?,"The calendar merge method in qlib restricts calendar dates to the range of the source DataFrame before reindexing by using the `data_merge_calendar` function from the `DumpDataBase` class. This function takes two parameters: `df`, which is the source DataFrame, and `calendars_list`, which is a list of calendars. It first checks if the `df` parameter is not None, and then it uses the `reindex` method to reindex the calendar dates based on the values in the `calendars_list` parameter. The function also checks if the frequency is set to '1min', and if so, it uses the 1-day data to fill in missing symbols relative to 1-minute data using a script from the `scripts/data_collector/contrib/fill_cn_1min_data/README.md` file.",In DumpDataBase.data_merge_calendar (scripts/dump_bin.py) the code first builds a `calendars_df` then immediately filters it to only those dates between `df[self.date_field_name].min()` and `df[self.date_field_name].max()` before setting the index and calling `df.reindex(...)`.,"['CLASS qlib.qlib.data.data.LocalCalendarProvider has method qlib.qlib.data.data.LocalCalendarProvider.__init__\nLocal calendar data provider class  Provide calendar data from local data source.', 'METHOD qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar.df', 'CLASS qlib.qlib.data.data.LocalCalendarProvider has method qlib.qlib.data.data.LocalCalendarProvider.load_calendar\nLoad original calendar timestamp from file.  Parameters ---------- freq : str     frequency of read calendar file. future: bool Returns ---------- list     list of timestamps\nLocal calendar data provider class  Provide calendar data from local data source.', 'METHOD qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar.calendars_list', ""CLASS qlib.qlib.data.data.LocalCalendarProvider inherits from or is a subclass of qlib.qlib.data.data.ProviderBackendMixin\nThis helper class tries to make the provider based on storage backend more convenient It is not necessary to inherent this class if that provider don't rely on the backend storage\nLocal calendar data provider class  Provide calendar data from local data source."", 'CLASS qlib.scripts.dump_bin.DumpDataBase has method qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar', 'CLASS qlib.qlib.data.data.LocalCalendarProvider inherits from or is a subclass of qlib.qlib.data.data.CalendarProvider\nCalendar provider base class  Provide calendar data.\nLocal calendar data provider class  Provide calendar data from local data source.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1656, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",55.33151650428772,760
What are the two different mechanisms employed in the RL logger tests to capture and verify log output?,"Two mechanisms are employed in the RL logger tests to capture and verify log output:

1. **Logger Object**: A `logger` object is passed as an argument to the `backtest` function in the `qlib.qlib.rl.trainer.api` module, responsible for recording backtest results.

2. **MetricsWriter Class**: The `MetricsWriter` class dumps training metrics to file.","The two mechanisms are:

1. In test_simple_env_logger you use pytest’s caplog fixture (after doing logging.config.dictConfig(C.logging_config) and running through ConsoleWriter) and then regex-match lines in caplog.text.

2. In both tests (and especially in test_logger_with_env_wrapper) you use CsvWriter (and in the wrapper test a LogCollector(LogLevel.DEBUG)) to dump metrics to “.output/result.csv” and then load it with pd.read_csv to assert on its columns and values.","['MODULE qlib.qlib.rl.trainer.api contains FUNCTION qlib.qlib.rl.trainer.api.backtest\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'CLASS qlib.qlib.rl.trainer.callbacks.MetricsWriter has method qlib.qlib.rl.trainer.callbacks.MetricsWriter.on_validate_end\nDump training metrics to file.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.concurrency\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'MODULE qlib.qlib.rl.utils.log contains CLASS qlib.qlib.rl.utils.log.LogLevel\nLog-levels for RL training. The behavior of handling each log level depends on the implementation of :class:`LogWriter`.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.logger\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'CLASS qlib.qlib.rl.trainer.callbacks.MetricsWriter has field or attribute or property qlib.qlib.rl.trainer.callbacks.MetricsWriter.valid_records\nDump training metrics to file.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.state_interpreter\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"", ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Changes in the R package\nThis section summarizes the new features, improvements, and bug fixes to the R package.\n\n* `load.raw` can optionally construct a booster as return. (#7686)\n* Fix parsing decision stump, which affects both transforming text representation to data\n  table and plotting. (#7689)\n* Implement feature weights. (#7660)\n* Some improvements for complying the CRAN release policy. (#7672, #7661, #7763)\n* Support CSR data for predictions (#7615)\n* Document update (#7263, #7606)\n* New maintainer for the CRAN package (#7691, #7649)\n* Handle non-standard installation of toolchain on macos (#7759)\n\n### Changes in JVM-packages\nSome new features for JVM-packages are introduced for a more integrated GPU pipeline and\nbetter compatibility with musl-based Linux. Aside from this, we have a few notable bug\nfixes.\n\n* User can specify the tracker IP address for training, which helps running XGBoost on\n  restricted network environments. (#7808)\n* Add support for detecting musl-based Linux (#7624)\n* Add `DeviceQuantileDMatrix` to Scala binding (#7459)\n* Add Rapids plugin support, now more of the JVM pipeline can be accelerated by RAPIDS (#7491, #7779, #7793, #7806)\n* The setters for CPU and GPU are more aligned (#7692, #7798)\n* Control logging for early stopping (#7326)\n* Do not repartition when nWorker = 1 (#7676)\n* Fix the prediction issue for `multi:softmax` (#7694)\n* Fix for serialization of custom objective and eval (#7274)\n* Update documentation about Python tracker (#7396)\n* Remove jackson from dependency, which fixes CVE-2020-36518. (#7791)\n* Some refactoring to the training pipeline for better compatibility between CPU and\n  GPU. (#7440, #7401, #7789, #7784)\n* Maintenance work. (#7550, #7335, #7641, #7523, #6792, #4676)\n\n### Deprecation\nOther than the changes in the Python package and serialization, we removed some deprecated\nfeatures in previous releases. Also, as mentioned in the previous section, we plan to\nphase out the old binary format in future releases.\n\n* Remove old warning in 1.3 (#7279)\n* Remove label encoder deprecated in 1.3. (#7357)\n* Remove old callback deprecated in 1.3. (#7280)\n* Pre-built binary will no longer support deprecated CUDA architectures including sm35 and\n  sm50. Users can continue to use these platforms with source build. (#7767)\n\n### Documentation\nThis section lists some of the general changes to XGBoost's document, for language binding\nspecific change please visit related sections.\n\n* Document is overhauled to use the new RTD theme, along with integration of Python\n  examples using Sphinx gallery. Also, we replaced most of the hard-coded URLs with sphinx\n  references. (#7347, #7346, #7468, #7522, #7530)\n* Small update along with fixes for broken links, typos, etc. (#7684, #7324, #7334, #7655,\n  #7628, #7623, #7487, #7532, #7500, #7341, #7648, #7311)\n* Update document for GPU. [skip ci] (#7403)\n* Document the status of RTD hosting. (#7353)\n* Update document for building from source. (#7664)\n* Add note about CRAN release [skip ci] (#7395)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1661, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",42.20258331298828,761
What is the flow for checking existing cache entries and storing new responses when caching is enabled in the LLM client?,"The flow for checking existing cache entries and storing new responses when caching is enabled in the LLM client in graphiti involves the following steps:

1. The `OpenAIGenericClient` class checks if caching is enabled by checking the value of its `cache` attribute.
2. If caching is enabled, the client uses the `LLMClient.cache_enabled` attribute to check if caching is already enabled for the LLM client.
3. If caching is not enabled for the LLM client, new responses are stored in a cache using the `LLMClient.cache_store_response` method.

Here's an example of how this flow works:

```python
default from graphiti_core import Graphiti
from graphiti_core.llm_client.openai_generic_client import OpenAIGenericClient

# Create an instance of the OpenAIGenericClient with caching enabled
client = OpenAIGenericClient(config=LLMConfig(api_key=""abc"", model=""gemini-2.0-flash""), cache=True)

# Check if caching is already enabled for the LLM client
if client.cache_enabled:
    # If caching is already enabled, store new responses in the cache
    client.cache_store_response(response)
else:
    # If caching is not enabled, do not store new responses in the cache
    pass
```

This flow assumes that the `cache` attribute and the `cache_enabled` attribute are properly implemented in the `OpenAIGenericClient` class and the `LLMClient` class, respectively.","In LLMClient (graphiti_core/llm_client/client.py) the flow is:

1. In __init__, if you passed cache=True, `self.cache_dir` is set to a Cache under DEFAULT_CACHE_DIR.  
2. In `generate_response(...)`:
   a) If `cache_enabled`, compute a key via `_get_cache_key(messages)` and call `self.cache_dir.get(key)`.  
      – On hit, return the cached dict.  
   b) Otherwise (cache miss):
      - Clean each `message.content` via `_clean_input`.  
      - Call `_generate_response_with_retry(...)`, which wraps the provider’s `_generate_response`.  
      - After you get the response, recompute the same cache key and call `self.cache_dir.set(key, response)`.  
      - Return the freshly fetched response.","['METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.cache\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.llm_client.client.LLMClient has field or attribute or property graphiti.graphiti_core.llm_client.client.cache', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.cache\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.llm_client.client.LLMClient has field or attribute or property graphiti.graphiti_core.llm_client.client.LLMClient.cache_enabled', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.client\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.client\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 11, 'n_files_pr': 21, 'pr': 39, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",69.72870516777039,762
How does the bulk ingestion method differ from the single-episode ingestion path regarding edge invalidation and date extraction?,"The bulk ingestion method differs from the single-episode ingestion path regarding edge invalidation and date extraction as follows:

*   In the bulk ingestion method, edge invalidation is handled through a process called ""cache invalidation,"" which ensures that the graph remains up-to-date with the latest data. This process involves checking for changes in the data and updating the graph accordingly.
*   The single-episode ingestion path does not have built-in support for cache invalidation but can be implemented manually using a hash function to track changes in the data and update the graph accordingly.

Regarding date extraction, both methods provide similar functionality, allowing users to extract dates from the graph data and perform various operations on them, such as filtering, sorting, and grouping.

In summary, while both methods have their strengths and weaknesses, the bulk ingestion method is better suited for large-scale data ingestion and provides built-in support for cache invalidation. The single-episode ingestion path is more suitable for smaller datasets and requires manual implementation of cache invalidation features.","In graphiti_core/graphiti.py the two paths diverge precisely on those steps:

• add_episode (single) runs resolve_extracted_edges, collects its invalidated_edges and invokes extract_attributes_from_nodes (which includes date‐field extraction) before writing to Neo4j.  
• add_episode_bulk skips both edge invalidation and any date‐extraction logic entirely (“Important: This method does not perform edge invalidation or date extraction steps.”).","['CLASS keras.keras.src.utils.file_utils_test.GetFileTest has method keras.keras.src.utils.file_utils_test.GetFileTest.test_cache_invalidation\nTest using a hash to force cache invalidation.', 'MODULE cognee.cognee.tasks.ingestion.ingest_data contains GLOBAL_VARIABLE cognee.cognee.tasks.ingestion.ingest_data.existing_data_points', 'CLASS keras.keras.src.utils.file_utils_test.GetFileTest has method keras.keras.src.utils.file_utils_test.GetFileTest.test_force_download\nTest using a hash to force cache invalidation.', 'MODULE cognee.cognee.tasks.ingestion.ingest_data contains GLOBAL_VARIABLE cognee.cognee.tasks.ingestion.ingest_data.dataset_new_data_points', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.blocking_query\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE cognee.cognee.tasks.ingestion.ingest_data contains GLOBAL_VARIABLE cognee.cognee.tasks.ingestion.ingest_data.existing_datasets', 'MODULE cognee.cognee.infrastructure.databases.graph.kuzu.adapter contains CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 5, 'pr': 40, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",51.371201515197754,763
How does the Graphiti.search method choose the edge reranking configuration when a center node identifier is passed?,"The Graphiti.search method chooses the edge reranking configuration when a center node identifier is passed by using the `center_node_uuid` parameter. This allows you to center your search around a specific node and explore related information. The `center_node_uuid` parameter is used in combination with other search parameters, such as node and fact searches, to build a complete picture of the user's preferences, procedures, and factual context. This approach enables Graphiti to provide personalized assistance that respects the user's established knowledge and preferences, allowing for more effective and efficient knowledge retrieval.","In Graphiti.search (graphiti_core/graphiti.py) you’ll see:  
```
search_config = (EDGE_HYBRID_SEARCH_RRF 
                   if center_node_uuid is None 
                   else EDGE_HYBRID_SEARCH_NODE_DISTANCE)
```
So as soon as you pass a center_node_uuid, it swaps out the default RRF config for EDGE_HYBRID_SEARCH_NODE_DISTANCE, and then uses that to rerank edges.","['CLASS cognee.cognee.tasks.graph.models.OntologyEdge inherits from or is a subclass of pydantic.BaseModel\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_edges', 'MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_nodes', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_node_names', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.source_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 45, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",38.13680577278137,764
How does extract_nodes determine which prompt to use for text episodes?,"Based on the provided information, `extract_nodes` determines which prompt to use for text episodes in Graphiti by using a combination of user input and system prompts. The function uses a system prompt defined in `cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names.system_prompt`, which is formatted with the user's input and system prompt to guide the model's response. This approach allows for dynamic and context-dependent prompting, enabling more accurate and relevant responses. Here is an example of how you could implement this functionality in your code:

```python
def get_prompt(context):
    # Get the system prompt from the context
    system_prompt = context['system_prompt']

    # Format the user input and system prompt into a single string
    text_input = f""{context['user_input']} {system_prompt}""

    # Use the show_prompt function to format the prompt for display
    formatted_prompt = cognee.cognee.infrastructure.llm.prompts.show_prompt.show_prompt(text_input, system_prompt)

    return formatted_prompt

def main():
    context = {
        'user_input': 'What is the meaning of life?',
        'system_prompt': 'Please provide a clear and concise question.'
    }

    prompt = get_prompt(context)
    print(prompt)
if __name__ == '__main__':
    main()
```
This code defines a function `get_prompt` that takes a context dictionary as input, formats the user's input with the system prompt, and uses the `show_prompt` function to format the prompt for display. The formatted prompt is then returned by the function.

Remember to replace the example context dictionary with your own user input and system prompts.","In extract_nodes (graphiti_core/utils/maintenance/node_operations.py) there’s a source‐type dispatch on episode.source. If it equals EpisodeType.text, it calls:

```python
llm_client.generate_response(
    prompt_library.extract_nodes.extract_text(context),
    response_model=ExtractedEntities
)
```

i.e. it uses the extract_text prompt template for text episodes.","[""FUNCTION cognee.cognee.infrastructure.llm.prompts.show_prompt.show_prompt has parameter or accepts argument cognee.cognee.infrastructure.llm.prompts.show_prompt.show_prompt.text_input\nFormat and display the prompt for a user query.  This method formats the prompt using the provided user input and system prompt, returning a string representation. Raises MissingSystemPromptPathError if the system prompt is not provided.  Parameters: -----------      - text_input (str): The input text provided by the user.     - system_prompt (str): The system's prompt to guide the model's response.  Returns: --------      - str: A formatted string representing the user input and system prompt."", 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains FUNCTION graphiti.graphiti_core.prompts.extract_nodes.extract_text', ""MODULE cognee.cognee.infrastructure.llm.prompts.show_prompt contains FUNCTION cognee.cognee.infrastructure.llm.prompts.show_prompt.show_prompt\nFormat and display the prompt for a user query.  This method formats the prompt using the provided user input and system prompt, returning a string representation. Raises MissingSystemPromptPathError if the system prompt is not provided.  Parameters: -----------      - text_input (str): The input text provided by the user.     - system_prompt (str): The system's prompt to guide the model's response.  Returns: --------      - str: A formatted string representing the user input and system prompt."", 'MODULE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names contains GLOBAL_VARIABLE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names.system_prompt', ""FUNCTION cognee.cognee.infrastructure.llm.prompts.show_prompt.show_prompt has parameter or accepts argument cognee.cognee.infrastructure.llm.prompts.show_prompt.show_prompt.system_prompt\nFormat and display the prompt for a user query.  This method formats the prompt using the provided user input and system prompt, returning a string representation. Raises MissingSystemPromptPathError if the system prompt is not provided.  Parameters: -----------      - text_input (str): The input text provided by the user.     - system_prompt (str): The system's prompt to guide the model's response.  Returns: --------      - str: A formatted string representing the user input and system prompt."", 'MODULE cognee.cognee.tasks.graph.cascade_extract.utils.extract_nodes contains GLOBAL_VARIABLE cognee.cognee.tasks.graph.cascade_extract.utils.extract_nodes.system_prompt', 'MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 46, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",75.48417258262634,765
Which attribute of the returned EntityEdge objects does the integration test extract for logging?,The integration test extracts the 'attributes' attribute of the returned EntityEdge objects for logging in graphiti.,The integration test uses the helper search_results_to_context_string (in tests/) which pulls out each EntityEdge’s context attribute and concatenates those for logging.,"['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS cognee.cognee.tests.unit.interfaces.graph.get_graph_from_model_unit_test.EntityType has field or attribute or property cognee.cognee.tests.unit.interfaces.graph.get_graph_from_model_unit_test.EntityType.metadata', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS cognee.cognee.tests.unit.interfaces.graph.get_graph_from_model_unit_test.Entity has field or attribute or property cognee.cognee.tests.unit.interfaces.graph.get_graph_from_model_unit_test.Entity.metadata', 'CLASS qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter has method qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter.split_data\nSplits data frame into training-validation-test data frames.  This also calibrates scaling object, and transforms data for each split.  Args:   df: Source data frame to split.   valid_boundary: Starting year for validation data   test_boundary: Starting year for test data  Returns:   Tuple of transformed (train, valid, test) data.\nDefines and formats data for the Alpha158 dataset.  Attributes:   column_definition: Defines input and data type of column used in the     experiment.   identifiers: Entity identifiers used in experiments.', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.attributes', 'CLASS keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyGlobalFunctionsTest has method keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyGlobalFunctionsTest.test_set_dtype_policy_valid_policy\nTest set_dtype_policy with a valid DTypePolicy object.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 48, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",29.059367656707764,766
What sequence of operations does Graphiti perform to generate node results for a textual query?,"**Sequence of Operations for Generating Node Results in Graphiti**

1. **Text Input**: The user inputs a textual query into the system.
2. **Preprocessing**: The input text is preprocessed to extract relevant features such as keywords, entities, and relationships.
3. **Knowledge Graph Construction**: A knowledge graph is constructed using the extracted features and existing knowledge in the database.
4. **Node Search**: The system searches for nodes that match the user's query in the constructed knowledge graph.
5. **Ranking**: The search results are ranked based on their relevance to the user's query.
6. **Result Processing**: The top-ranked search results are processed and presented to the user.","When you issue a text‐only node search, Graphiti does roughly the following:

1. Uses your EmbedderClient (e.g. `OpenAIEmbedder`) to turn the query string into an embedding.  
2. Calls `hybrid_node_search` (in `graphiti_core/search/search_utils.py`), passing both
   – the raw text query to `node_fulltext_search(driver, q, …)`, and  
   – its embedding to `node_similarity_search(driver, e, …)`, in parallel via `semaphore_gather`.  
3. Merges and deduplicates all returned `EntityNode` objects into a UUID map.  
4. Applies RRF reranking (`rrf(...)`) over the per‐query/per‐embedding result lists.  
5. Maps the final ranked UUIDs back to `EntityNode` instances and returns them.","[""MODULE keras.keras.src.ops.function contains FUNCTION keras.keras.src.ops.function.map_graph\nValidates a graph's topology and gather its operations and nodes.  Args:     inputs: List of input tensors.     outputs: List of outputs tensors.  Returns:     A tuple `(nodes, nodes_by_depth, operations, operations_by_depth)`.     - nodes: set of Node instances     - nodes_by_depth: dict mapping ints (depth) to lists of node instances.     - operations: list of Operation instances.     - operations_by_depth: dict mapping ints (depth) to lists of Operation         instances."", 'MODULE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names contains GLOBAL_VARIABLE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names.text_input', ""FUNCTION keras.keras.src.ops.function.map_graph has parameter or accepts argument keras.keras.src.ops.function.map_graph.inputs\nValidates a graph's topology and gather its operations and nodes.  Args:     inputs: List of input tensors.     outputs: List of outputs tensors.  Returns:     A tuple `(nodes, nodes_by_depth, operations, operations_by_depth)`.     - nodes: set of Node instances     - nodes_by_depth: dict mapping ints (depth) to lists of node instances.     - operations: list of Operation instances.     - operations_by_depth: dict mapping ints (depth) to lists of Operation         instances."", 'FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query has parameter or accepts argument graphiti.graphiti_core.graph_queries.NEO4J_TO_FALKORDB_MAPPING', ""FUNCTION keras.keras.src.ops.function.map_graph has parameter or accepts argument keras.keras.src.ops.function.map_graph.outputs\nValidates a graph's topology and gather its operations and nodes.  Args:     inputs: List of input tensors.     outputs: List of outputs tensors.  Returns:     A tuple `(nodes, nodes_by_depth, operations, operations_by_depth)`.     - nodes: set of Node instances     - nodes_by_depth: dict mapping ints (depth) to lists of node instances.     - operations: list of Operation instances.     - operations_by_depth: dict mapping ints (depth) to lists of Operation         instances."", 'MODULE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names contains GLOBAL_VARIABLE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names.existing_node_set', 'CLASS cognee.cognee.modules.sync.models.SyncOperation.SyncOperation inherits from or is a subclass of cognee.infrastructure.databases.relational.Base\nDatabase model for tracking sync operations.  This model stores information about background sync operations, allowing users to monitor progress and query the status of their sync requests.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 8, 'pr': 49, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",61.1823456287384,767
How does the system's hybrid node search integrate fulltext and vector similarity queries before producing the final ranked list?,"The system's hybrid node search in Graphiti integrates fulltext and vector similarity queries before producing the final ranked list by using a combination of graph-based and vector-based approaches. The process involves performing a full-text search, filtering results based on their similarity to the query vector, and sorting nodes based on their maximal marginal relevance scores. This is achieved through the following steps: first, a full-text search is performed using a function that computes the maximal marginal relevance of each node with respect to the query vector; next, the results are filtered based on their similarity to the query vector; finally, the nodes are sorted based on their maximal marginal relevance scores to produce the final ranked list. This approach combines the strengths of graph-based and vector-based methods to provide accurate and relevant search results.","In graphiti_core/search/search_utils.py the hybrid_node_search function does the following:

1. For each text query it calls node_fulltext_search(driver, q, …, 2*limit), and for each embedding it calls node_similarity_search(driver, e, …, 2*limit), all in parallel via semaphore_gather.  
2. It collects all returned EntityNode lists, builds a dict uuid→node to dedupe.  
3. It extracts the per-search lists of UUIDs and feeds them into the rrf() (reciprocal-rank fusion) reranker.  
4. Finally it looks up the nodes in rrf’s merged UUID order and returns that as the final ranked list.","['FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance', 'MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query', 'CLASS cognee.cognee.infrastructure.databases.hybrid.neptune_analytics.NeptuneAnalyticsAdapter.NeptuneAnalyticsAdapter inherits from or is a subclass of cognee.infrastructure.databases.graph.neptune_driver.adapter.NeptuneGraphDB\nHybrid adapter that combines Neptune Analytics Vector and Graph functionality.  This adapter extends NeptuneGraphDB and implements VectorDBInterface to provide a unified interface for working with Neptune Analytics as both a vector store and a graph database.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md\n# HIST\n* Code: [https://github.com/Wentao-Xu/HIST](https://github.com/Wentao-Xu/HIST)\n* Paper: [HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared InformationAdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/abs/2110.13716).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/XGBoost/README.md\n# XGBoost\n* Code: [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)\n* Paper: XGBoost: A Scalable Tree Boosting System. [https://dl.acm.org/doi/pdf/10.1145/2939672.2939785](https://dl.acm.org/doi/pdf/10.1145/2939672.2939785).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 13}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 13, 'n_files_pr': 7, 'pr': 51, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",63.74384927749634,768
Which utility operations are invoked to perform node and edge deduplication during bulk episode processing?,"**Node and Edge Deduplication Utility Operations**

During bulk episode processing in Graphiti, node and edge deduplication is performed using the following utility operations:

* `nodes_by_episode`: This variable contains a dictionary where the keys are episode IDs and the values are lists of EntityNode objects.

* `edges_by_episode`: This variable contains a dictionary where the keys are episode IDs and the values are lists of EntityEdge objects.

These variables store nodes and edges by episode ID, enabling efficient deduplication during bulk episode processing.","During add_episode_bulk, node and edge deduplication are handled by the bulk utilities in graphiti_core/utils/bulk_utils.py:

• dedupe_nodes_bulk (calls create_entity_node_embeddings, resolve_extracted_nodes, compress_uuid_map)  
• dedupe_edges_bulk (calls create_entity_edge_embeddings, resolve_extracted_edge, compress_uuid_map)","['CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.reopen\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.nodes_by_episode', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.blocking_query\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.close\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE graphiti.graphiti_core.nodes contains FUNCTION graphiti.graphiti_core.nodes.get_episodic_node_from_record', 'MODULE cognee.cognee.infrastructure.databases.graph.kuzu.adapter contains CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 10, 'n_files_pr': 8, 'pr': 67, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",39.58107542991638,769
What are the main steps performed by the node distance reranker to produce a sorted list of node UUIDs based on proximity and score filtering?,"The node distance reranker produces a sorted list of node UUIDs based on proximity and score filtering by following these steps:

1.  The reranker takes the input nodes and their corresponding scores as input.
2.  It calculates the proximity between each pair of nodes using a proximity metric (e.g., Euclidean distance, cosine similarity).
3.  The reranker then filters out nodes that do not meet certain conditions based on their scores or other criteria.
4.  For the remaining nodes, it calculates a weighted sum of their scores and proximities to determine their final score.
5.  Finally, the reranked node UUIDs are sorted in descending order of their final scores.

This process allows the reranker to produce a sorted list of node UUIDs that takes into account both the proximity between nodes and their individual scores, providing a more accurate ranking of nodes based on their relevance to the query.","In graphiti_core/search/search_utils.py the async node_distance_reranker does roughly the following:

1. Remove the center UUID from the candidate list.  
2. Initialize a score map with center→0.0.  
3. Run a Cypher query (MATCH …–[:RELATES_TO]–…) to get a “distance” score (always 1) for directly connected nodes.  
4. Assign ∞ to any nodes not returned by the query.  
5. Sort the remaining UUIDs by ascending score (i.e. closest first).  
6. If the center was originally in the input, prepend it back with a small score (0.1).  
7. Apply the min_score filter by keeping only UUIDs where (1/score) ≥ min_score.  

This yields a proximity-sorted, score-filtered list of node UUIDs.","['FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_node_uuids', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.examples.quickstart.quickstart_neo4j contains GLOBAL_VARIABLE graphiti.examples.quickstart.quickstart_neo4j.reranked_results', 'MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS graphiti.graphiti_core.search.search_config.NodeSearchConfig has field or attribute or property graphiti.graphiti_core.search.search_config.NodeSearchConfig.reranker', ""CLASS keras.keras.src.ops.node.Node has method keras.keras.src.ops.node.Node.parent_nodes\nThe parent `Node`s.  Returns:     all the `Node`s whose output this node immediately depends on.\nA `Node` describes an operation `__call__()` event.  A Keras Function is a DAG with `Node` instances as nodes, and `KerasTensor` instances as edges. Nodes aren't `Operation` instances, because a single operation could be called multiple times, which would result in graph cycles.  A `__call__()` event involves input tensors (and other input arguments), the operation that was called, and the resulting output tensors. A `Node` will include all this information.  Since a single `Operation` could be called multiple times, the `Node` instances are stored on operations as a list. Each time an operation is called, a node is added to `op._inbound_nodes`. Each time the output of an operation is used by another operation, a node is added to `op._outbound_nodes`.  Every `KerasTensor` instance has a `KerasHistory` object attached, which tracks the `Node` that records the `__call__()` event that created the tensor. By recursively walking through `Node` instances via the `KerasHistory` metadata of `KerasTensor` instances, once can retrieve the entire DAG of a Keras Function.  Args:     operation: The Operation that was called in the `op.__call__()`         event that this node represents.     call_args: The positional arguments the operation was called with.     call_kwargs: The keyword arguments the operation was called with.     outputs: The output tensors of the `op.__call__()` call."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 72, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",47.00052809715271,770
"How do the bulk utility functions chain retrieval, extraction, and deduplication to process multiple episodes into final node and edge mappings?","The bulk utility functions in Graphiti chain retrieval, extraction, and deduplication to process multiple episodes into final node and edge mappings as follows: **1. Retrieval**: The `get_entity_edge_save_bulk_query` function is used to retrieve the entity edges from the database. This function takes a `db_type` parameter, which specifies the type of database being queried. **2. Extraction**: After retrieval, the `edges_by_episode` and `nodes_by_episode` dictionaries are used to extract the relevant information from the retrieved data. These dictionaries map episode IDs to lists of entity edges and nodes, respectively. **3. Deduplication**: Finally, the extracted data is deduplicated using the `graphiti.graphiti_core.utils.bulk_utils.deduplicate_edges` function. This function takes the `edges_by_episode` dictionary as input and returns a new dictionary with duplicate edges removed. The final node and edge mappings are then constructed by iterating over the deduplicated edges and nodes, and creating corresponding OntologyEdge and OntologyNode objects, respectively.","The bulk pipeline in graphiti_core/utils/bulk_utils.py works in four stages, each feeding its output into the next:

1. retrieve_previous_episodes_bulk  
   – For each EpisodicNode in your input list, it fetches the last N episodes with semaphore_gather and returns a list of (episode, previous_episodes) tuples.

2. extract_nodes_and_edges_bulk  
   – Takes the episode tuples plus your GraphitiClients and edge_type_map, runs extract_nodes(...) and extract_edges(...) in parallel (again via semaphore_gather), and returns two nested lists: extracted_nodes_bulk and extracted_edges_bulk.

3. dedupe_nodes_bulk  
   – Embeds all extracted EntityNode objects, finds cross-episode candidates by word overlap or embedding similarity, calls resolve_extracted_nodes(...) to pick/merge duplicates, builds a compressed_map of UUIDs, and finally returns nodes_by_episode: a dict mapping each episode.uuid to its deduplicated EntityNode list.

4. dedupe_edges_bulk  
   – Mirrors the node step but for EntityEdge: embeds, finds semantic/lexical duplicates, calls resolve_extracted_edge(...), compresses UUIDs, and returns edges_by_episode keyed by episode.uuid.

In practice you chain them like:  
```python  
episode_tuples = await retrieve_previous_episodes_bulk(driver, episodes)  
nodes_bulk, edges_bulk = await extract_nodes_and_edges_bulk(clients, episode_tuples, edge_type_map, …)  
nodes_by_ep, node_map = await dedupe_nodes_bulk(clients, nodes_bulk, episode_tuples, …)  
edges_by_ep      = await dedupe_edges_bulk(clients, edges_bulk, episode_tuples, nodes_by_ep, edge_types, …)  
```","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_edge_save_bulk_query', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.nodes_by_episode', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.source_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 6, 'pr': 73, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",61.21262168884277,771
Which episode addition workflow variant processes multiple episodes in one batch without performing edge invalidation or extracting temporal dates?,"The episode addition workflow variant that processes multiple episodes in one batch without performing edge invalidation or extracting temporal dates in Graphiti is not explicitly mentioned in the provided information. However, based on the information provided about Graphiti, it seems that it supports custom graph schemas and has a MCP Server. If you could provide more context or clarify what you mean by ""episode addition workflow variant"", I would be happy to try and assist you further.","The bulk‐add workflow is the `Graphiti.add_episode_bulk` method (graphiti_core/graphiti.py). Per its docstring, it processes a list of `RawEpisode` in one batch and “does not perform edge invalidation or date extraction” (use `add_episode` for that).","['CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.eval contains FUNCTION graphiti.graphiti_core.prompts.eval.eval_add_episode_results', 'MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'FUNCTION graphiti.graphiti_core.prompts.eval.eval_add_episode_results has parameter or accepts argument graphiti.graphiti_core.prompts.eval.eval_add_episode_results.context', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS graphiti.graphiti_core.nodes.EpisodeType inherits from or is a subclass of enum.Enum\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 18}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 18, 'n_files_pr': 12, 'pr': 77, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",37.192105531692505,772
What steps are involved in the hybrid node search from executing fulltext and vector searches to combining and reranking results?,"The steps involved in hybrid node search from executing fulltext and vector searches to combining and reranking results in Graphiti are as follows:

1. **Search Nodes**: Look for relevant preferences, procedures, and requirements before beginning work.
2. **Search Facts**: Discover relationships and factual information that may be relevant to your task.
3. **Filter by Entity Type**: Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.
4. **Review All Matches**: Carefully examine any preferences, procedures, or facts that match your current task.
5. **Combine Node and Fact Searches**: For complex tasks, search both nodes and facts to build a complete picture.
6. **Rerank Results**: Use the `center_node_uuid` parameter when exploring related information, and prioritize specific matches over general information.

By following these steps, you can effectively execute hybrid node searches in Graphiti and provide personalized assistance that respects the user's established preferences, procedures, and factual context.","In graphiti_core/search/search_utils.py’s hybrid_node_search:

1. Concurrently fire off  
   – full-text searches (node_fulltext_search) for each query  
   – vector-similarity searches (node_similarity_search) for each embedding  
   (via semaphore_gather)  
2. Flatten all returned EntityNode lists and build a uuid→node map  
3. Build a list of uuid sequences (one list per search)  
4. Rerank with reciprocal-rank fusion (rrf(result_uuids))  
5. Map the fused uuid ranking back to EntityNode objects and return them","['CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.search.search_config_recipes contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_config_recipes.EDGE_HYBRID_SEARCH_NODE_DISTANCE', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search_config_recipes contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_config_recipes.NODE_HYBRID_SEARCH_NODE_DISTANCE', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 14}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 14, 'n_files_pr': 7, 'pr': 81, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",47.62356424331665,773
"Within the add_episode workflow, which utility handles embedding creation, related edge retrieval, and LLM-based deduplication and invalidation of extracted edges?","The utility responsible for embedding creation, related edge retrieval, and LLM-based deduplication and invalidation of extracted edges in Graphiti is likely handled by the `expand_with_nodes_and_edges` function within the `cognee` codebase.","The utility is the resolve_extracted_edges function in graphiti_core/utils/maintenance/edge_operations.py. It calls create_entity_edge_embeddings (to embed edges), get_relevant_edges / get_edge_invalidation_candidates (to fetch related/invalidation candidates), and then uses LLM via resolve_extracted_edge for deduplication and invalidation.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""CLASS qlib.qlib.rl.utils.log.CsvWriter has method qlib.qlib.rl.utils.log.CsvWriter.log_episode\nDump all episode metrics to a ``result.csv``.  This is not the correct implementation. It's only used for first iteration."", 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""MODULE qlib.qlib.rl.utils.log contains CLASS qlib.qlib.rl.utils.log.CsvWriter\nDump all episode metrics to a ``result.csv``.  This is not the correct implementation. It's only used for first iteration."", 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""CLASS qlib.qlib.rl.utils.log.CsvWriter has field or attribute or property qlib.qlib.rl.utils.log.contents\nDump all episode metrics to a ``result.csv``.  This is not the correct implementation. It's only used for first iteration."", 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 13}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 13, 'n_files_pr': 8, 'pr': 85, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",38.91366481781006,774
Which two modules' Cypher queries were updated to include the name_embedding field in their return statements?,The two modules' Cypher queries that were updated to include the `name_embedding` field in their return statements are `EntityNode` and `CommunityNode`. These nodes have been modified to include this field in their output.,"The Cypher queries in

• graphiti_core/nodes.py (EntityNode.get_by_uuid)  
• graphiti_core/search/search_utils.py (get_mentioned_nodes)  

were both updated to add `n.name_embedding` to their RETURN clauses.","['CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has method cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever.__init__\nInitialize retriever with optional custom prompt paths.\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.name_embedding', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.system_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'CLASS graphiti.graphiti_core.nodes.CommunityNode has field or attribute or property graphiti.graphiti_core.nodes.CommunityNode.name_embedding', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever.system_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'CLASS cognee.cognee.modules.graph.models.EdgeType.EdgeType has field or attribute or property cognee.cognee.modules.graph.models.EdgeType.EdgeType.relationship_name', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.user_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 87, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",40.88659739494324,775
How is the group ID parameter incorporated into both the message ingestion and the search/memory retrieval processes?,"The group ID parameter is incorporated into both the message ingestion and search/memory retrieval processes in Graphiti through various methods. In the `AddMessagesRequest` class, there is a field called `group_id` which is used to identify the group that the messages belong to, allowing for efficient grouping of related messages together.

In the search process, you can filter by entity type and specify `Preference`, `Procedure`, or `Requirement` to get targeted results. You can also use the `center_node_uuid` parameter to center your search around a specific node.

In addition, the `add_memory` function allows for real-time updates and incorporation of new information into the knowledge graph by capturing requirements and preferences immediately and storing them in the graph.

Overall, Graphiti's design enables efficient and flexible management of dynamic data, allowing for real-time interaction and precise historical queries.","In ingest.py (add_messages_task), the incoming request.group_id is passed straight into graphiti.add_episode(…, group_id=request.group_id, …), so every message is tagged by its group.  

In retrieve.py both search() and get_memory() scope their calls to graphiti.search by passing either query.group_ids (or in get_memory, a singleton list [request.group_id]) into the group_ids argument, ensuring only that group’s episodes are returned.","['CLASS xgboost.python-package.xgboost.core.DMatrix has method xgboost.python-package.xgboost.core.DMatrix.get_group\nGet the group of the DMatrix.  Returns ------- group\nData Matrix used in XGBoost.  DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed.  You can construct DMatrix from multiple different sources of data.', 'FUNCTION cognee.cognee.modules.ingestion.identify.identify has parameter or accepts argument cognee.cognee.modules.ingestion.identify.identify.data', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_daily_bin_group has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_daily_bin_group.group_n\nget_daily_bin_group Group the values of the stocks of benchmark into several bins in a day. Put the stocks into these bins.  :param bench_values: A series contains the value of stocks in benchmark.                      The index is the stock code. :param stock_values: A series contains the value of stocks of your portofolio                      The index is the stock code. :param group_n:      Bins will be produced  :return:             A series with the same size and index as the stock_value.                      The value in the series is the group id of the bins.                      The No.1 bin contains the biggest values.', 'FUNCTION cognee.cognee.modules.ingestion.identify.identify has parameter or accepts argument cognee.cognee.modules.ingestion.identify.identify.user', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_daily_bin_group has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_daily_bin_group.bench_values\nget_daily_bin_group Group the values of the stocks of benchmark into several bins in a day. Put the stocks into these bins.  :param bench_values: A series contains the value of stocks in benchmark.                      The index is the stock code. :param stock_values: A series contains the value of stocks of your portofolio                      The index is the stock code. :param group_n:      Bins will be produced  :return:             A series with the same size and index as the stock_value.                      The value in the series is the group id of the bins.                      The No.1 bin contains the biggest values.', 'CLASS graphiti.server.graph_service.dto.ingest.AddMessagesRequest has field or attribute or property graphiti.server.graph_service.dto.ingest.AddMessagesRequest.group_id', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_daily_bin_group has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_daily_bin_group.stock_values\nget_daily_bin_group Group the values of the stocks of benchmark into several bins in a day. Put the stocks into these bins.  :param bench_values: A series contains the value of stocks in benchmark.                      The index is the stock code. :param stock_values: A series contains the value of stocks of your portofolio                      The index is the stock code. :param group_n:      Bins will be produced  :return:             A series with the same size and index as the stock_value.                      The value in the series is the group id of the bins.                      The No.1 bin contains the biggest values.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 92, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",45.19620132446289,776
What steps does the graph client's save_entity_node method perform to embed and persist a new entity node?,"The `save_entity_node` method performs the following steps to embed and persist a new entity node:

1.  It calls the `get_entity_node_save_bulk_query` function, which returns a query for saving entity nodes.
2.  The query is executed on the database using the `ENTITY_NODE_SAVE` variable.
3.  The method then calls the `expand_with_nodes_and_edges` function, passing in the entity node and its relationships.
4.  This function expands the knowledge graph by adding ontology-derived nodes and edges to enhance the knowledge representation.
5.  Finally, the method saves the expanded graph structure to the database using the `add_node` method from the `CogneeAbstractGraph` class.","In ZepGraphiti.save_entity_node (server/graph_service/zep_graphiti.py) it:

1. Instantiates an EntityNode(name, uuid, group_id, summary)  
2. Calls new_node.generate_name_embedding(self.embedder) to compute its embedding  
3. Calls new_node.save(self.driver) to persist it in Neo4j  
4. Returns the saved EntityNode instance","['CLASS cognee.cognee.modules.graph.cognee_graph.CogneeAbstractGraph.CogneeAbstractGraph has method cognee.cognee.modules.graph.cognee_graph.CogneeAbstractGraph.CogneeAbstractGraph.add_node\nAdd a node to the graph.\nAbstract base class for representing a graph structure.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_node_save_bulk_query', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.name_embedding', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.models.nodes.node_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.nodes.node_db_queries.ENTITY_NODE_SAVE', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 9, 'pr': 100, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",50.89183259010315,777
What default edge type map does add_episode_bulk use if edge_types is not specified?,The default edge type map used by `add_episode_bulk` when `edge_types` is not specified in Graphiti is the one defined in the `edges_by_episode` dictionary.,"In Graphiti.add_episode_bulk (graphiti_core/graphiti.py), when `edge_types` is None it builds  
```python
edge_type_map_default = {('Entity','Entity'): []}
```","['MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.EdgeNotFoundError\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS graphiti.graphiti_core.errors.EdgeNotFoundError has method graphiti.graphiti_core.errors.EdgeNotFoundError.__init__\nRaised when an edge is not found.', 'MODULE cognee.cognee.tasks.storage.index_graph_edges contains GLOBAL_VARIABLE cognee.cognee.tasks.storage.index_graph_edges.edge_types', 'METHOD xgboost.python-package.xgboost.core.Booster.get_score has parameter or accepts argument xgboost.python-package.xgboost.core.Booster.get_score.fmap\nGet feature importance of each feature. For tree model Importance type can be defined as:  * \'weight\': the number of times a feature is used to split the data across all trees. * \'gain\': the average gain across all splits the feature is used in. * \'cover\': the average coverage across all splits the feature is used in. * \'total_gain\': the total gain across all splits the feature is used in. * \'total_cover\': the total coverage across all splits the feature is used in.  .. note::     For linear model, only ""weight"" is defined and it\'s the normalized    coefficients without bias.  .. note:: Zero-importance features will not be included     Keep in mind that this function does not include zero-importance feature,    i.e.  those features that have not been used in any split conditions.  Parameters ---------- fmap :    The name of feature map file. importance_type :     One of the importance types defined above.  Returns ------- A map between feature names and their scores.  When `gblinear` is used for multi-class classification the scores for each feature is a list with length `n_classes`, otherwise they\'re scalars.', 'CLASS cognee.cognee.modules.graph.models.EdgeType.EdgeType inherits from or is a subclass of cognee.infrastructure.engine.DataPoint', 'METHOD xgboost.python-package.xgboost.core.Booster.get_score has parameter or accepts argument xgboost.python-package.xgboost.core.Booster.get_score.importance_type\nGet feature importance of each feature. For tree model Importance type can be defined as:  * \'weight\': the number of times a feature is used to split the data across all trees. * \'gain\': the average gain across all splits the feature is used in. * \'cover\': the average coverage across all splits the feature is used in. * \'total_gain\': the total gain across all splits the feature is used in. * \'total_cover\': the total coverage across all splits the feature is used in.  .. note::     For linear model, only ""weight"" is defined and it\'s the normalized    coefficients without bias.  .. note:: Zero-importance features will not be included     Keep in mind that this function does not include zero-importance feature,    i.e.  those features that have not been used in any split conditions.  Parameters ---------- fmap :    The name of feature map file. importance_type :     One of the importance types defined above.  Returns ------- A map between feature names and their scores.  When `gblinear` is used for multi-class classification the scores for each feature is a list with length `n_classes`, otherwise they\'re scalars.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David Díaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ryū algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 101, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",37.052268266677856,778
Which exceptions are classified as server or retry errors and thus trigger exponential backoff in the LLM client's retry policy?,"The exceptions that are classified as server or retry errors and thus trigger exponential backoff in the LLM client's retry policy in graphiti are defined by the `is_server_or_retry_error` function. This function checks if an exception is a server or retry error, determining whether it should trigger exponential backoff in the LLM client's retry policy.

To implement this, you can use the `is_server_or_retry_error` function to determine which exceptions require exponential backoff. This will ensure that your application correctly handles server and retry errors in accordance with the LLM client's retry policy.","The retry decorator (in graphiti_core/llm_client/client.py on LLMClient._generate_response_with_retry) uses is_server_or_retry_error, which returns True for:

• RateLimitError  
• json.decoder.JSONDecodeError  
• httpx.HTTPStatusError with a 5xx status code (500–599)","['MODULE cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter contains FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff\nCalculate the backoff time for a retry attempt with jitter.  Parameters: -----------      - attempt: The current retry attempt (0-based).     - initial_backoff: The initial backoff time in seconds. (default       DEFAULT_INITIAL_BACKOFF)     - backoff_factor: The multiplier for exponential backoff. (default       DEFAULT_BACKOFF_FACTOR)     - jitter: The jitter factor to avoid thundering herd. (default DEFAULT_JITTER)  Returns: --------      The backoff time in seconds, calculated using the exponential backoff formula     adjusted by a jitter component.', 'MODULE graphiti.graphiti_core.llm_client.client contains FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff.initial_backoff\nCalculate the backoff time for a retry attempt with jitter.  Parameters: -----------      - attempt: The current retry attempt (0-based).     - initial_backoff: The initial backoff time in seconds. (default       DEFAULT_INITIAL_BACKOFF)     - backoff_factor: The multiplier for exponential backoff. (default       DEFAULT_BACKOFF_FACTOR)     - jitter: The jitter factor to avoid thundering herd. (default DEFAULT_JITTER)  Returns: --------      The backoff time in seconds, calculated using the exponential backoff formula     adjusted by a jitter component.', 'FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error has parameter or accepts argument graphiti.graphiti_core.llm_client.client.is_server_or_retry_error.exception', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff.jitter\nCalculate the backoff time for a retry attempt with jitter.  Parameters: -----------      - attempt: The current retry attempt (0-based).     - initial_backoff: The initial backoff time in seconds. (default       DEFAULT_INITIAL_BACKOFF)     - backoff_factor: The multiplier for exponential backoff. (default       DEFAULT_BACKOFF_FACTOR)     - jitter: The jitter factor to avoid thundering herd. (default DEFAULT_JITTER)  Returns: --------      The backoff time in seconds, calculated using the exponential backoff formula     adjusted by a jitter component.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff.attempt\nCalculate the backoff time for a retry attempt with jitter.  Parameters: -----------      - attempt: The current retry attempt (0-based).     - initial_backoff: The initial backoff time in seconds. (default       DEFAULT_INITIAL_BACKOFF)     - backoff_factor: The multiplier for exponential backoff. (default       DEFAULT_BACKOFF_FACTOR)     - jitter: The jitter factor to avoid thundering herd. (default DEFAULT_JITTER)  Returns: --------      The backoff time in seconds, calculated using the exponential backoff formula     adjusted by a jitter component.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 7, 'n_files_pr': 6, 'pr': 102, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",37.70856285095215,779
"After clustering nodes into communities, which function generates the membership edges and at what point in the build process are they persisted?","After clustering nodes into communities, the membership edges are generated by the `get_community_edge_from_record` function from the `edges.py` module in the `graphiti_core` package. These edges are persisted at the point when the `destroy_graph` configuration parameter is set to `True` in the `GraphitiConfig` class.","The “HAS_MEMBER” edges are built by build_community_edges (in graphiti_core/utils/maintenance/edge_operations.py).  They’re then persisted in Graphiti.build_communities (graphiti_core/graphiti.py), immediately after you save all the CommunityNode objects – i.e. in the semaphore_gather call that runs edge.save() on each CommunityEdge.","['CLASS graphiti.graphiti_core.errors.EdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a list of edges is not found.', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_community_edge_from_record', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.edges.CommunityEdge inherits from or is a subclass of graphiti.graphiti_core.edges.Edge', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.edges contains CLASS graphiti.graphiti_core.edges.CommunityEdge', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.destroy_graph\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 23}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 23, 'n_files_pr': 10, 'pr': 103, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.67008352279663,780
How does a missing entity edge lookup result in an HTTP 404 response?,"A missing entity edge lookup result in an HTTP 404 response is related to the graphiti codebase. The graphiti documentation explains that edge search results include EntityEdge objects with various properties, including UUID, fact, valid at/invalid at time periods, and source/target node UUIDs. However, it does not provide specific information on how a missing entity edge lookup results in an HTTP 404 response. This issue may be due to the absence of a matching edge in the graphiti database. To resolve this issue, you can check the graphiti documentation or contact the graphiti developers for further assistance.","When you call EntityEdge.get_by_uuid (in graphiti_core/edges.py) and no matching edge is found it raises an EdgeNotFoundError. In server/graph_service/zep_graphiti.py’s ZepGraphiti.get_entity_edge this is caught and re-thrown as FastAPI’s HTTPException(status_code=404), so a missing edge lookup yields a 404.","['MODULE cognee.cognee.api.v1.sync.sync contains CLASS cognee.cognee.api.v1.sync.sync.CheckHashesDiffResponse\nResponse model for missing hashes check', 'MODULE cognee.cognee.modules.graph.utils.retrieve_existing_edges contains GLOBAL_VARIABLE cognee.cognee.modules.graph.utils.retrieve_existing_edges.entity_node_id', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE cognee.cognee.modules.graph.utils.retrieve_existing_edges contains GLOBAL_VARIABLE cognee.cognee.modules.graph.utils.retrieve_existing_edges.type_entity_edges', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE cognee.cognee.modules.graph.utils.retrieve_existing_edges contains GLOBAL_VARIABLE cognee.cognee.modules.graph.utils.retrieve_existing_edges.entity_node_edges', 'CLASS cognee.cognee.api.v1.sync.sync.CheckHashesDiffResponse has field or attribute or property cognee.cognee.api.v1.sync.sync.CheckHashesDiffResponse.missing_on_remote\nResponse model for missing hashes check', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## 💫 Contributors\n\nThanks to our amazing contributors! 💖\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=topoteretes/cognee"" />\n</a>\n\n## 🏆 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 5, 'n_files_pr': 6, 'pr': 104, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",45.5211820602417,781
Describe how extract_nodes leverages the extract_text prompt to generate and return EntityNode objects for text episodes.,"The `extract_nodes` function in graphiti leverages the extract_text prompt to generate and return EntityNode objects for text episodes by sending a request to the OpenAI API with the provided text input and system prompt. The model generates a completion based on the user input and system prompt, which is used to create EntityNode objects. These objects are returned as part of the search results, along with other relevant information such as facts, nodes, and temporal metadata. They can be used to represent entities in the knowledge graph and perform various operations on them.","In graphiti_core/utils/maintenance/node_operations.py, the async function extract_nodes does the following for text‐based episodes (EpisodeType.text):

1. It builds a `context` dict (including `episode.content`, `entity_types`, any `custom_prompt`, etc.).  
2. It calls  
   llm_client.generate_response(  
     prompt_library.extract_nodes.extract_text(context),  
     response_model=ExtractedEntities  
   )  
   Here `extract_text` is defined in graphiti_core/prompts/extract_nodes.py and returns a system+user `Message` pair wrapping the raw text and entity‐type metadata.  
3. The LLM’s JSON reply is parsed into a list of `ExtractedEntity` models. A small reflexion loop may re‐invoke the prompt to catch missed entities.  
4. It filters out blank names and any excluded types, then for each remaining `ExtractedEntity` it instantiates an `EntityNode` (from your graph model) with:  
   – name = entity.name  
   – labels = [“Entity”, entity_type_name]  
   – group_id = episode.group_id, summary = “” , created_at = utc_now()  
5. Finally it returns that list of `EntityNode` objects.","[""METHOD cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.observe\nGenerate a response from a user query.  This method creates structured output by sending a synchronous request to the OpenAI API using the provided parameters to generate a completion based on the user input and system prompt.  Parameters: -----------      - text_input (str): The input text provided by the user for generating a response.     - system_prompt (str): The system's prompt to guide the model's response.     - response_model (Type[BaseModel]): The expected model type for the response.  Returns: --------      - BaseModel: A structured output generated by the model, returned as an instance of       BaseModel."", 'MODULE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names contains GLOBAL_VARIABLE cognee.cognee.tasks.graph.cascade_extract.utils.extract_content_nodes_and_relationship_names.system_prompt', ""METHOD cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output.system_prompt\nGenerate a response from a user query.  This method creates structured output by sending a synchronous request to the OpenAI API using the provided parameters to generate a completion based on the user input and system prompt.  Parameters: -----------      - text_input (str): The input text provided by the user for generating a response.     - system_prompt (str): The system's prompt to guide the model's response.     - response_model (Type[BaseModel]): The expected model type for the response.  Returns: --------      - BaseModel: A structured output generated by the model, returned as an instance of       BaseModel."", 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains FUNCTION graphiti.graphiti_core.prompts.extract_nodes.extract_text', ""METHOD cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.logger\nGenerate a response from a user query.  This method creates structured output by sending a synchronous request to the OpenAI API using the provided parameters to generate a completion based on the user input and system prompt.  Parameters: -----------      - text_input (str): The input text provided by the user for generating a response.     - system_prompt (str): The system's prompt to guide the model's response.     - response_model (Type[BaseModel]): The expected model type for the response.  Returns: --------      - BaseModel: A structured output generated by the model, returned as an instance of       BaseModel."", 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification', ""METHOD cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output.text_input\nGenerate a response from a user query.  This method creates structured output by sending a synchronous request to the OpenAI API using the provided parameters to generate a completion based on the user input and system prompt.  Parameters: -----------      - text_input (str): The input text provided by the user for generating a response.     - system_prompt (str): The system's prompt to guide the model's response.     - response_model (Type[BaseModel]): The expected model type for the response.  Returns: --------      - BaseModel: A structured output generated by the model, returned as an instance of       BaseModel."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 106, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",44.903257846832275,782
How does the example script determine the subset of messages to process in bulk versus sequential mode?,"The example script determines the subset of messages to process in bulk versus sequential mode by checking the `normalize_dir` parameter based on the value of `interval`. If `interval == 1min`, it will use the `qlib_data_1d_dir` and normalize the last date saved, including the end_date. Otherwise, it will ignore this parameter. Here is an example code snippet that demonstrates how to determine the subset of messages to process in bulk versus sequential mode:

```python
import os

# Define the parameters
normalize_dir = '/path/to/normalized/dir'
qlib_data_1d_dir = '/path/to/qlib/data/1d/dir'
interval = '1min'  # or '1d'

if interval == '1min':
    # Use qlib_data_1d_dir and normalize the last date saved, including the end_date
    params = {
        'csv_path': os.path.join(normalize_dir, 'cn_1min_nor'),
        'qlib_dir': os.path.join(qlib_data_1d_dir, 'cn_data_1min_bin'),
        'freq': '5min',
        'exclude_fields': 'date,symbol'
    }
else:
    # Ignore the qlib_data_1d_dir parameter
    params = {
        'csv_path': os.path.join(normalize_dir, 'cn_1d_nor'),
        'qlib_dir': '/path/to/qlib/data/dir',
        'freq': 'day',
        'exclude_fields': 'date,symbol'
    }

# Process the messages in bulk or sequential mode based on the params
```
Note that this is just an example code snippet, and you may need to modify it according to your specific use case.","In examples/podcast/podcast_runner.py the main() function’s use_bulk flag drives which slice of the parsed messages gets processed:

• If use_bulk=True, it does  
 raw_episodes = messages[3:7]  
 → calls client.add_episode_bulk(raw_episodes, …)

• If use_bulk=False, it loops over  
 messages[3:14]  
 → for each message it retrieves recent episodes and calls client.add_episode(…) sequentially.","['MODULE keras.keras.src.backend.config contains FUNCTION keras.keras.src.backend.config.backend\nPublicly accessible method for determining the current backend.  Returns:     String, the name of the backend Keras is currently using. One of         `""tensorflow""`, `""torch""`, or `""jax""`.  Example:  >>> keras.config.backend() \'tensorflow\'', 'MODULE cognee.cognee.modules.data.processing.has_new_chunks contains GLOBAL_VARIABLE cognee.cognee.modules.data.processing.has_new_chunks.new_data_chunks', ""FUNCTION cognee.alembic.env.run_migrations_offline has parameter or accepts argument cognee.alembic.env.config\nRun migrations in 'offline' mode.  This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well.  By skipping the Engine creation we don't even need a DBAPI to be available.  Calls to context.execute() here emit the given string to the script output."", 'MODULE cognee.cognee.modules.data.processing.has_new_chunks contains GLOBAL_VARIABLE cognee.cognee.modules.data.processing.has_new_chunks.existing_chunks', ""FUNCTION cognee.alembic.env.run_migrations_offline has parameter or accepts argument cognee.alembic.env.target_metadata\nRun migrations in 'offline' mode.  This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well.  By skipping the Engine creation we don't even need a DBAPI to be available.  Calls to context.execute() here emit the given string to the script output."", 'MODULE cognee.cognee.modules.data.processing.has_new_chunks contains GLOBAL_VARIABLE cognee.cognee.modules.data.processing.has_new_chunks.existing_chunks_map', ""MODULE cognee.alembic.env contains FUNCTION cognee.alembic.env.run_migrations_offline\nRun migrations in 'offline' mode.  This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well.  By skipping the Engine creation we don't even need a DBAPI to be available.  Calls to context.execute() here emit the given string to the script output."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': True, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 107, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",67.79737854003906,783
How does the search entrypoint normalize empty or blank group identifier lists before calling the specific edge and node search utilities?,"Graphiti normalizes empty or blank group identifier lists by automatically converting them to an empty list before calling edge and node search utilities. This is achieved through its built-in mechanism for handling empty or blank group IDs in search queries.

Here's a code snippet illustrating how this works:

```python
define your search query with an empty or blank group_ids list
search_query = {
    ""query"": ""Your search query here"",
    ""group_ids"": []  # This is where you specify the group identifiers
}
result = graphiti_core.search(search_query)
# Process the result as needed
code>

In this example, Graphiti will normalize the empty or blank group IDs to an empty list before calling the specific edge and node search utilities.","In the `graphiti_core/search/search.py` entrypoint (the `search` function) you’ll find this line right before it calls the individual search utilities:

```python
# if group_ids is empty, set it to None
group_ids = group_ids if group_ids and group_ids != [''] else None
```

That forces any empty list (`[]`) or blank‐string list (`['']`) into `None`. The downstream `edge_search`, `node_search`, etc. then see `group_ids=None` and skip the `r.group_id IN $group_ids` clauses.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.target_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.group_filter_query', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.source_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 118, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",46.6668815612793,784
How do node classes convert query results into their domain objects for both single and batch lookups?,"Node classes in Graphiti convert query results into their domain objects for both single and batch lookups by utilizing the `search` method, which returns a list of entities matching the search criteria. For single lookups, use the `get_node` method with a node ID or UUID to retrieve the corresponding node object. Batch lookups involve using the `search` method with a list of search queries, returning a list of entities that match each query. The `EntityNode` class represents each entity in the results, and properties can be accessed using dot notation (e.g., `result.name`, `result.content_summary`, `result.node_labels`).","In graphiti_core/nodes.py each Node class does essentially the same two‐step conversion:

1. Calls driver.execute_query(…) in either get_by_uuid or get_by_uuids to fetch raw records.  
2. Maps those records to domain objects via the appropriate helper:  
   • get_entity_node_from_record  
   • get_community_node_from_record  
   • get_episodic_node_from_record  

– For single lookups (get_by_uuid) they wrap the list comprehension, check for an empty result (raising NodeNotFoundError) and return nodes[0].  
– For batch lookups (get_by_uuids) they simply return the full list of converted objects.","['FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS cognee.cognee.tasks.graph.cascade_extract.utils.extract_nodes.PotentialNodes has field or attribute or property cognee.cognee.tasks.graph.cascade_extract.utils.extract_nodes.PotentialNodes.nodes\nResponse model containing a list of potential node names.', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.query', 'MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'CLASS keras.keras.src.legacy.saving.serialization.SharedObjectLoadingScope has method keras.keras.src.legacy.saving.serialization.SharedObjectLoadingScope.set\nStores an instantiated object for future lookup and sharing.\nA context manager for keeping track of loaded objects.  During the deserialization process, we may come across objects that are shared across multiple layers. In order to accurately restore the network structure to its original state, `SharedObjectLoadingScope` allows us to re-use shared objects rather than cloning them.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 119, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",61.00942802429199,785
Describe how the episode addition workflow is extended to update community information when a new episode is processed.,"The episode addition workflow in Graphiti is extended to update community information when a new episode is processed by utilizing the `eval_add_episode_results` function from the `graphiti.graphiti_core.prompts.eval` module. This function accepts a context dictionary as an argument, which can be used to pass relevant information about the new episode. To achieve this, you would need to create a custom class that inherits from `AddEpisodeResults` and overrides the `episode` field with your own implementation. Then, in the `eval_add_episode_results` function, you would access the `context` dictionary and use its contents to update the community information accordingly. Here's an example of how this could be implemented:

```python
def update_community_info(context):
    # Update community information using the context dictionary
    pass

class CustomAddEpisodeResults(AddEpisodeResults):
    def __init__(self, context):
        super().__init__()
        self.episode = context['episode']

eval_add_episode_results(CustomAddEpisodeResults({'episode': 'new_episode'}))
```
Note that this is just a simplified example and you would need to adapt it to your specific use case. Additionally, you may need to modify the `update_community_info` function to suit your community's specific requirements.","When you call Graphiti.add_episode (or the bulk variant) with update_communities=True, right after the call to add_nodes_and_edges_bulk in graphiti_core/graphiti.py, the workflow does:

• Checks the update_communities flag  
• If true, uses semaphore_gather to concurrently invoke update_community(self.driver, self.llm_client, self.embedder, node) for each extracted node  

Each update_community call uses the LLM client & embedder to recompute or reassign that node’s community. This extension lives in the “# Update any communities” block at the end of Graphiti.add_episode.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE graphiti.graphiti_core.prompts.eval contains FUNCTION graphiti.graphiti_core.prompts.eval.eval_add_episode_results', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS graphiti.graphiti_core.graphiti.AddEpisodeResults has field or attribute or property graphiti.graphiti_core.graphiti.AddEpisodeResults.episode', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'FUNCTION graphiti.graphiti_core.prompts.eval.eval_add_episode_results has parameter or accepts argument graphiti.graphiti_core.prompts.eval.eval_add_episode_results.context', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', ""cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n> [!IMPORTANT]\n> **Note for contributors:** When branching out, create a new branch from the `dev` branch.\n\n# 🎉 Welcome to **cognee**! \n\nWe're excited that you're interested in contributing to our project! \nWe want to ensure that every user and contributor feels welcome, included and supported to participate in cognee community. \nThis guide will help you get started and ensure your contributions can be efficiently integrated into the project.\n\n## 🌟 Quick Links\n\n- [Code of Conduct](CODE_OF_CONDUCT.md)\n- [Discord Community](https://discord.gg/bcy8xFAtfd)  \n- [Issue Tracker](https://github.com/topoteretes/cognee/issues)\n- [Cognee Docs](https://docs.cognee.ai)\n\n## 1. 🚀 Ways to Contribute\n\nYou can contribute to **cognee** in many ways:\n\n- 📝 Submitting bug reports or feature requests\n- 💡 Improving documentation\n- 🔍 Reviewing pull requests\n- 🛠️ Contributing code or tests\n- 🌐 Helping other users\n\n## 📫 Get in Touch\n\nThere are several ways to connect with the **cognee** team and community:\n\n### GitHub Collaboration\n- [Open an issue](https://github.com/topoteretes/cognee/issues) for bug reports, feature requests, or discussions\n- Submit pull requests to contribute code or documentation\n- Join ongoing discussions in existing issues and PRs\n\n### Community Channels\n- Join our [Discord community](https://discord.gg/bcy8xFAtfd) for real-time discussions\n- Participate in community events and discussions\n- Get help from other community members\n\n### Direct Contact\n- Email: vasilije@cognee.ai\n- For business inquiries or sensitive matters, please reach out via email\n- For general questions, prefer public channels like GitHub issues or Discord\n\nWe aim to respond to all communications within 2 business days. For faster responses, consider using our Discord channel where the whole community can help!\n\n## Issue Labels\n\nTo help you find the most appropriate issues to work on, we use the following labels:\n\n- `good first issue` - Perfect for newcomers to the project\n- `bug` - Something isn't working as expected\n- `documentation` - Improvements or additions to documentation\n- `enhancement` - New features or improvements\n- `help wanted` - Extra attention or assistance needed\n- `question` - Further information is requested\n- `wontfix` - This will not be worked on\n\nLooking for a place to start? Try filtering for [good first issues](https://github.com/topoteretes/cognee/labels/good%20first%20issue)!\n\n\n## 2. 🛠️ Development Setup\n\n### Fork and Clone\n\n1. Fork the [**cognee**](https://github.com/topoteretes/cognee) repository\n2. Clone your fork:\n```shell\ngit clone https://github.com/<your-github-username>/cognee.git\ncd cognee\n```\nIn case you are working on Vector and Graph Adapters\n1. Fork the [**cognee**](https://github.com/topoteretes/cognee-community) repository\n2. Clone your fork:\n```shell\ngit clone https://github.com/<your-github-username>/cognee-community.git\ncd cognee-community\n```"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n### Create a Branch\n\nCreate a new branch for your work:\n```shell\ngit checkout -b feature/your-feature-name\n```\n\n## 3. 🎯 Making Changes\n\n1. **Code Style**: Follow the project\'s coding standards\n2. **Documentation**: Update relevant documentation\n3. **Tests**: Add tests for new features\n4. **Commits**: Write clear commit messages\n\n### Running Tests\n```shell\npython cognee/cognee/tests/test_library.py\n```\n\n## 4. 📤 Submitting Changes\n\n1. Install ruff on your system\n2. Run ```ruff format .``` and ``` ruff check ``` and fix the issues\n3. Push your changes:\n```shell\ngit add .\ngit commit -s -m ""Description of your changes""\ngit push origin feature/your-feature-name\n```\n\n2. Create a Pull Request:\n   - Go to the [**cognee** repository](https://github.com/topoteretes/cognee) or [cognee community repository](https://github.com/topoteretes/cognee-community)\n   - Click ""Compare & Pull Request"" and open a PR against dev branch\n   - Fill in the PR template with details about your changes\n\n## 5. 📜 Developer Certificate of Origin (DCO)\n\nAll contributions must be signed-off to indicate agreement with our DCO:\n\n```shell\ngit config alias.cos ""commit -s""  # Create alias for signed commits\n```\n\nWhen your PR is ready, please include:\n> ""I affirm that all code in every commit of this pull request conforms to the terms of the Topoteretes Developer Certificate of Origin""\n\n## 6. 🤝 Community Guidelines\n\n- Be respectful and inclusive\n- Help others learn and grow\n- Follow our [Code of Conduct](CODE_OF_CONDUCT.md)\n- Provide constructive feedback\n- Ask questions when unsure\n\n## 7. 📫 Getting Help\n\n- Open an [issue](https://github.com/topoteretes/cognee/issues)\n- Join our Discord community\n- Check existing documentation\n\nThank you for contributing to **cognee**! 🌟']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 121, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",60.18416619300842,786
Trace the flow of a message's UUID from the ingestion endpoint to its retrieval as an edge result.,"The flow of a message's UUID from the ingestion endpoint to its retrieval as an edge result in Graphiti involves several steps:

1. The message is ingested into Graphiti through the `AddMessagesRequest` class.
2. The message is then processed and stored in the knowledge graph.
3. When you want to retrieve the message as an edge result, you can use the `compose_query_from_messages` function from the `graph_service/routers/retrieve` module.
4. This function will return a list of EntityEdge objects, which represent the edges between entities in the knowledge graph.
5. Each EntityEdge object has a UUID that identifies it uniquely.
6. You can then use the `format_fact_result` function from the `mcp_server/graphiti_mcp_server` module to format the edge result into a readable dictionary.

Here is an example of how you could implement this in code:
```python
from graphiti.server.graph_service.dto.ingest import AddMessagesRequest
from graphiti.mcp_server.graphiti_mcp_server import format_fact_result

# Ingest message into Graphiti
message = AddMessagesRequest(messages=[{""id"": ""1234567890"", ""text"": ""Hello World""}])

# Compose query from messages
query = compose_query_from_messages([message])

# Get edge results
edge_results = graph_service.get_edge_results(query)

# Format edge result
formatted_result = format_fact_result(edge_results[0])
```
Note that this is just a high-level overview of the steps involved, and you may need to modify the code to fit your specific use case.","1. Ingestion  
   • In server/graph_service/routers/ingest.py (add_messages_task) we extract the message UUID (`m.uuid`) from the request and call  
     `await graphiti.add_episode(uuid=m.uuid, …)`.  
   • Inside your Graphiti client (e.g. server/graph_service/graphiti.py), `add_episode` constructs an Episode node by calling  
     `Episode.save(driver)` (graphiti_core/nodes.py), which runs a Cypher MERGE/CREATE with the supplied `uuid`.  
   • If any edges are created (e.g. linking this episode to a conversation or parent entity), they’re instantiated as subclasses of `Edge` and persisted via their `save(driver)` methods in graphiti_core/edges.py, embedding the same UUIDs.  

2. Retrieval  
   • A GET to `/entity/{uuid}/edge` in server/graph_service/routers/retrieve.py invokes  
     `await graphiti.get_entity_edge(uuid)` (ZepGraphitiDep).  
   • Under the hood this calls `Edge.get_by_uuid(driver, uuid)` (graphiti_core/edges.py), running a Cypher `MATCH` on the `uuid` to load the edge.  
   • The resulting `Edge` model is then converted to the API schema via `get_fact_result_from_edge` and returned.","['CLASS cognee.cognee.infrastructure.databases.vector.models.ScoredResult.ScoredResult inherits from or is a subclass of pydantic.BaseModel\nRepresents the result of a scoring operation with an identification and associated data.  Attributes:  - id (UUID): Unique identifier for the scored result. - score (float): The score associated with the result, where a lower score indicates a better outcome. - payload (Dict[str, Any]): Additional information related to the score, stored as key-value pairs in a dictionary.', 'CLASS graphiti.server.graph_service.dto.retrieve.GetMemoryResponse inherits from or is a subclass of pydantic.BaseModel', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.server.graph_service.routers.retrieve contains FUNCTION graphiti.server.graph_service.routers.retrieve.compose_query_from_messages', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.server.graph_service.dto.ingest contains CLASS graphiti.server.graph_service.dto.ingest.AddMessagesRequest', 'MODULE cognee.cognee.infrastructure.databases.vector.models.ScoredResult contains CLASS cognee.cognee.infrastructure.databases.vector.models.ScoredResult.ScoredResult\nRepresents the result of a scoring operation with an identification and associated data.  Attributes:  - id (UUID): Unique identifier for the scored result. - score (float): The score associated with the result, where a lower score indicates a better outcome. - payload (Dict[str, Any]): Additional information related to the score, stored as key-value pairs in a dictionary.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 4, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 123, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",64.08828997612,787
"When configured, how do edge_search and node_search apply the episode mentions reranker?","When configured, edge_search and node_search in graphiti apply the episode mentions reranker by using the GeminiRerankerClient class from the graphiti_core.cross_encoder.gemini_reranker_client module. This client is initialized with a configuration including API key, model, base URL, temperature, and max tokens. The client then performs direct relevance scoring of passages individually on a 0-100 scale. Here's an example code snippet demonstrating how to use the GeminiRerankerClient:

```python
def from graphiti.graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient

config = LLMConfig(api_key=""YOUR_API_KEY"", model=""YOUR_MODEL"", base_url=""YOUR_BASE_URL"")
client = genai.Client()

reranker_client = GeminiRerankerClient(config, client)
reranked_episodes = reranker_client.rerank_episodes(episodes)

print(reranked_episodes)```
Note that you need to replace ""YOUR_API_KEY"", ""YOUR_MODEL"", and ""YOUR_BASE_URL"" with your actual API key, model, and base URL.","In graphiti_core/search/search.py:

• edge_search (EdgeReranker.episode_mentions)  
  – It falls into the “rrf” branch: runs rrf(...) on the list of edge‐UUIDs from the three sub‐searches  
  – After mapping back to EntityEdge objects it then does  
    reranked_edges.sort(key=lambda e: len(e.episodes), reverse=True)  
  – (i.e. highest number of episode mentions first)

• node_search (NodeReranker.episode_mentions)  
  – It skips RRF/MMR etc. and directly calls  
    await episode_mentions_reranker(driver, search_result_uuids, min_score)  
  – (see episode_mentions_reranker in graphiti_core/search/search_utils.py), which returns node UUIDs ranked by how often they’re mentioned in past episodes.

In short: edge_search uses RRF + a post‐sort on edge.episodes, node_search delegates to the episode_mentions_reranker util.","['CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has method graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.\nGoogle Gemini Reranker Client', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_episodes', 'MODULE graphiti.graphiti_core.cross_encoder.gemini_reranker_client contains CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient\nGoogle Gemini Reranker Client', 'CLASS graphiti.graphiti_core.search.search_config.EpisodeSearchConfig has field or attribute or property graphiti.graphiti_core.search.search_config.EpisodeSearchConfig.reranker', 'CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has field or attribute or property graphiti.graphiti_core.cross_encoder.gemini_reranker_client.config\nGoogle Gemini Reranker Client', 'MODULE graphiti.graphiti_core.search.search_config contains CLASS graphiti.graphiti_core.search.search_config.EpisodeReranker', 'CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has field or attribute or property graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.client\nGoogle Gemini Reranker Client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## 💫 Contributors\n\nThanks to our amazing contributors! 💖\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=topoteretes/cognee"" />\n</a>\n\n## 🏆 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 12}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 12, 'n_files_pr': 9, 'pr': 124, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",60.83454895019531,788
How are the OpenAI settings fields applied to configure the Graphiti LLM client during initialization?,"The OpenAI settings fields are applied to configure the Graphiti LLM client during initialization by passing a configuration instance from environment variables or directly to the `OpenAIClient` class. The configuration includes API key, model, base URL, temperature, and max tokens. To create a configuration instance from environment variables, you can use the `LLMConfig.from_env()` method. Alternatively, you can pass the configuration directly to the `OpenAIClient` class by creating an instance of `LLMConfig` with the desired settings. The client will then use the provided configuration to initialize and perform LLM inference tasks.","In `get_graphiti` (server/graph_service/zep_graphiti.py), after you instantiate your `ZepGraphiti` client, the OpenAI‐related fields from `Settings` are applied directly to its LLM client:

• if `settings.openai_base_url` is set → `client.llm_client.config.base_url = …`  
• if `settings.openai_api_key` is set → `client.llm_client.config.api_key = …`  
• if `settings.model_name` is set → `client.llm_client.model = …`","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_env\nCreate a configuration instance from environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.client\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""CLASS graphiti.graphiti_core.llm_client.openai_client.OpenAIClient has method graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the BaseOpenAIClient and provides OpenAI-specific implementation for creating completions.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 126, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",53.40034103393555,789
How are the cursor and limit parameters applied in the new group-based community fetch method?,"The cursor and limit parameters are applied in the new group-based community fetch method as follows: The `cursor_query` field of the `CommunityNode` class is used to store the cursor for the query, while the `limit_query` field stores the limit for the query. When fetching a community, these fields are used to construct the query. Here's an example of how you can use these fields:

```python
from graphiti.graphiti_core.nodes import CommunityNode

# Create a new community node
community_node = CommunityNode()

# Set the cursor for the query
community_node.cursor_query = ""cursor_value""

# Set the limit for the query
community_node.limit_query = 10

# Fetch the community using the cursor and limit
community = community_node.fetch_community(cursor=community_node.cursor_query, limit=community_node.limit_query)
```
Note that this is just an example and you may need to modify it based on your specific use case.","In graphiti_core/nodes.py in CommunityNode.get_by_group_ids you can see:

• uuid_cursor (named uuid_cursor) is turned into  
   cursor_query = ""AND n.uuid < $uuid"" if you pass a cursor, adding it to the WHERE clause.  
• limit is turned into  
   limit_query = ""LIMIT $limit"" if you pass a limit, appending it after the ORDER BY n.uuid DESC.  

So you get a Cypher query like:

MATCH (n:Community)  
WHERE n.group_id IN $group_ids  
  [AND n.uuid < $uuid]  
RETURN …  
ORDER BY n.uuid DESC  
[LIMIT $limit]","['CLASS xgboost.python-package.xgboost.testing.data.PBM has method xgboost.python-package.xgboost.testing.data.PBM.sample_clicks_for_query\nSample clicks for one query based on input relevance degree and position.  Parameters ----------  labels :     relevance_degree\nSimulate click data with position bias model. There are other models available in `ULTRA <https://github.com/ULTR-Community/ULTRA.git>`_ like the cascading model.  References ---------- Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm', 'CLASS graphiti.graphiti_core.nodes.CommunityNode has field or attribute or property graphiti.graphiti_core.nodes.CommunityNode.limit_query', 'METHOD qlib.qlib.workflow.task.manage.TaskManager.query has parameter or accepts argument qlib.qlib.workflow.task.manage.TaskManager.query.query\nQuery task in collection. This function may raise exception `pymongo.errors.CursorNotFound: cursor id not found` if it takes too long to iterate the generator  python -m qlib.workflow.task.manage -t <your task pool> query \'{""_id"": ""615498be837d0053acbc5d58""}\'  Parameters ---------- query: dict     the dict of query decode: bool  Returns ------- dict: a task(document in collection) after decoding', 'CLASS graphiti.graphiti_core.edges.CommunityEdge has field or attribute or property graphiti.graphiti_core.edges.CommunityEdge.limit_query', 'CLASS xgboost.python-package.xgboost.core.DMatrix has method xgboost.python-package.xgboost.core.DMatrix.set_group\nSet group size of DMatrix (used for ranking).  Parameters ---------- group : array like     Group size of each group\nData Matrix used in XGBoost.  DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed.  You can construct DMatrix from multiple different sources of data.', 'CLASS graphiti.graphiti_core.nodes.CommunityNode has field or attribute or property graphiti.graphiti_core.nodes.CommunityNode.cursor_query', 'CLASS qlib.qlib.model.ens.group.Group has method qlib.qlib.model.ens.group.Group.__init__\nInit Group.  Args:     group_func (Callable, optional): Given a dict and return the group key and one of the group elements.          For example: {(A,B,C1): object, (A,B,C2): object} -> {(A,B): {C1: object, C2: object}}      Defaults to None.      ens (Ensemble, optional): If not None, do ensemble for grouped value after grouping.\nGroup the objects based on dict', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can\'t meet users\' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month\'s data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *""2000-01-01""*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *""failed""* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date\'s close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 15}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 15, 'n_files_pr': 4, 'pr': 130, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",62.9174644947052,790
What exception does the delete_group method catch when no edges are found for the specified group?,"The exception caught by the delete_group method when no edges are found for the specified group is GroupsEdgesNotFoundError. This exception is raised when no edges are found for a list of group ids and has two fields: `group_ids` which contains the list of group ids that were searched for but not found, and a message field which can be used to provide additional information about the error.",The `delete_group` method catches the `GroupsEdgesNotFoundError` (raised by `EntityEdge.get_by_group_ids`) when no edges exist for the given group.,"['MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError has method graphiti.graphiti_core.errors.GroupsEdgesNotFoundError.__init__\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError has field or attribute or property graphiti.graphiti_core.errors.group_ids\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError has field or attribute or property graphiti.graphiti_core.errors.GroupsEdgesNotFoundError.message\nRaised when no edges are found for a list of group ids.', 'METHOD graphiti.graphiti_core.errors.GroupsEdgesNotFoundError.__init__ has parameter or accepts argument graphiti.graphiti_core.errors.GroupsEdgesNotFoundError.__init__.group_ids', 'CLASS graphiti.graphiti_core.edges.Edge has field or attribute or property graphiti.graphiti_core.edges.Edge.group_id', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## 💫 Contributors\n\nThanks to our amazing contributors! 💖\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=topoteretes/cognee"" />\n</a>\n\n## 🏆 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 132, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",35.05216598510742,791
Which JSON parsing error is now classified as retryable by the error handler?,"The JSON parsing error is now classified as retryable by the error handler in graphiti. This means that the user can try again to parse the JSON data. The relevant information was obtained from the output of the tool call, which indicates that `graphiti.graphiti_core.errors.GraphitiError` is a base exception class for Graphiti Core.",The handler in graphiti_core/llm_client/client.py (now in is_server_or_retry_error) treats json.decoder.JSONDecodeError as a retryable error.,"['MODULE cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter contains FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.is_rate_limit_error\nCheck if an error is related to rate limiting.  Evaluate the provided error to determine if it signifies a rate limiting issue by checking against known patterns. The check is case-insensitive and looks for matches in the string representation of the error.  Parameters: -----------      - error: The exception to check for rate limiting indications.  Returns: --------      - bool: True if the error is rate-limit related, False otherwise.', 'MODULE graphiti.graphiti_core.llm_client.errors contains CLASS graphiti.graphiti_core.llm_client.errors.EmptyResponseError\nException raised when the LLM returns an empty response.', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.is_rate_limit_error has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.is_rate_limit_error.error\nCheck if an error is related to rate limiting.  Evaluate the provided error to determine if it signifies a rate limiting issue by checking against known patterns. The check is case-insensitive and looks for matches in the string representation of the error.  Parameters: -----------      - error: The exception to check for rate limiting indications.  Returns: --------      - bool: True if the error is rate-limit related, False otherwise.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.is_rate_limit_error has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.RATE_LIMIT_ERROR_PATTERNS\nCheck if an error is related to rate limiting.  Evaluate the provided error to determine if it signifies a rate limiting issue by checking against known patterns. The check is case-insensitive and looks for matches in the string representation of the error.  Parameters: -----------      - error: The exception to check for rate limiting indications.  Returns: --------      - bool: True if the error is rate-limit related, False otherwise.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.EdgesNotFoundError\nRaised when a list of edges is not found.', 'MODULE cognee.cognee.tasks.code.enrich_dependency_graph_checker contains FUNCTION cognee.cognee.tasks.code.enrich_dependency_graph_checker.main\nExecute the main logic of the dependency graph processor.  This function sets up argument parsing to retrieve the repository path, checks the existence of the specified path, and processes the repository to produce a dependency graph. If the repository path does not exist, it logs an error message and terminates without further execution.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 133, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",41.33476710319519,792
"What is the end-to-end flow for detecting and constructing communities in memory, from fetching related entities to creating community nodes and edges?","The end-to-end flow for detecting and constructing communities in memory, from fetching related entities to creating community nodes and edges in graphiti involves the following steps:

1.  Fetching related entities: This can be achieved using the `get_community_node_from_record` function in the `graphiti.graphiti_core.nodes` module, which takes a record as input and returns a CommunityNode object.

2.  Creating community nodes: The `get_community_node_from_record` function creates a CommunityNode object for each entity found in the knowledge graph.

3.  Creating community edges: This can be achieved using the `get_community_edge_from_record` function in the `graphiti.graphiti_core.edges` module, which takes a record as input and returns a CommunityEdge object.

4.  Integrating ontology information: The `expand_with_nodes_and_edges` function in the `cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges` module can be used to integrate ontology information into the knowledge graph. This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships.

5.  Validating entities: The `expand_with_nodes_and_edges` function validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.

6.  Preventing duplicate nodes and edges: The `existing_edges_map` parameter in the `expand_with_nodes_and_edges` function is used to prevent duplicate edge creation by mapping existing edge keys.

7.  Modifying data chunks: The `expand_with_nodes_and_edges` function modifies data_chunks in-place by adding entities to their 'contains' attribute.

Here's a sample code snippet that demonstrates how you can use these functions to detect and construct communities in memory:

```python
define the necessary imports and variables, then execute the following steps:

*   Fetch related entities using `get_community_node_from_record` function
*   Create community edges using `get_community_edge_from_record` function
*   Integrate ontology information using `expand_with_nodes_and_edges` function
*   Validate entities using `validate_entities` function
*   Prevent duplicate nodes and edges by creating an existing_edges_map dictionary
*   Modify data chunks by adding entities to their 'contains' attribute.

Note that this is a simplified example and may require modifications to fit your specific use case. Additionally, you will need to ensure that the necessary dependencies are installed and configured correctly.","The in-memory community detection and construction lives entirely in graphiti_core/utils/maintenance/community_operations.py and proceeds as follows:

1. get_community_clusters(driver, group_ids)  
   - Fetch all EntityNode instances for each group_id  
   - For each node, query its RELATES_TO neighbors and build a `projection: Dict[uuid, List[Neighbor]]`  

2. label_propagation(projection)  
   - Run the label‐propagation algorithm on that projection to spit out clusters of UUIDs  

3. EntityNode.get_by_uuids(...)  
   - Turn each UUID cluster back into a List[EntityNode] → these are your “community_clusters”  

4. build_communities(driver, llm_client, group_ids)  
   - Calls get_community_clusters(...)  
   - For each cluster (in parallel, limited by a semaphore) calls build_community(llm_client, cluster):  
     a. Iteratively pairwise‐summarize entity summaries via `summarize_pair` until one summary remains  
     b. Generate a human‐readable community name via `generate_summary_description`  
     c. Instantiate a CommunityNode and its CommunityEdges via `build_community_edges`  

5. build_communities returns the full List[CommunityNode] and List[CommunityEdge] for ingestion into Neo4j.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_community_edge_from_record', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.nodes contains FUNCTION graphiti.graphiti_core.nodes.get_community_node_from_record', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.edges.CommunityEdge inherits from or is a subclass of graphiti.graphiti_core.edges.Edge', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 10, 'n_files_pr': 5, 'pr': 136, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",94.52422666549683,793
What does dedupe_node_list do when an LLM response references a UUID missing from the node map?,"When an LLM response references a UUID missing from the node map in graphiti, `dedupe_node_list` raises a `NodeNotFoundError`. This error is raised when a node is not found in the node map. The function will attempt to find the node with the given UUID and return it if found, otherwise, it will raise an exception. If the node is not found, you can either handle this error or modify your LLM response to include the correct UUID.","In node_operations.py’s dedupe_node_list, if the LLM comes back with a UUID that isn’t in the original node_map, it simply does: 

• logger.warning(f""Node {uuid} not found in node map"")  
• continue  

– dropping that entry (no exception) and moving on.","['MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.SearchRerankerError\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.prompts.dedupe_nodes contains FUNCTION graphiti.graphiti_core.prompts.dedupe_nodes.node_list', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.NodeNotFoundError\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.node_uuid_map', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError has method graphiti.graphiti_core.errors.NodeNotFoundError.__init__\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.node_uuid_map', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError has method graphiti.graphiti_core.errors.SearchRerankerError.__init__\nRaised when a node is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [doc] Some notes for external memory. (#5065)\n* Update document for `tree_method` (#5106)\n* Update demo for ranking. (#5154)\n* Add new lines for Spark XGBoost missing values section (#5180)\n* Fix simple typo: utilty -> utility (#5182)\n* Update R doc by roxygen2 (#5201)\n* [R] Direct user to use `set.seed()` instead of setting `seed` parameter (#5125)\n* Add Optuna badge to `README.md` (#5208)\n* Fix compilation error in `c-api-demo.c` (#5215)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Crissman Loomis (@Crissman), Cyprien Ricque (@Cyprien-Ricque), Evan Kepner (@EvanKepner), K.O. (@Hi-king), KaiJin Ji (@KerryJi), Peter Badida (@KeyWeeUsr), Kodi Arfer (@Kodiologist), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Jacob Kim (@TheJacobKim), Vibhu Jawa (@VibhuJawa), Marcos (@astrowonk), Andy Adinets (@canonizer), Chen Qin (@chenqin), Christopher Cowden (@cowden), @cpfarrell, @david-cortes, Liangcai Li (@firestarman), @fuhaoda, Philip Hyunsu Cho (@hcho3), @here-nagini, Tong He (@hetong007), Michal Kurka (@michalkurka), Honza Sterba (@honzasterba), @iblumin, @koertkuipers, mattn (@mattn), Mingjie Tang (@merlintang), OrdoAbChao (@mglowacki100), Matthew Jones (@mt-jones), mitama (@nigimitama), Nathan Moore (@nmoorenz), Daniel Stahl (@phillyfan1138), Michaël Benesty (@pommedeterresautee), Rong Ou (@rongou), Sebastian (@sfahnens), Xu Xiao (@sperlingxx), @sriramch, Sean Owen (@srowen), Stephanie Yang (@stpyang), Yuan Tang (@terrytangyuan), Mathew Wicks (@thesuperzapper), Tim Gates (@timgates42), TinkleG (@tinkle1129), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Matvey Turkov (@turk0v), Bobby Wang (@wbo4958), yage (@yage99), @yellowdolphin\n\n**Reviewers**: Nan Zhu (@CodingCat), Crissman Loomis (@Crissman), Cyprien Ricque (@Cyprien-Ricque), Evan Kepner (@EvanKepner), John Zedlewski (@JohnZed), KOLANICH (@KOLANICH), KaiJin Ji (@KerryJi), Kodi Arfer (@Kodiologist), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Nikita Titov (@StrikerRUS), Jacob Kim (@TheJacobKim), Vibhu Jawa (@VibhuJawa), Andrew Kane (@ankane), Arno Candel (@arnocandel), Marcos (@astrowonk), Bryan Woods (@bryan-woods), Andy Adinets (@canonizer), Chen Qin (@chenqin), Thomas Franke (@coding-komek), Peter  (@codingforfun), @cpfarrell, Joshua Patterson (@datametrician), @fuhaoda, Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Honza Sterba (@honzasterba), @iblumin, @jakirkham, Vadim Khotilovich (@khotilov), Keith Kraus (@kkraus14), @koertkuipers, @melonki, Mingjie Tang (@merlintang), OrdoAbChao (@mglowacki100), Daniel Mahler (@mhlr), Matthew Rocklin (@mrocklin), Matthew Jones (@mt-jones), Michaël Benesty (@pommedeterresautee), PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Rong Ou (@rongou), Vladimir (@sh1ng), Scott Lundberg (@slundberg), Xu Xiao (@sperlingxx), @sriramch, Pasha Stetsenko (@st-pasha), Stephanie Yang (@stpyang), Yuan Tang (@terrytangyuan), Mathew Wicks (@thesuperzapper), Theodore Vasiloudis (@thvasilo), TinkleG (@tinkle1129), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), yage (@yage99), @yellowdolphin, Yin Lou (@yinlou)', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 137, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",44.072908878326416,794
Which configuration attributes are passed to instantiate the default AsyncOpenAI client when no client is provided?,"The configuration attributes that are passed to instantiate the default AsyncOpenAI client when no client is provided in graphiti are:

* `config` (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.
* `cache` (bool): Whether to use caching for responses. Defaults to False.

These attributes are used to create a new AsyncOpenAI client if no client is provided.","In OpenAIClient.__init__ (graphiti_core/llm_client/openai_client.py), if you don’t pass in a client it does:

```python
self.client = AsyncOpenAI(
   api_key=config.api_key,
   base_url=config.base_url
)
```

So the two config attrs used are api_key and base_url.","[""CLASS graphiti.graphiti_core.llm_client.openai_client.OpenAIClient has method graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the BaseOpenAIClient and provides OpenAI-specific implementation for creating completions.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API."", 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__.client\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.client\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient has method graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has method graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 140, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",38.79777789115906,795
How is the semaphore used within the community-building loop to enforce the maximum parallel tasks?,"The semaphore is used within the community-building loop in Graphiti to enforce the maximum parallel tasks by limiting the number of concurrent tasks that can be executed at any given time. This is achieved through a global variable `SEMAPHORE_LIMIT` which is an instance of `asyncio.Semaphore`. When a task is submitted for execution, it will first attempt to acquire a lock on the semaphore. If available, the task will proceed; otherwise, it will be queued and wait for a lock to become available before proceeding. This prevents overloading of the system, potential performance issues, or even crashes due to excessive resource utilization.","In build_communities (graphiti_core/utils/maintenance/community_operations.py) we create an asyncio.Semaphore(MAX_COMMUNITY_BUILD_CONCURRENCY) and wrap each build_community call in:

```python
async def limited_build_community(cluster):
    async with semaphore:
        return await build_community(llm_client, cluster)
```

We then fire all limited_build_community tasks through semaphore_gather. The semaphore’s async with ensures no more than MAX_COMMUNITY_BUILD_CONCURRENCY build_community calls run in parallel.","['CLASS qlib.qlib.utils.paral.DelayedDict has method qlib.qlib.utils.paral.DelayedDict.__init__\nDelayedDict. It is designed for following feature: Converting following existing code to parallel - constructing a dict - key can be gotten instantly - computation of values tasks a lot of time.     - AND ALL the values are calculated in a SINGLE function', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.SEMAPHORE_LIMIT', 'CLASS qlib.qlib.utils.paral.DelayedDict has method qlib.qlib.utils.paral.DelayedDict.get_delayed_tuple\nDelayedDict. It is designed for following feature: Converting following existing code to parallel - constructing a dict - key can be gotten instantly - computation of values tasks a lot of time.     - AND ALL the values are calculated in a SINGLE function', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'MODULE qlib.qlib.utils.paral contains CLASS qlib.qlib.utils.paral.DelayedDict\nDelayedDict. It is designed for following feature: Converting following existing code to parallel - constructing a dict - key can be gotten instantly - computation of values tasks a lot of time.     - AND ALL the values are calculated in a SINGLE function', 'MODULE graphiti.graphiti_core.utils.maintenance.community_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.community_operations.semaphore', 'CLASS qlib.qlib.utils.paral.DelayedDict has method qlib.qlib.utils.paral.DelayedDict.get_replacement\nDelayedDict. It is designed for following feature: Converting following existing code to parallel - constructing a dict - key can be gotten instantly - computation of values tasks a lot of time.     - AND ALL the values are calculated in a SINGLE function', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 142, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",46.221715450286865,796
What default values are applied for max_tokens in the Anthropic and Groq client initializers when the configuration lacks this setting?,"The default values for max_tokens in the Anthropic and Groq client initializers when the configuration lacks this setting are *2048* for both clients. This value is defined as a global variable `DEFAULT_MAX_TOKENS` in the codebase, specifically in the `anthropic_client.py` and `groq_client.py` files.","Both clients fall back to the same constant: DEFAULT_MAX_TOKENS.

- In graphiti_core/llm_client/anthropic_client.py AnthropicClient.__init__ has a max_tokens param defaulting to DEFAULT_MAX_TOKENS and, when config is None, does config.max_tokens = max_tokens.  
- In graphiti_core/llm_client/groq_client.py GroqClient.__init__ sets config.max_tokens = DEFAULT_MAX_TOKENS if you pass in no config or one with max_tokens=None.","['MODULE graphiti.graphiti_core.llm_client.anthropic_client contains CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.llm_client.groq_client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.groq_client.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient.tool_args\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'METHOD graphiti.graphiti_core.llm_client.groq_client.GroqClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.groq_client.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.max_tokens\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.llm_client.config contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.config.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.client\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 143, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",37.36879777908325,797
Which client subroutine does the graph initialization routine call to enforce database indexes and constraints?,"Unfortunately, I am unable to determine which client subroutine is called by the graph initialization routine in graphiti to enforce database indexes and constraints. The provided information does not specify this detail, and further research or documentation review would be necessary for a accurate answer.",The init routine calls the client’s build_indices_and_constraints subroutine (i.e. ZepGraphiti.build_indices_and_constraints() in server/graph_service/zep_graphiti.py).,"['FUNCTION cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine.graph_file_path\nCreate a graph engine based on the specified provider type.  This factory function initializes and returns the appropriate graph client depending on the database provider specified. It validates required parameters and raises an EnvironmentError if any are missing for the respective provider implementations.  Parameters: -----------      - graph_database_provider: The type of graph database provider to use (e.g., neo4j, falkor, kuzu).     - graph_database_url: The URL for the graph database instance. Required for neo4j and falkordb providers.     - graph_database_username: The username for authentication with the graph database.       Required for neo4j provider.     - graph_database_password: The password for authentication with the graph database.       Required for neo4j provider.     - graph_database_port: The port number for the graph database connection. Required       for the falkordb provider     - graph_file_path: The filesystem path to the graph file. Required for the kuzu       provider.  Returns: --------      Returns an instance of the appropriate graph adapter depending on the provider type     specified.', 'MODULE cognee.cognee.infrastructure.databases.graph.graph_db_interface contains GLOBAL_VARIABLE cognee.cognee.infrastructure.databases.graph.graph_db_interface.Node', 'FUNCTION cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine.graph_database_provider\nCreate a graph engine based on the specified provider type.  This factory function initializes and returns the appropriate graph client depending on the database provider specified. It validates required parameters and raises an EnvironmentError if any are missing for the respective provider implementations.  Parameters: -----------      - graph_database_provider: The type of graph database provider to use (e.g., neo4j, falkor, kuzu).     - graph_database_url: The URL for the graph database instance. Required for neo4j and falkordb providers.     - graph_database_username: The username for authentication with the graph database.       Required for neo4j provider.     - graph_database_password: The password for authentication with the graph database.       Required for neo4j provider.     - graph_database_port: The port number for the graph database connection. Required       for the falkordb provider     - graph_file_path: The filesystem path to the graph file. Required for the kuzu       provider.  Returns: --------      Returns an instance of the appropriate graph adapter depending on the provider type     specified.', 'MODULE cognee.cognee.infrastructure.databases.graph.graph_db_interface contains GLOBAL_VARIABLE cognee.cognee.infrastructure.databases.graph.graph_db_interface.NodeData', 'FUNCTION cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine.graph_database_url\nCreate a graph engine based on the specified provider type.  This factory function initializes and returns the appropriate graph client depending on the database provider specified. It validates required parameters and raises an EnvironmentError if any are missing for the respective provider implementations.  Parameters: -----------      - graph_database_provider: The type of graph database provider to use (e.g., neo4j, falkor, kuzu).     - graph_database_url: The URL for the graph database instance. Required for neo4j and falkordb providers.     - graph_database_username: The username for authentication with the graph database.       Required for neo4j provider.     - graph_database_password: The password for authentication with the graph database.       Required for neo4j provider.     - graph_database_port: The port number for the graph database connection. Required       for the falkordb provider     - graph_file_path: The filesystem path to the graph file. Required for the kuzu       provider.  Returns: --------      Returns an instance of the appropriate graph adapter depending on the provider type     specified.', 'MODULE cognee.cognee.infrastructure.databases.graph.graph_db_interface contains GLOBAL_VARIABLE cognee.cognee.infrastructure.databases.graph.graph_db_interface.EdgeData', 'FUNCTION cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.get_graph_engine.create_graph_engine.graph_database_name\nCreate a graph engine based on the specified provider type.  This factory function initializes and returns the appropriate graph client depending on the database provider specified. It validates required parameters and raises an EnvironmentError if any are missing for the respective provider implementations.  Parameters: -----------      - graph_database_provider: The type of graph database provider to use (e.g., neo4j, falkor, kuzu).     - graph_database_url: The URL for the graph database instance. Required for neo4j and falkordb providers.     - graph_database_username: The username for authentication with the graph database.       Required for neo4j provider.     - graph_database_password: The password for authentication with the graph database.       Required for neo4j provider.     - graph_database_port: The port number for the graph database connection. Required       for the falkordb provider     - graph_file_path: The filesystem path to the graph file. Required for the kuzu       provider.  Returns: --------      Returns an instance of the appropriate graph adapter depending on the provider type     specified.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 144, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",32.49081897735596,798
How are multiple group IDs handled in the search flow from the incoming request model to the Graphiti client?,"Graphiti handles multiple group IDs in the search flow from the incoming request model to the Graphiti client by using a `group_ids` field in the `SearchQuery` class. This field is used to filter nodes and edges based on their group IDs. When a search query is executed, Graphiti uses its knowledge graph database (Neo4j) to perform the filtering. A Cypher query is constructed that filters nodes and edges based on their group IDs. For example: `MATCH (n:Node {id: 'node-1'})-[:EDGE]-(m:Node {id: 'node-2'}) WHERE n.group_id IN ['group-1', 'group-2'] AND m.group_id IN ['group-1', 'group-2'] RETURN n, m;` This query would filter the results to only include nodes and edges that belong to both `group-1` and `group-2`. Graphiti provides a flexible way to handle multiple group IDs in search queries by using the `group_ids` field in the `SearchQuery` class.","Incoming `group_ids` are declared as a list of strings on the Pydantic model (server/graph_service/dto/retrieve.py: SearchQuery.group_ids), so when you send multiple IDs in your JSON payload they’re parsed into a Python list. In server/graph_service/routers/retrieve.py the router simply does

```python
await graphiti.search(
  group_ids=query.group_ids,
  …
)
```

passing that list straight through to your ZepGraphiti client (instantiated in server/graph_service/zep_graphiti.py). The underlying Graphiti.search implementation then uses the list to build an `IN $group_ids` filter in the Neo4j query.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.group_id', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n# Graphiti MCP Server\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nThis is an experimental Model Context Protocol (MCP) server implementation for Graphiti. The MCP server exposes\nGraphiti's key functionality through the MCP protocol, allowing AI assistants to interact with Graphiti's knowledge\ngraph capabilities.\n\n## Features\n\nThe Graphiti MCP server exposes the following key high-level functions of Graphiti:\n\n- **Episode Management**: Add, retrieve, and delete episodes (text, messages, or JSON data)\n- **Entity Management**: Search and manage entity nodes and relationships in the knowledge graph\n- **Search Capabilities**: Search for facts (edges) and node summaries using semantic and hybrid search\n- **Group Management**: Organize and manage groups of related data with group_id filtering\n- **Graph Maintenance**: Clear the graph and rebuild indices\n\n## Quick Start\n\n### Clone the Graphiti GitHub repo\n\n```bash\ngit clone https://github.com/getzep/graphiti.git\n```\n\nor\n\n```bash\ngh repo clone getzep/graphiti\n```\n\n### For Claude Desktop and other `stdio` only clients\n\n1. Note the full path to this directory.\n\n```\ncd graphiti && pwd\n```\n\n2. Install the [Graphiti prerequisites](#prerequisites).\n\n3. Configure Claude, Cursor, or other MCP client to use [Graphiti with a `stdio` transport](#integrating-with-mcp-clients). See the client documentation on where to find their MCP configuration files.\n\n### For Cursor and other `sse`-enabled clients\n\n1. Change directory to the `mcp_server` directory\n\n`cd graphiti/mcp_server`\n\n2. Start the service using Docker Compose\n\n`docker compose up`\n\n3. Point your MCP client to `http://localhost:8000/sse`\n\n## Installation\n\n### Prerequisites\n\n1. Ensure you have Python 3.10 or higher installed.\n2. A running Neo4j database (version 5.26 or later required)\n3. OpenAI API key for LLM operations\n\n### Setup\n\n1. Clone the repository and navigate to the mcp_server directory\n2. Use `uv` to create a virtual environment and install dependencies:\n\n```bash\n# Install uv if you don't have it already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a virtual environment and install dependencies in one step\nuv sync\n```\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `NEO4J_URI`: URI for the Neo4j database (default: `bolt://localhost:7687`)\n- `NEO4J_USER`: Neo4j username (default: `neo4j`)\n- `NEO4J_PASSWORD`: Neo4j password (default: `demodemo`)\n- `OPENAI_API_KEY`: OpenAI API key (required for LLM operations)\n- `OPENAI_BASE_URL`: Optional base URL for OpenAI API\n- `MODEL_NAME`: OpenAI model name to use for LLM operations.\n- `SMALL_MODEL_NAME`: OpenAI model name to use for smaller LLM operations.\n- `LLM_TEMPERATURE`: Temperature for LLM responses (0.0-2.0).\n- `AZURE_OPENAI_ENDPOINT`: Optional Azure OpenAI LLM endpoint URL"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 151, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",60.61840009689331,799
At what point in the episode retrieval flow is the clause to filter by group_id conditionally added?,"The episode retrieval flow in graphiti involves searching for episodes and filtering them based on certain conditions. The `EpisodeReranker` class in `graphiti.graphiti_core.search.search_config` is responsible for ranking episodes based on their relevance, while the `log_episode` method in `qlib.qlib.rl.utils.log.LogWriter` seems to be related to logging episode metrics. However, the exact point at which the clause to filter by group_id is conditionally added cannot be determined without further analysis of the code and documentation. The filtering process may involve conditional statements or functions that are not explicitly mentioned in the provided information.","The `GROUP_ID` clause is injected in the helper retrieve_episodes (in graphiti_core/utils/maintenance/graph_data_operations.py). Right at the top of that function it does:

```python
group_id_filter = '\nAND e.group_id IN $group_ids' if group_ids and len(group_ids)>0 else ''
```

and then concatenates `group_id_filter` into the Cypher right after the `MATCH … WHERE e.valid_at <= $reference_time` line.","[""CLASS qlib.qlib.rl.utils.log.LogWriter has method qlib.qlib.rl.utils.log.LogWriter.log_episode\nThis is triggered at the end of each trajectory.  Parameters ---------- length     Length of this trajectory. rewards     A list of rewards at each step of this episode. contents     Logged contents for every step.\nBase class for log writers, triggered at every reset and step by finite env.  What to do with a specific log depends on the implementation of subclassing :class:`LogWriter`. The general principle is that, it should handle logs above its loglevel (inclusive), and discard logs that are not acceptable. For instance, console loggers obviously can't handle an image."", 'MODULE graphiti.examples.podcast.podcast_runner contains GLOBAL_VARIABLE graphiti.examples.podcast.podcast_runner.group_id', ""CLASS qlib.qlib.rl.utils.log.CsvWriter inherits from or is a subclass of qlib.qlib.rl.utils.log.LogWriter\nBase class for log writers, triggered at every reset and step by finite env.  What to do with a specific log depends on the implementation of subclassing :class:`LogWriter`. The general principle is that, it should handle logs above its loglevel (inclusive), and discard logs that are not acceptable. For instance, console loggers obviously can't handle an image.\nDump all episode metrics to a ``result.csv``.  This is not the correct implementation. It's only used for first iteration."", 'CLASS graphiti.graphiti_core.search.search_config.EpisodeReranker inherits from or is a subclass of enum.Enum', 'CLASS qlib.qlib.rl.utils.log.LogCollector has method qlib.qlib.rl.utils.log.LogCollector.logs\nLogs are first collected in each environment worker, and then aggregated to stream at the central thread in vector env.  In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step. The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.  ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.', 'CLASS keras.keras.src.layers.attention.grouped_query_attention_test.GroupedQueryAttentionTest has field or attribute or property keras.keras.src.layers.attention.grouped_query_attention_test.query_dims', 'MODULE qlib.qlib.rl.utils.log contains CLASS qlib.qlib.rl.utils.log.LogCollector\nLogs are first collected in each environment worker, and then aggregated to stream at the central thread in vector env.  In :class:`LogCollector`, every metric is added to a dict, which needs to be ``reset()`` at each step. The dict is sent via the ``info`` in ``env.step()``, and decoded by the :class:`LogWriter` at vector env.  ``min_loglevel`` is for optimization purposes: to avoid too much traffic on networks / in pipe.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 33}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 33, 'n_files_pr': 11, 'pr': 152, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",57.50292491912842,800
"In the search handler, how is the query’s max_facts value applied to limit the number of graph search results?","In Graphiti, the query's max_facts value is applied to limit the number of graph search results by using the `max_facts` field in the `SearchQuery` class. This field is an integer that specifies the maximum number of facts to return in the search results. To use this feature, you can create a `SearchQuery` object and set its `max_facts` field to the desired value, as demonstrated in the following code snippet from the Graphiti documentation:

```python
define from graphiti.server.graph_service.dto.retrieve import SearchQuery

query = SearchQuery(
    max_facts=10,  # Specify the maximum number of facts to return
    query=""your search query here""
)
```
This will limit the number of graph search results returned by the query to the specified value.","In server/graph_service/routers/retrieve.py’s search handler, `query.max_facts` is passed straight into the `num_results` parameter of `graphiti.search(...)`. That tells the graph search implementation (in ZepGraphitiDep) to return at most that many edges, effectively capping your results to `max_facts`.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.server.graph_service.dto.retrieve.GetMemoryRequest has field or attribute or property graphiti.server.graph_service.dto.retrieve.GetMemoryRequest.max_facts', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.max_facts', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.fact_type', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.neo4j\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 153, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",47.755205154418945,801
What is the default value for group_ids when it is not provided in a search query?,The default value for group_ids when it is not provided in a search query in graphiti is None.,"In SearchQuery (server/graph_service/dto/retrieve.py), group_ids is defined as list[str] | None = Field(None…), so if you don’t supply it, it defaults to None.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.password\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS qlib.qlib.model.ens.group.Group has field or attribute or property qlib.qlib.model.ens.group.ungrouped_dict\nGroup the objects based on dict', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.nodes.Node has field or attribute or property graphiti.graphiti_core.nodes.Node.group_id', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.uri\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.cross_encoder\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md\n# Crowd Source Data\n\n## Initiative\nPublic data source like yahoo is flawed, it might miss data for stock which is delisted and it might have data which is wrong. This can introduce survivorship bias into our training process.\n\nThe Crowd Source Data is introduced to merged data from multiple data source and cross validate against each other, so that:\n1. We will have a more complete history record.\n2. We can identify the anomaly data and apply correction when necessary.\n\n## Related Repo\nThe raw data is hosted on dolthub repo: https://www.dolthub.com/repositories/chenditc/investment_data\n\nThe processing script and sql is hosted on github repo: https://github.com/chenditc/investment_data\n\nThe packaged docker runtime is hosted on dockerhub: https://hub.docker.com/repository/docker/chenditc/investment_data\n\n## How to use it in qlib\n### Option 1: Download release bin data\nUser can download data in qlib bin format and use it directly: https://github.com/chenditc/investment_data/releases/latest\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```\n\n### Option 2: Generate qlib data from dolthub\nDolthub data will be update daily, so that if user wants to get up to date data, they can dump qlib bin using docker:\n```\ndocker run -v /<some output directory>:/output -it --rm chenditc/investment_data bash dump_qlib_bin.sh && cp ./qlib_bin.tar.gz /output/\n```\n\n## FAQ and other info\nSee: https://github.com/chenditc/investment_data/blob/main/README.md']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 154, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.64159321784973,802
What is the sequence of retrieval and deletion operations performed when a group is deleted?,"When a group is deleted in Graphiti, the sequence of retrieval and deletion operations performed can be summarized as follows:

1.  **Search for relevant preferences and procedures**: Before deleting a group, search for any existing preferences or procedures related to the group.
2.  **Review all matches**: Carefully examine any preferences, procedures, or facts that match your current task.
3.  **Capture requirements and preferences immediately**: If new requirements or preferences are expressed during the deletion process, store them right away.
4.  **Document procedures clearly**: Record how the group was deleted as a procedure for future reference.
5.  **Record factual relationships**: Store any connections between entities related to the deleted group as facts.
6.  **Respect discovered preferences**: Align your work with any preferences you've found during the deletion process.","In `server/graph_service/zep_graphiti.py` – ZepGraphiti.delete_group does:

1. Retrievals (all via `get_by_group_ids`):
   • Entity edges: EntityEdge.get_by_group_ids  
     (catches `GroupsEdgesNotFoundError`, falls back to `[]`)  
   • Entity nodes: EntityNode.get_by_group_ids  
   • Episodic nodes: EpisodicNode.get_by_group_ids  

2. Deletions (in order):
   • for each edge → `edge.delete(self.driver)`  
   • for each entity node → `node.delete(self.driver)`  
   • for each episodic node → `episode.delete(self.driver)`","['CLASS cognee.cognee.modules.retrieval.base_retriever.BaseRetriever inherits from or is a subclass of abc.ABC\nBase class for all retrieval operations.', 'MODULE cognee.cognee.modules.data.methods.get_deletion_counts contains GLOBAL_VARIABLE cognee.cognee.modules.data.methods.get_deletion_counts.datasets_query', 'MODULE cognee.cognee.modules.retrieval.base_retriever contains CLASS cognee.cognee.modules.retrieval.base_retriever.BaseRetriever\nBase class for all retrieval operations.', 'MODULE cognee.cognee.modules.data.methods.get_deletion_counts contains GLOBAL_VARIABLE cognee.cognee.modules.data.methods.get_deletion_counts.relational_engine', 'CLASS qlib.qlib.contrib.ops.high_freq.Cut inherits from or is a subclass of qlib.data.ops.ElemOperator\nCut Operator  Parameters ---------- feature : Expression     feature instance l : int     l > 0, delete the first l elements of feature (default is None, which means 0) r : int     r < 0, delete the last -r elements of feature (default is None, which means 0) Returns ---------- feature:     A series with the first l and last -r elements deleted from the feature.     Note: It is deleted from the raw data, not the sliced data', 'MODULE cognee.cognee.modules.data.methods.get_deletion_counts contains GLOBAL_VARIABLE cognee.cognee.modules.data.methods.get_deletion_counts.user_datasets', 'CLASS qlib.examples.highfreq.highfreq_ops.Cut inherits from or is a subclass of qlib.data.ops.ElemOperator\nCut Operator  Parameters ---------- feature : Expression     feature instance l : int     l > 0, delete the first l elements of feature (default is None, which means 0) r : int     r < 0, delete the last -r elements of feature (default is None, which means 0) Returns ---------- feature:     A series with the first l and last -r elements deleted from the feature.     Note: It is deleted from the raw data, not the sliced data', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n- `prune` (data reset)\n- `get_developer_rules` (developer rules retrieval)\n- `list_data` with specific dataset_id (detailed data listing)\n\nBasic operations like `cognify`, `search`, `delete`, and `list_data` (all datasets) work in both modes.\n\n## 💻 Basic Usage\n\nThe MCP server exposes its functionality through tools. Call them from any MCP client (Cursor, Claude Desktop, Cline, Roo and more).\n\n\n### Available Tools\n\n- **cognify**: Turns your data into a structured knowledge graph and stores it in memory\n\n- **codify**: Analyse a code repository, build a code graph, stores it in memory\n\n- **search**: Query memory – supports GRAPH_COMPLETION, RAG_COMPLETION, CODE, CHUNKS\n\n- **list_data**: List all datasets and their data items with IDs for deletion operations\n\n- **delete**: Delete specific data from a dataset (supports soft/hard deletion modes)\n\n- **prune**: Reset cognee for a fresh start (removes all data)\n\n- **cognify_status / codify_status**: Track pipeline progress\n\n**Data Management Examples:**\n```bash\n# List all available datasets and data items\nlist_data()\n\n# List data items in a specific dataset\nlist_data(dataset_id=""your-dataset-id-here"")\n\n# Delete specific data (soft deletion - safer, preserves shared entities)\ndelete(data_id=""data-uuid"", dataset_id=""dataset-uuid"", mode=""soft"")\n\n# Delete specific data (hard deletion - removes orphaned entities)\ndelete(data_id=""data-uuid"", dataset_id=""dataset-uuid"", mode=""hard"")\n```\n\n\n## Development and Debugging\n\n### Debugging\n\nTo use debugger, run:\n    ```bash\n    mcp dev src/server.py\n    ```\n\nOpen inspector with timeout passed:\n    ```\n    http://localhost:5173?timeout=120000\n    ```\n\nTo apply new changes while developing cognee you need to do:\n\n1. Update dependencies in cognee folder if needed\n2. `uv sync --dev --all-extras --reinstall`\n3. `mcp dev src/server.py`\n\n### Development\n\nIn order to use local cognee:\n\n1. Uncomment the following line in the cognee-mcp [`pyproject.toml`](pyproject.toml) file and set the cognee root path.\n    ```\n    #""cognee[postgres,codegraph,gemini,huggingface,docs,neo4j] @ file:/Users/<username>/Desktop/cognee""\n    ```\n    Remember to replace `file:/Users/<username>/Desktop/cognee` with your actual cognee root path.\n\n2. Install dependencies with uv in the mcp folder\n    ```\n    uv sync --reinstall\n    ```\n\n## Code of Conduct\n\nWe are committed to making open source an enjoyable and respectful experience for our community. See <a href=""https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md""><code>CODE_OF_CONDUCT</code></a> for more information.\n\n## 💫 Contributors\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img alt=""contributors"" src=""https://contrib.rocks/image?repo=topoteretes/cognee""/>\n</a>\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 4, 'pr': 155, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",50.27036905288696,803

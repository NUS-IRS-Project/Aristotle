user_input,response,reference,retrieved_contexts,metadata,time_taken,idx
"In the BERT benchmark data loader, how does the feature-splitting function convert a GLUE example's dictionary into the (features, label) tuple expected by KerasNLP?","The feature-splitting function in the BERT benchmark data loader converts a GLUE example's dictionary into the (features, label) tuple expected by KerasNLP by using the expression `tuple(x[name] for name in feature_names)` to build features and `x[","In benchmarks/model_benchmark/bert_benchmark.py the split_features function does exactly this: it takes the GLUE example dict x, builds features = tuple(x[name] for name in feature_names), pulls out label = x[""label""], and returns (features, label) so it matches the (inputs, target) format KerasNLP expects.","['MODULE keras.benchmarks.model_benchmark.bert_benchmark contains FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.load_data\nLoad data.  Load GLUE/MRPC dataset, and convert the dictionary format to (features, label), where `features` is a tuple of all input sentences.', 'FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.load_data has parameter or accepts argument keras.benchmarks.model_benchmark.bert_benchmark.FLAGS\nLoad data.  Load GLUE/MRPC dataset, and convert the dictionary format to (features, label), where `features` is a tuple of all input sentences.', 'MODULE keras.benchmarks.model_benchmark.bert_benchmark contains FUNCTION keras.benchmarks.model_benchmark.bert_benchmark.split_features', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'MODULE keras.keras.src.datasets.cifar contains FUNCTION keras.keras.src.datasets.cifar.load_batch\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\ninformation, please see our tutorial on [categorical\ndata](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html), along with\nexamples linked on that page. (#7380, #7708, #7695, #7330, #7307, #7322, #7705,\n#7652, #7592, #7666, #7576, #7569, #7529, #7575, #7393, #7465, #7385, #7371, #7745, #7810)\n\nIn the future, we will continue to improve categorical data support with new features and\noptimizations. Also, we are looking forward to bringing the feature beyond Python binding,\ncontributions and feedback are welcomed! Lastly, as a result of experimental status, the\nbehavior might be subject to change, especially the default value of related\nhyper-parameters.\n\n### Experimental support for multi-output model\n\nXGBoost 1.6 features initial support for the multi-output model, which includes\nmulti-output regression and multi-label classification. Along with this, the XGBoost\nclassifier has proper support for base margin without to need for the user to flatten the\ninput. In this initial support, XGBoost builds one model for each target similar to the\nsklearn meta estimator, for more details, please see our [quick\nintroduction](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html).\n\n(#7365, #7736, #7607, #7574, #7521, #7514, #7456, #7453, #7455, #7434, #7429, #7405, #7381)\n\n### External memory support\nExternal memory support for both approx and hist tree method is considered feature\ncomplete in XGBoost 1.6.  Building upon the iterator-based interface introduced in the\nprevious version, now both `hist` and `approx` iterates over each batch of data during\ntraining and prediction.  In previous versions, `hist` concatenates all the batches into\nan internal representation, which is removed in this version.  As a result, users can\nexpect higher scalability in terms of data size but might experience lower performance due\nto disk IO. (#7531, #7320, #7638, #7372)\n\n### Rewritten approx\n\nThe `approx` tree method is rewritten based on the existing `hist` tree method. The\nrewrite closes the feature gap between `approx` and `hist` and improves the performance.\nNow the behavior of `approx` should be more aligned with `hist` and `gpu_hist`. Here is a\nlist of user-visible changes:\n\n- Supports both `max_leaves` and `max_depth`.\n- Supports `grow_policy`.\n- Supports monotonic constraint.\n- Supports feature weights.\n- Use `max_bin` to replace `sketch_eps`.\n- Supports categorical data.\n- Faster performance for many of the datasets.\n- Improved performance and robustness for distributed training.\n- Supports prediction cache.\n- Significantly better performance for external memory when `depthwise` policy is used.\n\n### New serialization format\nBased on the existing JSON serialization format, we introduce UBJSON support as a more\nefficient alternative. Both formats will be available in the future and we plan to\ngradually [phase out](https://github.com/dmlc/xgboost/issues/7547) support for the old\nbinary model format.  Users can opt to use the different formats in the serialization\nfunction by providing the file extension `json` or `ubj`. Also, the `save_raw` function in\nall supported languages bindings gains a new parameter for exporting the model in different\nformats, available options are `json`, `ubj`, and `deprecated`, see document for the\nlanguage binding you are using for details. Lastly, the default internal serialization', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\nIn this example, we use tree booster for gradient boosting. If you would like to use linear booster for regression, you can keep all the parameters except booster and the tree booster parameters as below:\n```conf\n# General Parameters\n# choose the linear booster\nbooster = gblinear\n...\n\n# Change Tree Booster Parameters into Linear Booster Parameters\n# L2 regularization term on weights, default 0\nlambda = 0.01\n# L1 regularization term on weights, default 0\nalpha = 0.01\n# L2 regularization term on bias, default 0\nlambda_bias = 0.01\n\n# Regression Parameters\n...\n```\n\n#### Get Predictions\nAfter training, we can use the output model to get the prediction of the test data:\n```\n../../xgboost mushroom.conf task=pred model_in=0002.model\n```\nFor binary classification, the output predictions are probability confidence scores in [0,1], corresponds to the probability of the label to be positive.\n\n#### Dump Model\nThis is a preliminary feature, so only tree models support text dump. XGBoost can display the tree models in text or JSON files, and we can scan the model in an easy way:\n```\n../../xgboost mushroom.conf task=dump model_in=0002.model name_dump=dump.raw.txt\n../../xgboost mushroom.conf task=dump model_in=0002.model fmap=featmap.txt name_dump=dump.nice.txt\n```\n\nIn this demo, the tree boosters obtained will be printed in dump.raw.txt and dump.nice.txt, and the latter one is easier to understand because of usage of feature mapping featmap.txt\n\nFormat of ```featmap.txt: <featureid> <featurename> <q or i or int>\\n ```:\n  - Feature id must be from 0 to number of features, in sorted order.\n  - i means this feature is binary indicator feature\n  - q means this feature is a quantitative value, such as age, time, can be missing\n  - int means this feature is integer value (when int is hinted, the decision boundary will be integer)\n\n#### Monitoring Progress\nWhen you run training we can find there are messages displayed on screen\n```\ntree train end, 1 roots, 12 extra nodes, 0 pruned nodes ,max_depth=3\n[0]  test-error:0.016139\nboosting round 1, 0 sec elapsed\n\ntree train end, 1 roots, 10 extra nodes, 0 pruned nodes ,max_depth=3\n[1]  test-error:0.000000\n```\nThe messages for evaluation are printed into stderr, so if you want only to log the evaluation progress, simply type\n```\n../../xgboost mushroom.conf 2>log.txt\n```\nThen you can find the following content in log.txt\n```\n[0]     test-error:0.016139\n[1]     test-error:0.000000\n```\nWe can also monitor both training and test statistics, by adding following lines to configure\n```conf\neval[test] = ""agaricus.txt.test""\neval[trainname] = ""agaricus.txt.train""\n```\nRun the command again, we can find the log file becomes\n```\n[0]     test-error:0.016139     trainname-error:0.014433\n[1]     test-error:0.000000     trainname-error:0.001228\n```\nThe rule is eval[name-printed-in-log] = filename, then the file will be added to monitoring process, and evaluated each round.\n\nxgboost also supports monitoring multiple metrics, suppose we also want to monitor average log-likelihood of each prediction during training, simply add ```eval_metric=logloss``` to configure. Run again, we can find the log file becomes\n```\n[0]     test-error:0.016139     test-negllik:0.029795   trainname-error:0.014433        trainname-negllik:0.027023\n[1]     test-error:0.000000     test-negllik:0.000000   trainname-error:0.001228        trainname-negllik:0.002457\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 19138, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",36.62000036239624,121
"How does sparse categorical crossentropy handle samples of the ignored class through the initial preprocessing, the backend loss computation, and the final mask application?","Sparse categorical crossentropy handles samples of the ignored class through the following steps: **Preprocessing**: The `valid_mask` is computed as `(y_true != ignore_class)`, and both `y_true` and `y_pred` are updated by multiplying them with `valid_mask`. This effectively zeroes out both the target and logits for any ignored sample. **Backend Loss Computation**: The loss computation calls `ops.sparse_categorical_crossentropy(y_true, y_pred, …)`, which computes the raw loss on zeros for ignored entries. **Final Mask Application**: In the final masking step, `valid_mask` is reshaped to match the loss's shape and used in a `where` operation to set ignored losses to 0. Finally, `backend.set_keras_mask(res, mask=valid_mask)` is called to ensure that ignored samples contribute zero to any reduction or metric.","In keras/src/losses/losses.py, sparse_categorical_crossentropy does the following when ignore_class≠None:

1. **Preprocessing**  
   – valid_mask = (y_true != ignore_class)  
   – y_true ← y_true * valid_mask  
   – y_pred ← y_pred * expand_dims(valid_mask, –1)  
   This zeroes out both the target and logits for any “ignored” sample.

2. **Backend loss**  
   – Calls ops.sparse_categorical_crossentropy(y_true, y_pred, …)  
   – Because ignored entries were zeroed, their raw loss is computed on zeros.

3. **Final masking**  
   – Reshapes valid_mask to the loss’s shape  
   – res ← where(valid_mask, res, 0.0)  (sets ignored losses to 0)  
   – backend.set_keras_mask(res, mask=valid_mask)  
   This ensures ignored samples contribute zero to any reduction or metric.","['MODULE keras.keras.src.ops.nn contains FUNCTION keras.keras.src.ops.nn.sparse_categorical_crossentropy\nComputes sparse categorical cross-entropy loss.  The sparse categorical cross-entropy loss is similar to categorical cross-entropy, but it is used when the target tensor contains integer class labels instead of one-hot encoded vectors. It measures the dissimilarity between the target and output probabilities or logits.  Args:     target: The target tensor representing the true class labels as         integers. Its shape should match the shape of the `output`         tensor except for the last dimension.     output: The output tensor representing the predicted probabilities         or logits.         Its shape should match the shape of the `target` tensor except         for the last dimension.     from_logits: (optional) Whether `output` is a tensor of logits         or probabilities.         Set it to `True` if `output` represents logits; otherwise,         set it to `False` if `output` represents probabilities.         Defaults to `False`.     axis: (optional) The axis along which the sparse categorical         cross-entropy is computed.         Defaults to `-1`, which corresponds to the last dimension         of the tensors.  Returns:     Integer tensor: The computed sparse categorical cross-entropy     loss between `target` and `output`.  Example:  >>> target = keras.ops.convert_to_tensor([0, 1, 2], dtype=int32) >>> output = keras.ops.convert_to_tensor( ... [[0.9, 0.05, 0.05], ...  [0.1, 0.8, 0.1], ...  [0.2, 0.3, 0.5]]) >>> sparse_categorical_crossentropy(target, output) array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)', 'FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.losses.losses.sparse_categorical_crossentropy.ignore_class\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'FUNCTION keras.keras.src.ops.nn.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.ops.nn.sparse_categorical_crossentropy.axis\nComputes sparse categorical cross-entropy loss.  The sparse categorical cross-entropy loss is similar to categorical cross-entropy, but it is used when the target tensor contains integer class labels instead of one-hot encoded vectors. It measures the dissimilarity between the target and output probabilities or logits.  Args:     target: The target tensor representing the true class labels as         integers. Its shape should match the shape of the `output`         tensor except for the last dimension.     output: The output tensor representing the predicted probabilities         or logits.         Its shape should match the shape of the `target` tensor except         for the last dimension.     from_logits: (optional) Whether `output` is a tensor of logits         or probabilities.         Set it to `True` if `output` represents logits; otherwise,         set it to `False` if `output` represents probabilities.         Defaults to `False`.     axis: (optional) The axis along which the sparse categorical         cross-entropy is computed.         Defaults to `-1`, which corresponds to the last dimension         of the tensors.  Returns:     Integer tensor: The computed sparse categorical cross-entropy     loss between `target` and `output`.  Example:  >>> target = keras.ops.convert_to_tensor([0, 1, 2], dtype=int32) >>> output = keras.ops.convert_to_tensor( ... [[0.9, 0.05, 0.05], ...  [0.1, 0.8, 0.1], ...  [0.2, 0.3, 0.5]]) >>> sparse_categorical_crossentropy(target, output) array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)', 'MODULE keras.keras.src.losses.losses contains FUNCTION keras.keras.src.losses.losses.sparse_categorical_crossentropy\nComputes the sparse categorical crossentropy loss.  Args:     y_true: Ground truth values.     y_pred: The predicted values.     from_logits: Whether `y_pred` is expected to be a logits tensor. By         default, we assume that `y_pred` encodes a probability distribution.     ignore_class: Optional integer. The ID of a class to be ignored during         loss computation. This is useful, for example, in segmentation         problems featuring a ""void"" class (commonly -1 or 255) in         segmentation maps. By default (`ignore_class=None`), all classes are         considered.     axis: Defaults to `-1`. The dimension along which the entropy is         computed.  Returns:     Sparse categorical crossentropy loss value.  Examples:  >>> y_true = [1, 2] >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]] >>> loss = keras.losses.sparse_categorical_crossentropy(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.0513, 2.303], dtype=float32)', 'FUNCTION keras.keras.src.ops.nn.sparse_categorical_crossentropy has parameter or accepts argument keras.keras.src.ops.nn.sparse_categorical_crossentropy.from_logits\nComputes sparse categorical cross-entropy loss.  The sparse categorical cross-entropy loss is similar to categorical cross-entropy, but it is used when the target tensor contains integer class labels instead of one-hot encoded vectors. It measures the dissimilarity between the target and output probabilities or logits.  Args:     target: The target tensor representing the true class labels as         integers. Its shape should match the shape of the `output`         tensor except for the last dimension.     output: The output tensor representing the predicted probabilities         or logits.         Its shape should match the shape of the `target` tensor except         for the last dimension.     from_logits: (optional) Whether `output` is a tensor of logits         or probabilities.         Set it to `True` if `output` represents logits; otherwise,         set it to `False` if `output` represents probabilities.         Defaults to `False`.     axis: (optional) The axis along which the sparse categorical         cross-entropy is computed.         Defaults to `-1`, which corresponds to the last dimension         of the tensors.  Returns:     Integer tensor: The computed sparse categorical cross-entropy     loss between `target` and `output`.  Example:  >>> target = keras.ops.convert_to_tensor([0, 1, 2], dtype=int32) >>> output = keras.ops.convert_to_tensor( ... [[0.9, 0.05, 0.05], ...  [0.1, 0.8, 0.1], ...  [0.2, 0.3, 0.5]]) >>> sparse_categorical_crossentropy(target, output) array([0.10536056 0.22314355 0.6931472 ], shape=(3,), dtype=float32)', 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\ninformation, please see our tutorial on [categorical\ndata](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html), along with\nexamples linked on that page. (#7380, #7708, #7695, #7330, #7307, #7322, #7705,\n#7652, #7592, #7666, #7576, #7569, #7529, #7575, #7393, #7465, #7385, #7371, #7745, #7810)\n\nIn the future, we will continue to improve categorical data support with new features and\noptimizations. Also, we are looking forward to bringing the feature beyond Python binding,\ncontributions and feedback are welcomed! Lastly, as a result of experimental status, the\nbehavior might be subject to change, especially the default value of related\nhyper-parameters.\n\n### Experimental support for multi-output model\n\nXGBoost 1.6 features initial support for the multi-output model, which includes\nmulti-output regression and multi-label classification. Along with this, the XGBoost\nclassifier has proper support for base margin without to need for the user to flatten the\ninput. In this initial support, XGBoost builds one model for each target similar to the\nsklearn meta estimator, for more details, please see our [quick\nintroduction](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html).\n\n(#7365, #7736, #7607, #7574, #7521, #7514, #7456, #7453, #7455, #7434, #7429, #7405, #7381)\n\n### External memory support\nExternal memory support for both approx and hist tree method is considered feature\ncomplete in XGBoost 1.6.  Building upon the iterator-based interface introduced in the\nprevious version, now both `hist` and `approx` iterates over each batch of data during\ntraining and prediction.  In previous versions, `hist` concatenates all the batches into\nan internal representation, which is removed in this version.  As a result, users can\nexpect higher scalability in terms of data size but might experience lower performance due\nto disk IO. (#7531, #7320, #7638, #7372)\n\n### Rewritten approx\n\nThe `approx` tree method is rewritten based on the existing `hist` tree method. The\nrewrite closes the feature gap between `approx` and `hist` and improves the performance.\nNow the behavior of `approx` should be more aligned with `hist` and `gpu_hist`. Here is a\nlist of user-visible changes:\n\n- Supports both `max_leaves` and `max_depth`.\n- Supports `grow_policy`.\n- Supports monotonic constraint.\n- Supports feature weights.\n- Use `max_bin` to replace `sketch_eps`.\n- Supports categorical data.\n- Faster performance for many of the datasets.\n- Improved performance and robustness for distributed training.\n- Supports prediction cache.\n- Significantly better performance for external memory when `depthwise` policy is used.\n\n### New serialization format\nBased on the existing JSON serialization format, we introduce UBJSON support as a more\nefficient alternative. Both formats will be available in the future and we plan to\ngradually [phase out](https://github.com/dmlc/xgboost/issues/7547) support for the old\nbinary model format.  Users can opt to use the different formats in the serialization\nfunction by providing the file extension `json` or `ubj`. Also, the `save_raw` function in\nall supported languages bindings gains a new parameter for exporting the model in different\nformats, available options are `json`, `ubj`, and `deprecated`, see document for the\nlanguage binding you are using for details. Lastly, the default internal serialization']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 2, 'pr': 19838, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",44.29817509651184,122
Which exception is thrown when attempting to retrieve an attribute but the layer has never been called?,"An AttributeError is thrown when attempting to retrieve an attribute but the layer has never been called in keras. This exception occurs because the layer's inbound nodes are empty, indicating that it has not been used yet.","In Operation._get_node_attribute_at_index (keras/src/ops/operation.py), if self._inbound_nodes is empty it raises an AttributeError indicating the layer has never been called.","['CLASS keras.keras.src.layers.layer_test.LayerTest has method keras.keras.src.layers.layer_test.LayerTest.test_register_call_context_arguments_after_call\nValidate that registering call-context args after the layer has been called raises an error.', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.value', 'MODULE cognee.cognee.tests.unit.modules.graph.cognee_graph_test contains FUNCTION cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node\nTest retrieving edges for a nonexistent node raises an exception.', 'CLASS keras.keras.src.layers.layer.CallContext has field or attribute or property keras.keras.src.layers.layer.entry_layer', 'FUNCTION cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node has parameter or accepts argument cognee.cognee.tests.unit.modules.graph.cognee_graph_test.test_get_edges_nonexistent_node.setup_graph\nTest retrieving edges for a nonexistent node raises an exception.', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n#### Resource Allocation\n\nA denial of service caused by one model can impact the overall system health. Implement safeguards like rate limits, access controls, and health monitoring.\n\n#### Model Sharing\n\nIn a multitenant design that allows sharing models, make sure that tenants and users fully understand the potential security risks involved. They must be aware that they will essentially be running code provided by other users. Unfortunately, there are no reliable methods available to detect malicious models, graphs, or checkpoints. To mitigate this risk, the recommended approach is to sandbox the model execution, effectively isolating it from the rest of the system.\n\n#### Hardware Attacks\n\nBesides the virtual environment, the hardware (GPUs or TPUs) can also be attacked. [Research](https://scholar.google.com/scholar?q=gpu+side+channel) has shown that side channel attacks on GPUs are possible, which can make data leak from other models or processes running on the same system at the same time.\n\n## Reporting a Vulnerability\n\nBeware that none of the topics under [Using Keras Securely](#using-keras-securely) are considered vulnerabilities of Keras.\n\nIf you have discovered a security vulnerability in this project, please report it\nprivately. **Do not disclose it as a public issue.** This gives us time to work with you\nto fix the issue before public exposure, reducing the chance that the exploit will be\nused before a patch is released.\n\nYou may submit the report in the following ways:\n\n- send an email to francois.chollet@gmail.com; and/or\n- send a [private vulnerability report](https://github.com/keras-team/keras/security/advisories/new)\n\nPlease provide the following information in your report:\n\n- A description of the vulnerability and its impact\n- How to reproduce the issue\n\nThis project is maintained by volunteers on a reasonable-effort basis. As such,\nplease give us 90 days to work on a fix before public exposure.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20156, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",29.044025182724,123
How does the return_attention_scores flag passed to call affect the attention computation flow and the layer’s output?,"When `return_attention_scores=True` is passed to the call method of the MultiHeadAttention layer in Keras, it affects the attention computation flow and the layer's output as follows:

*   The `_compute_attention` function scales Q and K.
*   It uses an einsum to compute raw scores.
*   It applies masked softmax to the scores.
*   Optionally, it applies dropout to the scores.
*   It computes the context via another einsum.
*   It returns a tuple of `(context, attention_scores)` instead of `(context, None)`.
*   The call method returns a 2-tuple of `(output, scores)`. In summary, when `return_attention_scores` is True, the MultiHeadAttention layer in Keras computes and returns both the context and attention scores.","When you pass return_attention_scores=True into MultiHeadAttention.call:

• In _compute_attention, it short-circuits the “fast” dot-product path (and forbids flash attention), so it always:  
  – scales Q, K, does an einsum to get raw scores  
  – applies masked softmax  
  – (optionally) applies dropout to the scores  
  – computes the context via another einsum  
  – returns both (context, attention_scores) rather than (context, None).

• Back in call, because return_attention_scores=True, it returns the 2-tuple (output, scores); otherwise it only returns the output tensor.","['CLASS keras.keras.src.layers.reshaping.flatten.Flatten has method keras.keras.src.layers.reshaping.flatten.Flatten.get_config\nFlattens the input. Does not affect the batch size.  Note: If inputs are shaped `(batch,)` without a feature axis, then flattening adds an extra channel dimension and output shape is `(batch, 1)`.  Args:     data_format: A string, one of `""channels_last""` (default) or         `""channels_first""`. The ordering of the dimensions in the inputs.         `""channels_last""` corresponds to inputs with shape         `(batch, ..., channels)` while `""channels_first""` corresponds to         inputs with shape `(batch, channels, ...)`.         When unspecified, uses `image_data_format` value found in your Keras         config file at `~/.keras/keras.json` (if exists). Defaults to         `""channels_last""`.  Example:  >>> x = keras.Input(shape=(10, 64)) >>> y = keras.layers.Flatten()(x) >>> y.shape (None, 640)', 'METHOD keras.keras.src.layers.attention.attention.Attention.call has parameter or accepts argument keras.keras.src.layers.attention.attention.Attention.call.return_attention_scores', 'MODULE keras.keras.src.layers.reshaping.flatten contains CLASS keras.keras.src.layers.reshaping.flatten.Flatten\nFlattens the input. Does not affect the batch size.  Note: If inputs are shaped `(batch,)` without a feature axis, then flattening adds an extra channel dimension and output shape is `(batch, 1)`.  Args:     data_format: A string, one of `""channels_last""` (default) or         `""channels_first""`. The ordering of the dimensions in the inputs.         `""channels_last""` corresponds to inputs with shape         `(batch, ..., channels)` while `""channels_first""` corresponds to         inputs with shape `(batch, channels, ...)`.         When unspecified, uses `image_data_format` value found in your Keras         config file at `~/.keras/keras.json` (if exists). Defaults to         `""channels_last""`.  Example:  >>> x = keras.Input(shape=(10, 64)) >>> y = keras.layers.Flatten()(x) >>> y.shape (None, 640)', 'METHOD keras.keras.src.layers.attention.multi_head_attention.MultiHeadAttention.call has parameter or accepts argument keras.keras.src.layers.attention.multi_head_attention.MultiHeadAttention.call.return_attention_scores', 'CLASS keras.keras.src.layers.reshaping.flatten.Flatten has method keras.keras.src.layers.reshaping.flatten.Flatten.compute_output_shape\nFlattens the input. Does not affect the batch size.  Note: If inputs are shaped `(batch,)` without a feature axis, then flattening adds an extra channel dimension and output shape is `(batch, 1)`.  Args:     data_format: A string, one of `""channels_last""` (default) or         `""channels_first""`. The ordering of the dimensions in the inputs.         `""channels_last""` corresponds to inputs with shape         `(batch, ..., channels)` while `""channels_first""` corresponds to         inputs with shape `(batch, channels, ...)`.         When unspecified, uses `image_data_format` value found in your Keras         config file at `~/.keras/keras.json` (if exists). Defaults to         `""channels_last""`.  Example:  >>> x = keras.Input(shape=(10, 64)) >>> y = keras.layers.Flatten()(x) >>> y.shape (None, 640)', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 20482, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",39.34803748130798,124
How does the ONNX export function retrieve a concrete TensorFlow function without creating a temporary SavedModel?,"The ONNX export function retrieves a concrete TensorFlow function without creating a temporary SavedModel by using the `get_concrete_fn` method in the Keras library. This method returns a tf.function (concrete function) that is then passed to the `tf2onnx.convert.from_function` method for conversion to ONNX format. The `get_concrete_fn` method calls `ExportArchive.track_and_add_endpoint` and `_filter_and_track_resources` functions, which are used to track and filter resources in the TensorFlow graph. This approach allows for efficient retrieval of a concrete function without requiring the creation of a temporary SavedModel.","The ONNX exporter never writes out a SavedModel on disk. Instead it calls get_concrete_fn (in keras/src/export/onnx.py), which under the hood does:

• ExportArchive.track_and_add_endpoint(…)  
• (for TF) ExportArchive._filter_and_track_resources()  
• return ExportArchive._get_concrete_fn(DEFAULT_ENDPOINT_NAME)

That gives you a ready‐to‐go tf.function (concrete function) in memory, which is then handed straight to tf2onnx.convert.from_function. No temporary SavedModel is ever created.","['METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'MODULE keras.keras.src.export.onnx contains FUNCTION keras.keras.src.export.onnx.get_concrete_fn\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.verbose\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'FUNCTION keras.keras.src.export.onnx.get_concrete_fn has parameter or accepts argument keras.keras.src.export.onnx.kwargs\nGet the `tf.function` associated with the model.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.input_signature\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 4, 'pr': 20725, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",34.91649866104126,126
Which argument to the Keras ISTFT function requires a 32-bit integer dtype to avoid a conversion error?,The argument `fft_length` to the Keras ISTFT function requires a 32-bit integer (tf.int32) for fft_length to avoid a conversion error.,The fft_length argument must be a 32-bit integer (tf.int32) to avoid the conversion error.,"['FUNCTION qlib.qlib.rl.order_execution.interpreter.canonicalize has parameter or accepts argument qlib.qlib.rl.order_execution.interpreter.canonicalize.value\nTo 32-bit numeric types. Recursively.', 'FUNCTION keras.keras.src.backend.numpy.math.stft has parameter or accepts argument keras.keras.src.backend.numpy.math.stft.fft_length', 'MODULE qlib.qlib.rl.order_execution.interpreter contains FUNCTION qlib.qlib.rl.order_execution.interpreter.canonicalize\nTo 32-bit numeric types. Recursively.', 'METHOD keras.keras.src.ops.math_test.TestMathErrors.test_invalid_fft_length has parameter or accepts argument keras.keras.src.ops.math_test.x', 'FUNCTION keras.keras.src.ops.numpy.deg2rad has parameter or accepts argument keras.keras.src.ops.numpy.deg2rad.x\nConvert angles from degrees to radians.  The conversion is defined as: `rad = deg * (π / 180)`  Args:     x: Input tensor of angles in degrees.  Returns:     A tensor containing angles converted to radians.  Examples: >>> from keras import ops >>> ops.deg2rad(180.0) 3.141592653589793 >>> ops.deg2rad([0.0, 90.0, 180.0]) array([0., 1.57079633, 3.14159265])', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20728, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",26.829463481903076,127
What exception types are asserted for an unrecognized format and for an unsupported backend in the export error tests?,"In the export error tests for Keras, an unrecognized format is asserted to raise a `ValueError`, while an unsupported backend is asserted to raise a `NotImplementedError`. This is based on the implementation of `ModelTest.test_export_error` in `keras/src/models/model_test.py`.","In ModelTest.test_export_error (keras/src/models/model_test.py) they assert:

• ValueError for an unrecognized format  
• NotImplementedError for an unsupported backend","['METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'METHOD keras.keras.src.models.model_test.ModelTest.test_export_error has parameter or accepts argument keras.keras.src.models.model_test.model', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.input_signature\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'CLASS keras.keras.src.models.model_test.ModelTest has method keras.keras.src.models.model_test.ModelTest.test_export_error', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.format\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n### Option 2: Set up a local environment\n\nTo set up your local dev environment, you will need the following tools.\n\n1.  [git](https://github.com/) for code repository management.\n2.  [python](https://www.python.org/) to build and code in Keras.\n\nThe following commands check the tools above are successfully installed. Note\nthat Keras requires at least Python 3.10 to run.\n\n```shell\ngit --version\npython --version\n```\n\nClone your forked repo to your local machine. Go to the cloned directory to\ninstall the dependencies.\n\n```shell\ngit clone https://github.com/YOUR_GITHUB_USERNAME/keras.git\ncd keras\npip install -r requirements.txt\n```\n\nYou then need to configure the backend to use, see the\n[Configuring your backend](https://github.com/keras-team/keras/blob/master/README.md#configuring-your-backend)\nsection of the README.\n\nYou can also add GPU support to your environment, see the\n[Adding GPU support](https://github.com/keras-team/keras/blob/master/README.md#adding-gpu-support)\nsection of the README.\n\n## Generating public API and formatting the code\n\nFor the first time you are setting up the repo, please run `pre-commit install`.\nNote that this needs to be done only once at the beginning.\n\nNow, whenever you run `git commit -m ""<message>""`, three things are\nautomatically done:\n\n- Public API generation\n- Code formatting\n- Code linting\n\nIf there\'s any error, the commit will not go through. Please fix the error (\nmost of the times, the error is fixed automatically by the formatter/linter) and\nre-run the following:\n\n```\ngit add .\ngit commit -m ""<message>"" # This will not get logged as a duplicate commit.\n```\n\nIn case you want to run the above manually on all files, you can do the\nfollowing:\n\n```\npre-commit run --all-files\n```\n\nKerasHub uses [Ruff](https://docs.astral.sh/ruff/) to format the code.\n\n### Docstrings\n\nWe do not have an automated way to check docstring style, so if you write\nor edit any docstring, please make sure to check them manually.\nKeras docstrings follow the conventions below:\n\nA **class docstring** may contain the following items:\n\n* A one-line description of the class.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for arguments in `__init__()`.\n* If it\'s a layer:\n    * `Call arguments` section for arguments in `Layer.call()`.\n    * `Returns` section for the return values of `Layer.call()`.\n    * Optional `Raises` section for possible errors.\n\nYou can check out `MultiHeadAttention` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/layers/attention/multi_head_attention.py#L20).\n\nA **function docstring** may contain the following items:\n\n* One-line description of the function.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for the function arguments.\n* `Returns` section for the return values.\n* Optional `Raises` section for possible errors.\n\nYou can check out `text_dataset_from_directory` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/utils/text_dataset_utils.py#L27).', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20735, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",29.701108932495117,128
What sequence of tensor operations does the STFTSpectrogram layer perform to support variable-length inputs in both channel-first and channel-last formats?,"The STFTSpectrogram layer performs the following sequence of tensor operations to support variable-length inputs in both channel-first and channel-last formats:

1.  **Reshaping**: The input tensor is reshaped from [B, T, C] to [B, C, T] for channel-last format or [B, C, T] to [B·C, T, 1] for channel-first format.
2.  **Convolution**: A convolution operation is performed on the reshaped tensor using a fixed ""frame"" kernel.
3.  **Transposition**: The output of the convolution operation is transposed from [B·C, F, T′] to [B·C, T′, F].
4.  **Reshaping (again)**: The transposed tensor is reshaped back to [B, C, F, T′] for channel-last format or [B, C, F, T′] for channel-first format.
5.  **Adjusting shapes**: The final output tensor may need to be adjusted in shape depending on the input dimensions and whether `expand_dims` is used.

This sequence of operations allows the STFTSpectrogram layer to handle variable-length inputs in both channel-first and channel-last formats, making it a flexible and versatile tool for signal processing tasks.","The STFTSpectrogram layer first builds a fixed “frame” kernel and then in STFTSpectrogram._apply_conv:

• If data_format=""channels_last"":  
  1. Transpose inputs from [B, T, C] → [B, C, T]  
  2. Reshape to [B·C, T, 1]  
  3. ops.conv → [B·C, F, T′]  
  4. Transpose back [B·C, F, T′] → [B·C, T′, F]  
  5. Reshape to [B, C, F, T′]

• If data_format=""channels_first"":  
  1. Reshape [B, C, T] → [B·C, 1, T]  
  2. ops.conv → [B·C, F, T′]  
  3. Reshape to [B, C, F, T′]

Then in STFTSpectrogram._adjust_shapes it uses ops.reshape and ops.transpose to support both variable length and optional expand_dims:

• channels_last & expand_dims: transpose [0,3,2,1] → [B, T′, F, C]  
• channels_last & no expand_dims: reshape [B, C·F, T′] → transpose [0,2,1] → [B, T′, C·F]  
• channels_first & expand_dims: transpose [0,1,3,2] → [B, C, T′, F]  
• channels_first & no expand_dims: reshape [B, C·F, T′]

All reshapes use batch_size=–1 so T′ can vary at runtime.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.tensordot\nCompute the tensor dot product along specified axes.  Args:     x1: First tensor.     x2: Second tensor.     axes: - If an integer, N, sum over the last N axes of `x1` and the             first N axes of `x2` in order. The sizes of the corresponding             axes must match.           - Or, a list of axes to be summed over, first sequence applying             to `x1`, second to `x2`. Both sequences must be of the             same length.  Returns:     The tensor dot product of the inputs.', 'FUNCTION keras.keras.src.ops.math.stft has parameter or accepts argument keras.keras.src.ops.math.stft.x\nShort-Time Fourier Transform along the last axis of the input.  The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time.  Args:     x: Input tensor.     sequence_length: An integer representing the sequence length.     sequence_stride: An integer representing the sequence hop size.     fft_length: An integer representing the size of the FFT to apply. If not         specified, uses the smallest power of 2 enclosing `sequence_length`.     window: A string, a tensor of the window or `None`. If `window` is a         string, available values are `""hann""` and `""hamming""`. If `window`         is a tensor, it will be used directly as the window and its length         must be `sequence_length`. If `window` is `None`, no windowing is         used. Defaults to `""hann""`.     center: Whether to pad `x` on both sides so that the t-th sequence is         centered at time `t * sequence_stride`. Otherwise, the t-th sequence         begins at time `t * sequence_stride`. Defaults to `True`.  Returns:     A tuple containing two tensors - the real and imaginary parts of the     STFT output.  Example:  >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0]) >>> stft(x, 3, 2, 3) (array([[0.75, -0.375],    [3.75, -1.875],    [5.25, -2.625]]), array([[0.0, 0.64951905],    [0.0, 0.64951905],    [0.0, -0.64951905]]))', 'FUNCTION keras.keras.src.ops.numpy.tensordot has parameter or accepts argument keras.keras.src.ops.numpy.tensordot.axes\nCompute the tensor dot product along specified axes.  Args:     x1: First tensor.     x2: Second tensor.     axes: - If an integer, N, sum over the last N axes of `x1` and the             first N axes of `x2` in order. The sizes of the corresponding             axes must match.           - Or, a list of axes to be summed over, first sequence applying             to `x1`, second to `x2`. Both sequences must be of the             same length.  Returns:     The tensor dot product of the inputs.', 'FUNCTION keras.keras.src.ops.math.stft has parameter or accepts argument keras.keras.src.ops.math.stft.window\nShort-Time Fourier Transform along the last axis of the input.  The STFT computes the Fourier transform of short overlapping windows of the input. This giving frequency components of the signal as they change over time.  Args:     x: Input tensor.     sequence_length: An integer representing the sequence length.     sequence_stride: An integer representing the sequence hop size.     fft_length: An integer representing the size of the FFT to apply. If not         specified, uses the smallest power of 2 enclosing `sequence_length`.     window: A string, a tensor of the window or `None`. If `window` is a         string, available values are `""hann""` and `""hamming""`. If `window`         is a tensor, it will be used directly as the window and its length         must be `sequence_length`. If `window` is `None`, no windowing is         used. Defaults to `""hann""`.     center: Whether to pad `x` on both sides so that the t-th sequence is         centered at time `t * sequence_stride`. Otherwise, the t-th sequence         begins at time `t * sequence_stride`. Defaults to `True`.  Returns:     A tuple containing two tensors - the real and imaginary parts of the     STFT output.  Example:  >>> x = keras.ops.convert_to_tensor([0.0, 1.0, 2.0, 3.0, 4.0]) >>> stft(x, 3, 2, 3) (array([[0.75, -0.375],    [3.75, -1.875],    [5.25, -2.625]]), array([[0.0, 0.64951905],    [0.0, 0.64951905],    [0.0, -0.64951905]]))', 'FUNCTION keras.keras.src.ops.numpy.tensordot has parameter or accepts argument keras.keras.src.ops.numpy.tensordot.x2\nCompute the tensor dot product along specified axes.  Args:     x1: First tensor.     x2: Second tensor.     axes: - If an integer, N, sum over the last N axes of `x1` and the             first N axes of `x2` in order. The sizes of the corresponding             axes must match.           - Or, a list of axes to be summed over, first sequence applying             to `x1`, second to `x2`. Both sequences must be of the             same length.  Returns:     The tensor dot product of the inputs.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 20736, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",52.316837310791016,129
Under what conditions does _retrieve_class_or_fn prepend a '>' to the name when searching custom_objects?,"The `_retrieve_class_or_fn` function in Keras prepends a '>' to the name when searching `custom_objects` only in the case where the raw name contains no """">"" and it is part of a Keras 3.6 compatibility workaround. This workaround checks if the raw name contains no "">"", and if so, sets `separated_name` to "">"" concatenated with the original name, then searches for any key in `custom_objects` that ends with "">"" plus the original name.","In the `if obj_type == ""function"" and module == ""builtins""` block (in `keras/src/saving/serialization_lib.py`), after checking all `BUILTIN_MODULES`, there’s a Keras 3.6-compat workaround: if the raw `name` contains no “>”, it does  

    separated_name = "">"" + name  

and then looks for any key in `custom_objects` that ends with that `"">""+name""`. That’s the only time `_retrieve_class_or_fn` prepends a “>” to the name.","[""MODULE keras.keras.src.saving.object_registration contains FUNCTION keras.keras.src.saving.object_registration.get_registered_object\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'MODULE keras.keras.src.utils.naming_test contains CLASS keras.keras.src.utils.naming_test.ObjectWithoutName', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.GLOBAL_CUSTOM_OBJECTS\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'MODULE keras.keras.src.utils.naming_test contains CLASS keras.keras.src.utils.naming_test.ObjectWithoutNameOrClass', ""FUNCTION keras.keras.src.saving.object_registration.get_registered_object has parameter or accepts argument keras.keras.src.saving.object_registration.get_registered_object.name\nReturns the class associated with `name` if it is registered with Keras.  This function is part of the Keras serialization and deserialization framework. It maps strings to the objects associated with them for serialization/deserialization.  Example:  ```python def from_config(cls, config, custom_objects=None):     if 'my_custom_object_name' in config:         config['hidden_cls'] = tf.keras.saving.get_registered_object(             config['my_custom_object_name'], custom_objects=custom_objects) ```  Args:     name: The name to look up.     custom_objects: A dictionary of custom objects to look the name up in.         Generally, custom_objects is provided by the user.     module_objects: A dictionary of custom objects to look the name up in.         Generally, module_objects is provided by midlevel library         implementers.  Returns:     An instantiable class associated with `name`, or `None` if no such class         exists."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20755, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.971901178359985,130
"In TensorFlow graph mode, which dimension of the provided mask does the TimeDistributed layer still validate when omitting the batch size comparison?","In TensorFlow graph mode, the TimeDistributed layer still validates the second dimension of the provided mask when omitting the batch size comparison. This is because `mask_shape[1:2] != (timesteps,)` is used to check if the timesteps match.","In TimeDistributed.call (keras/src/layers/rnn/time_distributed.py), when running in TF graph mode it skips checking mask_shape[0] (batch) but still asserts that mask_shape[1] == timesteps (via `mask_shape[1:2] != (timesteps,)`).","['CLASS keras.keras.src.layers.reshaping.reshape.Reshape inherits from or is a subclass of keras.src.layers.layer.Layer\nLayer that reshapes inputs into the given shape.  Args:     target_shape: Target shape. Tuple of integers, does not include the         samples dimension (batch size).  Input shape:     Arbitrary, although all dimensions in the input shape must be     known/fixed. Use the keyword argument `input_shape` (tuple of integers,     does not include the samples/batch size axis) when using this layer as     the first layer in a model.  Output shape:     `(batch_size, *target_shape)`  Example:  >>> x = keras.Input(shape=(12,)) >>> y = keras.layers.Reshape((3, 4))(x) >>> y.shape (None, 3, 4)  >>> # also supports shape inference using `-1` as dimension >>> y = keras.layers.Reshape((-1, 2, 2))(x) >>> y.shape (None, 3, 2, 2)', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.TimeDistributed.supports_masking\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'CLASS keras.keras.src.layers.reshaping.reshape.Reshape has method keras.keras.src.layers.reshaping.reshape.Reshape.compute_output_spec\nLayer that reshapes inputs into the given shape.  Args:     target_shape: Target shape. Tuple of integers, does not include the         samples dimension (batch size).  Input shape:     Arbitrary, although all dimensions in the input shape must be     known/fixed. Use the keyword argument `input_shape` (tuple of integers,     does not include the samples/batch size axis) when using this layer as     the first layer in a model.  Output shape:     `(batch_size, *target_shape)`  Example:  >>> x = keras.Input(shape=(12,)) >>> y = keras.layers.Reshape((3, 4))(x) >>> y.shape (None, 3, 4)  >>> # also supports shape inference using `-1` as dimension >>> y = keras.layers.Reshape((-1, 2, 2))(x) >>> y.shape (None, 3, 2, 2)', 'CLASS keras.keras.src.layers.rnn.time_distributed.TimeDistributed has field or attribute or property keras.keras.src.layers.rnn.time_distributed.mask\nThis wrapper allows to apply a layer to every temporal slice of an input.  Every input should be at least 3D, and the dimension of index one of the first input will be considered to be the temporal dimension.  Consider a batch of 32 video samples, where each sample is a 128x128 RGB image with `channels_last` data format, across 10 timesteps. The batch input shape is `(32, 10, 128, 128, 3)`.  You can then use `TimeDistributed` to apply the same `Conv2D` layer to each of the 10 timesteps, independently:  >>> inputs = layers.Input(shape=(10, 128, 128, 3), batch_size=32) >>> conv_2d_layer = layers.Conv2D(64, (3, 3)) >>> outputs = layers.TimeDistributed(conv_2d_layer)(inputs) >>> outputs.shape (32, 10, 126, 126, 64)  Because `TimeDistributed` applies the same instance of `Conv2D` to each of the timestamps, the same set of weights are used at each timestamp.  Args:     layer: a `keras.layers.Layer` instance.  Call arguments:     inputs: Input tensor of shape (batch, time, ...) or nested tensors,         and each of which has shape (batch, time, ...).     training: Python boolean indicating whether the layer should behave in         training mode or in inference mode. This argument is passed to the         wrapped layer (only if the layer supports this argument).     mask: Binary tensor of shape `(samples, timesteps)` indicating whether         a given timestep should be masked. This argument is passed to the         wrapped layer (only if the layer supports this argument).', 'MODULE keras.keras.src.layers.reshaping.reshape contains CLASS keras.keras.src.layers.reshaping.reshape.Reshape\nLayer that reshapes inputs into the given shape.  Args:     target_shape: Target shape. Tuple of integers, does not include the         samples dimension (batch size).  Input shape:     Arbitrary, although all dimensions in the input shape must be     known/fixed. Use the keyword argument `input_shape` (tuple of integers,     does not include the samples/batch size axis) when using this layer as     the first layer in a model.  Output shape:     `(batch_size, *target_shape)`  Example:  >>> x = keras.Input(shape=(12,)) >>> y = keras.layers.Reshape((3, 4))(x) >>> y.shape (None, 3, 4)  >>> # also supports shape inference using `-1` as dimension >>> y = keras.layers.Reshape((-1, 2, 2))(x) >>> y.shape (None, 3, 2, 2)', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20765, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",29.6281316280365,131
Which TensorFlow quantization APIs are used in the backend shortcut for per-tensor and per-channel fake quantization?,"The TensorFlow quantization APIs used in the backend shortcut for per-tensor and per-channel fake quantization in Keras are `tf.quantization.fake_quant_with_min_max_vars` and `tf.quantization.fake_quant_with_min_max_vars_per_channel`, respectively.","In `keras/src/quantizers/quantizers.py` inside `fake_quant_with_min_max_vars`, the TensorFlow backend shortcut uses:

• Per-tensor: `tf.quantization.fake_quant_with_min_max_vars`  
• Per-channel: `tf.quantization.fake_quant_with_min_max_vars_per_channel`","['MODULE keras.keras.src.quantizers.quantizers contains FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.max_vals\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.num_bits\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.narrow_range\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'FUNCTION keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars has parameter or accepts argument keras.keras.src.quantizers.quantizers.fake_quant_with_min_max_vars.inputs\nPerform per-tensor or per-channel fake quantization.  `[min_vals, max_vals]` define the clamping range for the `inputs`.  The `inputs` are quantized into the quantization range: - `[0, 2^num_bits - 1]` when `narrow_range=False` - `[1, 2^num_bits - 1]` when `narrow_range=True`  After quantization, the values are dequantized and output as floats within the `[min_vals, max_vals]` interval.  This operation supports gradient computation, allowing `min_vals` and `max_vals` to be trained.  Args:     inputs: Input Keras tensor of float dtype.     min_vals: A global minimum scalar or a per-channel minimum tensor.     max_vals: A global maximum scalar or a per-channel maximum tensor.     num_bits: Quantization bit width (e.g., `8` for int8). Defaults to `8`.     narrow_range: Whether to use narrow quantization range. Defaults to         `False`.     axis: Axis along which to perform per-channel quantization. If `None`,           per-tensor quantization is performed. Defaults to `None`.   Returns:     Tensor: A Keras tensor with fake quantization applied.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 9, 'n_files_pr': 4, 'pr': 20772, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",29.52292561531067,132
How does the masking layer reconstruct a tensor mask value when loading a saved model?,"When loading a saved Masking layer in Keras, the mask_value is passed into `Masking.__init__` as a serialized dict. If `mask_value` is a dict with a ""config"" entry, `Masking.__init__` calls `deserialize_keras_object(mask_value)` to rebuild the original Tensor `mask_value`. This process involves deserializing the `mask_value` dictionary using the `deserialize_keras_object` function, which reconstructs the original Tensor object from the serialized data. This allows the Masking layer to correctly load and use the saved mask value when loading a saved model.","When you reload a saved Masking layer, its mask_value is passed into Masking.__init__ as a serialized dict. The constructor checks:

• in keras/src/layers/core/masking.py:Masking.__init__  
  if isinstance(mask_value, dict) and mask_value.get(""config""):  
      mask_value = deserialize_keras_object(mask_value)

That call to deserialize_keras_object rebuilds the original Tensor mask_value.","['CLASS keras.keras.src.layers.core.masking.Masking inherits from or is a subclass of keras.src.layers.layer.Layer\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', 'MODULE keras.guides.understanding_masking_and_padding contains GLOBAL_VARIABLE keras.guides.understanding_masking_and_padding.unmasked_embedding', 'CLASS keras.keras.src.layers.core.masking.Masking has field or attribute or property keras.keras.src.layers.core.masking.inputs\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', 'MODULE keras.guides.understanding_masking_and_padding contains GLOBAL_VARIABLE keras.guides.understanding_masking_and_padding.model', 'MODULE keras.keras.src.layers.core.masking contains CLASS keras.keras.src.layers.core.masking.Masking\nMasks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor), if all values in the input tensor at that timestep are equal to `mask_value`, then the timestep will be masked (skipped) in all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such an input mask, an exception will be raised.  Example:  Consider a NumPy data array `x` of shape `(samples, timesteps, features)`, to be fed to an LSTM layer. You want to mask timestep #3 and #5 because you lack data for these timesteps. You can:  - Set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.` - Insert a `Masking` layer with `mask_value=0.` before the LSTM layer:  ```python samples, timesteps, features = 32, 10, 8 inputs = np.random.random([samples, timesteps, features]).astype(np.float32) inputs[:, 3, :] = 0. inputs[:, 5, :] = 0.  model = keras.models.Sequential() model.add(keras.layers.Masking(mask_value=0.0)) model.add(keras.layers.LSTM(32)) output = model(inputs) # The time step 3 and 5 will be skipped from LSTM calculation. ```  Note: in the Keras masking convention, a masked timestep is denoted by a mask value of `False`, while a non-masked (i.e. usable) timestep is denoted by a mask value of `True`.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20791, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.94422769546509,133
How are lines read from the exclusion file filtered before being used to determine which tests to skip for the OpenVINO backend?,"The lines read from the exclusion file are filtered by stripping whitespace and removing empty or whitespace-only lines using a list comprehension before being used to determine which tests to skip for the OpenVINO backend in keras. This is achieved through the following steps:

1. `file.readlines()` reads all lines from the exclusion file.
2. A list comprehension `[line.strip() for line in openvino_skipped_tests if line.strip()]` filters out any lines that are empty or contain only whitespace by stripping whitespace and checking if the resulting string is not empty.

This means that any lines with leading, trailing, or interior whitespace will be removed from `openvino_skipped_tests`, leaving only the non-empty lines.","In `pytest_collection_modifyitems` (conftest.py), after doing `file.readlines()`, they run:

```python
openvino_skipped_tests = [
    line.strip() for line in openvino_skipped_tests
    if line.strip()
]
```

This both strips each line of whitespace and filters out any empty (or all-whitespace) lines before using them to skip tests.","['MODULE cognee.cognee.infrastructure.llm.prompts.read_query_prompt contains FUNCTION cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt\nRead a query prompt from a file.  Retrieve the contents of a specified prompt file, optionally using a provided base directory for the file path. If the base directory is not specified, a default path is used. Log errors if the file is not found or if another error occurs during file reading.  Parameters: -----------      - prompt_file_name (str): The name of the prompt file to be read.     - base_directory (str): The base directory from which to read the prompt file. If       None, a default path is used. (default None)  Returns: --------      Returns the contents of the prompt file as a string, or None if the file cannot be     read due to an error.', 'CLASS qlib.qlib.contrib.report.data.ana.FeaInfAna has method qlib.qlib.contrib.report.data.ana.FeaInfAna.skip', 'FUNCTION cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt has parameter or accepts argument cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt.base_directory\nRead a query prompt from a file.  Retrieve the contents of a specified prompt file, optionally using a provided base directory for the file path. If the base directory is not specified, a default path is used. Log errors if the file is not found or if another error occurs during file reading.  Parameters: -----------      - prompt_file_name (str): The name of the prompt file to be read.     - base_directory (str): The base directory from which to read the prompt file. If       None, a default path is used. (default None)  Returns: --------      Returns the contents of the prompt file as a string, or None if the file cannot be     read due to an error.', 'CLASS qlib.qlib.contrib.report.data.ana.FeaNanAnaRatio has method qlib.qlib.contrib.report.data.ana.FeaNanAnaRatio.skip', 'FUNCTION cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt has parameter or accepts argument cognee.cognee.infrastructure.llm.prompts.read_query_prompt.read_query_prompt.prompt_file_name\nRead a query prompt from a file.  Retrieve the contents of a specified prompt file, optionally using a provided base directory for the file path. If the base directory is not specified, a default path is used. Log errors if the file is not found or if another error occurs during file reading.  Parameters: -----------      - prompt_file_name (str): The name of the prompt file to be read.     - base_directory (str): The base directory from which to read the prompt file. If       None, a default path is used. (default None)  Returns: --------      Returns the contents of the prompt file as a string, or None if the file cannot be     read due to an error.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/data/README.md\nThis folder contains processed example dataset used by the demos.\nCopyright of the dataset belongs to the original copyright holder', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 20845, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",34.94467282295227,134
Under what condition do these functions immediately return the input tensor without any reduction?,The amax and amin functions in keras/src/backend/openvino/numpy.py short-circuit and return the input x immediately without any reduction when `axis` is either an empty tuple (`axis == ()`) or an empty list (`axis == []`).,"In both keras/src/backend/openvino/numpy.py (amax and amin), they short‐circuit and return the input `x` immediately if you pass an “empty” axis, i.e.  
 `axis == ()` or `axis == []`.","['CLASS keras.keras.src.dtype_policies.dtype_policy.QuantizedFloat8DTypePolicy has method keras.keras.src.dtype_policies.dtype_policy.QuantizedFloat8DTypePolicy.amax_history_length\nThe length of the amax history window.  This property is used for scaling factor computation in float8 training.', 'FUNCTION keras.keras.src.backend.openvino.numpy.amax has parameter or accepts argument keras.keras.src.backend.openvino.numpy.amax.axis', 'MODULE xgboost.ops.script.lint_cpp contains FUNCTION xgboost.ops.script.lint_cpp.get_header_guard_dmlc\nGet Header Guard Convention for DMLC Projects.  For headers in include, directly use the path For headers in src, use project name plus path  Examples: with project-name = dmlc     include/dmlc/timer.h -> DMLC_TIMTER_H_     src/io/libsvm_parser.h -> DMLC_IO_LIBSVM_PARSER_H_', 'FUNCTION keras.keras.src.backend.numpy.numpy.amax has parameter or accepts argument keras.keras.src.backend.numpy.numpy.amax.axis', 'FUNCTION xgboost.ops.script.lint_cpp.get_header_guard_dmlc has parameter or accepts argument xgboost.ops.script.lint_cpp.get_header_guard_dmlc.filename\nGet Header Guard Convention for DMLC Projects.  For headers in include, directly use the path For headers in src, use project name plus path  Examples: with project-name = dmlc     include/dmlc/timer.h -> DMLC_TIMTER_H_     src/io/libsvm_parser.h -> DMLC_IO_LIBSVM_PARSER_H_', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n#### Resource Allocation\n\nA denial of service caused by one model can impact the overall system health. Implement safeguards like rate limits, access controls, and health monitoring.\n\n#### Model Sharing\n\nIn a multitenant design that allows sharing models, make sure that tenants and users fully understand the potential security risks involved. They must be aware that they will essentially be running code provided by other users. Unfortunately, there are no reliable methods available to detect malicious models, graphs, or checkpoints. To mitigate this risk, the recommended approach is to sandbox the model execution, effectively isolating it from the rest of the system.\n\n#### Hardware Attacks\n\nBesides the virtual environment, the hardware (GPUs or TPUs) can also be attacked. [Research](https://scholar.google.com/scholar?q=gpu+side+channel) has shown that side channel attacks on GPUs are possible, which can make data leak from other models or processes running on the same system at the same time.\n\n## Reporting a Vulnerability\n\nBeware that none of the topics under [Using Keras Securely](#using-keras-securely) are considered vulnerabilities of Keras.\n\nIf you have discovered a security vulnerability in this project, please report it\nprivately. **Do not disclose it as a public issue.** This gives us time to work with you\nto fix the issue before public exposure, reducing the chance that the exploit will be\nused before a patch is released.\n\nYou may submit the report in the following ways:\n\n- send an email to francois.chollet@gmail.com; and/or\n- send a [private vulnerability report](https://github.com/keras-team/keras/security/advisories/new)\n\nPlease provide the following information in your report:\n\n- A description of the vulnerability and its impact\n- How to reproduce the issue\n\nThis project is maintained by volunteers on a reasonable-effort basis. As such,\nplease give us 90 days to work on a fix before public exposure.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20883, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",28.178452730178833,135
Which test in LossScaleOptimizerTest validates that calling apply or stateless_apply increases the iterations property by one on each step?,The test `test_iterations_update` in LossScaleOptimizerTest validates that calling either `apply` or `stateless_apply` increments the iterations property by one on each step.,The `LossScaleOptimizerTest.test_iterations_update` (in `keras/src/optimizers/loss_scale_optimizer_test.py`) verifies that each call to `apply` or `stateless_apply` increments the `iterations` property by one.,"['CLASS qlib.qlib.rl.trainer.vessel.TrainingVessel has field or attribute or property qlib.qlib.rl.trainer.vessel.TrainingVessel.test_initial_states\nThe default implementation of training vessel.  ``__init__`` accepts a sequence of initial states so that iterator can be created. ``train``, ``validate``, ``test`` each do one collect (and also update in train). By default, the train initial states will be repeated infinitely during training, and collector will control the number of episodes for each iteration. In validation and testing, the val / test initial states will be used exactly once.  Extra hyper-parameters (only used in train) include:  - ``buffer_size``: Size of replay buffer. - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run. - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.   For example, ``dict(repeat=10, batch_size=64)``.', 'METHOD keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_iterations_update has parameter or accepts argument keras.keras.src.optimizers.loss_scale_optimizer_test.LossScaleOptimizerTest.test_iterations_update.stateless', 'CLASS qlib.qlib.rl.trainer.vessel.TrainingVessel has field or attribute or property qlib.qlib.rl.trainer.vessel.TrainingVessel.train_initial_states\nThe default implementation of training vessel.  ``__init__`` accepts a sequence of initial states so that iterator can be created. ``train``, ``validate``, ``test`` each do one collect (and also update in train). By default, the train initial states will be repeated infinitely during training, and collector will control the number of episodes for each iteration. In validation and testing, the val / test initial states will be used exactly once.  Extra hyper-parameters (only used in train) include:  - ``buffer_size``: Size of replay buffer. - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run. - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.   For example, ``dict(repeat=10, batch_size=64)``.', 'METHOD keras.keras.src.optimizers.loss_scale_optimizer.LossScaleOptimizer.stateless_apply has parameter or accepts argument keras.keras.src.optimizers.loss_scale_optimizer.LossScaleOptimizer.stateless_apply.grads', 'CLASS qlib.qlib.rl.trainer.vessel.TrainingVessel has field or attribute or property qlib.qlib.rl.trainer.vessel.TrainingVessel.val_initial_states\nThe default implementation of training vessel.  ``__init__`` accepts a sequence of initial states so that iterator can be created. ``train``, ``validate``, ``test`` each do one collect (and also update in train). By default, the train initial states will be repeated infinitely during training, and collector will control the number of episodes for each iteration. In validation and testing, the val / test initial states will be used exactly once.  Extra hyper-parameters (only used in train) include:  - ``buffer_size``: Size of replay buffer. - ``episode_per_iter``: Episodes per collect at training. Can be overridden by fast dev run. - ``update_kwargs``: Keyword arguments appearing in ``policy.update``.   For example, ``dict(repeat=10, batch_size=64)``.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 20901, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",26.697601795196533,136
"When exporting, how does the resource-tracking step distinguish between generic TensorFlow trackables and Keras layers, and delegate to backend-specific logic?","The resource-tracking step in Keras distinguishes between generic TensorFlow trackables and Keras layers by checking if the resource is an instance of `tf.__internal__.tracking.Trackable` or `layers.Layer`. If it's a `Trackable`, it adds it to the `_tracked` list. If it's a `Layer`, it calls the `_track_layer` method, which is polymorphic and has backend-specific implementations for TensorFlow, JAX, and PyTorch. This allows Keras to delegate to backend-specific logic for tracking layers, enabling it to work with multiple backends while maintaining a consistent interface for users.","In keras/src/export/saved_model.py the ExportArchive.track() method does two isinstance checks:

  1. `isinstance(resource, tf.__internal__.tracking.Trackable)` → adds it to a generic `_tracked` list  
  2. `isinstance(resource, layers.Layer)` → calls `self._track_layer(resource)`

That `_track_layer` call is polymorphic: each backend supplies its own subclass of BackendExportArchive with a backend‐specific `_track_layer` (e.g.  
– TFExportArchive._track_layer in backend/tensorflow/export.py  
– JaxExportArchive._track_layer in backend/jax/export.py  
– TorchExportArchive._track_layer in backend/torch/export.py )  
so Keras layers go through the layer path and get handled by the right backend logic, while other Trackables just get recorded generically.","['MODULE keras.keras.src.backend.tensorflow.trackable contains CLASS keras.keras.src.backend.tensorflow.trackable.KerasAutoTrackable\nManages dependencies on other objects with Keras tracking.  Similar to TF AutoTrackable, but disabling tracking is based on tracking within Keras.  This serves as an interface between Keras tracking and TF tracking.', 'CLASS keras.keras.src.backend.tensorflow.core.Variable inherits from or is a subclass of tensorflow.tf.__internal__.tracking.Trackable', 'CLASS keras.keras.src.backend.tensorflow.trackable.KerasAutoTrackable has field or attribute or property keras.keras.src.backend.tensorflow.trackable.name\nManages dependencies on other objects with Keras tracking.  Similar to TF AutoTrackable, but disabling tracking is based on tracking within Keras.  This serves as an interface between Keras tracking and TF tracking.', 'CLASS keras.keras.src.backend.tensorflow.trackable.KerasAutoTrackable has field or attribute or property keras.keras.src.backend.tensorflow.trackable.value\nManages dependencies on other objects with Keras tracking.  Similar to TF AutoTrackable, but disabling tracking is based on tracking within Keras.  This serves as an interface between Keras tracking and TF tracking.', 'CLASS keras.keras.src.backend.tensorflow.trackable.KerasAutoTrackable has method keras.keras.src.backend.tensorflow.trackable.KerasAutoTrackable.__setattr__\nSupport self.foo = trackable syntax.\nManages dependencies on other objects with Keras tracking.  Similar to TF AutoTrackable, but disabling tracking is based on tracking within Keras.  This serves as an interface between Keras tracking and TF tracking.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 9, 'n_files_pr': 5, 'pr': 20906, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",38.6004433631897,137
"How is a cycle detected and handled in the dtype promotion lattice, and what exception is raised?","A cycle is detected and handled in the dtype promotion lattice by tracking the recursion stack during traversal. If a node is seen twice in the current path during traversal, it raises ValueError(""cycle detected in type promotion lattice for node <dtype>""). In keras, the function `dtypes._make_lattice_upper_bounds` uses depth-first search (DFS) to walk the graph and tracks the recursion stack during traversal. If a cycle is detected, it raises ValueError(""cycle detected in type promotion lattice for node <dtype>"").","Cycle detection happens in the function dtypes._make_lattice_upper_bounds (after calling dtypes._type_promotion_lattice). It walks the graph by DFS and tracks the recursion stack; if during traversal a node is seen twice in the current path, it bails out and raises

    ValueError(""cycle detected in type promotion lattice for node <dtype>"")

as tested in test_cycle_detection_in_make_lattice_upper_bounds.","['FUNCTION keras.keras.src.backend.common.dtypes.result_type has parameter or accepts argument keras.keras.src.backend.common.dtypes.dtype\nReturns the type from applying the Keras type promotion rules.  In general, each argument is first parsed by `backend.standardize_dtype`, and the resulting dtype is determined by the least upper bound of the type promotion lattice.  Note: This function attempts to match the result of `jnp.result_type`.  Args:     dtypes: Input dtypes.  Returns:     The result dtype.  Examples:  >>> x = keras.ops.ones((1,), dtype=""bfloat16"") >>> keras.backend.result_type(x.dtype, int) ""bfloat16""  >>> x = keras.ops.ones((1,), dtype=""int32"") >>> y = keras.ops.ones((1,), dtype=""float32"") >>> keras.backend.result_type(x.dtype, y.dtype) ""float32""  >>> z= keras.ops.ones((1,), dtype=\'complex64\') >>> keras.backend.result_type(z.dtype, int) ""float64""', 'CLASS keras.keras.src.backend.common.dtypes_test.DtypesTest has method keras.keras.src.backend.common.dtypes_test.DtypesTest.test_cycle_detection_in_make_lattice_upper_bounds\nTest the dtype to verify that the behavior matches JAX.', 'MODULE keras.keras.src.backend.common.dtypes contains FUNCTION keras.keras.src.backend.common.dtypes.result_type\nReturns the type from applying the Keras type promotion rules.  In general, each argument is first parsed by `backend.standardize_dtype`, and the resulting dtype is determined by the least upper bound of the type promotion lattice.  Note: This function attempts to match the result of `jnp.result_type`.  Args:     dtypes: Input dtypes.  Returns:     The result dtype.  Examples:  >>> x = keras.ops.ones((1,), dtype=""bfloat16"") >>> keras.backend.result_type(x.dtype, int) ""bfloat16""  >>> x = keras.ops.ones((1,), dtype=""int32"") >>> y = keras.ops.ones((1,), dtype=""float32"") >>> keras.backend.result_type(x.dtype, y.dtype) ""float32""  >>> z= keras.ops.ones((1,), dtype=\'complex64\') >>> keras.backend.result_type(z.dtype, int) ""float64""', 'CLASS cognee.cognee.modules.graph.exceptions.exceptions.DimensionOutOfRangeError inherits from or is a subclass of cognee.exceptions.CogneeValidationError', 'FUNCTION keras.keras.src.backend.common.dtypes.result_type has parameter or accepts argument keras.keras.src.backend.common.dtypes.FLOAT8_TYPES\nReturns the type from applying the Keras type promotion rules.  In general, each argument is first parsed by `backend.standardize_dtype`, and the resulting dtype is determined by the least upper bound of the type promotion lattice.  Note: This function attempts to match the result of `jnp.result_type`.  Args:     dtypes: Input dtypes.  Returns:     The result dtype.  Examples:  >>> x = keras.ops.ones((1,), dtype=""bfloat16"") >>> keras.backend.result_type(x.dtype, int) ""bfloat16""  >>> x = keras.ops.ones((1,), dtype=""int32"") >>> y = keras.ops.ones((1,), dtype=""float32"") >>> keras.backend.result_type(x.dtype, y.dtype) ""float32""  >>> z= keras.ops.ones((1,), dtype=\'complex64\') >>> keras.backend.result_type(z.dtype, int) ""float64""', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), John Zedlewski (@JohnZed), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Anthony D'Amato (@Totoketchup), @Wittty-Panda, Alexander Gugel (@alexanderGugel), Codecov Comments Bot (@codecov-commenter), Codecov (@codecov-io), DIVYA CHAUHAN (@divya661), Devin Robison (@drobison00), Geoffrey Blake (@geoffreyblake), Mark Harris (@harrism), Philip Hyunsu Cho (@hcho3), Honza Sterba (@honzasterba), Igor Moura (@igormp), @jakirkham, @jameskrach, James Lamb (@jameslamb), Janakarajan Natarajan (@janaknat), Jake Hemstad (@jrhemstad), Keith Kraus (@kkraus14), Kyle Nicholson (@kylejn27), Christian Lorentzen (@lorentzenchr), Michael Mayer (@mayer79), Nikolay Petrov (@napetrov), @odidev, PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Scott Lundberg (@slundberg), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vincent Nijs (@vnijs), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n## v1.2.0 (2020.08.22)\n\n### XGBoost4J-Spark now supports the GPU algorithm (#5171)\n* Now XGBoost4J-Spark is able to leverage NVIDIA GPU hardware to speed up training.\n* There is on-going work for accelerating the rest of the data pipeline with NVIDIA GPUs (#5950, #5972).\n\n### XGBoost now supports CUDA 11 (#5808)\n* It is now possible to build XGBoost with CUDA 11. Note that we do not yet distribute pre-built binaries built with CUDA 11; all current distributions use CUDA 10.0.\n\n### Better guidance for persisting XGBoost models in an R environment (#5940, #5964)\n* Users are strongly encouraged to use `xgb.save()` and `xgb.save.raw()` instead of `saveRDS()`. This is so that the persisted models can be accessed with future releases of XGBoost.\n* The previous release (1.1.0) had problems loading models that were saved with `saveRDS()`. This release adds a compatibility layer to restore access to the old RDS files. Note that this is meant to be a temporary measure; users are advised to stop using `saveRDS()` and migrate to `xgb.save()` and `xgb.save.raw()`.\n\n### New objectives and metrics\n* The pseudo-Huber loss `reg:pseudohubererror` is added (#5647). The corresponding metric is `mphe`. Right now, the slope is hard-coded to 1.\n* The Accelerated Failure Time objective for survival analysis (`survival:aft`) is now accelerated on GPUs (#5714, #5716). The survival metrics `aft-nloglik` and `interval-regression-accuracy` are also accelerated on GPUs.\n\n### Improved integration with scikit-learn\n* Added `n_features_in_` attribute to the scikit-learn interface to store the number of features used (#5780). This is useful for integrating with some scikit-learn features such as `StackingClassifier`.  See [this link](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for more details.\n* `XGBoostError` now inherits `ValueError`, which conforms scikit-learn's exception requirement (#5696)."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md\nDefining a Custom Data Iterator to Load Data from External Memory\n=================================================================\n\nA simple demo for using custom data iterator with XGBoost.  The feature is still\n**experimental** and not ready for production use.  If you are not familiar with C API,\nplease read its introduction in our tutorials and visit the basic demo first.\n\nDefining Data Iterator\n----------------------\n\nIn the example, we define a custom data iterator with 2 methods: `reset` and `next`.  The\n`next` method passes data into XGBoost and tells XGBoost whether the iterator has reached\nits end, and the `reset` method resets iterations. One important detail when using the C\nAPI for data iterator is users need to make sure that the data passed into `next` method\nmust be kept in memory until the next iteration or `reset` is called.  The external memory\nDMatrix is not limited to training, but also valid for other features like prediction.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 6, 'pr': 20927, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",36.58854794502258,138
What does the final assertion check about the remat wrapper calls?,"The final assertion checks that the wrapped function was actually called, which is done using `assert_called()` on the mock. This means that the test is verifying if the remat decorator successfully wrapped and called the original function.","The last two lines in `LayerTest.test_functional_model_with_remat` assert that:

• Exactly one function got wrapped by the `remat` decorator (i.e. `len(mock_remat.rematted_functions) == 1`)  
• And that this wrapped function was actually called (via `assert_called()` on the mock).","['MODULE keras.keras.src.layers.layer_test contains CLASS keras.keras.src.layers.layer_test.MockRemat\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has field or attribute or property keras.keras.src.layers.layer_test.MockRemat.rematted_functions\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has method keras.keras.src.layers.layer_test.MockRemat.__call__\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has field or attribute or property keras.keras.src.layers.layer_test.func\nMock remat by returning a wrapper Mock calling the original function', 'CLASS keras.keras.src.layers.layer_test.MockRemat has method keras.keras.src.layers.layer_test.MockRemat.__init__\nMock remat by returning a wrapper Mock calling the original function', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 20935, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",27.321969509124756,139
What two image statistics are asserted equal between the reference and backend outputs in the elastic transform test?,The two image statistics that are asserted equal between the reference and backend outputs in the elastic transform test in keras are the mean and variance.,In `ImageOpsCorrectnessTest.test_elastic_transform` (keras/src/ops/image_test.py) they assert that the transformed outputs have the same mean and the same variance (via `np.mean` and `np.var`) between the NumPy reference (`elastic_transform_np`) and the backend (`kimage.elastic_transform`).,"['CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.value_range\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', 'FUNCTION keras.keras.src.ops.image_test.elastic_transform_np has parameter or accepts argument keras.keras.src.ops.image_test.batch_size', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has field or attribute or property keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration.seed\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', 'FUNCTION keras.keras.src.backend.numpy.image.elastic_transform has parameter or accepts argument keras.keras.src.backend.numpy.image.x', 'CLASS keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration has method keras.keras.src.layers.preprocessing.image_preprocessing.random_color_degeneration.RandomColorDegeneration.__init__\nRandomly performs the color degeneration operation on given images.  The sharpness operation first converts an image to gray scale, then back to color. It then takes a weighted average between original image and the degenerated image. This makes colors appear more dull.  Args:     factor: A tuple of two floats or a single float.         `factor` controls the extent to which the         image sharpness is impacted. `factor=0.0` makes this layer perform a         no-op operation, while a value of 1.0 uses the degenerated result         entirely. Values between 0 and 1 result in linear interpolation         between the original image and the sharpened image.         Values should be between `0.0` and `1.0`. If a tuple is used, a         `factor` is sampled between the two values for every image         augmented. If a single float is used, a value between `0.0` and the         passed float is sampled. In order to ensure the value is always the         same, please pass a tuple with two identical floats: `(0.5, 0.5)`.     seed: Integer. Used to create a random seed.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 21007, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",27.016822814941406,140
How is a user-specified NumPy dtype translated into an OpenVINO element type for the fill operation?,"A user-specified NumPy dtype is translated into an OpenVINO element type for the fill operation in Keras as follows: when a `full_like` function is called, it standardizes the passed NumPy dtype using the `standardize_dtype` function. The standardized dtype is then looked up in the `OPENVINO_DTYPES` dictionary defined in `keras/src/backend/openvino/dtypes.py`, which maps NumPy dtypes to OpenVINO element types (ov_type). The resulting ov_type is used when building the `ov_opset.constant` for the fill operation.","When you pass a NumPy `dtype` into `full_like`, it does:

1. Calls standardize_dtype(dtype) (to normalize whatever NumPy gave you).
2. Looks up the result in OPENVINO_DTYPES (a dict in keras/src/backend/openvino/dtypes.py).
3. Uses that OpenVINO element type (`ov_type`) when building the `ov_opset.constant`.

So the user dtype → `standardize_dtype` → `OPENVINO_DTYPES[...]` → OpenVINO element type.","['CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.value\nThe current value of the variable (numpy array or backend tensor).\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'FUNCTION keras.keras.src.backend.openvino.numpy.full_like has parameter or accepts argument keras.keras.src.backend.openvino.numpy.full_like.dtype', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.dtype\nThe data type of the variable.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'FUNCTION keras.keras.src.backend.openvino.numpy.full_like has parameter or accepts argument keras.keras.src.backend.openvino.numpy.full_like.x', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.path\nThe path of the variable within the Keras model or layer.\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 21008, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",49.478752851486206,141
Which internal routines in the fit and evaluate workflows handle capturing and clearing the JAX training state sharding specifications?,"The internal routines that capture and clear JAX training state sharding specifications in the fit and evaluate workflows of Keras are `_record_training_state_sharding_spec()` for capturing and `_clear_jax_state_sharding()` for clearing, both located in `keras/src/backend/jax/trainer.py`. These routines handle sharding specs in both `JAXTrainer.fit` and `JAXTrainer.evaluate` methods.","In both `JAXTrainer.fit` and `JAXTrainer.evaluate` (in `keras/src/backend/jax/trainer.py`), the sharding specs are

• Captured by `self._record_training_state_sharding_spec()`  
• Cleared by `self._clear_jax_state_sharding()`","['CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.save_config\nOutput internal parameter configuration of Booster as a JSON string.  .. versionadded:: 1.0.0\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', 'MODULE keras.guides.distributed_training_with_jax contains GLOBAL_VARIABLE keras.guides.distributed_training_with_jax.data_sharding', 'CLASS keras.keras.src.random.seed_generator.SeedGenerator has field or attribute or property keras.keras.src.random.seed_generator.SeedGenerator.state\nGenerates variable seeds upon each call to a function generating random numbers.  In Keras, all random number generators (such as `keras.random.normal()`) are stateless, meaning that if you pass an integer seed to them (such as `seed=42`), they will return the same values for repeated calls. To get different values for each call, a `SeedGenerator` providing the state of the random generator has to be used.  Note that all the random number generators have a default seed of None, which implies that an internal global SeedGenerator is used. If you need to decouple the RNG from the global state you can provide a local `StateGenerator` with either a deterministic or random initial state.  Remark concerning the JAX backen: Note that the use of a local `StateGenerator` as seed argument is required for JIT compilation of RNG with the JAX backend, because the use of global state is not supported.  Example:  ```python seed_gen = keras.random.SeedGenerator(seed=42) values = keras.random.normal(shape=(2, 3), seed=seed_gen) new_values = keras.random.normal(shape=(2, 3), seed=seed_gen) ```  Usage in a layer:  ```python class Dropout(keras.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.seed_generator = keras.random.SeedGenerator(1337)      def call(self, x, training=False):         if training:             return keras.random.dropout(                 x, rate=0.5, seed=self.seed_generator             )         return x ```', 'MODULE keras.guides.distributed_training_with_jax contains GLOBAL_VARIABLE keras.guides.distributed_training_with_jax.sharded_x', 'CLASS keras.keras.src.random.seed_generator.SeedGenerator has method keras.keras.src.random.seed_generator.SeedGenerator.__init__\nGenerates variable seeds upon each call to a function generating random numbers.  In Keras, all random number generators (such as `keras.random.normal()`) are stateless, meaning that if you pass an integer seed to them (such as `seed=42`), they will return the same values for repeated calls. To get different values for each call, a `SeedGenerator` providing the state of the random generator has to be used.  Note that all the random number generators have a default seed of None, which implies that an internal global SeedGenerator is used. If you need to decouple the RNG from the global state you can provide a local `StateGenerator` with either a deterministic or random initial state.  Remark concerning the JAX backen: Note that the use of a local `StateGenerator` as seed argument is required for JIT compilation of RNG with the JAX backend, because the use of global state is not supported.  Example:  ```python seed_gen = keras.random.SeedGenerator(seed=42) values = keras.random.normal(shape=(2, 3), seed=seed_gen) new_values = keras.random.normal(shape=(2, 3), seed=seed_gen) ```  Usage in a layer:  ```python class Dropout(keras.Layer):     def __init__(self, **kwargs):         super().__init__(**kwargs)         self.seed_generator = keras.random.SeedGenerator(1337)      def call(self, x, training=False):         if training:             return keras.random.dropout(                 x, rate=0.5, seed=self.seed_generator             )         return x ```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 21019, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",31.74854016304016,142
Which class does the saving logic instantiate to manage sharded weight storage when max_shard_size is set?,"When `max_shard_size` is set, the `saving_lib.save_weights_only` function instantiates the `ShardedH5IOStore` class to manage sharded weight storage in Keras.","When `max_shard_size` is set, `saving_lib.save_weights_only` (in `keras/src/saving/saving_lib.py`) instantiates the `ShardedH5IOStore` class to manage the sharded weight files.","['METHOD keras.keras.src.models.model.Model.save_weights has parameter or accepts argument keras.keras.src.models.model.Model.save_weights.max_shard_size\nSaves all weights to a single file or sharded files.  By default, the weights will be saved in a single `.weights.h5` file. If sharding is enabled (`max_shard_size` is not `None`), the weights will be saved in multiple files, each with a size at most `max_shard_size` (in GB). Additionally, a configuration file `.weights.json` will contain the metadata for the sharded files.  The saved sharded files contain:  - `*.weights.json`: The configuration file containing \'metadata\' and     \'weight_map\'. - `*_xxxxxx.weights.h5`: The sharded files containing only the     weights.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`. If `.weights.h5` is provided, it will be         overridden.     overwrite: Whether to overwrite any existing weights at the target         location or instead ask the user via an interactive prompt.     max_shard_size: `int` or `float`. Maximum size in GB for each         sharded file. If `None`, no sharding will be done. Defaults to         `None`.  Example:  ```python # Instantiate a EfficientNetV2L model with about 454MB of weights. model = keras.applications.EfficientNetV2L(weights=None)  # Save the weights in a single file. model.save_weights(""model.weights.h5"")  # Save the weights in sharded files. Use `max_shard_size=0.25` means # each sharded file will be at most ~250MB. model.save_weights(""model.weights.json"", max_shard_size=0.25)  # Load the weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.h5"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x))  # Load the sharded weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.json"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x)) ```', 'FUNCTION keras.keras.src.saving.saving_api.save_weights has parameter or accepts argument keras.keras.src.saving.saving_api.save_weights.max_shard_size', 'METHOD keras.keras.src.models.model.Model.save_weights has parameter or accepts argument keras.keras.src.models.model.Model.save_weights.filepath\nSaves all weights to a single file or sharded files.  By default, the weights will be saved in a single `.weights.h5` file. If sharding is enabled (`max_shard_size` is not `None`), the weights will be saved in multiple files, each with a size at most `max_shard_size` (in GB). Additionally, a configuration file `.weights.json` will contain the metadata for the sharded files.  The saved sharded files contain:  - `*.weights.json`: The configuration file containing \'metadata\' and     \'weight_map\'. - `*_xxxxxx.weights.h5`: The sharded files containing only the     weights.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`. If `.weights.h5` is provided, it will be         overridden.     overwrite: Whether to overwrite any existing weights at the target         location or instead ask the user via an interactive prompt.     max_shard_size: `int` or `float`. Maximum size in GB for each         sharded file. If `None`, no sharding will be done. Defaults to         `None`.  Example:  ```python # Instantiate a EfficientNetV2L model with about 454MB of weights. model = keras.applications.EfficientNetV2L(weights=None)  # Save the weights in a single file. model.save_weights(""model.weights.h5"")  # Save the weights in sharded files. Use `max_shard_size=0.25` means # each sharded file will be at most ~250MB. model.save_weights(""model.weights.json"", max_shard_size=0.25)  # Load the weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.h5"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x))  # Load the sharded weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.json"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x)) ```', 'METHOD keras.keras.src.saving.saving_lib.ShardedH5IOStore.__init__ has parameter or accepts argument keras.keras.src.saving.saving_lib.ShardedH5IOStore.__init__.max_shard_size', 'METHOD keras.keras.src.models.model.Model.save_weights has parameter or accepts argument keras.keras.src.models.model.Model.save_weights.overwrite\nSaves all weights to a single file or sharded files.  By default, the weights will be saved in a single `.weights.h5` file. If sharding is enabled (`max_shard_size` is not `None`), the weights will be saved in multiple files, each with a size at most `max_shard_size` (in GB). Additionally, a configuration file `.weights.json` will contain the metadata for the sharded files.  The saved sharded files contain:  - `*.weights.json`: The configuration file containing \'metadata\' and     \'weight_map\'. - `*_xxxxxx.weights.h5`: The sharded files containing only the     weights.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`. If `.weights.h5` is provided, it will be         overridden.     overwrite: Whether to overwrite any existing weights at the target         location or instead ask the user via an interactive prompt.     max_shard_size: `int` or `float`. Maximum size in GB for each         sharded file. If `None`, no sharding will be done. Defaults to         `None`.  Example:  ```python # Instantiate a EfficientNetV2L model with about 454MB of weights. model = keras.applications.EfficientNetV2L(weights=None)  # Save the weights in a single file. model.save_weights(""model.weights.h5"")  # Save the weights in sharded files. Use `max_shard_size=0.25` means # each sharded file will be at most ~250MB. model.save_weights(""model.weights.json"", max_shard_size=0.25)  # Load the weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.h5"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x))  # Load the sharded weights in a new model with the same architecture. loaded_model = keras.applications.EfficientNetV2L(weights=None) loaded_model.load_weights(""model.weights.json"") x = keras.random.uniform((1, 480, 480, 3)) assert np.allclose(model.predict(x), loaded_model.predict(x)) ```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n### Option 2: Set up a local environment\n\nTo set up your local dev environment, you will need the following tools.\n\n1.  [git](https://github.com/) for code repository management.\n2.  [python](https://www.python.org/) to build and code in Keras.\n\nThe following commands check the tools above are successfully installed. Note\nthat Keras requires at least Python 3.10 to run.\n\n```shell\ngit --version\npython --version\n```\n\nClone your forked repo to your local machine. Go to the cloned directory to\ninstall the dependencies.\n\n```shell\ngit clone https://github.com/YOUR_GITHUB_USERNAME/keras.git\ncd keras\npip install -r requirements.txt\n```\n\nYou then need to configure the backend to use, see the\n[Configuring your backend](https://github.com/keras-team/keras/blob/master/README.md#configuring-your-backend)\nsection of the README.\n\nYou can also add GPU support to your environment, see the\n[Adding GPU support](https://github.com/keras-team/keras/blob/master/README.md#adding-gpu-support)\nsection of the README.\n\n## Generating public API and formatting the code\n\nFor the first time you are setting up the repo, please run `pre-commit install`.\nNote that this needs to be done only once at the beginning.\n\nNow, whenever you run `git commit -m ""<message>""`, three things are\nautomatically done:\n\n- Public API generation\n- Code formatting\n- Code linting\n\nIf there\'s any error, the commit will not go through. Please fix the error (\nmost of the times, the error is fixed automatically by the formatter/linter) and\nre-run the following:\n\n```\ngit add .\ngit commit -m ""<message>"" # This will not get logged as a duplicate commit.\n```\n\nIn case you want to run the above manually on all files, you can do the\nfollowing:\n\n```\npre-commit run --all-files\n```\n\nKerasHub uses [Ruff](https://docs.astral.sh/ruff/) to format the code.\n\n### Docstrings\n\nWe do not have an automated way to check docstring style, so if you write\nor edit any docstring, please make sure to check them manually.\nKeras docstrings follow the conventions below:\n\nA **class docstring** may contain the following items:\n\n* A one-line description of the class.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for arguments in `__init__()`.\n* If it\'s a layer:\n    * `Call arguments` section for arguments in `Layer.call()`.\n    * `Returns` section for the return values of `Layer.call()`.\n    * Optional `Raises` section for possible errors.\n\nYou can check out `MultiHeadAttention` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/layers/attention/multi_head_attention.py#L20).\n\nA **function docstring** may contain the following items:\n\n* One-line description of the function.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for the function arguments.\n* `Returns` section for the return values.\n* Optional `Raises` section for possible errors.\n\nYou can check out `text_dataset_from_directory` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/utils/text_dataset_utils.py#L27).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 23}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 23, 'n_files_pr': 5, 'pr': 21022, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",42.06050777435303,143
"When no axis is specified, how is the tensor reshaped before computing the index?","When no axis is specified, the tensor is reshaped by invoking `ov_opset.reshape(x, flatten_shape, False)` to reshape x. After this reshape, x has shape [-1, 1, 1, ..., 1]. This reshape operation flattens x along axis 0.","In both argmax and argmin (keras/src/backend/openvino/numpy.py), when axis=None we do:

• flatten_shape = constant([-1] + [1]*(rank−1))  
• x = ov_opset.reshape(x, flatten_shape, False)  

i.e. x is reshaped to [-1, 1, 1, …, 1], flattening it along axis 0 before calling topk.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.count_nonzero\nCounts the number of non-zero values in `x` along the given `axis`.  If no axis is specified then all non-zeros in the tensor are counted.  Args:     x: Input tensor.     axis: Axis or tuple of axes along which to count the number of         non-zeros. Defaults to `None`.  Returns:     int or tensor of ints.  Examples: >>> x = keras.ops.array([[0, 1, 7, 0], [3, 0, 2, 19]]) >>> keras.ops.count_nonzero(x) 5 >>> keras.ops.count_nonzero(x, axis=0) array([1, 1, 2, 1], dtype=int64) >>> keras.ops.count_nonzero(x, axis=1) array([2, 3], dtype=int64)', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.flip\nReverse the order of elements in the tensor along the given axis.  The shape of the tensor is preserved, but the elements are reordered.  Args:     x: Input tensor.     axis: Axis or axes along which to flip the tensor. The default,         `axis=None`, will flip over all of the axes of the input tensor.  Returns:     Output tensor with entries of `axis` reversed.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.var\nCompute the variance along the specified axes.  Args:     x: Input tensor.     axis: Axis or axes along which the variance is computed. The default         is to compute the variance of the flattened tensor.     keepdims: If this is set to `True`, the axes which are reduced are left         in the result as dimensions with size one.  Returns:     Output tensor containing the variance.', 'FUNCTION keras.keras.src.ops.numpy.flip has parameter or accepts argument keras.keras.src.ops.numpy.flip.axis\nReverse the order of elements in the tensor along the given axis.  The shape of the tensor is preserved, but the elements are reordered.  Args:     x: Input tensor.     axis: Axis or axes along which to flip the tensor. The default,         `axis=None`, will flip over all of the axes of the input tensor.  Returns:     Output tensor with entries of `axis` reversed.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.std\nCompute the standard deviation along the specified axis.  Args:     x: Input tensor.     axis: Axis along which to compute standard deviation.         Default is to compute the standard deviation of the         flattened tensor.     keepdims: If this is set to `True`, the axes which are reduced are left         in the result as dimensions with size one.  Returns:     Output tensor containing the standard deviation values.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 21060, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",29.65827226638794,144
How is the axis argument supplied to the loss wrapper ultimately used in the internal Dice loss calculation?,"The axis argument supplied to the loss wrapper is ultimately used in the internal Dice loss calculation by controlling which dimensions are reduced when computing the intersection and union for the Dice score. This is achieved through the use of `ops.sum` functions with the specified axis, as seen in the code snippet provided. The axis is passed from the loss wrapper to the underlying dice function via the LossFunctionWrapper, where it is used to reduce the dimensions of the inputs (`y_true`, `y_pred`) and targets before computing the Dice score.","The `axis` you pass to `keras.losses.Dice` is stored on the wrapper and then forwarded verbatim into the underlying `dice(y_true, y_pred, axis=…)` function (via `LossFunctionWrapper`). Inside `dice()` it’s used in each call to

• `ops.sum(inputs * targets, axis=axis)`  
• `ops.sum(y_true, axis=axis)`  
• `ops.sum(y_pred, axis=axis)`

i.e. it controls which dimensions are reduced when computing the intersection and union for the Dice score.","['CLASS keras.keras.src.losses.losses.Dice has method keras.keras.src.losses.losses.Dice.get_config\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', 'FUNCTION keras.keras.src.losses.losses.dice has parameter or accepts argument keras.keras.src.losses.losses.dice.axis\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     y_true: tensor of true targets.     y_pred: tensor of predicted targets.     axis: tuple for which dimensions the loss is calculated  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.dice(y_true, y_pred, axis=axis) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.dice(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)', 'CLASS keras.keras.src.losses.losses.Dice has field or attribute or property keras.keras.src.losses.losses.axis\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', 'MODULE keras.keras.src.losses.losses contains CLASS keras.keras.src.losses.losses.Dice\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', 'CLASS keras.keras.src.losses.losses.Dice has method keras.keras.src.losses.losses.Dice.__init__\nComputes the Dice loss value between `y_true` and `y_pred`.  Formula: ```python loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred)) ```  Args:     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     axis: Tuple for which dimensions the loss is calculated. Defaults to         `None`.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Returns:     Dice loss value.  Example:  >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]], ...           [[[1.0], [1.0]], [[0.0], [0.0]]]] >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]], ...           [[[0.4], [0.0]], [[0.0], [0.9]]]] >>> axis = (1, 2, 3) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)  >>> loss = keras.losses.Dice()(y_true, y_pred) >>> assert loss.shape == () >>> loss array(0.6164384, shape=(), dtype=float32)  >>> y_true = np.array(y_true) >>> y_pred = np.array(y_pred) >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred) >>> assert loss.shape == (2,) >>> loss array([0.5, 0.75757575], shape=(2,), dtype=float32)', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 21064, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",38.29504036903381,145
How does the transformation calculation handle cases where the sampled saturation factor reaches its maximum to avoid division by zero?,"The transformation calculation in the `RandomSaturation.get_random_transformation` method in Keras handles cases where the sampled saturation factor reaches its maximum to avoid division by zero by adding a small value, `epsilon()`, to the denominator of the normalization formula. This ensures that the denominator never becomes zero, even if the factor hits its maximum.

The specific line of code is:

```python
factor = factor / (1 – factor + epsilon())
```
This line normalizes the sampled saturation factor by dividing it by a value that is close to 1 but not exactly 1. The `epsilon()` function adds a small positive value to the denominator, preventing division by zero.

In summary, Keras uses a normalization formula with a small added value to prevent division by zero when the sampled saturation factor reaches its maximum.","In RandomSaturation.get_random_transformation (keras/src/layers/preprocessing/image_preprocessing/random_saturation.py) the sampled factor is normalized as

 factor = factor / (1 – factor + epsilon())

The addition of backend.epsilon() in the denominator guarantees it never becomes zero, even if factor hits its max.","['CLASS keras.keras.src.layers.normalization.rms_normalization.RMSNormalization has method keras.keras.src.layers.normalization.rms_normalization.RMSNormalization.build\nRoot Mean Square (RMS) Normalization layer.  This layer normalizes the input tensor based on its RMS value.  The Keras layer performs the operation as described in [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467) by Biao Zhang et al.   If `scale` is enabled, the layer will scale the normalized outputs via a learnable scaling factor.  So, with scaling enabled, the normalization equations are as follows:  Let the intermediate activations for a mini-batch to be the `inputs`.  ```python rms_normalization(x) = x * rsqrt(mean(square(x))) * scale ```  For example:  >>> layer = keras.layers.RMSNormalization() >>> layer.build([5, 20, 30, 10]) >>> print(layer.scale.shape) (10,) >>> layer(np.random.rand(1, 10)).numpy() array([[0.35098287, 1.0495652 , 1.4645109 , 1.2944688 , 0.31124955,         1.2768592 , 1.184331  , 0.17474432, 0.49955517, 1.2428929 ]],     dtype=float32)  Args:     axis: int. The axis on which to perform the normalization.     epsilon: float. A small number to add to avoid division by zero.', 'CLASS keras.keras.src.ops.core.SaturateCast inherits from or is a subclass of keras.src.ops.operation.Operation', 'CLASS keras.keras.src.layers.normalization.rms_normalization.RMSNormalization has field or attribute or property keras.keras.src.layers.normalization.rms_normalization.axis\nRoot Mean Square (RMS) Normalization layer.  This layer normalizes the input tensor based on its RMS value.  The Keras layer performs the operation as described in [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467) by Biao Zhang et al.   If `scale` is enabled, the layer will scale the normalized outputs via a learnable scaling factor.  So, with scaling enabled, the normalization equations are as follows:  Let the intermediate activations for a mini-batch to be the `inputs`.  ```python rms_normalization(x) = x * rsqrt(mean(square(x))) * scale ```  For example:  >>> layer = keras.layers.RMSNormalization() >>> layer.build([5, 20, 30, 10]) >>> print(layer.scale.shape) (10,) >>> layer(np.random.rand(1, 10)).numpy() array([[0.35098287, 1.0495652 , 1.4645109 , 1.2944688 , 0.31124955,         1.2768592 , 1.184331  , 0.17474432, 0.49955517, 1.2428929 ]],     dtype=float32)  Args:     axis: int. The axis on which to perform the normalization.     epsilon: float. A small number to add to avoid division by zero.', 'CLASS keras.keras.src.ops.numpy.Maximum inherits from or is a subclass of keras.src.ops.operation.Operation', 'CLASS keras.keras.src.layers.normalization.rms_normalization.RMSNormalization has field or attribute or property keras.keras.src.layers.normalization.rms_normalization.RMSNormalization.axis\nRoot Mean Square (RMS) Normalization layer.  This layer normalizes the input tensor based on its RMS value.  The Keras layer performs the operation as described in [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467) by Biao Zhang et al.   If `scale` is enabled, the layer will scale the normalized outputs via a learnable scaling factor.  So, with scaling enabled, the normalization equations are as follows:  Let the intermediate activations for a mini-batch to be the `inputs`.  ```python rms_normalization(x) = x * rsqrt(mean(square(x))) * scale ```  For example:  >>> layer = keras.layers.RMSNormalization() >>> layer.build([5, 20, 30, 10]) >>> print(layer.scale.shape) (10,) >>> layer(np.random.rand(1, 10)).numpy() array([[0.35098287, 1.0495652 , 1.4645109 , 1.2944688 , 0.31124955,         1.2768592 , 1.184331  , 0.17474432, 0.49955517, 1.2428929 ]],     dtype=float32)  Args:     axis: int. The axis on which to perform the normalization.     epsilon: float. A small number to add to avoid division by zero.', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 21066, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",38.5704607963562,146
"In the float-input branch, how is the sign bit isolated from the floating-point representation?","The sign bit is isolated from the floating-point representation by first casting `x` to `float32`, then bit-casting it to `int32`. The code sets a mask to `tf.constant(tf.int32.min)`, which has the hexadecimal value `0x80000000`. It then computes `bits` by AND-ing the result of the bitcast with the mask, effectively isolating the sign bit. Finally, it checks whether the masked bits are less than zero using `tf.less(bits, 0)`, which yields the sign of the original float.","In the float branch (in keras/src/backend/tensorflow/numpy.py signbit), x is first cast to float32, then bit-cast to an int32. The code does:

• mask = tf.constant(tf.int32.min)  # 0x80000000  
• bits = tf.bitcast(x, tf.int32) & mask  
• return tf.less(bits, 0)

By AND-ing the int32 view with INT32_MIN (0x80000000), only the sign bit remains, and checking <0 yields the sign.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.bitwise_not\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.signbit\nReturn the sign bit of the elements of `x`.  The output boolean tensor contains `True` where the sign of `x` is negative, and `False` otherwise.  Args:     x: Input tensor.  Returns:     Output boolean tensor of same shape as `x`.', 'FUNCTION keras.keras.src.ops.numpy.bitwise_not has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_not.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.bitwise_invert\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', 'FUNCTION keras.keras.src.ops.numpy.bitwise_invert has parameter or accepts argument keras.keras.src.ops.numpy.bitwise_invert.x\nCompute bit-wise inversion, or bit-wise NOT, element-wise.  Computes the bit-wise NOT of the underlying binary representation of the integers in the input arrays. This ufunc implements the C/Python operator `~`.  Args:     x: Input integer tensor.  Returns:     Result tensor.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 21077, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",51.58502197265625,147
How does the model workflow transition from the self-supervised pretraining with the auxiliary decoder to the supervised fine-tuning with the final linear layer?,"The model workflow transitions from self-supervised pretraining with the auxiliary decoder to supervised fine-tuning with the final linear layer as follows:

1.  The `TabnetModel` is initialized and a self-supervised pretraining process is performed using the `pretrain_fn()` method, which trains the `self.tabnet_model` and `self.tabnet_decoder` using a reconstruction loss.
2.  After pretraining, the pretrained weights are loaded into the `self.tabnet_model`.
3.  The `TabnetModel` is then wrapped in a `FinetuneModel`, which adds a new final linear layer on top of the frozen encoder (`self.tabnet_model`).
4.  During subsequent supervised epochs (train_epoch and test_epoch), the combined model is optimized with `self.train_optimizer`. This process allows the model to leverage the knowledge learned during self-supervised pretraining and fine-tune it for specific tasks using a final linear layer.","In qlib/contrib/model/pytorch_tabnet.py the switch happens in TabnetModel.fit():

1. If pretrain=True, fit() calls pretrain_fn(), which trains  
   – self.tabnet_model + self.tabnet_decoder  
   on a self-supervised reconstruction loss and saves only self.tabnet_model.state_dict().

2. Back in fit(), it reloads those pretrained weights into self.tabnet_model, then wraps it in a FinetuneModel:

   self.tabnet_model = FinetuneModel(self.out_dim, self.final_out_dim, self.tabnet_model)

   This adds a new final linear layer on top of the frozen encoder.

3. All subsequent supervised epochs (train_epoch/test_epoch) now optimize this combined model (the pretrained encoder + new head) with self.train_optimizer.","['CLASS qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel has method qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'METHOD qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn.S\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'METHOD qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn.f_hat\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'METHOD qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_tabnet.TabnetModel.pretrain_loss_fn.f\nPretrain loss function defined in the original paper, read ""Tabular self-supervised learning"" in https://arxiv.org/pdf/1908.07442.pdf', 'MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TabNet/README.md\n# TabNet\n* Code: [https://github.com/dreamquark-ai/tabnet](https://github.com/dreamquark-ai/tabnet)\n* Paper: [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/pdf/1908.07442.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 205, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.83016300201416,634
How is the new minute-frequency adjustment integrated into the existing Yahoo data normalization flow?,"The new minute-frequency adjustment is integrated into the existing Yahoo data normalization flow through the `adjusted_price` method within the `YahooNormalize1min` class in the `collector.py` file. This method calls the `calc_adjusted_price` function, passing in various parameters such as the input dataframe `df`, date field name `_date_field_name`, symbol field name `_symbol_field_name`, frequency `","The minute‐frequency adjustment is implemented in a new method right alongside the daily normalizer. In  scripts/data_collector/yahoo/collector.py you’ll find:

• Class: YahooNormalize1min  
• Method: adjusted_price(self, df)  
  – Calls calc_adjusted_price(  
     df=df,  
     _date_field_name=self._date_field_name,  
     _symbol_field_name=self._symbol_field_name,  
     frequence=""1min"",  
     consistent_1d=self.CONSISTENT_1d,  
     calc_paused=self.CALC_PAUSED_NUM,  
     _1d_data_all=self.all_1d_data  
   )

By passing the already-normalized 1d series (all_1d_data) plus the 1min flags into calc_adjusted_price, this hooks the minute‐level adjustment seamlessly into the existing Yahoo normalization flow.","['CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.scripts.data_collector.yahoo.collector.YahooNormalize1min inherits from or is a subclass of abc.ABC\nNormalised to 1min using local 1d data', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.symbol_field_name\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.scripts.data_collector.yahoo.collector.YahooNormalize1min inherits from or is a subclass of qlib.scripts.data_collector.yahoo.collector.YahooNormalize\nNormalised to 1min using local 1d data', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.old_qlib_data_dir\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Automatic update of daily frequency data(from yahoo finance)\n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --end_date <end date>\n      ```\n      * `end_date`: end of trading day(not included)\n      * `check_data_length`: check the number of rows per *symbol*, by default `None`\n        > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n\n  * `scripts/data_collector/yahoo/collector.py update_data_to_bin` parameters:\n      * `source_dir`: The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source""\n      * `normalize_dir`: Directory for normalize data, default ""Path(__file__).parent/normalize""\n      * `qlib_data_1d_dir`: the qlib data to be updated for yahoo, usually from: [download qlib data](https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)\n      * `end_date`: end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end)\n      * `region`: region, value from [""CN"", ""US""], default ""CN""\n      * `interval`: interval, default ""1d""(Currently only supports 1d data)\n      * `exists_skip`: exists skip, by default False\n\n## Using qlib data\n\n  ```python\n  import qlib\n  from qlib.data import D\n\n  # 1d data cn\n  # freq=day, freq default day\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data"", region=""cn"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data cn\n  # freq=1min\n  qlib.init(provider_uri=""~/.qlib/qlib_data/cn_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n\n  # 1d data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data"", region=""us"")\n  df = D.features(D.instruments(""all""), [""$close""], freq=""day"")\n\n  # 1min data us\n  qlib.init(provider_uri=""~/.qlib/qlib_data/us_data_1min"", region=""cn"")\n  inst = D.list_instruments(D.instruments(""all""), freq=""1min"", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [""$close""], freq=""1min"")\n  # get all symbol data\n  # df = D.features(D.instruments(""all""), [""$close""], freq=""1min"")\n  ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- [Collector Data](#collector-data)\n  - [Get Qlib data](#get-qlib-databin-file)\n  - [Collector *YahooFinance* data to qlib](#collector-yahoofinance-data-to-qlib)\n  - [Automatic update of daily frequency data](#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n- [Using qlib data](#using-qlib-data)\n\n\n# Collect Data From Yahoo Finance\n\n> *Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n**NOTE**:  Yahoo! Finance has blocked the access from China. Please change your network if you want to use the Yahoo data crawler.\n\n>  **Examples of abnormal data**\n\n- [SH000661](https://finance.yahoo.com/quote/000661.SZ/history?period1=1558310400&period2=1590796800&interval=1d&filter=history&frequency=1d)\n- [SZ300144](https://finance.yahoo.com/quote/300144.SZ/history?period1=1557446400&period2=1589932800&interval=1d&filter=history&frequency=1d)\n\nWe have considered **STOCK PRICE ADJUSTMENT**, but some price series seem still very abnormal.\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n### Get Qlib data(`bin file`)\n  > `qlib-data` from *YahooFinance*, is the data that has been dumped and can be used directly in `qlib`.\n  > This ready-made qlib-data is not updated regularly. If users want the latest data, please follow [these steps](#collector-yahoofinance-data-to-qlib) download the latest data. \n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data*\n    - `version`: dataset version, value from [`v1`, `v2`], by default `v1`\n      - `v2` end date is *2021-06*, `v1` end date is *2020-09*\n      - If users want to incrementally update data, they need to use yahoo collector to [collect data from scratch](#collector-yahoofinance-data-to-qlib).\n      - **the [benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks) for qlib use `v1`**, *due to the unstable access to historical data by YahooFinance, there are some differences between `v2` and `v1`*\n    - `interval`: `1d` or `1min`, by default `1d`\n    - `region`: `cn` or `us` or `in`, by default `cn`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # cn 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n    # cn 1min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n    # us 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us --interval 1d\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 221, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.82656145095825,635
What sequence of internal calls handles loading a point-in-time feature from the abstract expression down to the local PIT provider?,"The sequence of internal calls that handle loading a point-in-time feature from the abstract expression down to the local PIT provider in qlib is as follows:

1. `Expression.load` (located in `qlib/data/base.py`)
2. `PFeature._load_internal` (located in `qlib/data/base.py`)
3. `PITD.period_feature` (a wrapper located in `qlib/data/data.py`)
4. `LocalPITProvider.period_feature` (located in `qlib/data/data.py`)
5. `read_period_data` and NumPy I/O (assembled a pandas Series)

This sequence of calls is responsible for loading point-in-time features from the abstract expression down to the local PIT provider, handling caching data, error handling, processing and calculating data based on the specific expression, and querying specific periods.","When you call load on a PIT feature, the calls go roughly:

1. Expression.load (qlib/data/base.py)  
2. → PFeature._load_internal (qlib/data/base.py)  
3. → PITD.period_feature (wrapper in qlib/data/data.py)  
4. → LocalPITProvider.period_feature (qlib/data/data.py)  
5. → read_period_data + NumPy I/O → assemble pd.Series  

So the chain is:  
Expression.load → PFeature._load_internal → PITD.period_feature → LocalPITProvider.period_feature.","['CLASS qlib.qlib.data.base.Expression has method qlib.qlib.data.base.Expression.load\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).', 'CLASS qlib.qlib.data.base.Feature has method qlib.qlib.data.base.Feature.__init__\nStatic Expression  This kind of feature will load data from provider', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.end_index\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'CLASS qlib.qlib.data.base.Feature inherits from or is a subclass of qlib.qlib.data.base.Expression\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).\nStatic Expression  This kind of feature will load data from provider', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.start_index\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 39}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 39, 'n_files_pr': 19, 'pr': 343, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.03789710998535,636
"In the fit method, how are the train and valid metric histories extracted from the recorded evaluation results once training completes?",The train and valid metric histories are extracted from the recorded evaluation results once training completes by assigning `list(evals_result[,"At the end of HFLGBModel.fit (qlib/contrib/model/highfreq_gdbt_model.py), the code simply pulls out the first metric’s history from the nested evals_result dict:

• evals_result[""train""] = list(evals_result[""train""].values())[0]  
• evals_result[""valid""] = list(evals_result[""valid""].values())[0]  

This takes the single metric’s list of scores (one per boosting round) for both train and valid sets.","[""FUNCTION xgboost.python-package.xgboost.training.train has parameter or accepts argument xgboost.python-package.xgboost.training.train.num_boost_round\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", 'CLASS qlib.qlib.rl.trainer.callbacks.MetricsWriter has method qlib.qlib.rl.trainer.callbacks.MetricsWriter.on_validate_end\nDump training metrics to file.', ""MODULE xgboost.python-package.xgboost.training contains FUNCTION xgboost.python-package.xgboost.training.train\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", 'CLASS qlib.qlib.rl.trainer.callbacks.MetricsWriter has method qlib.qlib.rl.trainer.callbacks.MetricsWriter.on_train_end\nDump training metrics to file.', ""FUNCTION xgboost.python-package.xgboost.training.train has parameter or accepts argument xgboost.python-package.xgboost.training.train.params\nTrain a booster with given parameters.  Parameters ---------- params :     Booster params. dtrain :     Data to be trained. num_boost_round :     Number of boosting iterations. evals :     List of validation sets for which metrics will evaluated during training.     Validation metrics will help us track the performance of the model. obj     Custom objective function.  See :doc:`Custom Objective     </tutorials/custom_metric_obj>` for details. maximize :     Whether to maximize custom_metric.  early_stopping_rounds :      Activates early stopping. Validation metric needs to improve at least once in     every **early_stopping_rounds** round(s) to continue training.      Requires at least one item in **evals**.      The method returns the model from the last iteration (not the best one).  Use     custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model     slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's     more than one item in **evals**, the last entry will be used for early stopping.      If there's more than one metric in the **eval_metric** parameter given in     **params**, the last metric will be used for early stopping.      If early stopping occurs, the model will have two additional fields:     ``bst.best_score``, ``bst.best_iteration``.  evals_result :     This dictionary stores the evaluation results of all the items in watchlist.      Example: with a watchlist containing     ``[(dtest,'eval'), (dtrain,'train')]`` and     a parameter containing ``('eval_metric': 'logloss')``,     the **evals_result** returns      .. code-block:: python          {'train': {'logloss': ['0.48253', '0.35953']},          'eval': {'logloss': ['0.480385', '0.357756']}}  verbose_eval :     Requires at least one item in **evals**.      If **verbose_eval** is True then the evaluation metric on the validation set is     printed at each boosting stage.      If **verbose_eval** is an integer then the evaluation metric on the validation     set is printed at every given **verbose_eval** boosting stage. The last boosting     stage / the boosting stage found by using **early_stopping_rounds** is also     printed.      Example: with ``verbose_eval=4`` and at least one item in **evals**, an     evaluation metric is printed every 4 boosting stages, instead of every boosting     stage.  xgb_model :     Xgb model to be loaded before training (allows training continuation).  callbacks :     List of callback functions that are applied at end of each iteration.     It is possible to use predefined callbacks by using     :ref:`Callback API <callback_api>`.      .. note::         States in callback are not preserved during training, which means callback        objects can not be reused for multiple training sessions without        reinitialization or deepcopy.      .. code-block:: python          for params in parameters_grid:             # be sure to (re)initialize the callbacks before each run             callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]             xgboost.train(params, Xy, callbacks=callbacks)  custom_metric:      .. versionadded 1.6.0      Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`     for details. The metric receives transformed prediction (after applying the     reverse link function) when using a builtin objective, and raw output when using     a custom objective.  Returns ------- Booster : a trained booster model"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md\n# Crowd Source Data\n\n## Initiative\nPublic data source like yahoo is flawed, it might miss data for stock which is delisted and it might have data which is wrong. This can introduce survivorship bias into our training process.\n\nThe Crowd Source Data is introduced to merged data from multiple data source and cross validate against each other, so that:\n1. We will have a more complete history record.\n2. We can identify the anomaly data and apply correction when necessary.\n\n## Related Repo\nThe raw data is hosted on dolthub repo: https://www.dolthub.com/repositories/chenditc/investment_data\n\nThe processing script and sql is hosted on github repo: https://github.com/chenditc/investment_data\n\nThe packaged docker runtime is hosted on dockerhub: https://hub.docker.com/repository/docker/chenditc/investment_data\n\n## How to use it in qlib\n### Option 1: Download release bin data\nUser can download data in qlib bin format and use it directly: https://github.com/chenditc/investment_data/releases/latest\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```\n\n### Option 2: Generate qlib data from dolthub\nDolthub data will be update daily, so that if user wants to get up to date data, they can dump qlib bin using docker:\n```\ndocker run -v /<some output directory>:/output -it --rm chenditc/investment_data bash dump_qlib_bin.sh && cp ./qlib_bin.tar.gz /output/\n```\n\n## FAQ and other info\nSee: https://github.com/chenditc/investment_data/blob/main/README.md']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 908, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",41.12961506843567,637
Describe how US stock symbols are aggregated from the two data endpoints and how failures in each source are handled.,"The US stock symbols are aggregated from two data endpoints: EastMoney’s push2 API and NYSE’s /quotes/filter endpoint. The _get_eastmoney function sends a GET request to EastMoney's push2 API to retrieve approximately 10,000 tickers, while the _get_nyse function sends a POST request to NYSE's /quotes/filter endpoint. Both functions are wrapped in a retry decorator that retries calls throwing ValueError up to five times. The top-level function get_us_stock_symbols calls both helper functions and concatenates their outputs to return a combined list of symbols. Failures in each source are handled as follows: _get_eastmoney throws a ValueError for non-200 HTTP responses, JSON parse errors, or if fewer than 8,000 symbols are returned. The retry decorator retries these calls up to five times. In contrast, _get_nyse raises a ValueError for non-200 HTTP responses and catches JSON parse errors, logging them as warnings and returning an empty list.","US symbols are pulled in scripts/data_collector/utils.py by two helpers on top of a retry decorator (deco_retry.deco_func.wrapper):

1. _get_eastmoney  
   – GETs ~10 K tickers from EastMoney’s push2 API  
   – Maps each record’s f12 field (replacing “_”→“-P”)  
   – Non-200 responses, JSON errors or if fewer than 8 000 symbols are returned throw ValueError (and are retried up to 5× by the decorator)

2. _get_nyse  
   – POSTs a filter request to NYSE’s /quotes/filter endpoint  
   – Maps symbolTicker (replacing “-”→“-P”)  
   – Non-200 responses raise ValueError; JSON parse errors are caught, logged as warnings, and return an empty list

The top-level get_us_stock_symbols simply calls both (each wrapped in the retry logic), concatenates their outputs, and returns the combined symbol list.","['MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_us_stock_symbols\nget US stock symbols  Returns -------     stock symbols', 'FUNCTION qlib.scripts.data_collector.utils.get_us_stock_symbols has parameter or accepts argument qlib.scripts.data_collector.utils.get_us_stock_symbols.qlib_data_path\nget US stock symbols  Returns -------     stock symbols', 'MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_in_stock_symbols\nget IN stock symbols  Returns -------     stock symbols', 'FUNCTION qlib.scripts.data_collector.utils.get_in_stock_symbols has parameter or accepts argument qlib.scripts.data_collector.utils.get_in_stock_symbols.qlib_data_path\nget IN stock symbols  Returns -------     stock symbols', 'MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_hs_stock_symbols\nget SH/SZ stock symbols  Returns -------     stock symbols', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 4, 'pr': 938, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.65181231498718,638
How is the instrument identifier supplied to each processor during data calculation?,"The instrument identifier is supplied to each processor during data calculation in qlib through the `instrument` parameter, which is passed to the `__call__` method of the InstProcessor class. This parameter is set when calling the `_processor_obj` function with the `data` variable and the `instrument` keyword argument.

Here is an excerpt from the qlib codebase that shows how this works:

```python
def _processor_obj(data, processor_name, instrument):
    # ...
    return processor(instrument=instrument)
```
In this code, the `_processor_obj` function takes in a `data` variable and two keyword arguments: `processor_name` and `instrument`. It then calls the corresponding processor class with these arguments. The `instrument` parameter is passed to the processor's `__call__` method.

So, when calling the `_processor_obj` function, you would pass in the instrument identifier as a keyword argument like this:

```python
data = _processor_obj(data, 'my_processor', inst)
```
This would set the `instrument` parameter of the `my_processor` processor to the value of `inst`.","Each processor is called with the instrument ID as a keyword arg in `inst_calculator`. In qlib/data/data.py you’ll see:

    data = _processor_obj(data, instrument=inst)

so `inst` is passed into each `InstProcessor.__call__` via the `instrument` parameter.","['CLASS qlib.qlib.data.base.Expression has method qlib.qlib.data.base.Expression.__eq__\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).', 'CLASS qlib.qlib.data.inst_processor.InstProcessor has field or attribute or property qlib.qlib.data.inst_processor.instrument', 'CLASS qlib.qlib.data.base.Expression has method qlib.qlib.data.base.Expression.__ge__\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).', 'METHOD qlib.qlib.data.data.DatasetProvider.dataset_processor has parameter or accepts argument qlib.qlib.data.data.DatasetProvider.dataset_processor.instruments_d\nLoad and process the data, return the data set. - default using multi-kernel method.', 'CLASS qlib.qlib.data.base.Expression has method qlib.qlib.data.base.Expression.__le__\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 959, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",44.8063428401947,639
"If a Yahoo history request returns a dict without valid quote data, how does the higher‐level simple fetch method respond?","The Yahoo history request returned an error with status code 400 and error message ""Invalid parameters"".","If `history` returns a dict with no `""quote""` data, `get_data_from_remote` falls through and returns `None`. In `_get_simple` (in `YahooCollector.get_data`), that `None` triggers:

```python
if resp is None or resp.empty:
    raise ValueError(
        f""get data error: {symbol}--{start_}--{end_}""
        + ""The stock may be delisted, please check""
    )
```

So you get a `ValueError` telling you the stock may be delisted.","[""CLASS qlib.qlib.data.data.ClientProvider inherits from or is a subclass of qlib.qlib.data.data.BaseProvider\nLocal provider class It is a set of interface that allow users to access data. Because PITD is not exposed publicly to users, so it is not included in the interface.  To keep compatible with old qlib provider.\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", 'METHOD qlib.scripts.data_collector.yahoo.collector.YahooCollector.get_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.YahooCollector.get_data.end_datetime', ""CLASS qlib.qlib.data.data.ClientProvider has method qlib.qlib.data.data.ClientProvider.__init__\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", 'METHOD qlib.scripts.data_collector.yahoo.collector.YahooNormalize1dExtend.__init__ has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.YahooNormalize1dExtend.__init__.old_qlib_data_dir\nParameters ---------- old_qlib_data_dir: str, Path     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default is date symbol_field_name: str     symbol field name, default is symbol', ""CLASS qlib.qlib.data.data.ClientProvider has method qlib.qlib.data.data.ClientProvider.is_instance_of_provider\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can\'t meet users\' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month\'s data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *""2000-01-01""*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *""failed""* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date\'s close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 966, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",31.481945514678955,640
Which artifact repository type triggers deletion of the downloaded artifact directory after loading an object?,"There is no specific information about which artifact repository type triggers deletion of the downloaded artifact directory after loading an object in qlib. However, you can use the `download_artifact` method to download artifacts and then manage them as needed. The method returns the local path of the desired artifact, but it does not explicitly state what happens to the downloaded artifact directory after it is loaded.",The cleanup only happens when the artifact repo is an AzureBlobArtifactRepository (as checked in MLflowRecorder.load_object’s finally block).,"['METHOD qlib.qlib.workflow.__init__.QlibRecorder.download_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.download_artifact.path\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.', 'CLASS qlib.qlib.utils.objm.FileManager has method qlib.qlib.utils.objm.FileManager.remove\nUse file system to manage objects', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.download_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.download_artifact.dst_path\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.', 'CLASS qlib.scripts.data_collector.index.IndexBase has field or attribute or property qlib.scripts.data_collector.index.qlib_dir', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.download_artifact\nDownload an artifact file or directory from a run to a local directory if applicable, and return a local path for it.  Parameters ---------- path : str     Relative source path to the desired artifact. dst_path : Optional[str]     Absolute path of the local filesystem destination directory to which to     download the specified artifacts. This directory must already exist.     If unspecified, the artifacts will either be downloaded to a new     uniquely-named directory on the local filesystem.  Returns ------- str     Local path of desired artifact.\nA global system that helps to manage the experiments.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What\'s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=""https://github.com/microsoft/RD-Agent""><img src=""docs/_static/img/rdagent_logo.png"" alt=""RD_Agent"" style=""height: 2em""></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!\n\nTo learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (中文) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- 📃**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- 👾**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href=""https://github.com/microsoft/qlib/graphs/contributors""><img src=""https://contrib.rocks/image?repo=microsoft/qlib"" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib\'s document/code, you can follow the steps in the figure below.\n<p align=""center"">\n  <img src=""https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif"" />\n</p>\n\nIf you don\'t know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg \'TODO|FIXME\' qlib`\n \nIf you would like to become one of Qlib\'s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 967, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",38.53995609283447,641
Which flag controls whether the MultiIndex lexsorted check is skipped for recent pandas versions?,The flag that controls whether the MultiIndex lexsorted check is skipped for recent pandas versions in qlib is `is_deprecated_lexsorted_pandas`. This flag is defined in qlib/utils/__init__.py and applies to newer pandas versions.,The boolean flag `is_deprecated_lexsorted_pandas` (in qlib/utils/__init__.py) controls whether the MultiIndex `.is_lexsorted()` check is skipped for newer pandas versions.,"['METHOD graphiti.graphiti_core.prompts.lib.PromptTypeWrapper.__init__ has parameter or accepts argument graphiti.graphiti_core.prompts.lib.PromptTypeWrapper.__init__.versions', 'MODULE qlib.qlib.utils.__init__ contains GLOBAL_VARIABLE qlib.qlib.utils.__init__.is_deprecated_lexsorted_pandas', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.time_to_slc_point\nTime slicing in Qlib or Pandas is a frequently-used action. However, user often input all kinds of data format to represent time. This function will help user to convert these inputs into a uniform format which is friendly to time slicing.  Parameters ---------- t : Union[None, str, pd.Timestamp]     original time  Returns ------- Union[None, pd.Timestamp]:', 'FUNCTION qlib.qlib.utils.__init__.lazy_sort_index has parameter or accepts argument qlib.qlib.utils.__init__.is_deprecated_lexsorted_pandas\nmake the df index sorted  df.sort_index() will take a lot of time even when `df.is_lexsorted() == True` This function could avoid such case  Parameters ---------- df : pd.DataFrame  Returns ------- pd.DataFrame:     sorted dataframe', ""METHOD qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__.trade_range\nParameters ---------- file : Union[IO, str, Path, pd.DataFrame]     this parameters will specify the info of expected orders      Here is an example of the content      1) Amount (**adjusted**) based strategy          datetime,instrument,amount,direction         20200102,  SH600519,  1000,     sell         20200103,  SH600519,  1000,      buy         20200106,  SH600519,  1000,     sell  trade_range : Tuple[int, int]     the intra day time index range of the orders     the left and right is closed.      If you want to get the trade_range in intra-day     - `qlib/utils/time.py:def get_day_min_idx_range` can help you create the index range easier     # TODO: this is a trade_range level limitation. We'll implement a more detailed limitation later."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 973, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",28.037384271621704,642
"Which callbacks are passed to lightgbm.train in both models’ fit methods, and which single callback do their finetune methods use?","The callbacks used in the `lgb.train` function calls in both models' fit methods are `lgb.early_stopping(...)` and `lgb.log_evaluation(period=...)`. The single callback passed to their finetune methods is `lgb.log_evaluation(period=...). In addition, both models use the `record_evaluation(evals_result)` callback in their fit methods.","In both LGBModel.fit (qlib/contrib/model/gbdt.py) and HFLGBModel.fit (qlib/contrib/model/highfreq_gdbt_model.py) the call to lgb.train uses three callbacks:

• lgb.early_stopping(...)  
• lgb.log_evaluation(period=…)  
• lgb.record_evaluation(evals_result)  

In both finetune methods the only callback passed is:

• lgb.log_evaluation(period=…)","['MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'CLASS qlib.qlib.contrib.model.highfreq_gdbt_model.HFLGBModel has method qlib.qlib.contrib.model.highfreq_gdbt_model.HFLGBModel.fit\nLightGBM Model for high frequency prediction', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.model.highfreq_gdbt_model.HFLGBModel.fit has parameter or accepts argument qlib.qlib.contrib.model.highfreq_gdbt_model.HFLGBModel.fit.early_stopping_rounds', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has field or attribute or property qlib.qlib.workflow.record_temp.PortAnaRecord.strategy_config\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/LightGBM/README.md\n# LightGBM\n* Code: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)\n* Paper: LightGBM: A Highly Efficient Gradient Boosting\nDecision Tree. [https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf).\n\n\n# Introductions about the settings/configs.\n\n`workflow_config_lightgbm_multi_freq.yaml`\n- It uses data sources of different frequencies (i.e. multiple frequencies) for daily prediction.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 974, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",32.42443895339966,643
"What sequence of steps occurs when the temporary record loader fetches an artifact, including error fallback and artifact cleanup?","When the temporary record loader fetches an artifact in qlib, including error fallback and AzureBlobArtifactRepository cleanup, the following sequence of steps occurs:

1. The `RecordTemp.load(name)` function is called, which behaves similarly to `self.recorder.load_object(self.get_path(name))`.
2. `MLflowRecorder.load_object(name)` is called, which asserts that the recorder’s URI is not None.
3. If the underlying repository is AzureBlobArtifactRepository and a download path exists, the parent directory of the download path is removed using `shutil.rmtree` in the finally block.
4. The function attempts to download the artifact to a local path by calling `self.client.download_artifacts(self.id, name)`.
5. If the download fails due to any exception, including `LoadObjectError`, and parents=True and depend_cls is not None, the function temporarily casts self to depend_cls via class_casting and retries the load operation recursively.
6. If the load still fails after retry, or if parents=False or depend_cls is None, the LoadObjectError is propagated.","When you call RecordTemp.load(name):

1. RecordTemp.load(name) calls  
   → self.recorder.load_object(self.get_path(name))  
   (qlib/workflow/record_temp.py)

2. In MLflowRecorder.load_object(name):  
   a. Assert the recorder is started (uri ≠ None).  
   b. Download the artifact to a local path via self.client.download_artifacts(self.id, name).  
   c. Open the file and unpickle it.  
   d. Return the deserialized object.  
   e. In finally, if the underlying repo is AzureBlobArtifactRepository and a path was downloaded, remove its parent directory (shutil.rmtree) to clean up.

3. Error fallback in RecordTemp.load:  
   – If load_object raises LoadObjectError and parents=True and depend_cls≠ None, it temporarily casts self to depend_cls (via class_casting) and retries load(name) recursively.  
   – If that still fails (or parents=False/no depend_cls), the LoadObjectError is propagated.","['METHOD qlib.qlib.workflow.recorder.MLflowRecorder.load_object has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.load_object.unpickler\nLoad object such as prediction file or model checkpoint in mlflow.  Args:     name (str): the object name      unpickler: Supporting using custom unpickler  Raises:     LoadObjectError: if raise some exceptions when load the object  Returns:     object: the saved object in mlflow.', ""METHOD qlib.qlib.workflow.record_temp.RecordTemp.load has parameter or accepts argument qlib.qlib.workflow.record_temp.RecordTemp.load.name\nIt behaves the same as self.recorder.load_object. But it is an easier interface because users don't have to care about `get_path` and `artifact_path`  Parameters ---------- name : str     the name for the file to be load.  parents : bool     Each recorder has different `artifact_path`.     So parents recursively find the path in parents     Sub classes has higher priority  Return ------ The stored records."", 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.load_object has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.load_object.name\nLoad object such as prediction file or model checkpoint in mlflow.  Args:     name (str): the object name      unpickler: Supporting using custom unpickler  Raises:     LoadObjectError: if raise some exceptions when load the object  Returns:     object: the saved object in mlflow.', ""CLASS qlib.qlib.workflow.record_temp.RecordTemp has method qlib.qlib.workflow.record_temp.RecordTemp.load\nIt behaves the same as self.recorder.load_object. But it is an easier interface because users don't have to care about `get_path` and `artifact_path`  Parameters ---------- name : str     the name for the file to be load.  parents : bool     Each recorder has different `artifact_path`.     So parents recursively find the path in parents     Sub classes has higher priority  Return ------ The stored records.\nThis is the Records Template class that enables user to generate experiment results such as IC and backtest in a certain format."", 'CLASS qlib.qlib.rl.order_execution.interpreter.FullHistoryStateInterpreter has method qlib.qlib.rl.order_execution.interpreter.FullHistoryStateInterpreter.interpret\nThe observation of all the history, including today (until this moment), and yesterday.  Parameters ---------- max_step     Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps. data_ticks     Equal to the total number of records. For example, in SAOE per minute,     the total ticks is the length of day in minutes. data_dim     Number of dimensions in data. processed_data_provider     Provider of the processed data.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 977, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",45.568406105041504,644
What sequence of operations does the download_data entry point perform to fetch and assemble quarterly pit data?,"The sequence of operations performed by the download_data entry point to fetch and assemble quarterly pit data in qlib is as follows:

1. The `download_data` method is called with the required parameters, including the source directory, start date, end date, and interval.

2. The collector class (`PitCollector`) is dynamically looked up via the `self.collector_class_name` attribute.

3. An instance of the collector class is created with the specified parameters.

4. The `collector_data` method of the instantiated collector is called.

5. The `collector_data` method loops over all symbols and quarters, invoking the `PitCollector.get_data` method for each combination.

6. The `PitCollector.get_data` method splits and normalizes the symbol and exchange.

7. The `PitCollector.get_data` method formats the start_date and end_date.

8. The `PitCollector.get_data` method calls four functions: `get_performance_express_report_df`, `get_profit_df`, `get_forecast_report_df`, and `get_growth_df` to retrieve relevant data.

9. The `PitCollector.get_data` method concatenates the four DataFrames returned by the above-mentioned functions.

10. The concatenated DataFrame is then returned.","The `download_data` entry point (in scripts/data_collector/base.py: BaseRun.download_data) does roughly the following:

1.  Dynamically looks up your collector class (here PitCollector) via `self.collector_class_name`.  
2.  Instantiates it with parameters (`source_dir`, `max_workers`, `max_collector_count`, `delay`, `start`, `end`, `interval='quarterly'`, …).  
3.  Calls its `collector_data()` method, which internally loops over all symbols/quarters and for each invokes PitCollector.get_data (in scripts/data_collector/pit/collector.py).  

Inside PitCollector.get_data, for each symbol and quarter it:
-  Splits and normalizes the symbol/exchange.  
-  Formats `start_date`/`end_date`.  
-  Calls, in order:
   · get_performance_express_report_df  
   · get_profit_df  
   · get_forecast_report_df  
   · get_growth_df  
-  Concatenates those four DataFrames and returns the result.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.base.BaseRun has method qlib.scripts.data_collector.base.BaseRun.download_data\ndownload data from Internet  Parameters ---------- max_collector_count: int     default 2 delay: float     time.sleep(delay), default 0 start: str     start datetime, default ""2000-01-01"" end: str     end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`` check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. limit_nums: int     using for debug, by default None  Examples ---------     # get daily data     $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d     # get 1m data     $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1m', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.first\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'METHOD qlib.scripts.data_collector.base.BaseRun.download_data has parameter or accepts argument qlib.scripts.data_collector.base.BaseRun.download_data.start\ndownload data from Internet  Parameters ---------- max_collector_count: int     default 2 delay: float     time.sleep(delay), default 0 start: str     start datetime, default ""2000-01-01"" end: str     end datetime, default ``pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`` check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. limit_nums: int     using for debug, by default None  Examples ---------     # get daily data     $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d     # get 1m data     $ python collector.py download_data --source_dir ~/.qlib/instrument_data/source --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1m', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.last\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 979, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",46.30753207206726,645
Describe how the Run class orchestrates fetching the four PIT data types and then applies normalization for quarterly output.,"The Run class in qlib orchestrates fetching the four PIT data types and then applies normalization for quarterly output as follows:

1.  The BaseRun class instantiates PitCollector with the interval set to ""quarterly"".
2.  PitCollector.get_data is called, which invokes get_performance_express_report_df, get_profit_df, get_forecast_report_df, and get_growth_df.
3.  These four DataFrames are concatenated using pd.concat in PitCollector.get_data.
4.  After data collection, BaseRun looks up the normalize_class_name ""PitNormalize"" and instantiates it with the interval set to ""quarterly"".
5.  The raw DataFrame is passed into PitNormalize.normalize, which fills any missing ""date"" by adding 45 days to ""period"" and converts ""period"" into a quarterly code (YYYYQ).
6.  The workflow produces a single quarterly-normalized PIT table.

This process ensures that the data is collected and normalized for quarterly output, making it ready for further analysis or modeling in qlib.","Run simply wires together PitCollector and PitNormalize under the BaseRun workflow:

1. Run.collector_class_name returns “PitCollector”, so BaseRun will instantiate PitCollector(interval=""quarterly"") and call its get_data(symbol, “quarterly”, start, end).  
   – PitCollector.get_data invokes in order  
     • get_performance_express_report_df  
     • get_profit_df  
     • get_forecast_report_df  
     • get_growth_df  
   and then pd.concat’s those four DataFrames.

2. After data collection, BaseRun looks up normalize_class_name (“PitNormalize”), instantiates PitNormalize(interval=""quarterly"") and passes the raw df into PitNormalize.normalize.  
   – normalize fills any missing “date” by adding 45 days to “period”  
   – converts “period” into a quarterly code (YYYYQ)  

The result is a single quarterly‐normalized PIT table.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize has method qlib.scripts.data_collector.pit.collector.PitNormalize.normalize', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.quarterly\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.baostock_5min.collector.BaostockNormalizeHS3005min has method qlib.scripts.data_collector.baostock_5min.collector.BaostockNormalizeHS3005min.normalize', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.first\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships).""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 11, 'n_files_pr': 3, 'pr': 982, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.92082929611206,646
What are the main steps executed in the example's main method to run an online simulation and backtest?,"The main steps executed in the example's main method to run an online simulation and backtest are:

1. `self.reset()`: Clears tasks via TaskManager and deletes any existing recorders.
2. `self.rolling_online_manager.simulate(end_time)`: Runs a rolling-window online simulation.
3. `self.rolling_online_manager.get_collector()()`: Collects simulation metrics.
4. `self.rolling_online_manager.get_signals()`: Extracts a signal DataFrame.
5. Builds a TopkDropoutStrategy using the extracted signals.
6. `backtest_daily(start, end, strategy)`: Runs backtest_daily with the built strategy.
7. Computes excess returns using risk_analysis.
8. Pretty-prints the concatenated results of the backtest.","In examples/online_srv/online_management_simulate.py, OnlineSimulationExample.main does roughly:

1. Call self.reset() (to clear tasks via TaskManager and delete any existing recorders via R.get_exp/exp.delete_recorder).  
2. Call self.rolling_online_manager.simulate(end_time) to run the rolling‐window online simulation.  
3. Call and print self.rolling_online_manager.get_collector()() to collect simulation metrics.  
4. Call and print self.rolling_online_manager.get_signals() to extract the signal DataFrame.  
5. Build a TopkDropoutStrategy (using those signals) and run backtest_daily(start, end, strategy).  
6. Compute excess returns with risk_analysis and pprint the concatenated results.","['CLASS qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord inherits from or is a subclass of qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`\nThis is the Multiple Pass Portfolio Analysis Record class that run backtest multiple times and generates the analysis results such as those of backtest. This class inherits the ``PortAnaRecord`` class.  If shuffle_init_score enabled, the prediction score of the first backtest date will be shuffled, so that initial position will be random. The shuffle_init_score will only works when the signal is used as <PRED> placeholder. The placeholder will be replaced by pred.pkl saved in recorder.  Parameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.', 'CLASS qlib.examples.online_srv.online_management_simulate.OnlineSimulationExample has method qlib.examples.online_srv.online_management_simulate.OnlineSimulationExample.main', 'CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'MODULE qlib.examples.online_srv.online_management_simulate contains CLASS qlib.examples.online_srv.online_management_simulate.OnlineSimulationExample', ""CLASS qlib.qlib.workflow.online.utils.OnlineTool has method qlib.qlib.workflow.online.utils.OnlineTool.reset_online_tag\nOffline all models and set the recorders to 'online'.  Args:     recorder (Union[list,object]):         the recorder you want to reset to 'online'.\nOnlineTool will manage `online` models in an experiment that includes the model recorders."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 984, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.72844123840332,647
What happens when the start calendar index comes after the end calendar index in the base feature loader?,"When the start calendar index comes after the end calendar index in the base feature loader in qlib, a ValueError is raised with the message ""Invalid index range: {start_index} {end_index}"". This is because the Expression.load method has a guard that checks if `start_index` is not None, `end_index` is not None, and `start_index > end_index`. If these conditions are met, it raises a ValueError.","In Expression.load (qlib/data/base.py), there’s a guard:

```
if start_index is not None and end_index is not None and start_index > end_index:
    raise ValueError(f""Invalid index range: {start_index} {end_index}"")
```

So you’ll get a ValueError if the start calendar index comes after the end.","['CLASS qlib.qlib.data.data.CalendarProvider has method qlib.qlib.data.data.CalendarProvider.locate_index\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.\nCalendar provider base class  Provide calendar data.', 'CLASS qlib.qlib.data.storage.file_storage.FileCalendarStorage has method qlib.qlib.data.storage.file_storage.FileCalendarStorage.index', 'METHOD qlib.qlib.data.data.CalendarProvider.locate_index has parameter or accepts argument qlib.qlib.data.data.CalendarProvider.locate_index.freq\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.', 'METHOD qlib.qlib.data.data.BaseProvider.calendar has parameter or accepts argument qlib.qlib.data.data.BaseProvider.calendar.end_time', 'METHOD qlib.qlib.data.data.CalendarProvider.locate_index has parameter or accepts argument qlib.qlib.data.data.CalendarProvider.locate_index.end_time\nLocate the start time index and end time index in a calendar under certain frequency.  Parameters ---------- start_time : pd.Timestamp     start of the time range. end_time : pd.Timestamp     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ------- pd.Timestamp     the real start time. pd.Timestamp     the real end time. int     the index of start time. int     the index of end time.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 987, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.638482570648193,648
What additional processing does the general update method perform that the dataset-based retrieval bypasses?,"The general update method, RecordUpdater.update, performs additional processing compared to DSBasedUpdater.get_update_data by running the full online-update pipeline, preparing the Dataset using the recorder, running all pre-checks, post-checks, and sanity validations, handling logging, error handling, and persistence hooks. In contrast, DSBasedUpdater.get_update_data bypasses these steps and only pulls the new DataFrame from the Dataset.","The key is in qlib/workflow/online/update.py:

• RecordUpdater.update(...) runs the full “online‐update” pipeline – it  
  – prepares the Dataset (using the recorder)  
  – runs all the pre‐/post‐checks and sanity validations  
  – handles logging, error handling, persistence hooks, etc.  

• DSBasedUpdater.get_update_data(...) by contrast simply pulls the new DataFrame from the Dataset and skips all of those general routine steps (dataset prep, parameter checking, hooks, etc.).","['CLASS qlib.qlib.data.dataset.processor.Processor has method qlib.qlib.data.dataset.processor.Processor.readonly\nDoes the processor treat the input data readonly (i.e. does not write the input data) when processing  Knowning the readonly information is helpful to the Handler to avoid uncessary copy', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.RecordUpdater\nUpdate a specific recorders\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'MODULE qlib.qlib.data.dataset.__init__ contains CLASS qlib.qlib.data.dataset.__init__.DatasetH\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.get_update_data\nreturn the updated data based on the given dataset  The difference between `get_update_data` and `update` - `update_date` only include some data specific feature - `update` include some general routine steps(e.g. prepare dataset, checking)\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'CLASS qlib.qlib.data.dataset.__init__.DatasetH has method qlib.qlib.data.dataset.__init__.DatasetH.get_max_time\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 6, 'pr': 992, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",42.56769275665283,649
What argument provided by the new reference operator takes precedence over the relative start and end indices when retrieving point-in-time data?,"The period argument provided by the new P/PRef operator takes precedence over the relative start and end indices when retrieving point-in-time data in qlib, as it is used to pass into PITProvider.period_feature.",The `period` argument passed into PITProvider.period_feature (e.g. via the new `P`/`PRef` operator) takes priority over `start_index` and `end_index`.,"['MODULE qlib.qlib.contrib.ops.high_freq contains CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'METHOD qlib.scripts.dump_bin.DumpDataBase.get_datetime_index has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.get_datetime_index.df', 'CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum has method qlib.qlib.contrib.ops.high_freq.DayCumsum.__init__\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'FUNCTION qlib.qlib.utils.time.time_to_day_index has parameter or accepts argument qlib.qlib.utils.time.time_to_day_index.region', 'CLASS qlib.qlib.contrib.ops.high_freq.DayCumsum has method qlib.qlib.contrib.ops.high_freq.DayCumsum.period_cusum\nDayCumsum Operator during start time and end time.  Parameters ---------- feature : Expression     feature instance start : str     the start time of backtest in one day.     !!!NOTE: ""9:30"" means the time period of (9:30, 9:31) is in transaction. end : str     the end time of backtest in one day.     !!!NOTE: ""14:59"" means the time period of (14:59, 15:00) is in transaction,             but (15:00, 15:01) is not.     So start=""9:30"" and end=""14:59"" means trading all day.  Returns ---------- feature:     a series of that each value equals the cumsum value during start time and end time.     Otherwise, the value is zero.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 4, 'n_context_nodes': 7, 'n_files_pr': 5, 'pr': 1000, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",27.91383957862854,650
Which regex character class is defined to match Chinese punctuation in parse_field?,"The regex character class defined to match Chinese punctuation in the `parse_field` function of the `qlib/utils/__init__.py` file is `[^　-〿぀-ゟ゠-ヿ＀-）：-？＠-Ｚ［-＿｀-｛｜-･ｦ-ｯｰ-ﾝﾠ-ￜ￞-￿一-龥　-〿぀-ゟ゠-ヿ＀-）：-？＠-Ｚ［-＿｀-｛｜-･ｦ-ｯｰ-ﾝﾠ-ￜ￞-￿一-龥]`. This character class includes a range of Unicode code points that cover various Chinese punctuation marks, as well as some additional characters to ensure matching. It is used in the `parse_field` function to match Chinese punctuation.","In `qlib/utils/__init__.py`’s `parse_field`, the Chinese punctuation class is defined by:

```python
chinese_punctuation_regex = r""\u3001\uff1a\uff08\uff09""
```","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.parse_field', 'CLASS qlib.scripts.data_collector.pit.collector.PitCollector has field or attribute or property qlib.scripts.data_collector.pit.collector.symbol_regex', 'FUNCTION qlib.qlib.utils.__init__.parse_field has parameter or accepts argument qlib.qlib.utils.__init__.parse_field.field', 'CLASS qlib.scripts.data_collector.pit.collector.PitCollector has field or attribute or property qlib.scripts.data_collector.pit.collector.PitCollector.symbol_regex', ""METHOD qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__.file\nParameters ---------- file : Union[IO, str, Path, pd.DataFrame]     this parameters will specify the info of expected orders      Here is an example of the content      1) Amount (**adjusted**) based strategy          datetime,instrument,amount,direction         20200102,  SH600519,  1000,     sell         20200103,  SH600519,  1000,      buy         20200106,  SH600519,  1000,     sell  trade_range : Tuple[int, int]     the intra day time index range of the orders     the left and right is closed.      If you want to get the trade_range in intra-day     - `qlib/utils/time.py:def get_day_min_idx_range` can help you create the index range easier     # TODO: this is a trade_range level limitation. We'll implement a more detailed limitation later."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1003, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",43.380109548568726,651
"When invoking the custom load method, which instrument value is forwarded to the base operator's loader instead of the provided one?","When invoking the custom load method in qlib, the instrument value that is forwarded to the base operator's loader instead of the provided one is `self.instrument`, which was passed to the constructor. This means that the custom load method ignores its incoming argument and forwards the instance variable `self.instrument` to the base operator's loader.","In ChangeInstrument.load (qlib/data/ops.py), the incoming instrument argument is ignored and instead self.instrument (the one passed to the constructor) is forwarded to super().load.","['FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.data.ops.ChangeInstrument.load has parameter or accepts argument qlib.qlib.data.ops.ChangeInstrument.load.end_index', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.data.ops.ChangeInstrument.load has parameter or accepts argument qlib.qlib.data.ops.ChangeInstrument.load.start_index', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1005, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",28.20253586769104,652
"Under what conditions does the workflow function generate a default experiment manager URI, and how is that URI constructed?","The workflow function generates a default experiment manager URI when the `exp_manager` key is missing from `workflow.yml's qlib_init section`. The default URI is constructed by injecting a URI of the form `file:` + `Path(os.getcwd()).resolve()` / `uri_folder`, where `uri_folder` defaults to `","In qlib/workflow/cli.py’s workflow(), if your workflow.yml’s qlib_init section does *not* include an exp_manager key, it falls back to the default C[""exp_manager""] and injects a URI of the form:

  “file:” + Path(os.getcwd()).resolve() / uri_folder

(where uri_folder defaults to “mlruns”).  
So you get e.g.  
  file:/home/you/your_proj/mlruns","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.workflow.expm.ExpManager.__init__ has parameter or accepts argument qlib.qlib.workflow.expm.ExpManager.__init__.default_exp_name', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.workflow.expm.ExpManager.__init__ has parameter or accepts argument qlib.qlib.workflow.expm.ExpManager.__init__.uri', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/README.md\n# Requirements\n\nHere is the minimal hardware requirements to run the `workflow_by_code` example.\n- Memory: 16G\n- Free Disk: 5G\n\n\n# NOTE\nThe results will slightly vary on different OSs(the variance of annualized return will be less than 2%).\nThe evaluation results in the `README.md` page are from Linux OS.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1008, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.39806628227234,653
How does get_calendar_minute map each calendar timestamp to a half-hour bucket?,"The `get_calendar_minute` function in qlib maps each calendar timestamp to a half-hour bucket by dividing the minute of the timestamp by 30, where minutes 0-29 are mapped to 0 and minutes 30-59 are mapped to 1. The result is returned as a NumPy array.","The `get_calendar_minute` function (qlib/contrib/ops/high_freq.py) does:

1. Calls `Cal.load_calendar(freq, future)` to get a list of datetime stamps.  
2. Maps each timestamp `x` to `x.minute // 30` (i.e. 0 for minutes 0–29, 1 for minutes 30–59).  
3. Returns the result as a NumPy array (and caches it in `H[""c""]`).","['MODULE cognee.cognee.modules.engine.utils.generate_timestamp_datapoint contains FUNCTION cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int\nConverts a Timestamp model into an integer representation in milliseconds since the Unix epoch (UTC).  Args:     ts (Timestamp): The input Timestamp model containing year, month, day, hour, minute, and second.  Returns:     int: The UTC timestamp in milliseconds since January 1, 1970.', 'MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.generate_minutes_calendar_from_daily\ngenerate minutes calendar  Parameters ---------- calendars: Iterable     daily calendar freq: str     by default 1min am_range: Tuple[str, str]     AM Time Range, by default China-Stock: (""09:30:00"", ""11:29:00"") pm_range: Tuple[str, str]     PM Time Range, by default China-Stock: (""13:00:00"", ""14:59:00"")', 'FUNCTION cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int has parameter or accepts argument cognee.cognee.modules.engine.utils.generate_timestamp_datapoint.date_to_int.ts\nConverts a Timestamp model into an integer representation in milliseconds since the Unix epoch (UTC).  Args:     ts (Timestamp): The input Timestamp model containing year, month, day, hour, minute, and second.  Returns:     int: The UTC timestamp in milliseconds since January 1, 1970.', 'FUNCTION qlib.qlib.contrib.ops.high_freq.get_calendar_minute has parameter or accepts argument qlib.qlib.contrib.ops.high_freq.get_calendar_minute.future\nLoad High-Freq Calendar Minute Using Memcache', 'CLASS qlib.qlib.data.data.CalendarProvider has method qlib.qlib.data.data.CalendarProvider.load_calendar\nLoad original calendar timestamp from file.  Parameters ---------- freq : str     frequency of read calendar file. future: bool  Returns ---------- list     list of timestamps\nCalendar provider base class  Provide calendar data.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 4, 'pr': 1022, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.64634346961975,655
"In the cross-sectional rank normalization, what do the subtraction of 0.5 and multiplication by 3.46 achieve?","The subtraction of 0.5 and multiplication by 3.46 in the cross-sectional rank normalization achieve standardization of the ranks to zero mean and unit variance. This is done by first subtracting the mean of the ranks, which shifts the distribution so that it has a mean of 0, and then scaling the distribution by its standard deviation, which makes it have a standard deviation of 1. Specifically, this process removes the original distribution's mean (U(0,1) mean is 0.5), and rescales its standard deviation to 1 by multiplying with its reciprocal (standard deviation of U(0,1) is approximately 0.289).","In CSRankNorm (qlib/data/dataset/processor.py), after you `rank(pct=True)`:

• Subtracting 0.5 removes the mean of a U(0,1) distribution (centers it at zero).  
• Multiplying by ~3.46 (≈1/(1/√12)) rescales the uniform’s std (1/√12≈0.289) to 1.  

Together they standardize your cross‐sectional ranks to zero mean and unit variance.","['CLASS qlib.qlib.data.dataset.processor.CSRankNorm inherits from or is a subclass of qlib.qlib.data.dataset.processor.Processor\nCross Sectional Rank Normalization. ""Cross Sectional"" is often used to describe data operations. The operations across different stocks are often called Cross Sectional Operation.  For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.  Explanation about 3.46 & 0.5  .. code-block:: python      import numpy as np     import pandas as pd     x = np.random.random(10000)  # for any variable     x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed     x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution      x_rank.mean()   # accounts for 0.5     1 / x_rank.std()  # accounts for 3.46', 'CLASS qlib.qlib.data.dataset.processor.CSRankNorm has method qlib.qlib.data.dataset.processor.CSRankNorm.__call__\nCross Sectional Rank Normalization. ""Cross Sectional"" is often used to describe data operations. The operations across different stocks are often called Cross Sectional Operation.  For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.  Explanation about 3.46 & 0.5  .. code-block:: python      import numpy as np     import pandas as pd     x = np.random.random(10000)  # for any variable     x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed     x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution      x_rank.mean()   # accounts for 0.5     1 / x_rank.std()  # accounts for 3.46', 'CLASS qlib.qlib.data.dataset.processor.CSRankNorm has field or attribute or property qlib.qlib.data.dataset.processor.df\nCross Sectional Rank Normalization. ""Cross Sectional"" is often used to describe data operations. The operations across different stocks are often called Cross Sectional Operation.  For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.  Explanation about 3.46 & 0.5  .. code-block:: python      import numpy as np     import pandas as pd     x = np.random.random(10000)  # for any variable     x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed     x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution      x_rank.mean()   # accounts for 0.5     1 / x_rank.std()  # accounts for 3.46', 'CLASS qlib.qlib.data.dataset.processor.CSRankNorm has field or attribute or property qlib.qlib.data.dataset.processor.CSRankNorm.fields_group\nCross Sectional Rank Normalization. ""Cross Sectional"" is often used to describe data operations. The operations across different stocks are often called Cross Sectional Operation.  For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.  Explanation about 3.46 & 0.5  .. code-block:: python      import numpy as np     import pandas as pd     x = np.random.random(10000)  # for any variable     x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed     x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution      x_rank.mean()   # accounts for 0.5     1 / x_rank.std()  # accounts for 3.46', 'CLASS qlib.qlib.data.dataset.processor.CSRankNorm has method qlib.qlib.data.dataset.processor.CSRankNorm.__init__\nCross Sectional Rank Normalization. ""Cross Sectional"" is often used to describe data operations. The operations across different stocks are often called Cross Sectional Operation.  For example, CSRankNorm is an operation that grouping the data by each day and rank `across` all the stocks in each day.  Explanation about 3.46 & 0.5  .. code-block:: python      import numpy as np     import pandas as pd     x = np.random.random(10000)  # for any variable     x_rank = pd.Series(x).rank(pct=True)  # if it is converted to rank, it will be a uniform distributed     x_rank_norm = (x_rank - x_rank.mean()) / x_rank.std()  # Normally, we will normalize it to make it like normal distribution      x_rank.mean()   # accounts for 0.5     1 / x_rank.std()  # accounts for 3.46', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Validation sets now support instance weights (#2354)\n  - `XGBClassifier.predict_proba()` should not support `output_margin` option. (#3343) See BREAKING CHANGES below.\n* R package:\n  - Better handling of NULL in `print.xgb.Booster()` (#3338)\n  - Comply with CRAN policy by removing compiler warning suppression (#3329)\n  - Updated CRAN submission\n* JVM packages\n  - JVM packages will now use the same versioning scheme as other packages (#3253)\n  - Update Spark to 2.3 (#3254)\n  - Add scripts to cross-build and deploy artifacts (#3276, #3307)\n  - Fix a compilation error for Scala 2.10 (#3332)\n* BREAKING CHANGES\n  - `XGBClassifier.predict_proba()` no longer accepts parameter `output_margin`. The parameter makes no sense for `predict_proba()` because the method is to predict class probabilities, not raw margin scores.\n\n## v0.71 (2018.04.11)\n* This is a minor release, mainly motivated by issues concerning `pip install`, e.g. #2426, #3189, #3118, and #3194.\n  With this release, users of Linux and MacOS will be able to run `pip install` for the most part.\n* Refactored linear booster class (`gblinear`), so as to support multiple coordinate descent updaters (#3103, #3134). See BREAKING CHANGES below.\n* Fix slow training for multiclass classification with high number of classes (#3109)\n* Fix a corner case in approximate quantile sketch (#3167). Applicable for 'hist' and 'gpu_hist' algorithms\n* Fix memory leak in DMatrix (#3182)\n* New functionality\n  - Better linear booster class (#3103, #3134)\n  - Pairwise SHAP interaction effects (#3043)\n  - Cox loss (#3043)\n  - AUC-PR metric for ranking task (#3172)\n  - Monotonic constraints for 'hist' algorithm (#3085)\n* GPU support\n    - Create an abstract 1D vector class that moves data seamlessly between the main and GPU memory (#2935, #3116, #3068). This eliminates unnecessary PCIe data transfer during training time.\n  - Fix minor bugs (#3051, #3217)\n  - Fix compatibility error for CUDA 9.1 (#3218)\n* Python package:\n  - Correctly handle parameter `verbose_eval=0` (#3115)\n* R package:\n  - Eliminate segmentation fault on 32-bit Windows platform (#2994)\n* JVM packages\n  - Fix a memory bug involving double-freeing Booster objects (#3005, #3011)\n  - Handle empty partition in predict (#3014)\n  - Update docs and unify terminology (#3024)\n  - Delete cache files after job finishes (#3022)\n  - Compatibility fixes for latest Spark versions (#3062, #3093)\n* BREAKING CHANGES: Updated linear modelling algorithms. In particular L1/L2 regularisation penalties are now normalised to number of training examples. This makes the implementation consistent with sklearn/glmnet. L2 regularisation has also been removed from the intercept. To produce linear models with the old regularisation behaviour, the alpha/lambda regularisation parameters can be manually scaled by dividing them by the number of training examples."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1032, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.36451268196106,656
What is the general flow the workflow entrypoint follows from rendering the configuration template to saving the trained experiment?,"The general flow of the workflow entrypoint from rendering the configuration template to saving the trained experiment in qlib is as follows:

1. The workflow entrypoint calls `render_template(config_path)`, which loads a YAML into a config dict.
2. If `BASE_CONFIG_PATH` is set, the workflow entrypoint loads and merges it via `update_config()`.
3. The workflow entrypoint calls `sys_config(config, config_path)`.
4. The workflow entrypoint initializes Qlib by calling `qlib.init` with `qlib_init` and `exp_manager`.
5. The workflow entrypoint calls `task_train(config[""task""], experiment_name)`, which returns a Recorder.
6. The workflow entrypoint calls `recorder.save_objects(config=config)`.","The `workflow` entrypoint in `qlib/workflow/cli.py` proceeds roughly as follows:

1. render_template(config_path) → load YAML into a `config` dict  
2. If `BASE_CONFIG_PATH` is set, load & merge it via update_config()  
3. sys_config(config, config_path)  
4. Initialize Qlib (qlib.init) with `qlib_init` + `exp_manager`  
5. task_train(config[""task""], experiment_name) → returns a Recorder  
6. recorder.save_objects(config=config)  

In short: render → load & merge configs → sys_setups → qlib.init → task_train → save experiment.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.render_template\nrender the template based on the environment  Parameters ---------- config_path : str     configuration path  Returns ------- str     the rendered content', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.workflow.__init__ contains CLASS qlib.qlib.workflow.__init__.QlibRecorder\nA global system that helps to manage the experiments.', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 7, 'pr': 1037, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",42.190064668655396,657
Which internal method checks whether a processor list should avoid modifying the DataFrame before processing?,"The internal method that checks whether a processor list should avoid modifying the DataFrame before processing is DataHandlerLP._is_proc_readonly() in qlib, which is defined in qlib/data/dataset/handler.py.","The check is done by the internal helper DataHandlerLP._is_proc_readonly(), defined in qlib/data/dataset/handler.py.","['CLASS qlib.qlib.workflow.task.collect.Collector has method qlib.qlib.workflow.task.collect.Collector.process_collect\nDo a series of processing to the dict returned by collect and return a dict like {key: things} For example, you can group and ensemble.  Args:     collected_dict (dict): the dict return by `collect`     process_list (list or Callable): the list of processors or the instance of a processor to process dict.         The processor order is the same as the list order.         For example: [Group1(..., Ensemble1()), Group2(..., Ensemble2())]  Returns:     dict: the dict after processing.\nThe collector to collect different results', 'CLASS qlib.qlib.data.dataset.processor.Fillna has field or attribute or property qlib.qlib.data.dataset.processor.df\nProcess NaN', 'METHOD qlib.qlib.workflow.task.collect.Collector.process_collect has parameter or accepts argument qlib.qlib.workflow.task.collect.Collector.process_collect.collected_dict\nDo a series of processing to the dict returned by collect and return a dict like {key: things} For example, you can group and ensemble.  Args:     collected_dict (dict): the dict return by `collect`     process_list (list or Callable): the list of processors or the instance of a processor to process dict.         The processor order is the same as the list order.         For example: [Group1(..., Ensemble1()), Group2(..., Ensemble2())]  Returns:     dict: the dict after processing.', 'CLASS qlib.qlib.data.inst_processor.InstProcessor has method qlib.qlib.data.inst_processor.InstProcessor.__call__\nprocess the data  NOTE: **The processor could change the content of `df` inplace !!!!! ** User should keep a copy of data outside  Parameters ---------- df : pd.DataFrame     The raw_df of handler or result from previous processor.', 'METHOD qlib.qlib.workflow.task.collect.Collector.process_collect has parameter or accepts argument qlib.qlib.workflow.task.collect.Collector.process_collect.process_list\nDo a series of processing to the dict returned by collect and return a dict like {key: things} For example, you can group and ensemble.  Args:     collected_dict (dict): the dict return by `collect`     process_list (list or Callable): the list of processors or the instance of a processor to process dict.         The processor order is the same as the list order.         For example: [Group1(..., Ensemble1()), Group2(..., Ensemble2())]  Returns:     dict: the dict after processing.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Smarter choice of histogram construction for distributed `gpu_hist` (#4519)\n* Optimizations for quantization on device (#4572)\n* Introduce caching memory allocator to avoid latency associated with GPU memory allocation (#4554, #4615)\n* Optimize the initialization stage of the CPU `hist` algorithm for sparse datasets (#4625)\n* Prevent unnecessary data copies from GPU memory to the host (#4795)\n* Improve operation efficiency for single prediction (#5016)\n* Group builder modified for incremental building, to speed up building large `DMatrix` (#5098)\n\n### Bug-fixes\n* Eliminate `FutureWarning: Series.base is deprecated` (#4337)\n* Ensure pandas DataFrame column names are treated as strings in type error message (#4481)\n* [jvm-packages] Add back `reg:linear` for scala, as it is only deprecated and not meant to be removed yet (#4490)\n* Fix library loading for Cygwin users (#4499)\n* Fix prediction from loaded pickle (#4516)\n* Enforce exclusion between `pred_interactions=True` and `pred_interactions=True` (#4522)\n* Do not return dangling reference to local `std::string` (#4543)\n* Set the appropriate device before freeing device memory (#4566)\n* Mark `SparsePageDmatrix` destructor default. (#4568)\n* Choose the appropriate tree method only when the tree method is \'auto\' (#4571)\n* Fix `benchmark_tree.py` (#4593)\n* [jvm-packages] Fix silly bug in feature scoring (#4604)\n* Fix GPU predictor when the test data matrix has different number of features than the training data matrix used to train the model (#4613)\n* Fix external memory for get column batches. (#4622)\n* [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)\n* Fix early stopping in the Python package (#4638)\n* Fix AUC error in distributed mode caused by imbalanced dataset (#4645, #4798)\n* [jvm-packages] Expose `setMissing` method in `XGBoostClassificationModel` / `XGBoostRegressionModel` (#4643)\n* Remove initializing stringstream reference. (#4788)\n* [R] `xgb.get.handle` now checks all class listed of `object` (#4800)\n* Do not use `gpu_predictor` unless data comes from GPU (#4836)\n* Fix data loading (#4862)\n* Workaround `isnan` across different environments. (#4883)\n* [jvm-packages] Handle Long-type parameter (#4885)\n* Don\'t `set_params` at the end of `set_state` (#4947). Ensure that the model does not change after pickling and unpickling multiple times.\n* C++ exceptions should not crash OpenMP loops (#4960)\n* Fix `usegpu` flag in DART. (#4984)\n* Run training with empty `DMatrix` (#4990, #5159)\n* Ensure that no two processes can use the same GPU (#4990)\n* Fix repeated split and 0 cover nodes (#5010)\n* Reset histogram hit counter between multiple data batches (#5035)\n* Fix `feature_name` crated from int64index dataframe. (#5081)\n* Don\'t use 0 for ""fresh leaf"" (#5084)\n* Throw error when user attempts to use multi-GPU training and XGBoost has not been compiled with NCCL (#5170)\n* Fix metric name loading (#5122)\n* Quick fix for memory leak in CPU `hist` algorithm (#5153)\n* Fix wrapping GPU ID and prevent data copying (#5160)\n* Fix signature of Span constructor (#5166)\n* Lazy initialization of device vector, so that XGBoost compiled with CUDA can run on a machine without any GPU (#5173)\n* Model loading should not change system locale (#5314)\n* Distributed training jobs would sometimes hang; revert Rabit to fix this regression (dmlc/rabit#132, #5237)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593).""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1044, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",40.95487594604492,658
How does the collector decide whether to extract component changes from an Excel file or from the HTML table?,"The collector decides whether to extract component changes from an Excel file or from the HTML table based on the presence of an `excel_url` in the response. If an `excel_url` is found, it calls `self._parse_excel(excel_url, …)`. If no `excel_url` is found, it calls `self._parse_table(content, …)`.","In `CSIIndex._read_change_from_url` (scripts/data_collector/cn_index/collector.py) the logic is:

1. It extracts an `excel_url` from `resp[""enclosureList""]` or by regex on the HTML body.
2. If `excel_url` is found, it calls `self._parse_excel(excel_url, …)`.  
   – If that raises a `ValueError`, it falls back to `self._parse_table(content, …)`.  
3. If no `excel_url` is found at all, it directly calls `self._parse_table(content, …)`.","['CLASS qlib.scripts.data_collector.cn_index.collector.CSIIndex has method qlib.scripts.data_collector.cn_index.collector.CSIIndex.html_table_index\nWhich table of changes in html  CSI300: 0 CSI100: 1 :return:', 'CLASS qlib.scripts.data_collector.us_index.collector.SP400Index has method qlib.scripts.data_collector.us_index.collector.SP400Index.get_changes', 'FUNCTION cognee.cognee.infrastructure.files.utils.extract_text_from_file.extract_text_from_file has parameter or accepts argument cognee.cognee.infrastructure.files.utils.extract_text_from_file.extract_text_from_file.file\nExtract text from a file based on its type.  Supports extraction from PDF and plain text file formats. For PDF files, it reads the first three pages and returns the extracted text. For plain text files, it decodes the file content and returns it as a string. It will raise an error if the file format is unsupported or if there is an issue during reading.  Parameters: -----------      - file (BinaryIO): The file stream from which to extract text.     - file_type (filetype.Type): An object that provides the type of the file, including       its extension.  Returns: --------      - str: The extracted text as a string.', 'FUNCTION cognee.cognee.infrastructure.files.utils.extract_text_from_file.extract_text_from_file has parameter or accepts argument cognee.cognee.infrastructure.files.utils.extract_text_from_file.extract_text_from_file.file_type\nExtract text from a file based on its type.  Supports extraction from PDF and plain text file formats. For PDF files, it reads the first three pages and returns the extracted text. For plain text files, it decodes the file content and returns it as a string. It will raise an error if the file format is unsupported or if there is an issue during reading.  Parameters: -----------      - file (BinaryIO): The file stream from which to extract text.     - file_type (filetype.Type): An object that provides the type of the file, including       its extension.  Returns: --------      - str: The extracted text as a string.', 'CLASS qlib.scripts.data_collector.us_index.collector.DJIAIndex has method qlib.scripts.data_collector.us_index.collector.DJIAIndex.get_changes', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 1, 'pr': 1046, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",31.414920330047607,659
How are gradient computations toggled between the forecasting and weight networks across the training and validation stages?,"In the qlib codebase, gradient computations are toggled between the forecasting and weight networks across the training and validation stages as follows:

1. During the training forecaster phase, `weight_model.parameters().requires_grad` is set to False, while `fore_model.parameters().requires_grad` is set to True.
2. Optimization is performed with `self.fore_optimizer`.
3. In the validation (weight) update phase, `fore_model.parameters().requires_grad` is set to False, while `weight_model.parameters().requires_grad` is set to True.
4. Optimization is performed with `self.weight_optimizer`. This suggests that the forecasting model's weights are not updated during the training forecaster phase, but rather only during the validation (weight) update phase, where the weight model's weights are updated instead.","In TCTS.train_epoch (qlib/contrib/model/pytorch_tcts.py) we simply flip each network’s requires_grad flag around the two phases:

• Training forecaster:  
  – weight_model.parameters().requires_grad = False  
  – fore_model.parameters().requires_grad = True  
  – optimize with self.fore_optimizer

• Validation (weight) update:  
  – fore_model.parameters().requires_grad = False  
  – weight_model.parameters().requires_grad = True  
  – optimize with self.weight_optimizer","['FUNCTION keras.keras.src.trainers.data_adapters.array_slicing.train_validation_split has parameter or accepts argument keras.keras.src.trainers.data_adapters.array_slicing.train_validation_split.arrays\nSplit arrays into train and validation subsets in deterministic order.  The last part of data will become validation data.  Args:     arrays: Tensors to split. Allowed inputs are arbitrarily nested         structures of Tensors and NumPy arrays.     validation_split: Float between 0 and 1. The proportion of the dataset         to include in the validation split. The rest of the dataset will be         included in the training split.  Returns:     `(train_arrays, validation_arrays)`', 'MODULE keras.keras.src.backend.torch.core contains CLASS keras.keras.src.backend.torch.core.CustomGradientFunction\nEnables custom forward & backward passes for gradient computation.', 'MODULE keras.keras.src.trainers.data_adapters.array_slicing contains FUNCTION keras.keras.src.trainers.data_adapters.array_slicing.train_validation_split\nSplit arrays into train and validation subsets in deterministic order.  The last part of data will become validation data.  Args:     arrays: Tensors to split. Allowed inputs are arbitrarily nested         structures of Tensors and NumPy arrays.     validation_split: Float between 0 and 1. The proportion of the dataset         to include in the validation split. The rest of the dataset will be         included in the training split.  Returns:     `(train_arrays, validation_arrays)`', 'CLASS keras.keras.src.layers.core.dense.Dense has method keras.keras.src.layers.core.dense.Dense.matmul_with_inputs_gradient\nCustom gradient function for int4 quantized weights.  Automatic differentiation will not know how to handle the int4 quantized weights. So a custom gradient function is needed to handle the int4 quantized weights.  The custom gradient function will use the dequantized kernel to compute the gradient.\nJust your regular densely-connected NN layer.  `Dense` implements the operation: `output = activation(dot(input, kernel) + bias)` where `activation` is the element-wise activation function passed as the `activation` argument, `kernel` is a weights matrix created by the layer, and `bias` is a bias vector created by the layer (only applicable if `use_bias` is `True`).  Note: If the input to the layer has a rank greater than 2, `Dense` computes the dot product between the `inputs` and the `kernel` along the last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`). For example, if input has dimensions `(batch_size, d0, d1)`, then we create a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2 of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are `batch_size * d0` such sub-tensors). The output in this case will have shape `(batch_size, d0, units)`.  Args:     units: Positive integer, dimensionality of the output space.     activation: Activation function to use.         If you don\'t specify anything, no activation is applied         (ie. ""linear"" activation: `a(x) = x`).     use_bias: Boolean, whether the layer uses a bias vector.     kernel_initializer: Initializer for the `kernel` weights matrix.     bias_initializer: Initializer for the bias vector.     kernel_regularizer: Regularizer function applied to         the `kernel` weights matrix.     bias_regularizer: Regularizer function applied to the bias vector.     activity_regularizer: Regularizer function applied to         the output of the layer (its ""activation"").     kernel_constraint: Constraint function applied to         the `kernel` weights matrix.     bias_constraint: Constraint function applied to the bias vector.     lora_rank: Optional integer. If set, the layer\'s forward pass         will implement LoRA (Low-Rank Adaptation)         with the provided rank. LoRA sets the layer\'s kernel         to non-trainable and replaces it with a delta over the         original kernel, obtained via multiplying two lower-rank         trainable matrices. This can be useful to reduce the         computation cost of fine-tuning large dense layers.         You can also enable LoRA on an existing         `Dense` layer by calling `layer.enable_lora(rank)`.     lora_alpha: Optional integer. If set, this parameter scales the         low-rank adaptation delta (computed as the product of two lower-rank         trainable matrices) during the forward pass. The delta is scaled by         `lora_alpha / lora_rank`, allowing you to fine-tune the strength of         the LoRA adjustment independently of `lora_rank`.  Input shape:     N-D tensor with shape: `(batch_size, ..., input_dim)`.     The most common situation would be     a 2D input with shape `(batch_size, input_dim)`.  Output shape:     N-D tensor with shape: `(batch_size, ..., units)`.     For instance, for a 2D input with shape `(batch_size, input_dim)`,     the output would have shape `(batch_size, units)`.', 'FUNCTION keras.keras.src.trainers.data_adapters.array_slicing.train_validation_split has parameter or accepts argument keras.keras.src.trainers.data_adapters.array_slicing.train_validation_split.validation_split\nSplit arrays into train and validation subsets in deterministic order.  The last part of data will become validation data.  Args:     arrays: Tensors to split. Allowed inputs are arbitrarily nested         structures of Tensors and NumPy arrays.     validation_split: Float between 0 and 1. The proportion of the dataset         to include in the validation split. The rest of the dataset will be         included in the training split.  Returns:     `(train_arrays, validation_arrays)`', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1047, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.17540526390076,660
"When downsampling a file-based trading calendar, how is the region determining trading hours sourced?","The region determining trading hours is sourced from the `C['region']` global config property in Qlib. If no region is passed to the `resam_calendar` function, it defaults to `C['region']`. This means that the region used for downsampling a file-based trading calendar in Qlib is determined by the global config property or the default value if not specified.","The region comes straight from QThe region is pulled from Qlib’s global config (`C[""region""]`) in `FileCalendarStorage.__init__` (and if you ever pass `region=None` to `resam_calendar` it will again default to `C[""region""]`).","['MODULE qlib.qlib.contrib.ops.high_freq contains FUNCTION qlib.qlib.contrib.ops.high_freq.get_calendar_day\nLoad High-Freq Calendar Date Using Memcache. !!!NOTE: Loading the calendar is quite slow. So loading calendar before start multiprocessing will make it faster.  Parameters ---------- freq : str     frequency of read calendar file. future : bool     whether including future trading day.  Returns ------- _calendar:     array of date.', 'MODULE qlib.scripts.data_collector.utils contains FUNCTION qlib.scripts.data_collector.utils.get_calendar_list_by_ratio\nget calendar list by selecting the date when few funds trade in this day  Parameters ---------- source_dir: str or Path     The directory where the raw data collected from the Internet is saved date_field_name: str         date field name, default is date threshold: float     threshold to exclude some days when few funds trade in this day, default 0.5 minimum_count: int     minimum count of funds should trade in one day max_workers: int     Concurrent number, default is 16  Returns -------     history calendar list', 'FUNCTION qlib.qlib.utils.resam.resam_calendar has parameter or accepts argument qlib.qlib.utils.resam.resam_calendar.calendar_raw\nResample the calendar with frequency freq_raw into the calendar with frequency freq_sam Assumption:     - Fix length (240) of the calendar in each day.  Parameters ---------- calendar_raw : np.ndarray     The calendar with frequency  freq_raw freq_raw : str     Frequency of the raw calendar freq_sam : str     Sample frequency region: str     Region, for example, ""cn"", ""us"" Returns ------- np.ndarray     The calendar with frequency freq_sam', 'FUNCTION qlib.scripts.data_collector.utils.get_calendar_list_by_ratio has parameter or accepts argument qlib.scripts.data_collector.utils.get_calendar_list_by_ratio.threshold\nget calendar list by selecting the date when few funds trade in this day  Parameters ---------- source_dir: str or Path     The directory where the raw data collected from the Internet is saved date_field_name: str         date field name, default is date threshold: float     threshold to exclude some days when few funds trade in this day, default 0.5 minimum_count: int     minimum count of funds should trade in one day max_workers: int     Concurrent number, default is 16  Returns -------     history calendar list', 'FUNCTION qlib.qlib.utils.resam.resam_calendar has parameter or accepts argument qlib.qlib.utils.resam.resam_calendar.region\nResample the calendar with frequency freq_raw into the calendar with frequency freq_sam Assumption:     - Fix length (240) of the calendar in each day.  Parameters ---------- calendar_raw : np.ndarray     The calendar with frequency  freq_raw freq_raw : str     Frequency of the raw calendar freq_sam : str     Sample frequency region: str     Region, for example, ""cn"", ""us"" Returns ------- np.ndarray     The calendar with frequency freq_sam', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md\n# HIST\n* Code: [https://github.com/Wentao-Xu/HIST](https://github.com/Wentao-Xu/HIST)\n* Paper: [HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared InformationAdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/abs/2110.13716).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 9, 'n_files_pr': 3, 'pr': 1049, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",31.27449941635132,661
How does the TRA dataset subclass implement segment slicing differently from the base handler's fetch-based approach?,"The TRA dataset subclass implements segment slicing differently from the base handler's fetch-based approach by parsing the input slice (or tuple) into start and end dates, making a shallow copy of the dataset object, copying over the raw _data attribute, copying over the raw _label attribute, copying over the raw _index attribute, filtering its precomputed batch_slices to only those whose end-dates fall within the start–end range, and filtering its daily_slices to only those whose end-dates fall within the start–end range.","In the base class (qlib/data/dataset/__init__.py:DatasetH._prepare_seg) slicing just calls handler.fetch(slc) and returns the fetched data.  

In TRA’s subclass (examples/benchmarks/TRA/src/dataset.py:MTSDatasetH._prepare_seg) it:

• Parses the input slice (or tuple) into start/end dates  
• Makes a shallow copy of the dataset object  
• Copies over the raw _data, _label, _index  
• Filters its precomputed batch_slices and daily_slices to only those whose end‐dates fall within [start, end]  
• Returns this new filtered dataset view  

I.e. TRA does in-Python date‐range filtering on its slice indices, rather than delegating to handler.fetch.","['CLASS qlib.qlib.workflow.online.update.LabelUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the label in the recorder  Assumption - The label is generated from record_temp.SignalRecord.', 'CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH has field or attribute or property qlib.examples.benchmarks.TRA.src.dataset.segments\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'CLASS qlib.qlib.workflow.online.update.PredUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the prediction in the Recorder', 'CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH has field or attribute or property qlib.examples.benchmarks.TRA.src.dataset.vals\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.RecordUpdater\nUpdate a specific recorders\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 1050, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",34.03483200073242,662
"After converting power to a pair operator, how does the reverse-power method order its operands when instantiating the operator?","The reverse-power method, implemented as `Expression.__rpow__` in qlib/data/base.py, orders its operands when instantiating the operator by passing the incoming value as the left operand (`feature_left`) and self as the right operand (`feature_right`). This is consistent with the behavior of the pair operator after converting power to a pair operator.","In Expression.__rpow__ (qlib/data/base.py) it simply does

    return Power(other, self)

i.e. it passes the incoming value as the left operand (`feature_left`) and `self` as the right one (`feature_right`).","[""CLASS qlib.qlib.data.ops.Power inherits from or is a subclass of qlib.qlib.data.ops.NpPairOperator\nNumpy Pair-wise operator  Parameters ---------- feature_left : Expression     feature instance or numeric value feature_right : Expression     feature instance or numeric value func : str     operator function  Returns ---------- Feature:     two features' operation output\nPower Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     The bases in feature_left raised to the exponents in feature_right"", 'CLASS qlib.qlib.data.ops.Power has method qlib.qlib.data.ops.Power.__init__\nPower Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     The bases in feature_left raised to the exponents in feature_right', 'MODULE qlib.qlib.data.ops contains CLASS qlib.qlib.data.ops.Power\nPower Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     The bases in feature_left raised to the exponents in feature_right', 'CLASS keras.keras.src.backend.common.variables_test.VariableOpsCorrectnessTest has method keras.keras.src.backend.common.variables_test.VariableOpsCorrectnessTest.test_variable_rpow\nTest reverse power operation on a variable.\nTests for operations on Variable.', 'CLASS qlib.qlib.data.ops.Power has field or attribute or property qlib.qlib.data.ops.feature_left\nPower Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     The bases in feature_left raised to the exponents in feature_right', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1052, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",61.49096751213074,663
Which DataFrame method offsets the unstacked predictions by the given lag before correlation?,The DataFrame method to offset unstacked predictions by the given lag before correlation in qlib is `pred_ustk.shift(lag)`. This method reverts any normalisation and gives predictions in the original scale.,The unstacked predictions are offset by calling the DataFrame’s `shift(lag)` method (i.e. `pred_ustk.shift(lag)`) before computing the correlations.,"['METHOD qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter.format_predictions has parameter or accepts argument qlib.examples.benchmarks.TFT.data_formatters.qlib_Alpha158.Alpha158Formatter.format_predictions.predictions\nReverts any normalisation to give predictions in original scale.  Args:   predictions: Dataframe of model predictions.  Returns:   Data frame of unnormalised predictions.', 'METHOD qlib.qlib.contrib.model.linear.LinearModel.predict has parameter or accepts argument qlib.qlib.contrib.model.linear.LinearModel.predict.segment', 'METHOD qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.predict has parameter or accepts argument qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.predict.df\nComputes predictions for a given input dataset.  Args:   df: Input dataframe   return_targets: Whether to also return outputs aligned with predictions to     facilitate evaluation  Returns:   Input dataframe or tuple of (input dataframe, aligned output dataframe).', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.predict_sub has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.predict_sub.df_data', 'METHOD qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.predict has parameter or accepts argument qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.predict.return_targets\nComputes predictions for a given input dataset.  Args:   df: Input dataframe   return_targets: Whether to also return outputs aligned with predictions to     facilitate evaluation  Returns:   Input dataframe or tuple of (input dataframe, aligned output dataframe).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md\n# ADD\n* Paper: [ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting](https://arxiv.org/abs/2012.06289).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1072, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",28.265854835510254,664
Which methods read existing CSV files and concatenate new pandas DataFrames before writing back to disk?,"The methods that read existing CSV files and concatenate new pandas DataFrames before writing back to disk in qlib are:

*   `BaseCollector.save_instrument` (from scripts/data_collector/base.py)
*   `YahooCollectorCN1d.download_index_data` (from scripts/data_collector/yahoo/collector.py)

These methods check if a CSV file exists, read it using `pd.read_csv`, concatenate the old and new DataFrames, and then write the concatenated DataFrame back to the CSV file using `to_csv`.","The two methods that do this are:

• BaseCollector.save_instrument (scripts/data_collector/base.py)  
  – Checks if `<symbol>.csv` exists, reads it with `pd.read_csv`, concatenates the old + new DataFrames, then `to_csv`.  

• YahooCollectorCN1d.download_index_data (scripts/data_collector/yahoo/collector.py)  
  – For each index CSV, if the file exists, loads it via `pd.read_csv`, `pd.concat([_old_df, df])`, then overwrites it with `to_csv`.","['MODULE qlib.qlib.contrib.strategy.rule_strategy contains CLASS qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy\nMotivation: - This class provides an interface for user to read orders from csv files.', 'MODULE qlib.qlib.utils.__init__ contains GLOBAL_VARIABLE qlib.qlib.utils.__init__.is_deprecated_lexsorted_pandas', 'CLASS qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy has field or attribute or property qlib.qlib.contrib.strategy.rule_strategy.trade_range\nMotivation: - This class provides an interface for user to read orders from csv files.', 'METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.merge has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.merge.df_dict', 'CLASS qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy has field or attribute or property qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.trade_range\nMotivation: - This class provides an interface for user to read orders from csv files.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 5, 'pr': 1073, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",34.07428765296936,665
Which class handles applying the configured fillna strategy to the data frames?,The class that handles applying the configured fillna strategy to data frames in qlib is `Fillna` located in `qlib/data/dataset/processor.py`. This class uses its `__call__` method to apply the fill-na strategy.,The `Fillna` processor (class Fillna in qlib/data/dataset/processor.py) is responsible for applying the configured fill-na strategy to the DataFrames via its `__call__` method.,"['MODULE qlib.examples.benchmarks.TFT.tft contains FUNCTION qlib.examples.benchmarks.TFT.tft.process_predicted\nTransform the TFT predicted data into Qlib format.  Args:   df: Original DataFrame.   fillna: New column name.  Returns:   Transformed DataFrame.', 'CLASS qlib.qlib.data.dataset.processor.Fillna inherits from or is a subclass of qlib.qlib.data.dataset.processor.Processor\nProcess NaN', 'FUNCTION qlib.examples.benchmarks.TFT.tft.process_predicted has parameter or accepts argument qlib.examples.benchmarks.TFT.tft.process_predicted.df\nTransform the TFT predicted data into Qlib format.  Args:   df: Original DataFrame.   fillna: New column name.  Returns:   Transformed DataFrame.', 'CLASS qlib.qlib.data.dataset.processor.Fillna has method qlib.qlib.data.dataset.processor.Fillna.__call__\nProcess NaN', 'FUNCTION qlib.examples.benchmarks.TFT.tft.process_predicted has parameter or accepts argument qlib.examples.benchmarks.TFT.tft.process_predicted.col_name\nTransform the TFT predicted data into Qlib format.  Args:   df: Original DataFrame.   fillna: New column name.  Returns:   Transformed DataFrame.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1074, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",27.80323314666748,666
What mechanism ensures that every agent action is validated against its action space before being interpreted for the simulator?,"The mechanism that ensures every agent action is validated against its action space before interpretation in qlib is the `validate` method of the `ActionInterpreter` class. This method checks if the action belongs to the pre-defined action space using `_gym_space_contains(self.action_space, action)`, and only then calls the `interpret` method.","Every action goes through ActionInterpreter.__call__, which is marked @final. The first thing __call__ does is call validate(action), and validate uses `_gym_space_contains(self.action_space, action)` to ensure the action lies in the declared gym.Space before calling interpret().","['CLASS qlib.qlib.rl.interpreter.ActionInterpreter has method qlib.qlib.rl.interpreter.ActionInterpreter.validate\nValidate whether an action belongs to the pre-defined action space.\nAction Interpreter that interpret rl agent action into qlib orders', 'MODULE qlib.qlib.rl.interpreter contains CLASS qlib.qlib.rl.interpreter.ActionInterpreter\nAction Interpreter that interpret rl agent action into qlib orders', 'CLASS qlib.qlib.rl.interpreter.ActionInterpreter has method qlib.qlib.rl.interpreter.ActionInterpreter.action_space\nAction Interpreter that interpret rl agent action into qlib orders', 'METHOD qlib.qlib.rl.interpreter.ActionInterpreter.validate has parameter or accepts argument qlib.qlib.rl.interpreter.ActionInterpreter.validate.action\nValidate whether an action belongs to the pre-defined action space.', 'CLASS qlib.qlib.rl.interpreter.ActionInterpreter has method qlib.qlib.rl.interpreter.ActionInterpreter.__call__\nAction Interpreter that interpret rl agent action into qlib orders', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=""#main-challenges--solutions-in-quant-research"">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=""circle""><a href=""#forecasting-finding-valuable-signalspatterns"">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=""disc""><a href=""#quant-model-paper-zoo""><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=""circle""><a href=""#run-a-single-model"">Run a Single Model</a></li>\n                    <li type=""circle""><a href=""#run-multiple-models"">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=""circle""><a href=""#adapting-to-market-dynamics"">Adapting to Market Dynamics</a></li>\n          <li type=""circle""><a href=""#reinforcement-learning-modeling-continuous-decisions"">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=""align: center"">\n<img src=""docs/_static/img/framework-abstract.jpg"" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib\'s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 35, 'pr': 1076, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.945163249969482,667
"When assigning a DataFrame under a new tuple key, how does the setter build the MultiIndex for the resulting DataFrame’s columns?","When assigning a DataFrame under a new tuple key, the setter builds the MultiIndex for the resulting DataFrame’s columns by prefixing each original column tuple with `col_name` using `pd.MultiIndex.from_tuples` with a list comprehension: `[(*col_name, *orig_idx) for orig_idx in df.columns]`. This process involves copying the assigned DataFrame and rebuilding its columns to accommodate the new key structure.","In SepDataFrame.__setitem__ (qlib/contrib/data/utils/sepdf.py), when you assign a DataFrame under a new tuple key, it does roughly:

• Split your key into `_df_dict_key, *col_name`  
• Copy the DataFrame and rebuild its columns via  
  `pd.MultiIndex.from_tuples([(*col_name, *orig_idx) for orig_idx in df.columns])`  

So each original column tuple `orig_idx` gets “prefixed” with your `col_name` tuple to form the new MultiIndex.","['CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__setitem__\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS keras.keras.src.backend.common.variables_test.VariableNumpyValueAndAssignmentTest has method keras.keras.src.backend.common.variables_test.VariableNumpyValueAndAssignmentTest.test_variable_assign\nTest assigning a new value to a variable.\ntests for Variable.numpy(), Variable.value() and Variable.assign()', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS qlib.qlib.data.dataset.handler.DataHandler has method qlib.qlib.data.dataset.handler.DataHandler.fetch\nfetch data from underlying data source  Design motivation: - providing a unified interface for underlying data. - Potential to make the interface more friendly. - User can improve performance when fetching data in this extra layer  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index     It can be categories as following      - fetch single index     - fetch a range of index          - a slice range         - pd.Index for specific indexes      Following conflicts may occur      - Does [""20200101"", ""20210101""] mean selecting this slice or these two days?          - slice have higher priorities  level : Union[str, int]     which index level to select the data  col_set : Union[str, List[str]]      - if isinstance(col_set, str):          select a set of meaningful, pd.Index columns.(e.g. features, columns)          - if col_set == CS_RAW:              the raw dataset will be returned.      - if isinstance(col_set, List[str]):          select several sets of meaningful columns, the returned data has multiple levels  proc_func: Callable      - Give a hook for processing data before fetching     - An example to explain the necessity of the hook:          - A Dataset learned some processors to process data which is related to data segmentation         - It will apply them every time when preparing data.         - The learned processor require the dataframe remains the same format when fitting and applying         - However the data format will change according to the parameters.         - So the processors should be applied to the underlayer data.  squeeze : bool     whether squeeze columns and index  Returns ------- pd.DataFrame.\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Usability Improvements, Documentation\n* [jvm-packages] add example to handle missing value other than 0 (#5677)\n* Add DMatrix usage examples to the C API demo (#5854)\n* List `DaskDeviceQuantileDMatrix` in the doc. (#5975)\n* Update Python custom objective demo. (#5981)\n* Update the JSON model schema to document more objective functions. (#5982)\n* [Python] Fix warning when `missing` field is not used. (#5969)\n* Fix typo in tracker logging (#5994)\n* Move a warning about empty dataset, so that it's shown for all objectives and metrics (#5998)\n* Fix the instructions for installing the nightly build. (#6004)\n* [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)\n* [jvm-packages] [doc] Update install doc for JVM packages (#6051)\n* Fix typo in `xgboost.callback.early_stop` docstring (#6071)\n* Add cache suffix to the files used in the external memory demo. (#6088)\n* [Doc] Document the parameter `kill_spark_context_on_worker_failure` (#6097)\n* Fix link to the demo for custom objectives (#6100)\n* Update Dask doc. (#6108)\n* Validate weights are positive values. (#6115)\n* Document the updated CMake version requirement. (#6123)\n* Add demo for `DaskDeviceQuantileDMatrix`. (#6156)\n* Cosmetic fixes in `faq.rst` (#6161)\n* Fix error message. (#6176)\n* [Doc] Add list of winning solutions in data science competitions using XGBoost (#6177)\n* Fix a comment in demo to use correct reference (#6190)\n* Update the list of winning solutions using XGBoost (#6192)\n* Consistent style for build status badge (#6203)\n* [Doc] Add info on GPU compiler (#6204)\n* Update the list of winning solutions (#6222, #6254)\n* Add link to XGBoost's Twitter handle (#6244)\n* Fix minor typos in XGBClassifier methods' docstrings (#6247)\n* Add sponsors link to FUNDING.yml (#6252)\n* Group CLI demo into subdirectory. (#6258)\n* Reduce warning messages from `gbtree`. (#6273)\n* Create a tutorial for using the C API in a C/C++ application (#6285)\n* Update plugin instructions for CMake build (#6289)\n* [doc] make Dask distributed example copy-pastable (#6345)\n* [Python] Add option to use `libxgboost.so` from the system path (#6362)\n* Fixed few grammatical mistakes in doc (#6393)\n* Fix broken link in CLI doc (#6396)\n* Improve documentation for the Dask API (#6413)\n* Revise misleading exception information: no such param of `allow_non_zero_missing` (#6418)\n* Fix CLI ranking demo. (#6439)\n* Fix broken links. (#6455)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @FelixYBW, Jack Dunn (@JackDunnNZ), Jean Lescut-Muller (@JeanLescut),  Boris Feld (@Lothiraldan), Nikhil Choudhary (@Nikhil1O1), Rory Mitchell (@RAMitchell), @ShvetsKS, Anthony D'Amato (@Totoketchup), @Wittty-Panda, neko (@akiyamaneko), Alexander Gugel (@alexanderGugel), @dependabot[bot], DIVYA CHAUHAN (@divya661), Daniel Steinberg (@dstein64), Akira Funahashi (@funasoul), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Hristo Iliev (@hiliev), Honza Sterba (@honzasterba), @hzy001, Igor Moura (@igormp), @jameskrach, James Lamb (@jameslamb), Naveed Ahmed Saleem Janvekar (@janvekarnaveed), Kyle Nicholson (@kylejn27), lacrosse91 (@lacrosse91), Christian Lorentzen (@lorentzenchr), Manikya Bardhan (@manikyabard), @nabokovas, John Quitto-Graham (@nvidia-johnq), @odidev, Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Tanuja Kirthi Doddapaneni (@tanuja3), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), Zeno Gantner (@zenogantner), zhang_jf (@zuston)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 1080, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.731507778167725,668
How does get_instruments dynamically select and execute the appropriate collector class and method for a given market index?,"The `get_instruments` function dynamically selects and executes the appropriate collector class and method for a given market index by using string names at runtime. Here's how it works: The function uses `importlib.import_module` to import the selected collector module from the `data_collector.{market_index}.collector` package, then retrieves a class named `f""{index_name.upper()}Index""` from the imported module using `getattr`. It instantiates the retrieved class and calls `getattr(obj, method)()` on it to invoke a method on the object. This approach allows the function to dynamically select and execute the appropriate collector class and method for a given market index without requiring explicit code changes.","In scripts/data_collector/utils.py, get_instruments does three things:

1. importlib.import_module(""data_collector.{}.collector"".format(market_index))  
2. getattr on that module for the class named f""{index_name.upper()}Index"" (e.g. IBOVIndex or CSI300Index) and instantiate it  
3. getattr(obj, method)() to call the requested method (e.g. parse_instruments, save_new_companies)  

So it picks the right collector module, class and method entirely by string names at runtime.","['CLASS qlib.qlib.workflow.task.collect.MergeCollector has method qlib.qlib.workflow.task.collect.MergeCollector.__init__\nInit MergeCollector.  Args:     collector_dict (Dict[str,Collector]): the dict like {collector_key, Collector}     process_list (List[Callable]): the list of processors or the instance of processor to process dict.     merge_func (Callable): a method to generate outermost key. The given params are ``collector_key`` from collector_dict and ``key`` from every collector after collecting.         None for using tuple to connect them, such as ""ABC""+(""a"",""b"") -> (""ABC"", (""a"",""b"")).\nA collector to collect the results of other Collectors  For example:      We have 2 collector, which named A and B.     A can collect {""prediction"": pd.Series} and B can collect {""IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}.     Then after this class\'s collect, we can collect {""A_prediction"": pd.Series, ""B_IC"": {""Xgboost"": pd.Series, ""LSTM"": pd.Series}}      ...', 'FUNCTION qlib.scripts.data_collector.utils.get_instruments has parameter or accepts argument qlib.scripts.data_collector.utils.get_instruments.market_index\nParameters ---------- qlib_dir: str     qlib data dir, default ""Path(__file__).parent/qlib_data"" index_name: str     index name, value from [""csi100"", ""csi300""] method: str     method, value from [""parse_instruments"", ""save_new_companies""] freq: str     freq, value from [""day"", ""1min""] request_retry: int     request retry, by default 5 retry_sleep: int     request sleep, by default 3 market_index: str     Where the files to obtain the index are located,     for example data_collector.cn_index.collector  Examples -------     # parse instruments     $ python collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments      # parse new companies     $ python collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies', 'METHOD qlib.qlib.data.dataset.handler.DataHandler.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandler.fetch.proc_func\nfetch data from underlying data source  Design motivation: - providing a unified interface for underlying data. - Potential to make the interface more friendly. - User can improve performance when fetching data in this extra layer  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index     It can be categories as following      - fetch single index     - fetch a range of index          - a slice range         - pd.Index for specific indexes      Following conflicts may occur      - Does [""20200101"", ""20210101""] mean selecting this slice or these two days?          - slice have higher priorities  level : Union[str, int]     which index level to select the data  col_set : Union[str, List[str]]      - if isinstance(col_set, str):          select a set of meaningful, pd.Index columns.(e.g. features, columns)          - if col_set == CS_RAW:              the raw dataset will be returned.      - if isinstance(col_set, List[str]):          select several sets of meaningful columns, the returned data has multiple levels  proc_func: Callable      - Give a hook for processing data before fetching     - An example to explain the necessity of the hook:          - A Dataset learned some processors to process data which is related to data segmentation         - It will apply them every time when preparing data.         - The learned processor require the dataframe remains the same format when fitting and applying         - However the data format will change according to the parameters.         - So the processors should be applied to the underlayer data.  squeeze : bool     whether squeeze columns and index  Returns ------- pd.DataFrame.', 'CLASS qlib.scripts.data_collector.fund.collector.FundollectorCN has method qlib.scripts.data_collector.fund.collector.FundollectorCN.get_instrument_list', 'METHOD qlib.qlib.data.dataset.handler.DataHandler.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandler.fetch.level\nfetch data from underlying data source  Design motivation: - providing a unified interface for underlying data. - Potential to make the interface more friendly. - User can improve performance when fetching data in this extra layer  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index     It can be categories as following      - fetch single index     - fetch a range of index          - a slice range         - pd.Index for specific indexes      Following conflicts may occur      - Does [""20200101"", ""20210101""] mean selecting this slice or these two days?          - slice have higher priorities  level : Union[str, int]     which index level to select the data  col_set : Union[str, List[str]]      - if isinstance(col_set, str):          select a set of meaningful, pd.Index columns.(e.g. features, columns)          - if col_set == CS_RAW:              the raw dataset will be returned.      - if isinstance(col_set, List[str]):          select several sets of meaningful columns, the returned data has multiple levels  proc_func: Callable      - Give a hook for processing data before fetching     - An example to explain the necessity of the hook:          - A Dataset learned some processors to process data which is related to data segmentation         - It will apply them every time when preparing data.         - The learned processor require the dataframe remains the same format when fitting and applying         - However the data format will change according to the parameters.         - So the processors should be applied to the underlayer data.  squeeze : bool     whether squeeze columns and index  Returns ------- pd.DataFrame.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 7, 'n_files_pr': 5, 'pr': 1081, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.51205325126648,669
"When all sub-dataframes are deleted, how is the primary join key recalculated?","When all sub-dataframes are deleted, the primary join key is recalculated by assigning `self.join = None` in qlib. This means that after deleting all keys from `_df_dict`, the _update_join method assigns `None` to `self.join`, indicating that there is no longer a primary join key.","The magic happens in SepDataFrame.__delitem__ → SepDataFrame._update_join (qlib/contrib/data/utils/sepdf.py). After you delete a key, `_update_join` checks:

• If there are still any keys left, it does  
```python
self.join = next(iter(self._df_dict.keys()))
```  
i.e. picks the first remaining dataframe’s key as the new join.

• If `_df_dict` is now empty, it sets  
```python
self.join = None
```","['METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__ has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__.join\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index', 'METHOD qlib.qlib.data.dataset.loader.StaticDataLoader.__init__ has parameter or accepts argument qlib.qlib.data.dataset.loader.StaticDataLoader.__init__.join\nParameters ---------- config : dict     {fields_group: <path or object>} join : str     How to align different dataframes', 'METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__ has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__.skip_align\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index', 'METHOD qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__ has parameter or accepts argument qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__.df_dict\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index', 'METHOD qlib.qlib.data.dataset.loader.StaticDataLoader.__init__ has parameter or accepts argument qlib.qlib.data.dataset.loader.StaticDataLoader.__init__.config\nParameters ---------- config : dict     {fields_group: <path or object>} join : str     How to align different dataframes', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 1082, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",30.151559829711914,670
"Describe the sequence of data-fetching, normalization, and dumping operations performed in the TestPIT class setup to prepare PIT data for feature queries.","The sequence of operations performed in the TestPIT class setup to prepare PIT data for feature queries is as follows:

1. Fetch Qlib market data from the specified directory.
2. Fetch PIT data from the specified directory.
3. Normalize the PIT CSVs using the `normalize_data()` method of the `Run` class.
4. Dump the normalized PIT data into Qlib format using the `dump()` method of the `DumpPitData` class.","In setUpClass (tests/test_pit.py):

1. Fetch Qlib market data  
   • GetData().qlib_data(name=""qlib_data_simple"", target_dir=cn_data_dir, region=""cn"")  
   • GetData().qlib_data(name=""qlib_data"",       target_dir=pit_dir,    region=""pit"")  

2. Normalize PIT CSVs  
   • Run(source_dir=pit_dir, normalize_dir=pit_normalized_dir, interval=""quarterly"").normalize_data()  

3. Dump normalized PIT into Qlib format  
   • DumpPitData(csv_path=pit_normalized_dir, qlib_dir=cn_data_dir).dump(interval=""quarterly"")","[""METHOD qlib.qlib.data.dataset.__init__.DatasetH.prepare has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.prepare.data_key\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:"", 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize has method qlib.scripts.data_collector.pit.collector.PitNormalize.normalize', ""METHOD qlib.qlib.data.dataset.__init__.DatasetH.prepare has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.prepare.segments\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:"", 'MODULE qlib.qlib.data.data contains CLASS qlib.qlib.data.data.PITProvider', ""METHOD qlib.qlib.data.dataset.__init__.DatasetH.prepare has parameter or accepts argument qlib.qlib.data.dataset.__init__.DatasetH.prepare.col_set\nPrepare the data for learning and inference.  Parameters ---------- segments : Union[List[Text], Tuple[Text], Text, slice]     Describe the scope of the data to be prepared     Here are some examples:      - 'train'      - ['train', 'valid']  col_set : str     The col_set will be passed to self.handler when fetching data.     TODO: make it automatic:      - select DK_I for test data     - select DK_L for training data. data_key : str     The data to fetch:  DK_*     Default is DK_I, which indicate fetching data for **inference**.  kwargs :     The parameters that kwargs may contain:         flt_col : str             It only exists in TSDatasetH, can be used to add a column of data(True or False) to filter data.             This parameter is only supported when it is an instance of TSDatasetH.  Returns ------- Union[List[pd.DataFrame], pd.DataFrame]:  Raises ------ NotImplementedError:"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 1089, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",45.85720252990723,671
How is the annualized return calculated when using geometric accumulation?,"When using geometric accumulation in qlib, the annualized return is calculated as follows: cumulative_return = (1 + r).cumprod().iloc[-1] - 1, and then annualized_return = (1 + cumulative_return)^(N/len(r)) - 1, where N denotes the annualization factor.","In  qlib/contrib/evaluate.py’s risk_analysis (when mode=""product""), once you have the total compounded return 

    cumulative_return = (1+r).cumprod().iloc[-1] – 1

the annualized return is computed as

    annualized_return = (1 + cumulative_return)^(N/len(r)) – 1

where N is your annualization factor (e.g. 252 for daily).","['FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.r\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'MODULE qlib.qlib.contrib.evaluate contains FUNCTION qlib.qlib.contrib.evaluate.risk_analysis\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.mode\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.N\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'MODULE qlib.qlib.contrib.evaluate_portfolio contains FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_annual_return_from_positions\nAnnualized Returns  p_r = (p_end / p_start)^{(250/n)} - 1  p_r     annual return p_end   final value p_start init value n       days of backtest', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n# Benchmarks Performance\nThis page lists a batch of methods designed for alpha seeking. Each method tries to give scores/predictions for all stocks each day(e.g. forecasting the future excess return of stocks). The scores/predictions of the models will be used as the mined alpha. Investing in stocks with higher scores is expected to yield more profit.  \n\nThe alpha is evaluated in two ways.\n1. The correlation between the alpha and future return.\n1. Constructing portfolio based on the alpha and evaluating the final total return.\n   - The explanation of metrics can be found [here](https://qlib.readthedocs.io/en/latest/component/report.html#id4)\n\nHere are the results of each benchmark model running on Qlib's `Alpha360` and `Alpha158` dataset with China's A shared-stock & CSI300 data respectively. The values of each metric are the mean and std calculated based on 20 runs with different random seeds.\n\nThe numbers shown below demonstrate the performance of the entire `workflow` of each model. We will update the `workflow` as well as models in the near future for better results.\n<!-- \n> If you need to reproduce the results below, please use the **v1** dataset: `python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --version v1`\n>\n> In the new version of qlib, the default dataset is **v2**. Since the data is collected from the YahooFinance API (which is not very stable), the results of *v2* and *v1* may differ -->\n\n> NOTE:\n> The backtest start from 0.8.0 is quite different from previous version. Please check out the changelog for the difference.\n\n> NOTE:\n> We have very limited resources to implement and finetune the models. We tried our best effort to fairly compare these models.  But some models may have greater potential than what it looks like in the table below.  Your contribution is highly welcomed to explore their potential.\n\n## Results on CSI300\n\n### Alpha158 dataset\n\n| Model Name                               | Dataset                             | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------------------------------------|-------------------------------------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| TCN(Shaojie Bai, et al.)                 | Alpha158                            | 0.0279±0.00 | 0.2181±0.01 | 0.0421±0.00 | 0.3429±0.01 | 0.0262±0.02       | 0.4133±0.25       | -0.1090±0.03 |\n| TabNet(Sercan O. Arik, et al.)           | Alpha158                            | 0.0204±0.01 | 0.1554±0.07 | 0.0333±0.00 | 0.2552±0.05 | 0.0227±0.04       | 0.3676±0.54       | -0.1089±0.08 |\n| Transformer(Ashish Vaswani, et al.)      | Alpha158                            | 0.0264±0.00 | 0.2053±0.02 | 0.0407±0.00 | 0.3273±0.02 | 0.0273±0.02       | 0.3970±0.26       | -0.1101±0.02 |\n| GRU(Kyunghyun Cho, et al.)               | Alpha158(with selected 20 features) | 0.0315±0.00 | 0.2450±0.04 | 0.0428±0.00 | 0.3440±0.03 | 0.0344±0.02       | 0.5160±0.25       | -0.1017±0.02 |\n| LSTM(Sepp Hochreiter, et al.)            | Alpha158(with selected 20 features) | 0.0318±0.00 | 0.2367±0.04 | 0.0435±0.00 | 0.3389±0.03 | 0.0381±0.03       | 0.5561±0.46       | -0.1207±0.04 |\n| Localformer(Juyong Jiang, et al.)        | Alpha158                            | 0.0356±0.00 | 0.2756±0.03 | 0.0468±0.00 | 0.3784±0.03 | 0.0438±0.02       | 0.6600±0.33       | -0.0952±0.02 |""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1090, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.69658041000366,672
How does the updater determine the time window for the test segment and load the dataset accordingly?,"The updater determines the time window for the test segment by setting `start_time` to the result of calling `get_date_by_shift(last_end, +1)` and `to_date` as the user-supplied value or `dataset.step_len - 1`. The dataset spanning from the historical buffer through the test window [last_end+1, to_date] is loaded by calling `self.rmdl.get_dataset` with parameters `start_time=start_time_buffer`, `end_time=self.to_date`, and `segments=seg`. This process involves computing `hist_ref` as either the user-supplied value or `dataset.step_len - 1`, setting `start_time_buffer` by calling `get_date_by_shift(last_end, −hist_ref+1)`, and defining a segment dictionary `seg` with the key ""test"" mapping to the tuple `(start_time, to_date)`.","In qlib/workflow/online/update.py’s DSBasedUpdater.prepare_data, the updater:

1. Computes hist_ref (either the user‐supplied value or `dataset.step_len-1` for a TSDatasetH).  
2. Sets  
   • start_time_buffer = get_date_by_shift(last_end, -hist_ref+1)  
   • start_time        = get_date_by_shift(last_end, +1)  
3. Defines the test segment as  
   seg = { ""test"": (start_time, to_date) }  
4. Calls  
   `self.rmdl.get_dataset(start_time=start_time_buffer, end_time=self.to_date, segments=seg, …)`  

This loads the dataset spanning from the historical buffer up through the test window [last_end+1, to_date].","[""CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.__init__\nInit PredUpdater.  Expected behavior in following cases:  - if `to_date` is greater than the max date in the calendar, the data will be updated to the latest date - if there are data before `from_date` or after `to_date`, only the data between `from_date` and `to_date` are affected.  Args:     record : Recorder     to_date :         update to prediction to the `to_date`          if to_date is None:              data will updated to the latest date.     from_date :         the update will start from `from_date`          if from_date is None:              the updating will occur on the next tick after the latest data in historical data     hist_ref : int         Sometimes, the dataset will have historical depends.         Leave the problem to users to set the length of historical dependency         If user doesn't specify this parameter, Updater will try to load dataset to automatically determine the hist_ref          .. note::              the start_time is not included in the `hist_ref`; So the `hist_ref` will be `step_len - 1` in most cases      loader_cls : type         the class to load the model and dataset\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321"", ""METHOD qlib.qlib.workflow.online.update.RMDLoader.get_dataset has parameter or accepts argument qlib.qlib.workflow.online.update.RMDLoader.get_dataset.segments\nLoad, config and setup dataset.  This dataset is for inference.  Args:     start_time :         the start_time of underlying data     end_time :         the end_time of underlying data     segments : dict         the segments config for dataset         Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time     unprepared_dataset: Optional[DatasetH]         if user don't want to load dataset from recorder, please specify user's dataset  Returns:     DatasetH: the instance of DatasetH"", ""CLASS qlib.qlib.workflow.online.update.RMDLoader has method qlib.qlib.workflow.online.update.RMDLoader.get_dataset\nLoad, config and setup dataset.  This dataset is for inference.  Args:     start_time :         the start_time of underlying data     end_time :         the end_time of underlying data     segments : dict         the segments config for dataset         Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time     unprepared_dataset: Optional[DatasetH]         if user don't want to load dataset from recorder, please specify user's dataset  Returns:     DatasetH: the instance of DatasetH\nRecorder Model Dataset Loader"", 'CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has method qlib.qlib.model.meta.dataset.MetaTaskDataset.prepare_tasks\nPrepare the data in each meta-task and ready for training.  The following code example shows how to retrieve a list of meta-tasks from the `meta_dataset`:      .. code-block:: Python          # get the train segment and the test segment, both of them are lists         train_meta_tasks, test_meta_tasks = meta_dataset.prepare_tasks([""train"", ""test""])  Parameters ---------- segments: Union[List[Text], Tuple[Text], Text]     the info to select data  Returns ------- list:     A list of the prepared data of each meta-task for training the meta-model. For multiple segments [seg1, seg2, ... , segN], the returned list will be [[tasks in seg1], [tasks in seg2], ... , [tasks in segN]].     Each task is a meta task\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', ""METHOD qlib.qlib.workflow.online.update.RMDLoader.get_dataset has parameter or accepts argument qlib.qlib.workflow.online.update.RMDLoader.get_dataset.unprepared_dataset\nLoad, config and setup dataset.  This dataset is for inference.  Args:     start_time :         the start_time of underlying data     end_time :         the end_time of underlying data     segments : dict         the segments config for dataset         Due to the time series dataset (TSDatasetH), the test segments maybe different from start_time and end_time     unprepared_dataset: Optional[DatasetH]         if user don't want to load dataset from recorder, please specify user's dataset  Returns:     DatasetH: the instance of DatasetH"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1096, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.68151140213013,673
How does the grouping-aware DataFrame respond when both its group labels are deleted?,"When both group keys are deleted from a SepDataFrame, the internal _df_dict becomes empty. An empty internal _df_dict corresponds to an empty SepDataFrame. Therefore, deleting both group keys does not cause an error and results in an empty SepDataFrame.","The `SepDataFrame` simply removes each group key from its internal `_df_dict` via its `__delitem__` method, and when you `del sdf['g1']` and `del sdf['g2']` it doesn’t error out—it just ends up with an empty `_df_dict` (i.e. an empty SepDataFrame).","['CLASS qlib.qlib.data.storage.storage.InstrumentStorage has method qlib.qlib.data.storage.storage.InstrumentStorage.update\nD.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.  Notes ------     If E present and has a .keys() method, does:     for k in E: D[k] = E[k]      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v      In either case, this is followed by: for k, v in F.items(): D[k] = v', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__delitem__\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'MODULE qlib.qlib.__init__ contains FUNCTION qlib.qlib.__init__.get_project_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.loc\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'FUNCTION qlib.qlib.__init__.get_project_path has parameter or accepts argument qlib.qlib.__init__.get_project_path.cur_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1103, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.940434455871582,674
"When a versioned dataset URL returns a 404, how does the code adjust the filename before downloading?","When a versioned dataset URL returns a 404, the code adjusts the filename by swapping in 'latest' instead of using the real qlib.__version__ when regenerating the file name. The regenerated file name has the suffix '_latest.zip' before calling download_data.","In GetData.qlib_data (qlib/tests/data.py), after building the “vX/…_<qlib_version>.zip” name it calls check_dataset(), and if that returns 404 it does:

```python
# instead of using the real qlib.__version__, swap in “latest”
file_name = _get_file_name_with_version(""latest"", dataset_version=version)
```

i.e. it regenerates the path so the suffix becomes `_latest.zip` before calling download_data.","['CLASS qlib.qlib.tests.data.GetData has method qlib.qlib.tests.data.GetData.qlib_data\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'MODULE qlib.qlib.contrib.data.dataset contains GLOBAL_VARIABLE qlib.qlib.contrib.data.dataset.x', 'METHOD qlib.qlib.tests.data.GetData.qlib_data has parameter or accepts argument qlib.qlib.tests.data.GetData.qlib_data.version\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'MODULE qlib.qlib.config contains GLOBAL_VARIABLE qlib.qlib.config.SIMPLE_DATASET_CACHE', 'METHOD qlib.qlib.tests.data.GetData.qlib_data has parameter or accepts argument qlib.qlib.tests.data.GetData.qlib_data.region\ndownload cn qlib data from remote  Parameters ---------- target_dir: str     data save directory name: str     dataset name, value from [qlib_data, qlib_data_simple], by default qlib_data version: str     data version, value from [v1, ...], by default None(use script to specify version) interval: str     data freq, value from [1d], by default 1d region: str     data region, value from [cn, us], by default cn delete_old: bool     delete an existing directory, by default True exists_skip: bool     exists skip, by default False  Examples --------- # get 1d data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}  # get 1min data python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token} -------', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': True, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1109, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",32.207314014434814,675
How does the rolling strategy determine which prediction to keep when there are duplicate datetime entries?,"The rolling strategy in qlib determines which prediction to keep when there are duplicate datetime entries by using the `duplicated` function from pandas, which returns a boolean mask denoting duplicate rows. The `keep=""last""` argument is used to keep only the last entry for each duplicated datetime index. This means that if there are multiple predictions with the same datetime index, the rolling strategy will keep the prediction from the most recently-sorted artifact, which corresponds to the last entry in the sorted list of predictions.","In RollingEnsemble (qlib/model/ens/ensemble.py), after concatenating all prediction DataFrames (sorted by their earliest datetime), it calls

```python
artifact = artifact[~artifact.index.duplicated(keep=""last"")]
```

so for any duplicated datetime index it drops earlier entries and keeps the last one (i.e. the prediction from the most recently‐sorted artifact).","['METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.rtype\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', 'MODULE qlib.qlib.contrib.report.analysis_position.cumulative_return contains GLOBAL_VARIABLE qlib.qlib.contrib.report.analysis_position.cumulative_return.date', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.seg\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', 'CLASS qlib.qlib.data.ops.Cov has method qlib.qlib.data.ops.Cov.__init__\nRolling Covariance  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance N : int     rolling window size  Returns ---------- Expression     a feature instance with rolling max of two input features', 'METHOD qlib.qlib.workflow.task.utils.TimeAdjuster.shift has parameter or accepts argument qlib.qlib.workflow.task.utils.TimeAdjuster.shift.step\nShift the datetime of segment  If there are None (which indicates unbounded index) in the segment, this method will return None.  Parameters ---------- seg :     datetime segment step : int     rolling step rtype : str     rolling type (""sliding"" or ""expanding"")  Returns -------- tuple: new segment  Raises ------ KeyError:     shift will raise error if the index(both start and end) is out of self.cal', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/baseline/README.md\n# Introduction\n\nThis is the framework of periodically Rolling Retrain (RR) forecasting models. RR adapts to market dynamics by utilizing the up-to-date data periodically.\n\n## Run the Code\nUsers can try RR by running the following command:\n```bash\n    python rolling_benchmark.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `model_type` parameter.\nFor example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python rolling_benchmark.py --conf_path=workflow_config_lightgbm_Alpha158.yaml run\n\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1113, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.57685899734497,676
Describe how TrainerR.train orchestrates running tasks in subprocesses to release memory.,"TrainerR.train orchestrates running tasks in subprocesses to release memory by using the `call_in_subproc` parameter when constructing TrainerR. When `call_in_subproc` is set to True, TrainerR wraps the train function with `call_in_subproc`, which forks a fresh Python process and re-imports the config. The original train function is then run inside the subprocess, and the Recorder result is returned from the subprocess. This approach allows all memory to be reclaimed by the OS when the subprocess exits.","In qlib/model/trainer.py’s TrainerR.train, you can enable subprocess‐based execution by setting `call_in_subproc=True` when you construct your TrainerR. Inside the task loop, TrainerR does roughly:

1. if `self._call_in_subproc` is true, it wraps your `train_func` with  
   `train_func = call_in_subproc(train_func, C)`  
   (where `C` is the global config from qlib/config.py)
2. it then calls `train_func(task, …)` as usual.

`call_in_subproc` (in qlib/utils/parallel.py) forks a fresh Python process, re‐imports the config `C`, runs your original `train_func` there, returns its result (the Recorder), and exits the subprocess. By scoping each training run to its own process, all memory is reclaimed by the OS as soon as that child exits.","['CLASS qlib.qlib.model.trainer.TrainerR has method qlib.qlib.model.trainer.TrainerR.__init__\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release\nTrainer based on (R)ecorder. It will train a list of tasks and return a list of model recorders in a linear way.  Assumption: models were defined by `task` and the results will be saved to `Recorder`.', 'FUNCTION qlib.qlib.workflow.task.manage.run_task has parameter or accepts argument qlib.qlib.workflow.task.manage.run_task.task_func\nWhile the task pool is not empty (has WAITING tasks), use task_func to fetch and run tasks in task_pool  After running this method, here are 4 situations (before_status -> after_status):      STATUS_WAITING -> STATUS_DONE: use task[""def""] as `task_func` param, it means that the task has not been started      STATUS_WAITING -> STATUS_PART_DONE: use task[""def""] as `task_func` param      STATUS_PART_DONE -> STATUS_PART_DONE: use task[""res""] as `task_func` param, it means that the task has been started but not completed      STATUS_PART_DONE -> STATUS_DONE: use task[""res""] as `task_func` param  Parameters ---------- task_func : Callable     def (task_def, \\**kwargs) -> <res which will be committed>      the function to run the task task_pool : str     the name of the task pool (Collection in MongoDB) query: dict     will use this dict to query task_pool when fetching task force_release : bool     will the program force to release the resource before_status : str:     the tasks in before_status will be fetched and trained. Can be STATUS_WAITING, STATUS_PART_DONE. after_status : str:     the tasks after trained will become after_status. Can be STATUS_WAITING, STATUS_PART_DONE. kwargs     the params for `task_func`', 'METHOD qlib.qlib.model.trainer.TrainerR.__init__ has parameter or accepts argument qlib.qlib.model.trainer.TrainerR.__init__.call_in_subproc\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release', 'FUNCTION qlib.qlib.workflow.task.manage.run_task has parameter or accepts argument qlib.qlib.workflow.task.manage.run_task.task_pool\nWhile the task pool is not empty (has WAITING tasks), use task_func to fetch and run tasks in task_pool  After running this method, here are 4 situations (before_status -> after_status):      STATUS_WAITING -> STATUS_DONE: use task[""def""] as `task_func` param, it means that the task has not been started      STATUS_WAITING -> STATUS_PART_DONE: use task[""def""] as `task_func` param      STATUS_PART_DONE -> STATUS_PART_DONE: use task[""res""] as `task_func` param, it means that the task has been started but not completed      STATUS_PART_DONE -> STATUS_DONE: use task[""res""] as `task_func` param  Parameters ---------- task_func : Callable     def (task_def, \\**kwargs) -> <res which will be committed>      the function to run the task task_pool : str     the name of the task pool (Collection in MongoDB) query: dict     will use this dict to query task_pool when fetching task force_release : bool     will the program force to release the resource before_status : str:     the tasks in before_status will be fetched and trained. Can be STATUS_WAITING, STATUS_PART_DONE. after_status : str:     the tasks after trained will become after_status. Can be STATUS_WAITING, STATUS_PART_DONE. kwargs     the params for `task_func`', 'METHOD qlib.qlib.model.trainer.TrainerR.__init__ has parameter or accepts argument qlib.qlib.model.trainer.TrainerR.__init__.train_func\nInit TrainerR.  Args:     experiment_name (str, optional): the default name of experiment.     train_func (Callable, optional): default training method. Defaults to `task_train`.     call_in_subproc (bool): call the process in subprocess to force memory release', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 5, 'pr': 1116, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",35.26405739784241,677
What is the overall sequence of steps performed in the history-based PyTorch model's fit method?,"The overall sequence of steps performed in the HIST.fit method is as follows:

1. Calls `dataset.prepare(","In HIST.fit (qlib/contrib/model/pytorch_hist.py) the method:

1. Calls dataset.prepare(“train”, “valid”, “test”) and checks for empty splits.  
2. Ensures the stock2concept file exists (downloads if not), loads stock_index and maps it onto the train/valid DataFrames.  
3. Extracts x_train/y_train/stock_index_train and x_valid/y_valid/stock_index_valid.  
4. Creates the output folder and initializes early‐stop vars (`best_score`, `stop_steps`, `evals_result`).  
5. Instantiates the base model (LSTM or GRU), loads `self.model_path` if given, then merges its state dict into `self.HIST_model`.  
6. Enters the epoch loop for `self.n_epochs`:  
   a. Calls `train_epoch(...)` on the training set.  
   b. Calls `test_epoch(...)` on train and valid sets, appends scores to `evals_result`.  
   c. Updates `best_param` on validation improvement or increments `stop_steps` and breaks if `stop_steps >= early_stop`.  
7. After training, reloads `best_param` into `HIST_model` and saves it to disk.","['METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.step\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'METHOD qlib.qlib.contrib.model.pytorch_hist.HIST.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_hist.HIST.fit.save_path', 'METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.task_tpl\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'METHOD qlib.qlib.contrib.model.pytorch_hist.HIST.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_hist.HIST.stock_index', 'METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.trunc_days\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 10, 'n_files_pr': 11, 'pr': 1119, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.02871513366699,678
How do the feature dumping script and file storage mixin each handle and report errors during execution?,"The feature dumping script and FileStorageMixin in qlib handle errors during execution by raising informative ValueError exceptions and logging error messages when necessary. The script uses a try-except block to catch exceptions, captures tracebacks using `traceback.format_exc()`, maps them to instrument codes, and logs all errors after processing. The FileStorageMixin checks for supported frequencies, raises ValueErrors for unsupported frequencies or non-existent storage, and logs clear error messages. Both components prioritize informative exception handling and logging.","Feature dumping (scripts/dump_bin.py → DumpDataUpdate._dump_features):

• Tasks are submitted to a ProcessPoolExecutor and collected as futures.  
• In the as_completed loop, each future.result() is wrapped in try/except.  
• On exception, it captures traceback.format_exc(), maps it to the instrument code in an error_code dict, and continues.  
• At the end it logs all errors via logger.info(f""dump bin errors: {error_code}"").

File storage mixin (qlib/data/storage/file_storage.py → FileStorageMixin):

• The uri property checks if the requested freq is supported; if not, it immediately raises ValueError(f""{storage_name}: … does not contain data for {freq}"").  
• The check() method calls uri.exists(), and if False, raises ValueError(f""{storage_name} not exists: {uri}"").  
• All errors are surfaced as ValueError with clear, self-descriptive messages.","['CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.reset\nreset freq and report of account  Parameters ---------- freq : str, optional     frequency of account & report, by default None benchmark_config : {}, optional     benchmark config of report, by default None port_metr_enabled: bool\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'CLASS qlib.scripts.dump_bin.DumpDataAll has method qlib.scripts.dump_bin.DumpDataAll.dump', 'MODULE cognee.cognee.infrastructure.databases.relational.get_migration_relational_engine contains FUNCTION cognee.cognee.infrastructure.databases.relational.get_migration_relational_engine.get_migration_relational_engine\nCreate and return a relational database engine using the migration configuration.  This function fetches the migration configuration details and uses them to create a relational database engine. It does not explicitly handle exceptions but may propagate any errors raised in the underlying functions.  Returns: --------      A relational database engine instance configured with the migration settings.', 'MODULE qlib.scripts.dump_bin contains CLASS qlib.scripts.dump_bin.DumpDataAll', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.end_index\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 6, 'pr': 1123, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.115349769592285,679
What mechanism does the FiniteVectorEnv use to stop the collector when all data from the shared queue is exhausted?,"The FiniteVectorEnv uses a mechanism where it sets its `_zombie` attribute to True and raises a StopIteration exception when its `_alive_env_ids` becomes empty in its `reset()` method. This exception is caught by the `collector_guard()` function, which cleanly stops collection when all data from the shared queue is exhausted.","FiniteVectorEnv keeps track of “alive” workers in its `_alive_env_ids`, and in `reset()` (qlib/rl/utils/finite_env.py) when that set becomes empty it sets `_zombie=True` and raises a `StopIteration`. The `collector_guard()` around the collector then catches that exception and cleanly stops collection.","['CLASS qlib.qlib.rl.utils.finite_env.FiniteVectorEnv has method qlib.qlib.rl.utils.finite_env.FiniteVectorEnv.collector_guard\nGuard the collector. Recommended to guard every collect.  This guard is for two purposes.  1. Catch and ignore the StopIteration exception, which is the stopping signal    thrown by FiniteEnv to let tianshou know that ``collector.collect()`` should exit. 2. Notify the loggers that the collect is ready / done what it\'s ready / done.  Examples -------- >>> with finite_env.collector_guard(): ...     collector.collect(n_episode=INF)\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteShmemVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend.old_qlib_data_dir\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteSubprocVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 17}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 17, 'n_files_pr': 14, 'pr': 1125, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.26252031326294,680
"Which helper functions are used by the run method to retrieve configuration files, strip out seed parameters, and create the conda environment before model execution?","The run method uses the following helper functions to retrieve configuration files, strip out seed parameters, and create the conda environment before model execution: get_all_files (to fetch .yaml and .txt paths), gen_yaml_file_without_seed_kwargs (to strip out seed parameters from YAML), and create_env (to spin up a temporary conda environment). These helper functions are located in the examples/run_all_model.py module within qlib.","The `run` method uses three helpers:

• get_all_files (examples/run_all_model.py) – to fetch the .yaml and .txt paths for a given dataset  
• create_env (same module) – to spin up a temporary conda environment  
• gen_yaml_file_without_seed_kwargs (same module) – to strip out seed parameters from the YAML before execution","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.from_cli_and_env\nCreate LLM configuration from CLI arguments, falling back to environment variables.\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'METHOD qlib.examples.run_all_model.ModelRunner.run has parameter or accepts argument qlib.examples.run_all_model.ModelRunner.run.wait_before_rm_env\nPlease be aware that this function can only work under Linux. MacOS and Windows will be supported in the future. Any PR to enhance this method is highly welcomed. Besides, this script doesn\'t support parallel running the same model for multiple times, and this will be fixed in the future development.  Parameters: ----------- times : int     determines how many times the model should be running. models : str or list     determines the specific model or list of models to run or exclude. exclude : boolean     determines whether the model being used is excluded or included. dataset : str     determines the dataset to be used for each model. universe  : str     the stock universe of the dataset.     default """" indicates that qlib_uri : str     the uri to install qlib with pip     it could be URI on the remote or local path (NOTE: the local path must be an absolute path) exp_folder_name: str     the name of the experiment folder wait_before_rm_env : bool     wait before remove environment. wait_when_err : bool     wait when errors raised when executing commands  Usage: ------- Here are some use cases of the function in the bash:  The run_all_models  will decide which config to run based no `models` `dataset`  `universe` Example 1):      models=""lightgbm"", dataset=""Alpha158"", universe="""" will result in running the following config     examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml      models=""lightgbm"", dataset=""Alpha158"", universe=""csi500"" will result in running the following config     examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158_csi500.yaml  .. code-block:: bash      # Case 1 - run all models multiple times     python run_all_model.py run 3      # Case 2 - run specific models multiple times     python run_all_model.py run 3 mlp      # Case 3 - run specific models multiple times with specific dataset     python run_all_model.py run 3 mlp Alpha158      # Case 4 - run other models except those are given as arguments for multiple times     python run_all_model.py run 3 [mlp,tft,lstm] --exclude=True      # Case 5 - run specific models for one time     python run_all_model.py run --models=[mlp,lightgbm]      # Case 6 - run other models except those are given as arguments for one time     python run_all_model.py run --models=[mlp,tft,sfm] --exclude=True      # Case 7 - run lightgbm model on csi500.     python run_all_model.py run 3 lightgbm Alpha158 csi500', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.from_env\nCreate LLM configuration from environment variables.\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'METHOD qlib.examples.run_all_model.ModelRunner.run has parameter or accepts argument qlib.examples.run_all_model.ModelRunner.run.exp_folder_name\nPlease be aware that this function can only work under Linux. MacOS and Windows will be supported in the future. Any PR to enhance this method is highly welcomed. Besides, this script doesn\'t support parallel running the same model for multiple times, and this will be fixed in the future development.  Parameters: ----------- times : int     determines how many times the model should be running. models : str or list     determines the specific model or list of models to run or exclude. exclude : boolean     determines whether the model being used is excluded or included. dataset : str     determines the dataset to be used for each model. universe  : str     the stock universe of the dataset.     default """" indicates that qlib_uri : str     the uri to install qlib with pip     it could be URI on the remote or local path (NOTE: the local path must be an absolute path) exp_folder_name: str     the name of the experiment folder wait_before_rm_env : bool     wait before remove environment. wait_when_err : bool     wait when errors raised when executing commands  Usage: ------- Here are some use cases of the function in the bash:  The run_all_models  will decide which config to run based no `models` `dataset`  `universe` Example 1):      models=""lightgbm"", dataset=""Alpha158"", universe="""" will result in running the following config     examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml      models=""lightgbm"", dataset=""Alpha158"", universe=""csi500"" will result in running the following config     examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158_csi500.yaml  .. code-block:: bash      # Case 1 - run all models multiple times     python run_all_model.py run 3      # Case 2 - run specific models multiple times     python run_all_model.py run 3 mlp      # Case 3 - run specific models multiple times with specific dataset     python run_all_model.py run 3 mlp Alpha158      # Case 4 - run other models except those are given as arguments for multiple times     python run_all_model.py run 3 [mlp,tft,lstm] --exclude=True      # Case 5 - run specific models for one time     python run_all_model.py run --models=[mlp,lightgbm]      # Case 6 - run other models except those are given as arguments for one time     python run_all_model.py run --models=[mlp,tft,sfm] --exclude=True      # Case 7 - run lightgbm model on csi500.     python run_all_model.py run 3 lightgbm Alpha158 csi500', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.reset\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nPreviously, users often ran into issues where the model file produced by one machine could not load or run on another machine. For example, models trained using a machine with an NVIDIA GPU could not run on another machine without a GPU (#5291, #5234). The reason is that the old binary format saved some internal configuration that were not universally applicable to all machines, e.g. `predictor='gpu_predictor'`.\n\n  Now, model saving function (`Booster.save_model()` in Python) will save only the model, without internal configuration. This will guarantee that your model file would be used anywhere. Internal configuration will be serialized in limited circumstances such as:\n  * Multiple nodes in a distributed system exchange model details over the network.\n  * Model checkpointing, to recover from possible crashes.\n\n  This work proved to be useful for parameter validation as well (see below).\n* Starting with 1.0.0 release, we will use semantic versioning to indicate whether the model produced by one version of XGBoost would be compatible with another version of XGBoost. Any change in the major version indicates a breaking change in the serialization format.\n* We now provide a robust method to save and load scikit-learn related attributes (#5245). Previously, we used Python pickle to save Python attributes related to `XGBClassifier`, `XGBRegressor`, and `XGBRanker` objects. The attributes are necessary to properly interact with scikit-learn. See #4639 for more details. The use of pickling hampered interoperability, as a pickle from one machine may not necessarily work on another machine. Starting with this release, we use an alternative method to serialize the scikit-learn related attributes. The use of Python pickle is now discouraged (#5236, #5281).\n\n### Parameter validation: detection of unused or incorrect parameters (#4553, #4577, #4738, #4801, #4961, #5101, #5157, #5167, #5256)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. Currently, parameter validation is available to R users and Python XGBoost API users. We are working to extend its support to scikit-learn users.\n* Configuration steps now have well-defined semantics (#4542, #4738), so we know exactly where and how the internal configurable parameters are changed.\n* The user can now use `save_config()` function to inspect all (used) training parameters. This is helpful for debugging model performance.\n\n### Allow individual workers to recover from faults (#4808, #4966)\n* Status quo: if a worker fails, all workers are shut down and restarted, and learning resumes from the last checkpoint. This involves requesting resources from the scheduler (e.g. Spark) and shuffling all the data again from scratch. Both of these operations can be quite costly and block training for extended periods of time, especially if the training data is big and the number of worker nodes is in the hundreds.\n* The proposed solution is to recover the single node that failed, instead of shutting down all workers. The rest of the clusters wait until the single failed worker is bootstrapped and catches up with the rest.\n* See roadmap at #4753. Note that this is work in progress. In particular, the feature is not yet available from XGBoost4J-Spark."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n### Option 2: Set up a local environment\n\nTo set up your local dev environment, you will need the following tools.\n\n1.  [git](https://github.com/) for code repository management.\n2.  [python](https://www.python.org/) to build and code in Keras.\n\nThe following commands check the tools above are successfully installed. Note\nthat Keras requires at least Python 3.10 to run.\n\n```shell\ngit --version\npython --version\n```\n\nClone your forked repo to your local machine. Go to the cloned directory to\ninstall the dependencies.\n\n```shell\ngit clone https://github.com/YOUR_GITHUB_USERNAME/keras.git\ncd keras\npip install -r requirements.txt\n```\n\nYou then need to configure the backend to use, see the\n[Configuring your backend](https://github.com/keras-team/keras/blob/master/README.md#configuring-your-backend)\nsection of the README.\n\nYou can also add GPU support to your environment, see the\n[Adding GPU support](https://github.com/keras-team/keras/blob/master/README.md#adding-gpu-support)\nsection of the README.\n\n## Generating public API and formatting the code\n\nFor the first time you are setting up the repo, please run `pre-commit install`.\nNote that this needs to be done only once at the beginning.\n\nNow, whenever you run `git commit -m ""<message>""`, three things are\nautomatically done:\n\n- Public API generation\n- Code formatting\n- Code linting\n\nIf there\'s any error, the commit will not go through. Please fix the error (\nmost of the times, the error is fixed automatically by the formatter/linter) and\nre-run the following:\n\n```\ngit add .\ngit commit -m ""<message>"" # This will not get logged as a duplicate commit.\n```\n\nIn case you want to run the above manually on all files, you can do the\nfollowing:\n\n```\npre-commit run --all-files\n```\n\nKerasHub uses [Ruff](https://docs.astral.sh/ruff/) to format the code.\n\n### Docstrings\n\nWe do not have an automated way to check docstring style, so if you write\nor edit any docstring, please make sure to check them manually.\nKeras docstrings follow the conventions below:\n\nA **class docstring** may contain the following items:\n\n* A one-line description of the class.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for arguments in `__init__()`.\n* If it\'s a layer:\n    * `Call arguments` section for arguments in `Layer.call()`.\n    * `Returns` section for the return values of `Layer.call()`.\n    * Optional `Raises` section for possible errors.\n\nYou can check out `MultiHeadAttention` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/layers/attention/multi_head_attention.py#L20).\n\nA **function docstring** may contain the following items:\n\n* One-line description of the function.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for the function arguments.\n* `Returns` section for the return values.\n* Optional `Raises` section for possible errors.\n\nYou can check out `text_dataset_from_directory` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/utils/text_dataset_utils.py#L27).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 5, 'pr': 1126, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.77867817878723,681
"In the Windows auto-mount branch, which subprocess return code triggers logging that the share is already mounted?","In the Windows auto-mount branch of the `_mount_nfs_uri` function, a CalledProcessError with returncode equal to 85 triggers logging that the share is already mounted.","In `_mount_nfs_uri` (qlib/__init__.py)’s Windows branch, a `CalledProcessError` with `returncode == 85` triggers the “already mounted” warning.","['FUNCTION keras.keras.src.ops.numpy.einsum has parameter or accepts argument keras.keras.src.ops.numpy.einsum.subscripts\nEvaluates the Einstein summation convention on the operands.  Args:     subscripts: Specifies the subscripts for summation as comma separated         list of subscript labels. An implicit (classical Einstein         summation) calculation is performed unless the explicit indicator         `->` is included as well as subscript labels of the precise         output form.     operands: The operands to compute the Einstein sum of.  Returns:     The calculation based on the Einstein summation convention.  Example: >>> from keras.src import ops >>> a = ops.arange(25).reshape(5, 5) >>> b = ops.arange(5) >>> c = ops.arange(6).reshape(2, 3)  Trace of a matrix:  >>> ops.einsum(""ii"", a) 60 >>> ops.einsum(a, [0, 0]) 60 >>> ops.trace(a) 60  Extract the diagonal:  >>> ops.einsum(""ii -> i"", a) array([ 0,  6, 12, 18, 24]) >>> ops.einsum(a, [0, 0], [0]) array([ 0,  6, 12, 18, 24]) >>> ops.diag(a) array([ 0,  6, 12, 18, 24])  Sum over an axis:  >>> ops.einsum(""ij -> i"", a) array([ 10,  35,  60,  85, 110]) >>> ops.einsum(a, [0, 1], [0]) array([ 10,  35,  60,  85, 110]) >>> ops.sum(a, axis=1) array([ 10,  35,  60,  85, 110])  For higher dimensional tensors summing a single axis can be done with ellipsis:  >>> ops.einsum(""...j -> ..."", a) array([ 10,  35,  60,  85, 110]) >>> np.einsum(a, [..., 1], [...]) array([ 10,  35,  60,  85, 110])  Compute a matrix transpose or reorder any number of axes:  >>> ops.einsum(""ji"", c) array([[0, 3],        [1, 4],        [2, 5]]) >>> ops.einsum(""ij -> ji"", c) array([[0, 3],        [1, 4],        [2, 5]]) >>> ops.einsum(c, [1, 0]) array([[0, 3],        [1, 4],        [2, 5]]) >>> ops.transpose(c) array([[0, 3],        [1, 4],        [2, 5]])  Matrix vector multiplication:  >>> ops.einsum(""ij, j"", a, b) array([ 30,  80, 130, 180, 230]) >>> ops.einsum(a, [0, 1], b, [1]) array([ 30,  80, 130, 180, 230]) >>> ops.einsum(""...j, j"", a, b) array([ 30,  80, 130, 180, 230])', 'MODULE qlib.qlib.utils.exceptions contains CLASS qlib.qlib.utils.exceptions.QlibException', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.einsum\nEvaluates the Einstein summation convention on the operands.  Args:     subscripts: Specifies the subscripts for summation as comma separated         list of subscript labels. An implicit (classical Einstein         summation) calculation is performed unless the explicit indicator         `->` is included as well as subscript labels of the precise         output form.     operands: The operands to compute the Einstein sum of.  Returns:     The calculation based on the Einstein summation convention.  Example: >>> from keras.src import ops >>> a = ops.arange(25).reshape(5, 5) >>> b = ops.arange(5) >>> c = ops.arange(6).reshape(2, 3)  Trace of a matrix:  >>> ops.einsum(""ii"", a) 60 >>> ops.einsum(a, [0, 0]) 60 >>> ops.trace(a) 60  Extract the diagonal:  >>> ops.einsum(""ii -> i"", a) array([ 0,  6, 12, 18, 24]) >>> ops.einsum(a, [0, 0], [0]) array([ 0,  6, 12, 18, 24]) >>> ops.diag(a) array([ 0,  6, 12, 18, 24])  Sum over an axis:  >>> ops.einsum(""ij -> i"", a) array([ 10,  35,  60,  85, 110]) >>> ops.einsum(a, [0, 1], [0]) array([ 10,  35,  60,  85, 110]) >>> ops.sum(a, axis=1) array([ 10,  35,  60,  85, 110])  For higher dimensional tensors summing a single axis can be done with ellipsis:  >>> ops.einsum(""...j -> ..."", a) array([ 10,  35,  60,  85, 110]) >>> np.einsum(a, [..., 1], [...]) array([ 10,  35,  60,  85, 110])  Compute a matrix transpose or reorder any number of axes:  >>> ops.einsum(""ji"", c) array([[0, 3],        [1, 4],        [2, 5]]) >>> ops.einsum(""ij -> ji"", c) array([[0, 3],        [1, 4],        [2, 5]]) >>> ops.einsum(c, [1, 0]) array([[0, 3],        [1, 4],        [2, 5]]) >>> ops.transpose(c) array([[0, 3],        [1, 4],        [2, 5]])  Matrix vector multiplication:  >>> ops.einsum(""ij, j"", a, b) array([ 30,  80, 130, 180, 230]) >>> ops.einsum(a, [0, 1], b, [1]) array([ 30,  80, 130, 180, 230]) >>> ops.einsum(""...j, j"", a, b) array([ 30,  80, 130, 180, 230])', 'MODULE qlib.qlib.config contains GLOBAL_VARIABLE qlib.qlib.config.NUM_USABLE_CPU', 'CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data_1d_extend\nnormalize data extend; extending yahoo qlib data(from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)  Notes -----     Steps to extend yahoo qlib data:          1. download qlib data: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data; save to <dir1>          2. collector source data: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#collector-data; save to <dir2>          3. normalize new source data(from step 2): python scripts/data_collector/yahoo/collector.py normalize_data_1d_extend --old_qlib_dir <dir1> --source_dir <dir2> --normalize_dir <dir3> --region CN --interval 1d          4. dump data: python scripts/dump_bin.py dump_update --csv_path <dir3> --qlib_dir <dir1> --freq day --date_field_name date --symbol_field_name symbol --exclude_fields symbol,date          5. update instrument(eg. csi300): python python scripts/data_collector/cn_index/collector.py --index_name CSI300 --qlib_dir <dir1> --method parse_instruments  Parameters ---------- old_qlib_data_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol  Examples ---------     $ python collector.py normalize_data_1d_extend --old_qlib_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region CN --interval 1d', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR\'s for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/csi300/csi500/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/SH000300/SH000905/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332±0.00 | 0.3044±0.00 | 0.0462±0.00 | 0.4326±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n| MLP        | Alpha158 | 0.0229±0.01 | 0.2181±0.05 | 0.0360±0.00 | 0.3409±0.02 | 0.0043±0.02       | 0.0602±0.27       | -0.2184±0.04 |\n| LightGBM   | Alpha158 | 0.0399±0.00 | 0.4065±0.00 | 0.0482±0.00 | 0.5101±0.00 | 0.1284±0.00       | 1.5650±0.00       | -0.0635±0.00 |\n| CatBoost   | Alpha158 | 0.0345±0.00 | 0.2855±0.00 | 0.0417±0.00 | 0.3740±0.00 | 0.0496±0.00       | 0.5977±0.00       | -0.1496±0.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380±0.00 | 0.3659±0.00 | 0.0442±0.00 | 0.4324±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258±0.00 | 0.2021±0.02 | 0.0426±0.00 | 0.3840±0.02 | 0.0022±0.02       | 0.0301±0.26       | -0.2064±0.02 |\n| LightGBM   | Alpha360 | 0.0400±0.00 | 0.3605±0.00 | 0.0536±0.00 | 0.5431±0.00 | 0.0505±0.00       | 0.7658±0.02       | -0.1880±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1129, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",42.91244649887085,682
"What type of copy is made for a segmented view, and how are the original data, labels, and indices preserved?","In the `MTSDatasetH._prepare_seg` method, a Python shallow copy is made via `copy.copy(self)`. This type of copy creates a new object that references the original data, labels, and indices by reference. As a result, the underlying arrays are preserved, but changes to the segment's batch and daily slice indices `_batch_slices`, `_daily_slices`, and `_daily_index` do not affect the original data. The original payloads `_data`, `_label`, `_index`, `_memory`, and `_zeros` are updated by reference in the shallow copy, ensuring that the segment's state is correctly maintained.","In MTSDatasetH._prepare_seg (qlib/contrib/data/dataset.py) a Python shallow copy is made (via copy.copy(self)). After that, the original payloads (_data, _label, _index, _memory, _zeros) are re-assigned by reference onto the new object to keep the underlying arrays intact, while only the batch and daily slice indices (_batch_slices, _daily_slices, _daily_index) are updated for the segment.","['CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.data\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'MODULE qlib.qlib.utils.index_data contains CLASS qlib.qlib.utils.index_data.Index\nThis is for indexing(rows or columns)  Read-only operations has higher priorities than others. So this class is designed in a **read-only** way to shared data for queries. Modifications will results in new Index.  NOTE: the indexing has following flaws - duplicated index value is not well supported (only the first appearance will be considered) - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered', 'MODULE keras.keras.src.backend.tensorflow.sparse contains FUNCTION keras.keras.src.backend.tensorflow.sparse.elementwise_binary_union\nDecorator to add support for `tf.SparseTensor` and `tf.IndexedSlices` to an element-wise binary operator such that the indices present in the result are the union of the indices in the two operand.  The primary use case for this is the `add` and `subtract` operators.  There are requirements on the operator for this decorator to work correctly:  - The operator must be element-wise. - The operator must be binary (two input tensors and one output tensor). - Both inputs must be of the same shape or one input must be a scalar. - The output must be of the same shape as the (non scalar) inputs. - The indices of the output must be the union of the indices of the inputs.   This implies that func(0, 0) must be 0. As a result, if one operand is   dense or a scalar, then the result will be dense.  Additional arguments to the function (besides the input tensors) are not supported.  Note that if the result of the operation is zero at some indices, including because the operands were zero at these indices, the zeros and indices are preserved.  Args:     sparse_op: implementation of the operation for `tf.SparseTensor`. Must         work if both of the operands are `tf.SparseTensor`s and can         optionally work if one of the operand is a `tf.SparseTensor` and         the other one is dense tensor, see `densify_mixed`.     densify_mixed: if `True`, `sparse_op` does not support a mix of         `tf.SparseTensor` and dense tensor or dense tensor with         `tf.SparseTensor` and the `tf.SparseTensor` tensor is densified. Returns:     Wrapped function that supports `tf.SparseTensor` and `tf.IndexedSlices`.', 'CLASS qlib.qlib.utils.index_data.Index has field or attribute or property qlib.qlib.utils.index_data.Index.index_map\nThis is for indexing(rows or columns)  Read-only operations has higher priorities than others. So this class is designed in a **read-only** way to shared data for queries. Modifications will results in new Index.  NOTE: the indexing has following flaws - duplicated index value is not well supported (only the first appearance will be considered) - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.TSDataSampler.idx_df\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1135, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",47.589539766311646,683
What is the sequence of high-level operations performed by the end-to-end pipeline tests from model training through backtesting to experiment URI management?,"The sequence of high-level operations performed by the end-to-end pipeline tests from model training through backtesting to experiment URI management in qlib is as follows:

1. **Model Training**: The `train(uri)` function fits the model and logs predictions and IC/RIC to an MLflow run.
2. **Backtesting**: The `backtest_analysis(pred_scores, run_id, uri)` function loads the predictions, runs the backtest, and checks performance metrics.
3. **Experiment URI Management**: The `fake_experiment()` function uses the Experiment URI manager, verifies both the default URI and the current URI resolution, and tears down the MLflow folder.","The “all‐in‐one” pipeline tests in tests/test_all_pipeline.py (and its contrib variant) execute in three high-level steps:

1. test_0_train → calls train(uri)  
   – fits your model, logs predictions & IC/RIC to an MLflow run, returns (pred_scores, ic_ric, run_id, uri)

2. test_1_backtest → calls backtest_analysis(pred_scores, run_id, uri)  
   – loads those predictions, runs the backtest, and checks performance metrics

3. test_2_expmanager → calls fake_experiment() (uses the Experiment URI manager)  
   – verifies both the default URI and the “current” URI resolution, then tears down the MLflow folder","['CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.end_exp\nEnd an active experiment.  Maintaining `_active_exp_uri` is included in end_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_name : str     name of the active experiment. recorder_status : str     the status of the active recorder of the experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'MODULE qlib.qlib.backtest.utils contains CLASS qlib.qlib.backtest.utils.LevelInfrastructure\nlevel infrastructure is created by executor, and then shared to strategies on the same level', ""CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.start_exp\nLower level method for starting an experiment. When use this method, one should end the experiment manually and the status of the recorder may not be handled properly. Here is the example code:  .. code-block:: Python      R.start_exp(experiment_name='test', recorder_name='recorder_1')     ... # further operations     R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)   Parameters ---------- experiment_id : str     id of the experiment one wants to start. experiment_name : str     the name of the experiment to be started recorder_id : str     id of the recorder under the experiment one wants to start. recorder_name : str     name of the recorder under the experiment one wants to start. uri : str     the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.     The default uri are set in the qlib.config. resume : bool     whether to resume the specific recorder with given name under the given experiment.  Returns ------- An experiment instance being started.\nA global system that helps to manage the experiments."", 'MODULE qlib.qlib.rl.contrib.backtest contains FUNCTION qlib.qlib.rl.contrib.backtest.backtest', 'CLASS keras.guides.making_new_layers_and_models_via_subclassing.VariationalAutoEncoder inherits from or is a subclass of keras.keras.Model\nCombines the encoder and decoder into an end-to-end model for training.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships).""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 11, 'pr': 1141, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.782207012176514,684
Which git commands does the MLflow recorder run to capture and log uncommitted code changes?,"The MLflow recorder in qlib runs the following git commands to capture and log uncommitted code changes:

1. `git diff`
2. `git status`
3. `git diff --cached` 
These commands are run by the `MLflowRecorder._log_uncommitted_code` method, which logs the output of each command to a file: `code_diff.txt`, `code_status.txt`, and `code_cached.txt`, respectively.","In qlib/workflow/recorder.py, MLflowRecorder._log_uncommitted_code runs:

• git diff  
• git status  
• git diff --cached  

and logs their outputs to code_diff.txt, code_status.txt and code_cached.txt.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.start_run has parameter or accepts argument qlib.qlib.workflow.recorder.logger', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.delete_tags\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 2, 'pr': 1167, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",34.52646732330322,685
What flags does the PIT data fetch use to preserve existing files and skip re-downloading?,"The flags used to preserve existing files and skip re-downloading in qlib are `delete_old=False` and `exists_skip=True`. This is achieved by setting `delete_old` to False, which preserves existing files, and setting `exists_skip` to True, which skips re-downloading.","In `tests/test_pit.py::TestPIT.setUpClass`, the calls to `GetData().qlib_data` use

• delete_old=False  
• exists_skip=True

to preserve existing files and skip re-downloading.","['METHOD xgboost.ops.script.run_clang_tidy.ClangTidy.convert_nvcc_command_to_clang has parameter or accepts argument xgboost.ops.script.run_clang_tidy.ClangTidy.convert_nvcc_command_to_clang.command\nConvert nvcc flags to corresponding clang flags.', 'MODULE qlib.qlib.utils.file contains FUNCTION qlib.qlib.utils.file.get_tmp_file_with_buffer', 'CLASS xgboost.ops.script.run_clang_tidy.ClangTidy has method xgboost.ops.script.run_clang_tidy.ClangTidy.convert_nvcc_command_to_clang\nConvert nvcc flags to corresponding clang flags.\nclang tidy wrapper. Args:   args:  Command line arguments.       cpp_lint: Run linter on C++ source code.       cuda_lint: Run linter on CUDA source code.       use_dmlc_gtest: Whether to use gtest bundled in dmlc-core.', 'MODULE qlib.qlib.rl.contrib.naive_config_parser contains FUNCTION qlib.qlib.rl.contrib.naive_config_parser.check_file_exist', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What\'s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=""https://github.com/microsoft/RD-Agent""><img src=""docs/_static/img/rdagent_logo.png"" alt=""RD_Agent"" style=""height: 2em""></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!\n\nTo learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (中文) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- 📃**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- 👾**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1171, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.438749313354492,686
"When auto mounting on Unix-like systems, how many levels of parent paths does the function check for an existing mount, and which variable defines this limit?","When auto mounting on Unix-like systems, the function checks for an existing mount at most two parent-path levels. This limit is defined by the `_check_level_num` variable, which is initialized to 2 in the `qlib/__init__.py` file.","In qlib/__init__.py’s `_mount_nfs_uri` (Unix branch), it will climb at most 2 parent‐path levels when checking for an existing mount. That limit is set by the `_check_level_num` variable (initialized to 2).","['CLASS qlib.scripts.data_collector.base.BaseRun has method qlib.scripts.data_collector.base.BaseRun.__init__\nParameters ---------- source_dir: str     The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source"" normalize_dir: str     Directory for normalize data, default ""Path(__file__).parent/normalize"" max_workers: int     Concurrent number, default is 1; Concurrent number, default is 1; when collecting data, it is recommended that max_workers be set to 1 interval: str     freq, value from [1min, 1d], default 1d', 'METHOD qlib.qlib.config.DataPathManager.__init__ has parameter or accepts argument qlib.qlib.config.DataPathManager.__init__.mount_path\nThe relation of `provider_uri` and `mount_path` - `mount_path` is used only if provider_uri is an NFS path - otherwise, provider_uri will be used for accessing data', 'METHOD qlib.scripts.data_collector.base.BaseRun.__init__ has parameter or accepts argument qlib.scripts.data_collector.base.BaseRun.__init__.max_workers\nParameters ---------- source_dir: str     The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source"" normalize_dir: str     Directory for normalize data, default ""Path(__file__).parent/normalize"" max_workers: int     Concurrent number, default is 1; Concurrent number, default is 1; when collecting data, it is recommended that max_workers be set to 1 interval: str     freq, value from [1min, 1d], default 1d', 'METHOD qlib.qlib.config.DataPathManager.__init__ has parameter or accepts argument qlib.qlib.config.DataPathManager.__init__.provider_uri\nThe relation of `provider_uri` and `mount_path` - `mount_path` is used only if provider_uri is an NFS path - otherwise, provider_uri will be used for accessing data', 'METHOD qlib.scripts.data_collector.base.BaseRun.__init__ has parameter or accepts argument qlib.scripts.data_collector.base.BaseRun.__init__.source_dir\nParameters ---------- source_dir: str     The directory where the raw data collected from the Internet is saved, default ""Path(__file__).parent/source"" normalize_dir: str     Directory for normalize data, default ""Path(__file__).parent/normalize"" max_workers: int     Concurrent number, default is 1; Concurrent number, default is 1; when collecting data, it is recommended that max_workers be set to 1 interval: str     freq, value from [1min, 1d], default 1d', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1177, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.955493211746216,687
Which call in the setup is responsible for normalizing the downloaded quarterly pit source data?,"The call responsible for normalizing the downloaded quarterly pit source data is `normalize_data()` in the Run object, which is called on a Run object initialized with source_dir set to `pit_dir`, normalize_dir set to `pit_normalized_dir`, and interval set to `quarterly`. This process involves calling the `normalize_data()` function from the collector.py script with the specified interval and directories. Refer to the qlib documentation for more information on how to use this function.","The normalization is done by the call to Run(...).normalize_data(), i.e.:

```python
Run(
    source_dir=pit_dir,
    normalize_dir=pit_normalized_dir,
    interval=""quarterly"",
).normalize_data()
```","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'METHOD qlib.scripts.data_collector.pit.collector.PitNormalize.normalize has parameter or accepts argument qlib.scripts.data_collector.pit.collector.PitNormalize.normalize.df', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.first\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'CLASS qlib.scripts.data_collector.pit.collector.PitNormalize inherits from or is a subclass of data_collector.base.BaseNormalize', 'FUNCTION qlib.qlib.utils.__init__.get_period_list has parameter or accepts argument qlib.qlib.utils.__init__.get_period_list.last\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1178, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",34.91951656341553,688
"What is the overall process for training the ensemble, including how sample weights and features change across sub-model iterations?","The overall process for training the ensemble, including how sample weights and features change across sub-model iterations, in qlib is as follows:

1.  The training loop is implemented in `DEnsembleModel.fit` located in `qlib/contrib/model/double_ensemble.py`. 
2.  In initialization, weights are set to all ones of size N.
3.  In initialization, features are set to all columns of X_train.
4.  In initialization, pred_sub is set to zeros of shape N×K.
5.  The training loop iterates for each sub-model k from 1 to num_models.
6.  For each iteration, model_k is trained by calling `train_submodel(df_train, df_valid, weights, features)`. 
7.  After training, model_k is appended to self.ensemble.
8.  After training, the current features are appended to self.sub_features.
9.  If k equals num_models, the loop breaks.
10. After training model_k, loss_curve is computed by `retrieve_loss_curve(model_k, df_train, features)`. 
11. After training model_k, pred_k is computed by `predict_sub(model_k, df_train, features)`. 
12. After computing pred_k, pred_sub[:, k] is updated with pred_k.
13. The ensemble prediction is computed as the weighted sum of pred_sub[:, 0…k] divided by sum(sub_weights[0…k]).
14. Loss_values are computed by `get_loss(y_train, ensemble prediction)`. 
15. If enable_sr is true, weights is updated by `sample_reweight(loss_curve, loss_values, k)`. This function ranks losses, computes h-values, bins samples into bins_sr, and applies decay.
16. If enable_fs is true, features is updated by `feature_selection(df_train, loss_values)`. This function uses bins_fs and sample_ratios to drop less informative features.
17. At the end of training, the ensemble contains K sub-models trained on dynamically re-weighted samples.
18. At the end of training, the ensemble uses shrinking feature sets.
19. The final predict() method aggregates sub-model outputs using sub_weights.

This process is designed to solve both the low signal-to-noise ratio and increasing number of features problems in financial market prediction by leveraging learning trajectory-based sample reweighting and shuffling-based feature selection.","The training loop is implemented in DEnsembleModel.fit (qlib/contrib/model/double_ensemble.py):

1. Initialization  
   • weights ← all ones (size N)  
   • features ← all columns of X_train  
   • pred_sub ← zeros(N×K)  

2. For each sub‐model k=1…num_models:  
   a. Train sub‐model:  
      model_k = train_submodel(df_train, df_valid, weights, features)  
      append model_k to self.ensemble and current features to self.sub_features  
   b. If k == num_models, break  
   c. Compute:  
      – loss_curve = retrieve_loss_curve(model_k, df_train, features)  
      – pred_k = predict_sub(model_k, df_train, features) → update pred_sub[:,k]  
      – ensemble prediction = weighted sum of pred_sub[:,0…k] / sum(sub_weights[0…k])  
      – loss_values = get_loss(y_train, ensemble prediction)  
   d. Sample re‐weighting (if enable_sr):  
      weights = sample_reweight(loss_curve, loss_values, k)  
      (ranks losses, computes h‐values, bins into bins_sr, applies decay)  
   e. Feature selection (if enable_fs):  
      features = feature_selection(df_train, loss_values)  
      (uses bins_fs and sample_ratios to drop less informative features)  

At the end you have an ensemble of K sub‐models trained on dynamically re‐weighted samples and shrinking feature sets. The final predict() method then aggregates sub‐model outputs using sub_weights.","['MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'MODULE qlib.qlib.contrib.model.double_ensemble contains CLASS qlib.qlib.contrib.model.double_ensemble.DEnsembleModel\nDouble Ensemble Model', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'CLASS qlib.qlib.contrib.model.double_ensemble.DEnsembleModel has method qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel\nDouble Ensemble Model', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has field or attribute or property qlib.qlib.workflow.record_temp.indicator_analysis_method\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=""#main-challenges--solutions-in-quant-research"">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=""circle""><a href=""#forecasting-finding-valuable-signalspatterns"">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=""disc""><a href=""#quant-model-paper-zoo""><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=""circle""><a href=""#run-a-single-model"">Run a Single Model</a></li>\n                    <li type=""circle""><a href=""#run-multiple-models"">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=""circle""><a href=""#adapting-to-market-dynamics"">Adapting to Market Dynamics</a></li>\n          <li type=""circle""><a href=""#reinforcement-learning-modeling-continuous-decisions"">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=""align: center"">\n<img src=""docs/_static/img/framework-abstract.jpg"" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib\'s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 4, 'pr': 1205, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",67.982168674469,689
What impact does setting the forbid-all-trade-at-limit flag to true have on trades when a stock hits its daily price limit?,"When `forbid_all_trade_at_limit` is set to True in qlib, no trades will be executed for a stock when it hits its daily price limit. However, this setting may have implications for the overall performance of the strategy, and users should carefully consider whether to use it or not.","In TopkDropoutStrategy (qlib/contrib/strategy/signal_strategy.py), turning on forbid_all_trade_at_limit=True means that whenever a stock hits its daily limit‐up or limit‐down price, the strategy will skip all orders for that stock (no buys at limit‐down and no sells at limit‐up), even though such trades would be allowed in reality.","['METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.method_sell\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.forbid_all_trade_at_limit\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.check_stock_limit\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.n_drop\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.backtest.exchange.Exchange.check_stock_limit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.check_stock_limit.stock_id\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1209, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",41.167784452438354,690
What new market data fields are added in the order book handler's feature configuration that are not present in the standard high frequency backtest handler?,"The new market data fields added in the order book handler's feature configuration that are not present in the standard high frequency backtest handler in qlib are:

* bid0: $bid
* bidV0: $bidV
* ask0: $ask
* askV0: $askV
* median0: ($bid + $ask) / 2
* downlimitmarket0: $downlimitmarket
* uplimitmarket0: $uplimitmarket
* highmarket0: $highmarket
* lowmarket0: $lowmarket","In HighFreqBacktestOrderHandler.get_feature_config (qlib/contrib/data/highfreq_handler.py), the following fields are added on top of the standard close0, vwap0, volume0, factor0:

• bid0  (​$bid​)  
• bidV0 (​$bidV​)  
• ask0  (​$ask​)  
• askV0 (​$askV​)  
• median0 (​($bid+$ask)/2​)  
• downlimitmarket0 (​$downlimitmarket​)  
• uplimitmarket0 (​$uplimitmarket​)  
• highmarket0 (​$highmarket​)  
• lowmarket0 (​$lowmarket​)","['METHOD qlib.qlib.data.cache.DatasetCache.cache_to_origin_data has parameter or accepts argument qlib.qlib.data.cache.DatasetCache.cache_to_origin_data.fields\ncache data to origin data  :param data: pd.DataFrame, cache data. :param fields: feature fields. :return: pd.DataFrame.', 'CLASS qlib.qlib.contrib.data.highfreq_handler.HighFreqOrderHandler has field or attribute or property qlib.qlib.contrib.data.highfreq_handler.HighFreqOrderHandler.get_feature_config.price_field', 'METHOD qlib.qlib.data.cache.DatasetCache.cache_to_origin_data has parameter or accepts argument qlib.qlib.data.cache.DatasetCache.cache_to_origin_data.data\ncache data to origin data  :param data: pd.DataFrame, cache data. :param fields: feature fields. :return: pd.DataFrame.', 'CLASS qlib.qlib.contrib.data.highfreq_handler.HighFreqHandler has field or attribute or property qlib.qlib.contrib.data.highfreq_handler.HighFreqHandler.get_feature_config.price_field', 'MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/highfreq/README.md\n# Introduction\nThis folder contains 2 examples\n- A high-frequency dataset example\n- An example of predicting the price trend in high-frequency data\n\n## High-Frequency Dataset\n\nThis dataset is an example for RL high frequency trading.\n\n### Get High-Frequency Data\n\nGet high-frequency data by running the following command:\n```bash\n    python workflow.py get_data\n```\n\n### Dump & Reload & Reinitialize the Dataset\n\n\nThe High-Frequency Dataset is implemented as `qlib.data.dataset.DatasetH` in the `workflow.py`. `DatatsetH` is the subclass of [`qlib.utils.serial.Serializable`](https://qlib.readthedocs.io/en/latest/advanced/serial.html), whose state can be dumped in or loaded from disk in `pickle` format.\n\n### About Reinitialization\n\nAfter reloading `Dataset` from disk, `Qlib` also support reinitializing the dataset. It means that users can reset some states of `Dataset` or `DataHandler` such as `instruments`, `start_time`, `end_time` and `segments`, etc.,  and generate new data according to the states.\n\nThe example is given in `workflow.py`, users can run the code as follows.\n\n### Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py dump_and_load_dataset\n```\n\n## Benchmarks Performance (predicting the price trend in high-frequency data)\n\nHere are the results of models for predicting the price trend in high-frequency data. We will keep updating benchmark models in future.\n\n| Model Name | Dataset | IC | ICIR | Rank IC | Rank ICIR | Long precision| Short Precision | Long-Short Average Return | Long-Short Average Sharpe |\n|---|---|---|---|---|---|---|---|---|---|\n| LightGBM | Alpha158 | 0.0349±0.00 | 0.3805±0.00| 0.0435±0.00 | 0.4724±0.00 | 0.5111±0.00 | 0.5428±0.00 | 0.000074±0.00 | 0.2677±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1212, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",36.571152687072754,691
How are non-string elements handled by remove_fields_space?,"Non-string elements in the list comprehension inside `remove_fields_space` are wrapped in `str(i)`, meaning they are converted to strings. This is done to ensure that all elements in the resulting list are of the same type, which is likely a requirement for further processing or manipulation.","In qlib/utils/__init__.py (remove_fields_space), if an element isn’t a str it isn’t “space‐cleaned” but simply wrapped in str(i) in the list comprehension.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.remove_fields_space\nremove fields space  :param fields: features fields :return: list or str', 'FUNCTION qlib.qlib.utils.__init__.remove_fields_space has parameter or accepts argument qlib.qlib.utils.__init__.remove_fields_space.fields\nremove fields space  :param fields: features fields :return: list or str', ""METHOD qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__.trade_range\nParameters ---------- file : Union[IO, str, Path, pd.DataFrame]     this parameters will specify the info of expected orders      Here is an example of the content      1) Amount (**adjusted**) based strategy          datetime,instrument,amount,direction         20200102,  SH600519,  1000,     sell         20200103,  SH600519,  1000,      buy         20200106,  SH600519,  1000,     sell  trade_range : Tuple[int, int]     the intra day time index range of the orders     the left and right is closed.      If you want to get the trade_range in intra-day     - `qlib/utils/time.py:def get_day_min_idx_range` can help you create the index range easier     # TODO: this is a trade_range level limitation. We'll implement a more detailed limitation later."", 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.parse_field', ""METHOD qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.rule_strategy.FileOrderStrategy.__init__.file\nParameters ---------- file : Union[IO, str, Path, pd.DataFrame]     this parameters will specify the info of expected orders      Here is an example of the content      1) Amount (**adjusted**) based strategy          datetime,instrument,amount,direction         20200102,  SH600519,  1000,     sell         20200103,  SH600519,  1000,      buy         20200106,  SH600519,  1000,     sell  trade_range : Tuple[int, int]     the intra day time index range of the orders     the left and right is closed.      If you want to get the trade_range in intra-day     - `qlib/utils/time.py:def get_day_min_idx_range` can help you create the index range easier     # TODO: this is a trade_range level limitation. We'll implement a more detailed limitation later."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1213, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.080237865447998,692
"In the fallback implementation of the rolling rank, by what constant is the percentile score divided to scale it between 0 and 1?","The result of `percentileofscore(...)` is divided by 100, which scales it into the range [0, 1].","In the fallback `Rank._load_internal` (in qlib/data/ops.py), the result of `percentileofscore(...)` is divided by 100 to scale it into [0, 1].","['CLASS keras.keras.src.metrics.f_score_metrics.F1Score inherits from or is a subclass of keras.keras.src.metrics.f_score_metrics.FBetaScore\nComputes F-Beta score.  Formula:  ```python b2 = beta ** 2 f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall) ``` This is the weighted harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed across per-class results         in the multi-class case.         Acceptable values are `None`, `""micro""`, `""macro""` and         `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     beta: Determines the weight of given to recall         in the harmonic mean between precision and recall (see pseudocode         equation above). Defaults to `1`.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-Beta Score: float.  Example:  >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() >>> result [0.3846154 , 0.90909094, 0.8333334 ]\nComputes F-1 Score.  Formula:  ```python f1_score = 2 * (precision * recall) / (precision + recall) ``` This is the harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed on data.         Acceptable values are `None`, `""micro""`, `""macro""`         and `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-1 Score: float.  Example:  >>> metric = keras.metrics.F1Score(threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() array([0.5      , 0.8      , 0.6666667], dtype=float32)', 'METHOD qlib.qlib.data.ops.Quantile.__init__ has parameter or accepts argument qlib.qlib.data.ops.Quantile.__init__.qscore', 'CLASS keras.keras.src.metrics.f_score_metrics.FBetaScore has field or attribute or property keras.keras.src.metrics.f_score_metrics.name\nComputes F-Beta score.  Formula:  ```python b2 = beta ** 2 f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall) ``` This is the weighted harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed across per-class results         in the multi-class case.         Acceptable values are `None`, `""micro""`, `""macro""` and         `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     beta: Determines the weight of given to recall         in the harmonic mean between precision and recall (see pseudocode         equation above). Defaults to `1`.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-Beta Score: float.  Example:  >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() >>> result [0.3846154 , 0.90909094, 0.8333334 ]', 'FUNCTION qlib.qlib.utils.data.zscore has parameter or accepts argument qlib.qlib.utils.data.zscore.x', 'CLASS keras.keras.src.metrics.f_score_metrics.FBetaScore inherits from or is a subclass of keras.src.metrics.metric.Metric\nComputes F-Beta score.  Formula:  ```python b2 = beta ** 2 f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall) ``` This is the weighted harmonic mean of precision and recall. Its output range is `[0, 1]`. It works for both multi-class and multi-label classification.  Args:     average: Type of averaging to be performed across per-class results         in the multi-class case.         Acceptable values are `None`, `""micro""`, `""macro""` and         `""weighted""`. Defaults to `None`.         If `None`, no averaging is performed and `result()` will return         the score for each class.         If `""micro""`, compute metrics globally by counting the total         true positives, false negatives and false positives.         If `""macro""`, compute metrics for each label,         and return their unweighted mean.         This does not take label imbalance into account.         If `""weighted""`, compute metrics for each label,         and return their average weighted by support         (the number of true instances for each label).         This alters `""macro""` to account for label imbalance.         It can result in an score that is not between precision and recall.     beta: Determines the weight of given to recall         in the harmonic mean between precision and recall (see pseudocode         equation above). Defaults to `1`.     threshold: Elements of `y_pred` greater than `threshold` are         converted to be 1, and the rest 0. If `threshold` is         `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.     name: Optional. String name of the metric instance.     dtype: Optional. Data type of the metric result.  Returns:     F-Beta Score: float.  Example:  >>> metric = keras.metrics.FBetaScore(beta=2.0, threshold=0.5) >>> y_true = np.array([[1, 1, 1], ...                    [1, 0, 0], ...                    [1, 1, 0]], np.int32) >>> y_pred = np.array([[0.2, 0.6, 0.7], ...                    [0.2, 0.6, 0.6], ...                    [0.6, 0.8, 0.0]], np.float32) >>> metric.update_state(y_true, y_pred) >>> result = metric.result() >>> result [0.3846154 , 0.90909094, 0.8333334 ]', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 1, 'pr': 1228, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",43.26680040359497,693
What sequence of calls happens when logging an artifact with no active recorder?,"When logging an artifact with no active recorder in qlib, the sequence of calls is as follows:

1. QlibRecorder.log_artifact is called with local_path and artifact_path.
2. QlibRecorder.log_artifact delegates to MLflowRecorder.log_artifact.
3. MLflowRecorder.log_artifact sets tags through the active recorder if it exists.
4. If no active recorder exists, a default experiment is created as well as a new recorder, and tags are set under it.
5. The system will create a new MLflowRecorder when no active recorder is present.
6. MLflowRecorder.start_run is called to configure asynchronous logging, log parameters and tags, and set the MLflow tracking URI via mlflow.set_tracking_uri.
7. MLflowRecorder.start_run calls mlflow.start_run to start the run.
8. MlflowClient.log_artifact is called by MLflowRecorder.log_artifact to log the artifact.","When you call QlibRecorder.log_artifact with no active recorder, the following happens:

1. In qlib/workflow/__init__.py: QlibRecorder.log_artifact calls  
   → self.get_exp(start=True)  
2. In qlib/workflow/exp.py: Experiment.get_recorder(create=True, start=True) sees no active recorder, so it  
   → calls _get_or_create_rec to make a new MLflowRecorder  
   → because start=True, sets it as active and invokes MLflowRecorder.start_run()  
3. In qlib/workflow/recorder.py: MLflowRecorder.start_run sets up the MLflow run (mlflow.set_tracking_uri, mlflow.start_run, async logging, params/tags, etc.)  
4. Back in QlibRecorder.log_artifact it finally calls  
   → MLflowRecorder.log_artifact(local_path, artifact_path)  
   → which delegates to MlflowClient.log_artifact under the hood.","['METHOD qlib.qlib.workflow.__init__.QlibRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.log_artifact.artifact_path\nLog a local file or directory as an artifact of the currently active run  - If `active recorder` exists: it will set tags through the active recorder. - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.', 'CLASS qlib.qlib.workflow.recorder.Recorder has method qlib.qlib.workflow.recorder.Recorder.log_artifact\nLog a local file or directory as an artifact of the currently active run.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.log_artifact.local_path\nLog a local file or directory as an artifact of the currently active run  - If `active recorder` exists: it will set tags through the active recorder. - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.', 'CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.log_artifact\nLog a local file or directory as an artifact of the currently active run  - If `active recorder` exists: it will set tags through the active recorder. - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.  Parameters ---------- local_path : str     Path to the file to write. artifact_path : Optional[str]     If provided, the directory in ``artifact_uri`` to write to.\nA global system that helps to manage the experiments.', 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.log_artifact.artifact_path', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 11, 'n_files_pr': 5, 'pr': 1248, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",40.85296893119812,694
Which exchange-provided metric is fetched to determine the rounding unit for buy quantities?,"The exchange-provided metric fetched to determine the rounding unit for buy quantities is the 'factor' provided by the trade_exchange.get_factor(...) function, which is used in the round_amount_by_trade_unit method.",The strategy uses the exchange’s “factor” (fetched via trade_exchange.get_factor(...)) to determine the rounding unit when calling round_amount_by_trade_unit.,"['METHOD qlib.qlib.backtest.exchange.Exchange.__init__ has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.__init__.limit_threshold\n__init__ :param freq:             frequency of data :param start_time:       closed start time for backtest :param end_time:         closed end time for backtest :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50) :param deal_price:      Union[str, Tuple[str, str], List[str]]                         The `deal_price` supports following two types of input                         - <deal_price> : str                         - (<buy_price>, <sell_price>): Tuple[str] or List[str]                         <deal_price>, <buy_price> or <sell_price> := <price>                         <price> := str                         - for example \'$close\', \'$open\', \'$vwap\' (""close"" is OK. `Exchange` will help to prepend                           ""$"" to the expression) :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.                          It is useful when users want more fields to be queried :param limit_threshold: Union[Tuple[str, str], float, None]                         1) `None`: no limitation                         2) float, 0.1 for example, default None                         3) Tuple[str, str]: (<the expression for buying stock limitation>,                                              <the expression for sell stock limitation>)                                             `False` value indicates the stock is tradable                                             `True` value indicates the stock is limited and not tradable :param volume_threshold: Union[                             Dict[                                 ""all"": (""cum"" or ""current"", limit_str),                                 ""buy"": (""cum"" or ""current"", limit_str),                                 ""sell"":(""cum"" or ""current"", limit_str),                             ],                             (""cum"" or ""current"", limit_str),                          ]                         1) (""cum"" or ""current"", limit_str) denotes a single volume limit.                             - limit_str is qlib data expression which is allowed to define your own Operator.                             Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for                             high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom                             operator, you need to register it in qlib_init.                             - ""cum"" means that this is a cumulative value over time, such as cumulative market                             volume. So when it is used as a volume limit, it is necessary to subtract the dealt                             amount.                             - ""current"" means that this is a real-time value and will not accumulate over time,                             so it can be directly used as a capacity limit.                             e.g. (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""), (""current"", ""$bidV1"")                         2) ""all"" means the volume limits are both buying and selling.                         ""buy"" means the volume limits of buying. ""sell"" means the volume limits of selling.                         Different volume limits will be aggregated with min(). If volume_threshold is only                         (""cum"" or ""current"", limit_str) instead of a dict, the volume limits are for                         both by default. In other words, it is same as {""all"": (""cum"" or ""current"", limit_str)}.                         3) e.g. ""volume_threshold"": {                                     ""all"": (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""),                                     ""buy"": (""current"", ""$askV1""),                                     ""sell"": (""current"", ""$bidV1""),                                 } :param open_cost:        cost rate for open, default 0.0015 :param close_cost:       cost rate for close, default 0.0025 :param trade_unit:       trade unit, 100 for China A market.                          None for disable trade unit.                          **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must                          distinguish `not set` and `disable trade_unit` :param min_cost:         min cost, default 5 :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1. :param extra_quote:     pandas, dataframe consists of                             columns: like [\'$vwap\', \'$close\', \'$volume\', \'$factor\', \'limit_sell\', \'limit_buy\'].                                     The limit indicates that the etf is tradable on a specific day.                                     Necessary fields:                                         $close is for calculating the total value at end of each day.                                     Optional fields:                                         $volume is only necessary when we limit the trade amount or calculate                                         PA(vwap) indicator                                         $vwap is only necessary when we use the $vwap price as the deal price                                         $factor is for rounding to the trading unit                                         limit_sell will be set to False by default (False indicates we can sell                                         this target on this day).                                         limit_buy will be set to False by default (False indicates we can buy                                         this target on this day).                             index: MultipleIndex(instrument, pd.Datetime)', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit\nParameter Please refer to the docs of get_amount_of_trade_unit deal_amount : float, adjusted amount factor : float, adjusted factor return : float, real amount', 'METHOD qlib.qlib.backtest.exchange.Exchange.__init__ has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.__init__.volume_threshold\n__init__ :param freq:             frequency of data :param start_time:       closed start time for backtest :param end_time:         closed end time for backtest :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50) :param deal_price:      Union[str, Tuple[str, str], List[str]]                         The `deal_price` supports following two types of input                         - <deal_price> : str                         - (<buy_price>, <sell_price>): Tuple[str] or List[str]                         <deal_price>, <buy_price> or <sell_price> := <price>                         <price> := str                         - for example \'$close\', \'$open\', \'$vwap\' (""close"" is OK. `Exchange` will help to prepend                           ""$"" to the expression) :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.                          It is useful when users want more fields to be queried :param limit_threshold: Union[Tuple[str, str], float, None]                         1) `None`: no limitation                         2) float, 0.1 for example, default None                         3) Tuple[str, str]: (<the expression for buying stock limitation>,                                              <the expression for sell stock limitation>)                                             `False` value indicates the stock is tradable                                             `True` value indicates the stock is limited and not tradable :param volume_threshold: Union[                             Dict[                                 ""all"": (""cum"" or ""current"", limit_str),                                 ""buy"": (""cum"" or ""current"", limit_str),                                 ""sell"":(""cum"" or ""current"", limit_str),                             ],                             (""cum"" or ""current"", limit_str),                          ]                         1) (""cum"" or ""current"", limit_str) denotes a single volume limit.                             - limit_str is qlib data expression which is allowed to define your own Operator.                             Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for                             high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom                             operator, you need to register it in qlib_init.                             - ""cum"" means that this is a cumulative value over time, such as cumulative market                             volume. So when it is used as a volume limit, it is necessary to subtract the dealt                             amount.                             - ""current"" means that this is a real-time value and will not accumulate over time,                             so it can be directly used as a capacity limit.                             e.g. (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""), (""current"", ""$bidV1"")                         2) ""all"" means the volume limits are both buying and selling.                         ""buy"" means the volume limits of buying. ""sell"" means the volume limits of selling.                         Different volume limits will be aggregated with min(). If volume_threshold is only                         (""cum"" or ""current"", limit_str) instead of a dict, the volume limits are for                         both by default. In other words, it is same as {""all"": (""cum"" or ""current"", limit_str)}.                         3) e.g. ""volume_threshold"": {                                     ""all"": (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""),                                     ""buy"": (""current"", ""$askV1""),                                     ""sell"": (""current"", ""$bidV1""),                                 } :param open_cost:        cost rate for open, default 0.0015 :param close_cost:       cost rate for close, default 0.0025 :param trade_unit:       trade unit, 100 for China A market.                          None for disable trade unit.                          **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must                          distinguish `not set` and `disable trade_unit` :param min_cost:         min cost, default 5 :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1. :param extra_quote:     pandas, dataframe consists of                             columns: like [\'$vwap\', \'$close\', \'$volume\', \'$factor\', \'limit_sell\', \'limit_buy\'].                                     The limit indicates that the etf is tradable on a specific day.                                     Necessary fields:                                         $close is for calculating the total value at end of each day.                                     Optional fields:                                         $volume is only necessary when we limit the trade amount or calculate                                         PA(vwap) indicator                                         $vwap is only necessary when we use the $vwap price as the deal price                                         $factor is for rounding to the trading unit                                         limit_sell will be set to False by default (False indicates we can sell                                         this target on this day).                                         limit_buy will be set to False by default (False indicates we can buy                                         this target on this day).                             index: MultipleIndex(instrument, pd.Datetime)', 'METHOD qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.round_amount_by_trade_unit.end_time\nParameter Please refer to the docs of get_amount_of_trade_unit deal_amount : float, adjusted amount factor : float, adjusted factor return : float, real amount', 'METHOD qlib.qlib.backtest.exchange.Exchange.__init__ has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.__init__.start_time\n__init__ :param freq:             frequency of data :param start_time:       closed start time for backtest :param end_time:         closed end time for backtest :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50) :param deal_price:      Union[str, Tuple[str, str], List[str]]                         The `deal_price` supports following two types of input                         - <deal_price> : str                         - (<buy_price>, <sell_price>): Tuple[str] or List[str]                         <deal_price>, <buy_price> or <sell_price> := <price>                         <price> := str                         - for example \'$close\', \'$open\', \'$vwap\' (""close"" is OK. `Exchange` will help to prepend                           ""$"" to the expression) :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.                          It is useful when users want more fields to be queried :param limit_threshold: Union[Tuple[str, str], float, None]                         1) `None`: no limitation                         2) float, 0.1 for example, default None                         3) Tuple[str, str]: (<the expression for buying stock limitation>,                                              <the expression for sell stock limitation>)                                             `False` value indicates the stock is tradable                                             `True` value indicates the stock is limited and not tradable :param volume_threshold: Union[                             Dict[                                 ""all"": (""cum"" or ""current"", limit_str),                                 ""buy"": (""cum"" or ""current"", limit_str),                                 ""sell"":(""cum"" or ""current"", limit_str),                             ],                             (""cum"" or ""current"", limit_str),                          ]                         1) (""cum"" or ""current"", limit_str) denotes a single volume limit.                             - limit_str is qlib data expression which is allowed to define your own Operator.                             Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for                             high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom                             operator, you need to register it in qlib_init.                             - ""cum"" means that this is a cumulative value over time, such as cumulative market                             volume. So when it is used as a volume limit, it is necessary to subtract the dealt                             amount.                             - ""current"" means that this is a real-time value and will not accumulate over time,                             so it can be directly used as a capacity limit.                             e.g. (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""), (""current"", ""$bidV1"")                         2) ""all"" means the volume limits are both buying and selling.                         ""buy"" means the volume limits of buying. ""sell"" means the volume limits of selling.                         Different volume limits will be aggregated with min(). If volume_threshold is only                         (""cum"" or ""current"", limit_str) instead of a dict, the volume limits are for                         both by default. In other words, it is same as {""all"": (""cum"" or ""current"", limit_str)}.                         3) e.g. ""volume_threshold"": {                                     ""all"": (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""),                                     ""buy"": (""current"", ""$askV1""),                                     ""sell"": (""current"", ""$bidV1""),                                 } :param open_cost:        cost rate for open, default 0.0015 :param close_cost:       cost rate for close, default 0.0025 :param trade_unit:       trade unit, 100 for China A market.                          None for disable trade unit.                          **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must                          distinguish `not set` and `disable trade_unit` :param min_cost:         min cost, default 5 :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1. :param extra_quote:     pandas, dataframe consists of                             columns: like [\'$vwap\', \'$close\', \'$volume\', \'$factor\', \'limit_sell\', \'limit_buy\'].                                     The limit indicates that the etf is tradable on a specific day.                                     Necessary fields:                                         $close is for calculating the total value at end of each day.                                     Optional fields:                                         $volume is only necessary when we limit the trade amount or calculate                                         PA(vwap) indicator                                         $vwap is only necessary when we use the $vwap price as the deal price                                         $factor is for rounding to the trading unit                                         limit_sell will be set to False by default (False indicates we can sell                                         this target on this day).                                         limit_buy will be set to False by default (False indicates we can buy                                         this target on this day).                             index: MultipleIndex(instrument, pd.Datetime)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1251, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.41606044769287,695
In what cases does the collector treat a dict response as invalid and log a warning instead of returning data?,"The collector treats a dict response as invalid and logs a warning instead of returning data when `_resp[symbol]` is a string, or when `_resp[symbol][","In YahooCollector.get_data_from_remote (scripts/data_collector/yahoo/collector.py), if `Ticker.history` returns a dict, it only “succeeds” when

• `_resp[symbol]` is a dict  
• and `_resp[symbol][""indicators""][""quote""]` exists and is not None  

If instead  
1) `_resp[symbol]` is a string, or  
2) `_resp[symbol][""indicators""][""quote""]` is missing/None  

then it calls `_show_logging_func()` (logs a warning) rather than returning the data.","[""CLASS qlib.qlib.data.data.ClientProvider inherits from or is a subclass of qlib.qlib.data.data.BaseProvider\nLocal provider class It is a set of interface that allow users to access data. Because PITD is not exposed publicly to users, so it is not included in the interface.  To keep compatible with old qlib provider.\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", 'FUNCTION qlib.qlib.rl.utils.finite_env.is_invalid has parameter or accepts argument qlib.qlib.rl.utils.finite_env.is_invalid.arr', ""CLASS qlib.qlib.data.data.ClientProvider has method qlib.qlib.data.data.ClientProvider.__init__\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", 'FUNCTION qlib.qlib.rl.utils.finite_env.is_invalid has parameter or accepts argument qlib.qlib.rl.utils.finite_env.T', ""CLASS qlib.qlib.data.data.ClientProvider has field or attribute or property qlib.qlib.data.data.ClientProvider.client\nClient Provider  Requesting data from server as a client. Can propose requests:      - Calendar : Directly respond a list of calendars     - Instruments (without filter): Directly respond a list/dict of instruments     - Instruments (with filters):  Respond a list/dict of instruments     - Features : Respond a cache uri  The general workflow is described as follows: When the user use client provider to propose a request, the client provider will connect the server and send the request. The client will start to wait for the response. The response will be made instantly indicating whether the cache is available. The waiting procedure will terminate only when the client get the response saying `feature_available` is true. `BUG` : Everytime we make request for certain data we need to connect to the server, wait for the response and disconnect from it. We can't make a sequence of requests within one connection. You can refer to https://python-socketio.readthedocs.io/en/latest/client.html for documentation of python-socketIO client."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crypto/README.md\n# Collect Crypto Data\n\n> *Please pay **ATTENTION** that the data is collected from [Coingecko](https://www.coingecko.com/en/api) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage of the dataset\n> *Crypto dataset only support Data retrieval function but not support backtest function due to the lack of OHLC data.*\n\n## Collector Data\n\n\n### Crypto Data\n\n#### 1d from Coingecko\n\n```bash\n\n# download from https://api.coingecko.com/api/v3/\npython collector.py download_data --source_dir ~/.qlib/crypto_data/source/1d --start 2015-01-01 --end 2021-11-30 --delay 1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/crypto_data/source/1d --normalize_dir ~/.qlib/crypto_data/source/1d_nor --interval 1d --date_field_name date\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/crypto_data/source/1d_nor --qlib_dir ~/.qlib/qlib_data/crypto_data --freq day --date_field_name date --include_fields prices,total_volumes,market_caps\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/crypto_data"")\ndf = D.features(D.instruments(market=""all""), [""$prices"", ""$total_volumes"",""$market_caps""], freq=""day"")\n```\n\n\n### Help\n```bash\npython collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- delay: 1', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1257, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.327369928359985,696
"How are the trading account, market exchange, strategy, and executor components assembled when starting a backtest?","The trading account, market exchange, strategy, and executor components are assembled when starting a backtest in qlib as follows:

1. The `get_strategy_executor` function is called to initialize the strategy and executor.
2. The `init_instance_by_config` function is used to instantiate the strategy of type `BaseStrategy`. 
3. The `init_instance_by_config` function is also used to instantiate an executor of type `BaseExecutor`. 
4. Both the strategy and the executor call the `reset_common_infra` method to integrate with the Account and Exchange.
5. A CommonInfrastructure object is created from an Account and an Exchange.
The resulting components are then ready for backtesting.","When you call get_strategy_executor (in qlib/backtest/__init__.py):

1. create_account_instance(...) builds the Account (cash + positions).  
2. get_exchange(...) builds the Exchange with your start/end times.  
3. A CommonInfrastructure is created from the Account + Exchange.  
4. init_instance_by_config(...) instantiates your strategy (BaseStrategy) and executor (BaseExecutor).  
5. Both strategy and executor call reset_common_infra(common_infra) to hook into the Account/Exchange.  
6. Finally, get_strategy_executor returns (trade_strategy, trade_executor) ready for backtest.","[""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.exchange_kwargs\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'CLASS qlib.qlib.strategy.base.BaseStrategy has method qlib.qlib.strategy.base.BaseStrategy.executor\nBase strategy for trading', ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.end_time\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'CLASS qlib.qlib.strategy.base.BaseStrategy has method qlib.qlib.strategy.base.BaseStrategy.trade_calendar\nBase strategy for trading', ""FUNCTION qlib.qlib.backtest.__init__.backtest has parameter or accepts argument qlib.qlib.backtest.__init__.backtest.account\ninitialize the strategy and executor, then backtest function for the interaction of the outermost strategy and executor in the nested decision execution  Parameters ---------- start_time : Union[pd.Timestamp, str]     closed start time for backtest     **NOTE**: This will be applied to the outmost executor's calendar. end_time : Union[pd.Timestamp, str]     closed end time for backtest     **NOTE**: This will be applied to the outmost executor's calendar.     E.g. Executor[day](Executor[1min]),   setting `end_time == 20XX0301` will include all the minutes on 20XX0301 strategy : Union[str, dict, object, Path]     for initializing outermost portfolio strategy. Please refer to the docs of init_instance_by_config for more     information. executor : Union[str, dict, object, Path]     for initializing the outermost executor. benchmark: str     the benchmark for reporting. account : Union[float, int, Position]     information for describing how to create the account     For `float` or `int`:         Using Account with only initial cash     For `Position`:         Using Account with a Position exchange_kwargs : dict     the kwargs for initializing Exchange pos_type : str     the type of Position.  Returns ------- portfolio_dict: PORT_METRIC     it records the trading portfolio_metrics information indicator_dict: INDICATOR_METRIC     it computes the trading indicator     It is organized in a dict format"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 29}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 29, 'n_files_pr': 19, 'pr': 1263, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.2587628364563,697
What mechanism excludes the aggregated account value field from individual instrument entries before building the DataFrame?,"The mechanism that excludes the aggregated account value field from individual instrument entries before building the DataFrame in qlib is the line `_value.pop(_item, None)` inside a for loop that iterates over `[","In parse_position (qlib/contrib/report/analysis_position/parse_position.py), before building the DataFrame it explicitly drops the aggregated account value by doing:

```python
for _item in [""now_account_value""]:
    _value.pop(_item, None)
```

i.e. it uses dict.pop on “now_account_value” to exclude it from the per-instrument entries.","['CLASS qlib.qlib.backtest.account.Account has field or attribute or property qlib.qlib.backtest.account.Account.init_cash\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'CLASS qlib.qlib.data.dataset.processor.FilterCol has field or attribute or property qlib.qlib.data.dataset.processor.fields_group', 'CLASS qlib.qlib.backtest.account.Account has field or attribute or property qlib.qlib.backtest.account.port_metr_enabled\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'CLASS qlib.qlib.data.dataset.processor.FilterCol has field or attribute or property qlib.qlib.data.dataset.processor.FilterCol.fields_group', 'CLASS qlib.qlib.backtest.account.Account has field or attribute or property qlib.qlib.backtest.account.Account.benchmark_config\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1267, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.275233030319214,698
Which HTTP response code causes the dataset availability check to return false?,"The HTTP response code that causes the dataset availability check to return False in qlib is 404, which indicates a 'Not Found' status.","In GetData.check_dataset (qlib/tests/data.py), it returns False when resp.status_code is 404.","['MODULE qlib.qlib.__init__ contains FUNCTION qlib.qlib.__init__.get_project_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'CLASS qlib.qlib.tests.data.GetData has method qlib.qlib.tests.data.GetData.check_dataset', 'FUNCTION qlib.qlib.__init__.get_project_path has parameter or accepts argument qlib.qlib.__init__.get_project_path.cur_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'METHOD qlib.qlib.tests.data.GetData.check_dataset has parameter or accepts argument qlib.qlib.tests.data.GetData.check_dataset.file_name', 'FUNCTION qlib.qlib.__init__.get_project_path has parameter or accepts argument qlib.qlib.__init__.get_project_path.config_name\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = ""~/.qlib/qlib_data/cn_data""  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments(\'csi500\')\n  print(D.list_instruments(instruments=instruments, start_time=\'2010-01-01\', end_time=\'2017-12-31\', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = [\'SH600000\']\n  fields = [\'$close\', \'$volume\', \'Ref($close, 1)\', \'Mean($close, 3)\', \'$high-$low\']\n  print(D.features(instruments, fields, start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1270, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.01906394958496,699
Which attribute is incorrectly referenced in the initialization log that causes the URI to be reported wrong?,The attribute that is incorrectly referenced in the initialization log that causes the URI to be reported wrong is `self.uri`. The correct attribute should be `self.default_uri` which is set from the `uri` parameter.,"The debug log is referencing `self.uri`, which doesn’t exist. It should be using `self.default_uri` (set from the `uri` parameter).","[""METHOD qlib.qlib.workflow.__init__.QlibRecorder.uri_context has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.uri_context.uri\nTemporarily set the exp_manager's **default_uri** to uri  NOTE: - Please refer to the NOTE in the `set_uri`  Parameters ---------- uri : Text     the temporal uri"", 'MODULE qlib.docs.conf contains GLOBAL_VARIABLE qlib.docs.conf.copyright', ""CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.uri_context\nTemporarily set the exp_manager's **default_uri** to uri  NOTE: - Please refer to the NOTE in the `set_uri`  Parameters ---------- uri : Text     the temporal uri\nA global system that helps to manage the experiments."", 'MODULE qlib.qlib.contrib.report.data.ana contains CLASS qlib.qlib.contrib.report.data.ana.ValueCNT', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.end_exp\nEnd an active experiment.  Maintaining `_active_exp_uri` is included in end_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_name : str     name of the active experiment. recorder_status : str     the status of the active recorder of the experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href=""https://github.com/microsoft/qlib/graphs/contributors""><img src=""https://contrib.rocks/image?repo=microsoft/qlib"" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib\'s document/code, you can follow the steps in the figure below.\n<p align=""center"">\n  <img src=""https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif"" />\n</p>\n\nIf you don\'t know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg \'TODO|FIXME\' qlib`\n \nIf you would like to become one of Qlib\'s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR\'s for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/csi300/csi500/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/SH000300/SH000905/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332±0.00 | 0.3044±0.00 | 0.0462±0.00 | 0.4326±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n| MLP        | Alpha158 | 0.0229±0.01 | 0.2181±0.05 | 0.0360±0.00 | 0.3409±0.02 | 0.0043±0.02       | 0.0602±0.27       | -0.2184±0.04 |\n| LightGBM   | Alpha158 | 0.0399±0.00 | 0.4065±0.00 | 0.0482±0.00 | 0.5101±0.00 | 0.1284±0.00       | 1.5650±0.00       | -0.0635±0.00 |\n| CatBoost   | Alpha158 | 0.0345±0.00 | 0.2855±0.00 | 0.0417±0.00 | 0.3740±0.00 | 0.0496±0.00       | 0.5977±0.00       | -0.1496±0.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380±0.00 | 0.3659±0.00 | 0.0442±0.00 | 0.4324±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258±0.00 | 0.2021±0.02 | 0.0426±0.00 | 0.3840±0.02 | 0.0022±0.02       | 0.0301±0.26       | -0.2064±0.02 |\n| LightGBM   | Alpha360 | 0.0400±0.00 | 0.3605±0.00 | 0.0536±0.00 | 0.5431±0.00 | 0.0505±0.00       | 0.7658±0.02       | -0.1880±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1271, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.14001750946045,700
What condition causes the method to log that the data is not in calendars and return without writing a bin file?,"The condition that causes the method to log that the data is not in calendars and return without writing a bin file is when the merged DataFrame `_df` is empty. This means that no dates overlap the calendar, resulting in an empty DataFrame. The code checks for this condition using `if _df.empty is True`, and if it's true, it logs a warning message and exits without writing a bin file.","In DumpDataBase._data_to_bin (scripts/dump_bin.py), right after calling self.data_merge_calendar(df, calendar_list), it checks:

```
_df = self.data_merge_calendar(...)
if _df.empty:
    logger.warning(f""{features_dir.name} data is not in calendars"")
    return
```

So whenever the merged DataFrame has no rows (i.e. no dates overlap the calendar), it logs that message and exits without writing a bin file.","['MODULE keras.keras.src.utils.io_utils contains FUNCTION keras.keras.src.utils.io_utils.is_interactive_logging_enabled\nCheck if interactive logging is enabled.  To switch between writing logs to stdout and `absl.logging`, you may use `keras.config.enable_interactive_logging()` and `keras.config.disable_interactive_logging()`.  Returns:     Boolean, `True` if interactive logging is enabled,     and `False` otherwise.', 'MODULE qlib.qlib.utils.file contains GLOBAL_VARIABLE qlib.qlib.utils.file.log', 'MODULE keras.keras.src.utils.io_utils contains FUNCTION keras.keras.src.utils.io_utils.set_logging_verbosity\nSets the verbosity level for logging.  Supported log levels are as follows:  - `""FATAL""` (least verbose) - `""ERROR""` - `""WARNING""` - `""INFO""` - `""DEBUG""` (most verbose)  Args:     level: A string corresponding to the level of verbosity for logging.', 'MODULE qlib.qlib.data.storage.file_storage contains GLOBAL_VARIABLE qlib.qlib.data.storage.file_storage.logger', 'FUNCTION keras.keras.src.utils.io_utils.set_logging_verbosity has parameter or accepts argument keras.keras.src.utils.io_utils.set_logging_verbosity.level\nSets the verbosity level for logging.  Supported log levels are as follows:  - `""FATAL""` (least verbose) - `""ERROR""` - `""WARNING""` - `""INFO""` - `""DEBUG""` (most verbose)  Args:     level: A string corresponding to the level of verbosity for logging.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1273, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",32.05219483375549,701
"After filtering rows to the specified hour and minute, how does the processor convert each timestamp index entry to a date-only value?","After filtering rows to the specified hour and minute, Resample1minProcessor.__call__ converts each timestamp index entry to a date-only value by executing `df.index = df.index.normalize()`, which strips the time component from each index entry using DatetimeIndex.normalize(). This leaves only the date for each index entry.","In Resample1minProcessor.__call__, once rows are filtered by hour/minute, it does:

```python
df.index = df.index.normalize()
```

Calling DatetimeIndex.normalize() strips the time component, leaving only the date for each index entry.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.read_period_data\nAt `cur_date`(e.g. 20190102), read the information at `period`(e.g. 201803). Only the updating info before cur_date or at cur_date will be used.  Parameters ---------- period: int     date period represented by interger, e.g. 201901 corresponds to the first quarter in 2019 cur_date_int: int     date which represented by interger, e.g. 20190102 last_period_index: int     it is a optional parameter; it is designed to avoid repeatedly access the .index data of PIT database when     sequentially observing the data (Because the latest index of a specific period of data certainly appear in after the one in last observation).  Returns ------- the query value and byte index the index value', 'MODULE qlib.qlib.utils.resam contains FUNCTION qlib.qlib.utils.resam.resam_ts_data\nResample value from time-series data      - If `feature` has MultiIndex[instrument, datetime], apply the `method` to each instruemnt data with datetime in [start_time, end_time]         Example:          .. code-block::              print(feature)                                     $close      $volume             instrument  datetime             SH600000    2010-01-04  86.778313   16162960.0                         2010-01-05  87.433578   28117442.0                         2010-01-06  85.713585   23632884.0                         2010-01-07  83.788803   20813402.0                         2010-01-08  84.730675   16044853.0              SH600655    2010-01-04  2699.567383  158193.328125                         2010-01-08  2612.359619   77501.406250                         2010-01-11  2712.982422  160852.390625                         2010-01-12  2788.688232  164587.937500                         2010-01-13  2790.604004  145460.453125              print(resam_ts_data(feature, start_time=""2010-01-04"", end_time=""2010-01-05"", fields=[""$close"", ""$volume""], method=""last""))                         $close      $volume             instrument             SH600000    87.433578 28117442.0             SH600655    2699.567383  158193.328125      - Else, the `feature` should have Index[datetime], just apply the `method` to `feature` directly         Example:          .. code-block::             print(feature)                         $close      $volume             datetime             2010-01-04  86.778313   16162960.0             2010-01-05  87.433578   28117442.0             2010-01-06  85.713585   23632884.0             2010-01-07  83.788803   20813402.0             2010-01-08  84.730675   16044853.0              print(resam_ts_data(feature, start_time=""2010-01-04"", end_time=""2010-01-05"", method=""last""))              $close 87.433578             $volume 28117442.0              print(resam_ts_data(feature[\'$close\'], start_time=""2010-01-04"", end_time=""2010-01-05"", method=""last""))              87.433578  Parameters ---------- ts_feature : Union[pd.DataFrame, pd.Series]     Raw time-series feature to be resampled start_time : Union[str, pd.Timestamp], optional     start sampling time, by default None end_time : Union[str, pd.Timestamp], optional     end sampling time, by default None method : Union[str, Callable], optional     sample method, apply method function to each stock series data, by default ""last""     - If type(method) is str or callable function, it should be an attribute of SeriesGroupBy or DataFrameGroupby, and applies groupy.method for the sliced time-series data     - If method is None, do nothing for the sliced time-series data. method_kwargs : dict, optional     arguments of method, by default {}  Returns -------     The resampled DataFrame/Series/value, return None when the resampled data is empty.', 'FUNCTION qlib.qlib.utils.__init__.read_period_data has parameter or accepts argument qlib.qlib.utils.__init__.read_period_data.index_path\nAt `cur_date`(e.g. 20190102), read the information at `period`(e.g. 201803). Only the updating info before cur_date or at cur_date will be used.  Parameters ---------- period: int     date period represented by interger, e.g. 201901 corresponds to the first quarter in 2019 cur_date_int: int     date which represented by interger, e.g. 20190102 last_period_index: int     it is a optional parameter; it is designed to avoid repeatedly access the .index data of PIT database when     sequentially observing the data (Because the latest index of a specific period of data certainly appear in after the one in last observation).  Returns ------- the query value and byte index the index value', 'FUNCTION qlib.qlib.utils.resam.resam_ts_data has parameter or accepts argument qlib.qlib.utils.resam.resam_ts_data.start_time\nResample value from time-series data      - If `feature` has MultiIndex[instrument, datetime], apply the `method` to each instruemnt data with datetime in [start_time, end_time]         Example:          .. code-block::              print(feature)                                     $close      $volume             instrument  datetime             SH600000    2010-01-04  86.778313   16162960.0                         2010-01-05  87.433578   28117442.0                         2010-01-06  85.713585   23632884.0                         2010-01-07  83.788803   20813402.0                         2010-01-08  84.730675   16044853.0              SH600655    2010-01-04  2699.567383  158193.328125                         2010-01-08  2612.359619   77501.406250                         2010-01-11  2712.982422  160852.390625                         2010-01-12  2788.688232  164587.937500                         2010-01-13  2790.604004  145460.453125              print(resam_ts_data(feature, start_time=""2010-01-04"", end_time=""2010-01-05"", fields=[""$close"", ""$volume""], method=""last""))                         $close      $volume             instrument             SH600000    87.433578 28117442.0             SH600655    2699.567383  158193.328125      - Else, the `feature` should have Index[datetime], just apply the `method` to `feature` directly         Example:          .. code-block::             print(feature)                         $close      $volume             datetime             2010-01-04  86.778313   16162960.0             2010-01-05  87.433578   28117442.0             2010-01-06  85.713585   23632884.0             2010-01-07  83.788803   20813402.0             2010-01-08  84.730675   16044853.0              print(resam_ts_data(feature, start_time=""2010-01-04"", end_time=""2010-01-05"", method=""last""))              $close 87.433578             $volume 28117442.0              print(resam_ts_data(feature[\'$close\'], start_time=""2010-01-04"", end_time=""2010-01-05"", method=""last""))              87.433578  Parameters ---------- ts_feature : Union[pd.DataFrame, pd.Series]     Raw time-series feature to be resampled start_time : Union[str, pd.Timestamp], optional     start sampling time, by default None end_time : Union[str, pd.Timestamp], optional     end sampling time, by default None method : Union[str, Callable], optional     sample method, apply method function to each stock series data, by default ""last""     - If type(method) is str or callable function, it should be an attribute of SeriesGroupBy or DataFrameGroupby, and applies groupy.method for the sliced time-series data     - If method is None, do nothing for the sliced time-series data. method_kwargs : dict, optional     arguments of method, by default {}  Returns -------     The resampled DataFrame/Series/value, return None when the resampled data is empty.', 'FUNCTION qlib.qlib.utils.__init__.read_period_data has parameter or accepts argument qlib.qlib.utils.__init__.read_period_data.quarterly\nAt `cur_date`(e.g. 20190102), read the information at `period`(e.g. 201803). Only the updating info before cur_date or at cur_date will be used.  Parameters ---------- period: int     date period represented by interger, e.g. 201901 corresponds to the first quarter in 2019 cur_date_int: int     date which represented by interger, e.g. 20190102 last_period_index: int     it is a optional parameter; it is designed to avoid repeatedly access the .index data of PIT database when     sequentially observing the data (Because the latest index of a specific period of data certainly appear in after the one in last observation).  Returns ------- the query value and byte index the index value', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1285, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",42.88751435279846,702
"What column name prefixes distinguish metrics, parameters, and tags in the DataFrame returned by a records search?","The column name prefixes that distinguish metrics, parameters, and tags in the DataFrame returned by a records search in qlib are:
* Metrics: `metrics.<metric_name>`
* Parameters: `params.<param_name>`
* Tags: `tags.<tag_name>`","In QlibRecorder.search_records (qlib/workflow/__init__.py), the DataFrame expands metrics, params and tags into columns prefixed with:

• metrics.<metric_name>  
• params.<param_name>  
• tags.<tag_name>","['CLASS qlib.qlib.workflow.__init__.QlibRecorder has method qlib.qlib.workflow.__init__.QlibRecorder.search_records\nGet a pandas DataFrame of records that fit the search criteria.  The arguments of this function are not set to be rigid, and they will be different with different implementation of ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the example code of the method with the ``MLflowExpManager``:  .. code-block:: Python      R.log_metrics(m=2.50, step=0)     records = R.search_records([experiment_id], order_by=[""metrics.m DESC""])  Parameters ---------- experiment_ids : list     list of experiment IDs. filter_string : str     filter query string, defaults to searching all runs. run_view_type : int     one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType). max_results  : int     the maximum number of runs to put in the dataframe. order_by : list     list of columns to order by (e.g., “metrics.rmse”).  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don\'t have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nA global system that helps to manage the experiments.', 'METHOD qlib.qlib.workflow.__init__.QlibRecorder.search_records has parameter or accepts argument qlib.qlib.workflow.__init__.QlibRecorder.search_records.experiment_ids\nGet a pandas DataFrame of records that fit the search criteria.  The arguments of this function are not set to be rigid, and they will be different with different implementation of ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the example code of the method with the ``MLflowExpManager``:  .. code-block:: Python      R.log_metrics(m=2.50, step=0)     records = R.search_records([experiment_id], order_by=[""metrics.m DESC""])  Parameters ---------- experiment_ids : list     list of experiment IDs. filter_string : str     filter query string, defaults to searching all runs. run_view_type : int     one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType). max_results  : int     the maximum number of runs to put in the dataframe. order_by : list     list of columns to order by (e.g., “metrics.rmse”).  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don\'t have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.', ""CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.search_records\nGet a pandas DataFrame of records that fit the search criteria of the experiment. Inputs are the search criteria user want to apply.  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don't have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)"", 'METHOD qlib.qlib.data.data.DatasetProvider.get_column_names has parameter or accepts argument qlib.qlib.data.data.DatasetProvider.get_column_names.fields\nGet column names from input fields', ""CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.search_records\nGet a pandas DataFrame of records that fit the search criteria of the experiment. Inputs are the search criteria user want to apply.  Returns ------- A pandas.DataFrame of records, where each metric, parameter, and tag are expanded into their own columns named metrics.*, params.*, and tags.* respectively. For records that don't have a particular metric, parameter, or tag, their value will be (NumPy) Nan, None, or None respectively.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1287, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.02769446372986,703
What steps does the simulator executor follow to execute and track orders within a single trading step?,"The simulator executor follows these steps to execute and track orders within a single trading step in qlib:

1. The executor calls `_get_order_iterator` with `trade_decision` to pull orders.
2. It sorts orders according to `self.trade_type`, which can be serial or parallel by direction.
3. For each order, it floors the timestamp to determine `now_deal_day` and checks if it's a new day.
4. If it's a new day, it resets `self.dealt_order_amount` and updates `self.deal_day`.
5. It executes each order by calling `trade_exchange.deal_order` with `order`, `trade_account`, and `dealt_order_amount`.
6. The executor appends the execution result to a local results list and increments `dealt_order_amount[order.stock_id]` by `order.deal_amount`.
7. If verbose is enabled, it prints a summary line including time, buy or sell, price, amounts, and cash.
8. Finally, it returns the list of execution tuples and a dictionary containing ""trade_info"".","Within a single step (SimulatorExecutor._collect_data), the executor:

1. Calls _get_order_iterator(trade_decision) to pull & sort orders according to self.trade_type (serial or parallel by direction).  
2. Retrieves the current step timestamp via trade_calendar.get_step_time().  
3. For each order:  
   a. Floors the timestamp to “now_deal_day” and, if it’s a new day, resets self.dealt_order_amount and updates self.deal_day.  
   b. Executes the order via trade_exchange.deal_order(order, trade_account, dealt_order_amount), which returns (trade_val, trade_cost, trade_price).  
   c. Appends (order, trade_val, trade_cost, trade_price) to a local results list.  
   d. Increments dealt_order_amount[order.stock_id] by order.deal_amount.  
   e. If verbose, prints a summary line (time, buy/sell, price, amounts, cash).  
4. Returns the list of execution tuples plus a dict {""trade_info"": …}.","['CLASS qlib.qlib.backtest.executor.NestedExecutor has method qlib.qlib.backtest.executor.NestedExecutor.__init__\nParameters ---------- inner_executor : BaseExecutor     trading env in each trading bar. inner_strategy : BaseStrategy     trading strategy in each trading bar skip_empty_decision: bool     Will the executor skip call inner loop when the decision is empty.     It should be False in following cases     - The decisions may be updated by steps     - The inner executor may not follow the decisions from the outer strategy align_range_limit: bool     force to align the trade_range decision     It is only for nested executor, because range_limit is given by outer strategy\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple has method qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple.step\nExecute one step or SAOE.  Parameters ---------- amount     The amount you wish to deal. The simulator doesn\'t guarantee all the amount to be successfully dealt.\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', ""CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.step\nExecute one step or SAOE.  Parameters ---------- action (float):     The amount you wish to deal. The simulator doesn't guarantee all the amount to be successfully dealt.\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit."", 'MODULE qlib.qlib.rl.order_execution.simulator_simple contains CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'CLASS qlib.qlib.backtest.executor.NestedExecutor inherits from or is a subclass of qlib.qlib.backtest.executor.BaseExecutor\nBase executor for trading\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 1291, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",44.708635330200195,704
How does the current implementation handle values beyond the 3σ threshold when outlier clipping is enabled?,"The current implementation in qlib handles values beyond the 3σ threshold when outlier clipping is enabled by setting them to -3 or 3 using `np.clip(X, -3, 3)`. This means that any value of X greater than 3 will be set to 3, and any value less than -3 will be set to -3.","In RobustZScoreNorm.__call__ (qlib/data/dataset/processor.py), once you’ve z-scored X, if clip_outlier is True it simply does:

    X = np.clip(X, –3, 3)

i.e. any z-score > 3 is set to 3 and any < –3 set to –3.","[""CLASS qlib.qlib.rl.utils.log.LogWriter has method qlib.qlib.rl.utils.log.LogWriter.load_state_dict\nLoad the states of current logger from a dict.\nBase class for log writers, triggered at every reset and step by finite env.  What to do with a specific log depends on the implementation of subclassing :class:`LogWriter`. The general principle is that, it should handle logs above its loglevel (inclusive), and discard logs that are not acceptable. For instance, console loggers obviously can't handle an image."", 'METHOD qlib.qlib.data.dataset.processor.RobustZScoreNorm.__init__ has parameter or accepts argument qlib.qlib.data.dataset.processor.RobustZScoreNorm.__init__.clip_outlier', 'MODULE cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate contains FUNCTION cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate.rename_databases\nWhen overwrite is enabled, back up the original old_db (file with .lock and .wal or directory) by renaming it to *_old, and replace it with the newly imported new_db files.  When delete_old is enabled replace the old database with the new one and delete old database', 'FUNCTION keras.keras.src.legacy.backend.clip has parameter or accepts argument keras.keras.src.legacy.backend.clip.max_value\nDEPRECATED.', 'MODULE keras.keras.src.activations.activations contains FUNCTION keras.keras.src.activations.activations.threshold\nThreshold activation function.  It is defined as:  `threshold(x) = x` if `x > threshold`, `threshold(x) = default_value` otherwise.  Args:     x: Input tensor.     threshold: The value that decides when to retain or replace x.     default_value: Value to assign when `x <= threshold`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1294, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.958881378173828,705
How does the main backtest function schedule simulator-based execution across individual stocks when the simulator flag is enabled?,"The main backtest function schedules simulator-based execution across individual stocks when the simulator flag is enabled by using the `single_with_simulator` function from the qlib library. This function takes in a backtest config, orders to be executed, and other parameters such as split method, cash limit, and generate report flag. It then uses the `SingleAssetOrderExecution` class from the qlib library to execute the orders for each stock in parallel using multiple processes.

In this process, `joblib.Parallel` is invoked with `n_jobs` set to `backtest_config[","In backtest (qlib/rl/contrib/backtest.py), when you call  
```python
backtest(..., with_simulator=True)
```  
it does:

1. Picks  
   ```python
   single = single_with_simulator
   ```  
2. Builds a stock‐sorted list  
   ```python
   stock_pool = order_df[""instrument""].unique()
   ```  
3. Uses joblib’s Parallel with  
   ```
   n_jobs=backtest_config[""concurrency""], backend=""multiprocessing""
   ```  
   to launch one process per stock:  
   ```python
   res = Parallel(**mp_config)(
       delayed(single)(
           backtest_config,
           orders=order_df[order_df.instrument == stock],
           split=""stock"",
           cash_limit=…,
           generate_report=…
       )
       for stock in stock_pool
   )
   ```  
Each worker runs single_with_simulator on that stock’s orders, which in turn instantiates a SingleAssetOrderExecution (via _get_multi_level_executor_config) for each order/date and runs the simulator sequentially.","['MODULE qlib.qlib.rl.order_execution.simulator_qlib contains CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.single_with_simulator.backtest_config\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.get_state\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'FUNCTION qlib.qlib.rl.contrib.backtest.single_with_simulator has parameter or accepts argument qlib.qlib.rl.contrib.backtest.report\nRun backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day. A new simulator will be created and used for every single-day order.  Parameters ---------- backtest_config:     Backtest config orders:     Orders to be executed. Example format:              datetime instrument  amount  direction         0  2020-06-01       INST   600.0          0         1  2020-06-02       INST   700.0          1         ... split     Method to split orders. If it is ""stock"", split orders by stock. If it is ""day"", split orders by date. cash_limit     Limitation of cash. generate_report     Whether to generate reports.  Returns -------     If generate_report is True, return execution records and the generated report. Otherwise, return only records.', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.twap_price\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 19}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 19, 'n_files_pr': 11, 'pr': 1299, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",46.195197343826294,706
Which handler should you use to generate paused-filtered orderbook features in backtesting?,"To generate paused-filtered orderbook features in backtesting, you should use **HighFreqBacktestOrderHandler**. This class is located in `qlib/contrib/data/highfreq_handler.py` and defines a method called `get_feature_config` that applies the paused-filtered Select(…) to all orderbook fields.","You want to use the backtest‐only orderbook handler:  
In qlib/contrib/data/highfreq_handler.py, use the HighFreqBacktestOrderHandler class (its get_feature_config applies the paused‐filter Select(…) to all orderbook fields).","['FUNCTION qlib.qlib.workflow.task.utils.replace_task_handler_with_cache has parameter or accepts argument qlib.qlib.workflow.task.utils.replace_task_handler_with_cache.cache_dir\nReplace the handler in task with a cache handler. It will automatically cache the file and save it in cache_dir.  >>> import qlib >>> qlib.auto_init() >>> import datetime >>> # it is simplified task >>> task = {""dataset"": {""kwargs"":{\'handler\': {\'class\': \'Alpha158\', \'module_path\': \'qlib.contrib.data.handler\', \'kwargs\': {\'start_time\': datetime.date(2008, 1, 1), \'end_time\': datetime.date(2020, 8, 1), \'fit_start_time\': datetime.date(2008, 1, 1), \'fit_end_time\': datetime.date(2014, 12, 31), \'instruments\': \'CSI300\'}}}}} >>> new_task = replace_task_handler_with_cache(task) >>> print(new_task) {\'dataset\': {\'kwargs\': {\'handler\': \'file...Alpha158.3584f5f8b4.pkl\'}}}', 'CLASS qlib.qlib.contrib.data.highfreq_handler.HighFreqOrderHandler has method qlib.qlib.contrib.data.highfreq_handler.HighFreqOrderHandler.get_normalized_price_feature', 'MODULE qlib.qlib.workflow.task.utils contains FUNCTION qlib.qlib.workflow.task.utils.replace_task_handler_with_cache\nReplace the handler in task with a cache handler. It will automatically cache the file and save it in cache_dir.  >>> import qlib >>> qlib.auto_init() >>> import datetime >>> # it is simplified task >>> task = {""dataset"": {""kwargs"":{\'handler\': {\'class\': \'Alpha158\', \'module_path\': \'qlib.contrib.data.handler\', \'kwargs\': {\'start_time\': datetime.date(2008, 1, 1), \'end_time\': datetime.date(2020, 8, 1), \'fit_start_time\': datetime.date(2008, 1, 1), \'fit_end_time\': datetime.date(2014, 12, 31), \'instruments\': \'CSI300\'}}}}} >>> new_task = replace_task_handler_with_cache(task) >>> print(new_task) {\'dataset\': {\'kwargs\': {\'handler\': \'file...Alpha158.3584f5f8b4.pkl\'}}}', 'CLASS qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy has method qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.filter_stock', 'FUNCTION qlib.qlib.workflow.task.utils.replace_task_handler_with_cache has parameter or accepts argument qlib.qlib.workflow.task.utils.replace_task_handler_with_cache.task\nReplace the handler in task with a cache handler. It will automatically cache the file and save it in cache_dir.  >>> import qlib >>> qlib.auto_init() >>> import datetime >>> # it is simplified task >>> task = {""dataset"": {""kwargs"":{\'handler\': {\'class\': \'Alpha158\', \'module_path\': \'qlib.contrib.data.handler\', \'kwargs\': {\'start_time\': datetime.date(2008, 1, 1), \'end_time\': datetime.date(2020, 8, 1), \'fit_start_time\': datetime.date(2008, 1, 1), \'fit_end_time\': datetime.date(2014, 12, 31), \'instruments\': \'CSI300\'}}}}} >>> new_task = replace_task_handler_with_cache(task) >>> print(new_task) {\'dataset\': {\'kwargs\': {\'handler\': \'file...Alpha158.3584f5f8b4.pkl\'}}}', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=""#main-challenges--solutions-in-quant-research"">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=""circle""><a href=""#forecasting-finding-valuable-signalspatterns"">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=""disc""><a href=""#quant-model-paper-zoo""><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=""circle""><a href=""#run-a-single-model"">Run a Single Model</a></li>\n                    <li type=""circle""><a href=""#run-multiple-models"">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=""circle""><a href=""#adapting-to-market-dynamics"">Adapting to Market Dynamics</a></li>\n          <li type=""circle""><a href=""#reinforcement-learning-modeling-continuous-decisions"">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=""align: center"">\n<img src=""docs/_static/img/framework-abstract.jpg"" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib\'s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 1, 'pr': 1302, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",30.734810829162598,707
Describe the flow by which a BaseTradeDecision is refreshed each step and how its execution window is enforced when nested executors apply range limits.,"The flow by which a BaseTradeDecision is refreshed each step and how its execution window is enforced when nested executors apply range limits in qlib is as follows:

1.  At the start of each backtest step, NestedExecutor calls `decision.update(inner_calendar)`, where `inner_calendar.get_trade_len()` sets `self.total_step`.
2.  This triggers a two-phase refresh of `BaseTradeDecision`: first, it updates its trade decision using `strategy.update_trade_decision(self, inner_calendar)`; then, it gets the range limit via `decision.get_range_limit(default_value=parent_limit, inner_calendar=inner_calendar)`, where `default_value` is set to `parent_limit`. This represents the outer executor’s window.
3.  The `get_range_limit` method internally calls `_get_range_limit` via `TradeRange` or `IdxTradeRange` to get a `[start,end]` index, which is then clipped by `self.total_step` and issues a warning if out of bounds.
4.  If no trade range is defined, it falls back to `default_value`. The returned `[start,end]` slice is used to restrict the inner executor’s loop over the decision.
5.  Any outer trade range is propagated down via `BaseTradeDecision.mod_inner_decision`, allowing deeply nested strategies to inherit the window.","Every backtest step, the NestedExecutor drives a two‐phase refresh of a BaseTradeDecision:

1. At the start of the step, NestedExecutor calls  
     decision.update(inner_calendar)  
   which in BaseTradeDecision.update  
    – sets self.total_step = inner_calendar.get_trade_len()  
    – invokes strategy.update_trade_decision(self, inner_calendar)  
   allowing the strategy to produce or revise its concrete orders.

2. Before kicking off any sub‐executor, NestedExecutor calls  
     decision.get_range_limit(  
       default_value=parent_limit,    # the outer executor’s window  
       inner_calendar=inner_calendar  
     )  
   which under the hood does:  
    – _get_range_limit(...) via your TradeRange (or IdxTradeRange) to get a [start,end] index  
    – clips that interval by self.total_step (warning if out of bounds)  
   and if no trade_range is defined, falls back to default_value.  

That [start,end] slice is then used to restrict the inner executor’s loop over the decision. Any outer trade_range is also propagated down via BaseTradeDecision.mod_inner_decision so deeply nested strategies inherit the window.","['CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.update_bar_end\nupdate account at each trading bar step  Parameters ---------- trade_start_time : pd.Timestamp     closed start time of step trade_end_time : pd.Timestamp     closed end time of step trade_exchange : Exchange     trading exchange, used to update current atomic : bool     whether the trading executor is atomic, which means there is no higher-frequency trading executor inside it     - if atomic is True, calculate the indicators with trade_info     - else, aggregate indicators with inner indicators outer_trade_decision: BaseTradeDecision     external trade decision trade_info : List[(Order, float, float, float)], optional     trading information, by default None     - necessary if atomic is True     - list of tuple(order, trade_val, trade_cost, trade_price) inner_order_indicators : Indicator, optional     indicators of inner executor, by default None     - necessary if atomic is False     - used to aggregate outer indicators decision_list: List[Tuple[BaseTradeDecision, pd.Timestamp, pd.Timestamp]] = None,     The decision list of the inner level: List[Tuple[<decision>, <start_time>, <end_time>]]     The inner level indicator_config : dict, optional     config of calculating indicators, by default {}\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'CLASS qlib.qlib.backtest.decision.BaseTradeDecision has method qlib.qlib.backtest.decision.BaseTradeDecision.get_range_limit\nreturn the expected step range for limiting the decision execution time Both left and right are **closed**  if no available trade_range, `default_value` will be returned  It is only used in `NestedExecutor` - The outmost strategy will not follow any range limit (but it may give range_limit) - The inner most strategy\'s range_limit will be useless due to atomic executors don\'t have such   features.  **NOTE**: 1) This function must be called after `self.update` in following cases(ensured by NestedExecutor): - user relies on the auto-clip feature of `self.update`  2) This function will be called after _init_sub_trading in NestedExecutor.  Parameters ---------- **kwargs:     {         ""default_value"": <default_value>, # using dict is for distinguish no value provided or None provided         ""inner_calendar"": <trade calendar of inner strategy>         # because the range limit  will control the step range of inner strategy, inner calendar will be a         # important parameter when trade_range is callable     }  Returns ------- Tuple[int, int]:  Raises ------ NotImplementedError:     If the following criteria meet     1) the decision can\'t provide a unified start and end     2) default_value is not provided\nTrade decisions are made by strategy and executed by executor  Motivation:     Here are several typical scenarios for `BaseTradeDecision`      Case 1:     1. Outer strategy makes a decision. The decision is not available at the start of current interval     2. After a period of time, the decision are updated and become available     3. The inner strategy try to get the decision and start to execute the decision according to `get_range_limit`     Case 2:     1. The outer strategy\'s decision is available at the start of the interval     2. Same as `case 1.3`', 'CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple has method qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple.done\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'METHOD qlib.qlib.backtest.executor.NestedExecutor.__init__ has parameter or accepts argument qlib.qlib.backtest.executor.NestedExecutor.__init__.time_per_step\nParameters ---------- inner_executor : BaseExecutor     trading env in each trading bar. inner_strategy : BaseStrategy     trading strategy in each trading bar skip_empty_decision: bool     Will the executor skip call inner loop when the decision is empty.     It should be False in following cases     - The decisions may be updated by steps     - The inner executor may not follow the decisions from the outer strategy align_range_limit: bool     force to align the trade_range decision     It is only for nested executor, because range_limit is given by outer strategy', 'CLASS qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple has method qlib.qlib.rl.order_execution.simulator_simple.SingleAssetOrderExecutionSimple.get_backtest_data\nSingle-asset order execution (SAOE) simulator.  As there\'s no ""calendar"" in the simple simulator, ticks are used to trade. A tick is a record (a line) in the pickle-styled data file. Each tick is considered as a individual trading opportunity. If such fine granularity is not needed, use ``ticks_per_step`` to lengthen the ticks for each step.  In each step, the traded amount are ""equally"" separated to each tick, then bounded by volume maximum execution volume (i.e., ``vol_threshold``), and if it\'s the last step, try to ensure all the amount to be executed.  Parameters ---------- order     The seed to start an SAOE simulator is an order. data_dir     Path to load backtest data. feature_columns_today     Columns of today\'s feature. feature_columns_yesterday     Columns of yesterday\'s feature. data_granularity     Number of ticks between consecutive data entries. ticks_per_step     How many ticks per step. vol_threshold     Maximum execution volume (divided by market execution volume).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1308, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",50.05616879463196,708
How are the mid-close and mid-open times determined for regions outside China in the sample-minute function?,The mid-close and mid-open times are determined by setting `mid_close_time` equal to `close_time` and `mid_open_time` equal to `open_time` in the `else` branch of the `cal_sam_minute` function. The `open_time` and `close_time` were computed as `day_time + region_time[0] / region_time[-1] minus the shift`. This computation is used for regions outside China in the sample-minute function.,"In cal_sam_minute (tests/misc/test_utils.py), for any region ≠ CN (i.e. in the `else` branch) you simply do:

• mid_close_time = close_time  
• mid_open_time  = open_time  

(where open_time and close_time were already computed as  
day_time + region_time[0] / region_time[-1] minus the shift).","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'FUNCTION qlib.qlib.utils.time.get_min_cal has parameter or accepts argument qlib.qlib.utils.time.CN_TIME\nget the minute level calendar in day period  Parameters ---------- shift : int     the shift direction would be like pandas shift.     series.shift(1) will replace the value at `i`-th with the one at `i-1`-th region: str     Region, for example, ""cn"", ""us""  Returns ------- List[time]:', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'FUNCTION qlib.qlib.utils.time.time_to_day_index has parameter or accepts argument qlib.qlib.utils.time.time_to_day_index.region', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 1310, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",33.84624409675598,709
How does the LP handler route fetch calls to the appropriate underlying dataset based on the requested data key?,"The LP handler routes fetch calls to the appropriate underlying dataset based on the requested data key by using a dictionary called ATTR_MAP. This dictionary maps DK_R, DK_I, and DK_L to raw, infer, and learn respectively. When the `fetch()` method is called with a specific data key, it uses the ATTR_MAP to select the correct DataFrame from the DataHandlerLP instance. The selected DataFrame is then passed to the shared `_fetch_data()` routine for further processing. This mechanism allows the LP handler to route fetch calls to the appropriate underlying dataset based on the requested data key, making it easier to manage and process different types of data.","DataHandlerLP keeps three internal DataFrames—raw (`self._data`), infer (`self._infer`) and learn (`self._learn`)—and a class‐level ATTR_MAP that maps `DK_R`, `DK_I`, `DK_L` to those names. Its override of fetch() simply does:

1.  `_get_df_by_key(data_key)` (uses ATTR_MAP) to pick the right DataFrame.
2.  Passes that into the shared `_fetch_data(...)` routine.

So by calling `dh.fetch(…, data_key=DK_L)` you’ll get slices of `self._learn`, `data_key=DK_I` gives you `self._infer`, etc.","['MODULE qlib.qlib.workflow.online.update contains CLASS qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has field or attribute or property qlib.qlib.workflow.online.update.loader_cls\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerABC.DK_I', 'CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has field or attribute or property qlib.qlib.workflow.online.update.to_date\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 1, 'pr': 1312, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.58860468864441,710
What are the three sequential stages executed in the TestAllFlow suite?,"The three sequential stages executed in the TestAllFlow suite are:

1. `train()`: This function fits the model, generates predictions, IC, and RIC.
2. `backtest_analysis()`: This function is called on the predictions generated by the `train()` function.
3. `fake_experiment()`: This function validates the default URI and the current URI.","The suite runs, in order:

1. TestAllFlow.test_0_train – calls train() to fit the model, generate predictions and IC/RIC.  
2. TestAllFlow.test_1_backtest – calls backtest_analysis() on those predictions.  
3. TestAllFlow.test_2_expmanager – calls fake_experiment() to validate default and current URIs.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.backtest.executor contains GLOBAL_VARIABLE qlib.qlib.backtest.executor.decisions', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'FUNCTION qlib.qlib.backtest.__init__.get_strategy_executor has parameter or accepts argument qlib.qlib.backtest.__init__.get_strategy_executor.end_time', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 11, 'pr': 1314, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",29.29783344268799,711
What is the sequence of steps from calling the public training API to collecting episodes and updating the policy?,"The sequence of steps from calling the public training API to collecting episodes and updating the policy in qlib/rl/trainer/api.py are as follows:

1. The `train` function builds a `TrainingVessel`.
2. The `train` function builds a `Trainer`.
3. The `train` function calls `Trainer.fit` with the `TrainingVessel` as an argument.
4. In each iteration of the training loop, `Trainer.fit` builds a vectorized environment via `Trainer.venv_from_iterator(vessel.train_seed_iterator())`.
5. In each iteration of the training loop, `Trainer.fit` calls `vessel.train` with the vectorized environment.
6. `TrainingVessel.train` calls `policy.train()`.
7. `TrainingVessel.train` creates a `Collector` with `policy` and `vector_env`, setting `exploration_noise` to `True`.
8. `TrainingVessel.train` calls `collector.collect` with `n_episode` equal to `episode_per_iter`.
9. `TrainingVessel.train` calls `policy.update` with `sample_size` equal to 0, `buffer` equal to `collector.buffer`, and additional update kwargs.
10. `TrainingVessel.train` logs the combined collect/update metrics.
11. `TrainingVessel.train` returns the combined collect/update metrics.","The call chain is roughly:

1. User calls qlib/rl/trainer/api.py → train(…).
2. This builds a TrainingVessel and a Trainer, then calls Trainer.fit(vessel).
3. In Trainer.fit (qlib/rl/trainer/trainer.py):
   - initialize training loop
   - each iteration:
     a. build a vectorized env via Trainer.venv_from_iterator(vessel.train_seed_iterator())
     b. call vessel.train(vector_env)
4. In TrainingVessel.train (qlib/rl/trainer/vessel.py):
   - policy.train()
   - create a Collector(policy, vector_env, …, exploration_noise=True)
   - collector.collect(n_episode=episode_per_iter)  ← collect episodes
   - policy.update(sample_size=0, buffer=collector.buffer, **update_kwargs)  ← update policy
   - log and return the combined collect/update metrics.","['CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has field or attribute or property qlib.qlib.rl.utils.env_wrapper.state_interpreter\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'MODULE qlib.qlib.rl.trainer.api contains FUNCTION qlib.qlib.rl.trainer.api.train\nTrain a policy with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to train against. reward     Reward function. vessel_kwargs     Keyword arguments passed to :class:`TrainingVessel`, like ``episode_per_iter``. trainer_kwargs     Keyword arguments passed to :class:`Trainer`, like ``finite_env_type``, ``concurrency``.', 'CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has field or attribute or property qlib.qlib.rl.utils.env_wrapper.simulator_fn\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'METHOD qlib.qlib.rl.trainer.trainer.Trainer.__init__ has parameter or accepts argument qlib.qlib.rl.trainer.trainer.Trainer.fast_dev_run', 'CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has method qlib.qlib.rl.utils.env_wrapper.EnvWrapper.__init__\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 25}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 25, 'n_files_pr': 18, 'pr': 1316, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",52.71206784248352,712
What steps does cache_to_origin_data perform on the DataFrame columns using the supplied fields list?,"The cache_to_origin_data function performs the following steps on the DataFrame columns using the supplied fields list:

1. Remove spaces from field names: The function `remove_fields_space(fields)` is called, which strips spaces out of each field name.

2. Subset the DataFrame: The function subsets the DataFrame to only the no-space columns using `data = data.loc[:, not_space_fields]`.

3. Rename columns: The DataFrame columns are renamed back to the original fields in stringified order using `data.columns = [str(i) for i in fields]`.","The function does the following to your DataFrame’s columns:

1. Calls remove_fields_space(fields) to strip any spaces out of each field name.  
2. Subsets the DataFrame to only those “no‐space” columns:  
   `data = data.loc[:, not_space_fields]`  
3. Renames the resulting columns back to the original `fields` (stringified) order:  
   `data.columns = [str(i) for i in fields]`","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.data.cache contains GLOBAL_VARIABLE qlib.qlib.data.cache.H', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.data.cache.CacheUtils.visit has parameter or accepts argument qlib.qlib.data.cache.CacheUtils.visit.cache_path', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1329, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",36.91270613670349,713
How does the order of subtraction differ between the tests for the absolute and sign element operators?,"The order of subtraction differs between the tests for the absolute and sign element operators because `test_Abs` computes change as `prev_close` minus `close`, whereas `test_Sign` computes change as `close` minus `prev_close`. This difference in calculation is due to the definition of absolute value and sign, where absolute value is the distance from zero (i.e., `prev_close` - `close`) and sign is the direction of change (i.e., `close` - `prev_close`).","In tests/ops/test_elem_operator.py (TestElementOperator):

- test_Abs computes  
  change = prev_close – close  
- test_Sign computes  
  change = close – prev_close","['CLASS qlib.qlib.data.ops.Sign inherits from or is a subclass of qlib.qlib.data.ops.NpElemOperator\nNumpy Element-wise Operator  Parameters ---------- feature : Expression     feature instance func : str     numpy feature operation method  Returns ---------- Expression     feature operation output\nFeature Sign  Parameters ---------- feature : Expression     feature instance  Returns ---------- Expression     a feature instance with sign', ""CLASS qlib.qlib.data.ops.Sub has method qlib.qlib.data.ops.Sub.__init__\nSubtract Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     two features' subtraction"", 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.slogdet\nCompute the sign and natural logarithm of the determinant of a matrix.  Args:     x: Input matrix. It must 2D and square.  Returns:     A tuple `(sign, logabsdet)`. `sign` is a number representing     the sign of the determinant. For a real matrix, this is 1, 0, or -1.     For a complex matrix, this is a complex number with absolute value 1     (i.e., it is on the unit circle), or else 0.     `logabsdet` is the natural log of the absolute value of the determinant.', 'CLASS qlib.qlib.data.ops.Lt has method qlib.qlib.data.ops.Lt.__init__\nLess Than Operator  Parameters ---------- feature_left : Expression     feature instance feature_right : Expression     feature instance  Returns ---------- Feature:     bool series indicate `left < right`', 'FUNCTION keras.keras.src.ops.numpy.slogdet has parameter or accepts argument keras.keras.src.ops.numpy.slogdet.x\nCompute the sign and natural logarithm of the determinant of a matrix.  Args:     x: Input matrix. It must 2D and square.  Returns:     A tuple `(sign, logabsdet)`. `sign` is a number representing     the sign of the determinant. For a real matrix, this is 1, 0, or -1.     For a complex matrix, this is a complex number with absolute value 1     (i.e., it is on the unit circle), or else 0.     `logabsdet` is the natural log of the absolute value of the determinant.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR\'s for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/csi300/csi500/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/SH000300/SH000905/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332±0.00 | 0.3044±0.00 | 0.0462±0.00 | 0.4326±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n| MLP        | Alpha158 | 0.0229±0.01 | 0.2181±0.05 | 0.0360±0.00 | 0.3409±0.02 | 0.0043±0.02       | 0.0602±0.27       | -0.2184±0.04 |\n| LightGBM   | Alpha158 | 0.0399±0.00 | 0.4065±0.00 | 0.0482±0.00 | 0.5101±0.00 | 0.1284±0.00       | 1.5650±0.00       | -0.0635±0.00 |\n| CatBoost   | Alpha158 | 0.0345±0.00 | 0.2855±0.00 | 0.0417±0.00 | 0.3740±0.00 | 0.0496±0.00       | 0.5977±0.00       | -0.1496±0.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380±0.00 | 0.3659±0.00 | 0.0442±0.00 | 0.4324±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258±0.00 | 0.2021±0.02 | 0.0426±0.00 | 0.3840±0.02 | 0.0022±0.02       | 0.0301±0.26       | -0.2064±0.02 |\n| LightGBM   | Alpha360 | 0.0400±0.00 | 0.3605±0.00 | 0.0536±0.00 | 0.5431±0.00 | 0.0505±0.00       | 0.7658±0.02       | -0.1880±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1330, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",78.99125289916992,714
What are the two provided implementations for generating orders from target weight positions and how do they fundamentally differ in price sourcing?,"The two provided implementations for generating orders from target weight positions in qlib are OrderGenWInteract and OrderGenWOInteract, which fundamentally differ in price sourcing.

**OrderGenWInteract (With interaction)**: uses the real trade-date price via `trade_exchange.get_close` to generate order list. It also checks tradability via `is_stock_tradable`. This implementation interacts with the live exchange on the trade date.

**OrderGenWOInteract (Without interaction)**: never touches the live exchange on the trade date. Instead, it uses either the prediction-date close (`pred_start_time`) or the stored price in the current Position as a proxy price to generate order list.","There are two concrete subclasses of OrderGenerator.generate_order_list_from_target_weight_position in qlib/contrib/strategy/order_generator.py:

• OrderGenWInteract.generate_order_list_from_target_weight_position  
  – “With interaction” version. It queries the real trade‐date price and tradability via trade_exchange.get_close / calculate_amount_position_value / is_stock_tradable to size orders.

• OrderGenWOInteract.generate_order_list_from_target_weight_position  
  – “Without interaction” version. It never touches the live exchange on trade date; it uses the prediction‐date close (pred_start_time) or the stored price in the current Position as a proxy.","['CLASS qlib.qlib.workflow.record_temp.PortAnaRecord inherits from or is a subclass of qlib.qlib.workflow.record_temp.ACRecordTemp\nAutomatically checking record template\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.strategy.order_generator.OrderGenWInteract.generate_order_list_from_target_weight_position has parameter or accepts argument qlib.qlib.contrib.strategy.order_generator.OrderGenWInteract.generate_order_list_from_target_weight_position.current\ngenerate_order_list_from_target_weight_position  No adjustment for for the nontradable share. All the tadable value is assigned to the tadable stock according to the weight. if interact == True, will use the price at trade date to generate order list else, will only use the price before the trade date to generate order list  :param current: :type current: Position :param trade_exchange: :type trade_exchange: Exchange :param target_weight_position: :type target_weight_position: dict :param risk_degree: :type risk_degree: float :param pred_start_time: :type pred_start_time: pd.Timestamp :param pred_end_time: :type pred_end_time: pd.Timestamp :param trade_start_time: :type trade_start_time: pd.Timestamp :param trade_end_time: :type trade_end_time: pd.Timestamp  :rtype: list', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.__init__\nconfig[""strategy""] : dict     define the strategy class as well as the kwargs. config[""executor""] : dict     define the executor class as well as the kwargs. config[""backtest""] : dict     define the backtest kwargs. risk_analysis_freq : str|List[str]     risk analysis freq of report indicator_analysis_freq : str|List[str]     indicator analysis freq of report indicator_analysis_method : str, optional, default by None     the candidate values include \'mean\', \'amount_weighted\', \'value_weighted\'\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.strategy.order_generator.OrderGenWInteract.generate_order_list_from_target_weight_position has parameter or accepts argument qlib.qlib.contrib.strategy.order_generator.OrderGenWInteract.generate_order_list_from_target_weight_position.risk_degree\ngenerate_order_list_from_target_weight_position  No adjustment for for the nontradable share. All the tadable value is assigned to the tadable stock according to the weight. if interact == True, will use the price at trade date to generate order list else, will only use the price before the trade date to generate order list  :param current: :type current: Position :param trade_exchange: :type trade_exchange: Exchange :param target_weight_position: :type target_weight_position: dict :param risk_degree: :type risk_degree: float :param pred_start_time: :type pred_start_time: pd.Timestamp :param pred_end_time: :type pred_end_time: pd.Timestamp :param trade_start_time: :type trade_start_time: pd.Timestamp :param trade_end_time: :type trade_end_time: pd.Timestamp  :rtype: list', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 4, 'pr': 1338, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.677650928497314,715
Which attribute specifies the list of custom operations applied to high-frequency data in the workflow?,"The attribute that specifies the list of custom operations applied to high-frequency data in the workflow is HighfreqWorkflow.SPEC_CONF['custom_ops']. This attribute includes a list under the 'custom_ops' key, which can be accessed with HighfreqWorkflow.SPEC_CONF['custom_ops'].","The list is defined in the `SPEC_CONF` attribute of `HighfreqWorkflow` under the `""custom_ops""` key (i.e. `HighfreqWorkflow.SPEC_CONF[""custom_ops""]`).","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.qlib.workflow.task.gen.RollingGen has field or attribute or property qlib.qlib.workflow.task.gen.RollingGen.step', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.qlib.workflow.task.gen.RollingGen has field or attribute or property qlib.qlib.workflow.task.gen.RollingGen.rtype', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1348, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.958211183547974,716
What are the main stages executed by the update_data_to_bin command from data validation to index parsing?,"The main stages executed by the update_data_to_bin command from data validation to index parsing in qlib are:

1. QLib data validation/download: This is the first step in the update_data_to_bin process. It involves checking if the QLib data is available and downloading it if necessary.

2. Date setup: After validating the QLib data, the next step is to set up the date information for the data collection process. This includes reading the calendars/day.txt file to compute the trading_date and end_date.

3. Data download: Once the date information is set up, the next step is to download the actual data from the Yahoo collector. This involves executing the Run.download_data(...) function, which calls the Yahoo collector and Normalize._executor functions.

4. Data normalization: After downloading the data, the next step is to normalize it. This involves invoking the Run.normalize_data_1d_extend function, which in turn calls the YahooNormalizeUS1dExtend function.

5. Binary dump: The final stage of the update_data_to_bin process is to dump the normalized data into binary format. This involves calling the DumpDataUpdate().dump() function with the csv_path and qlib_dir parameters.

6. Index parsing: Finally, the index parsing stage involves importing the dynamic get_instruments function from the data_collector.{region}_index.collector module and invoking it for each index, such as SP500 and NASDAQ100.","The `update_data_to_bin` call in scripts/data_collector/yahoo/collector.py runs roughly these steps:

1. **QLib data validation/download**  
   – exists_qlib_data (qlib/utils/__init__.py) → if missing, GetData().qlib_data(...)  
2. **Date setup**  
   – Read calendars/day.txt to compute `trading_date` and `end_date`  
3. **Data download**  
   – Run.download_data(...) (calls your Yahoo collector and Normalize._executor)  
4. **Data normalization**  
   – Run.normalize_data_1d_extend (via YahooNormalizeUS1dExtend)  
5. **Binary dump**  
   – DumpDataUpdate(csv_path=…, qlib_dir=…).dump()  
6. **Index parsing**  
   – Dynamically import data_collector.{region}_index.collector.get_instruments and invoke it for each index (e.g. SP500, NASDAQ100)","['CLASS qlib.scripts.data_collector.yahoo.collector.Run has method qlib.scripts.data_collector.yahoo.collector.Run.normalize_data\nnormalize data  Parameters ---------- date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol end_date: str     if not None, normalize the last date saved (including end_date); if None, it will ignore this parameter; by default None qlib_data_1d_dir: str     if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;          qlib_data_1d can be obtained like this:             $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d             $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --trading_date 2021-06-01         or:             download 1d data, reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#1d-from-yahoo  Examples ---------     $ python collector.py normalize_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region cn --interval 1d     $ python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source_cn_1min --normalize_dir ~/.qlib/stock_data/normalize_cn_1min --region CN --interval 1min', 'MODULE qlib.scripts.dump_bin contains CLASS qlib.scripts.dump_bin.DumpDataUpdate', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data.symbol_field_name\nnormalize data  Parameters ---------- date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol end_date: str     if not None, normalize the last date saved (including end_date); if None, it will ignore this parameter; by default None qlib_data_1d_dir: str     if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;          qlib_data_1d can be obtained like this:             $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d             $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --trading_date 2021-06-01         or:             download 1d data, reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#1d-from-yahoo  Examples ---------     $ python collector.py normalize_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region cn --interval 1d     $ python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source_cn_1min --normalize_dir ~/.qlib/stock_data/normalize_cn_1min --region CN --interval 1min', 'FUNCTION qlib.qlib.utils.__init__.read_bin has parameter or accepts argument qlib.qlib.utils.__init__.read_bin.end_index', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.normalize_data has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.normalize_data.qlib_data_1d_dir\nnormalize data  Parameters ---------- date_field_name: str     date field name, default date symbol_field_name: str     symbol field name, default symbol end_date: str     if not None, normalize the last date saved (including end_date); if None, it will ignore this parameter; by default None qlib_data_1d_dir: str     if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;          qlib_data_1d can be obtained like this:             $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d             $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --trading_date 2021-06-01         or:             download 1d data, reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#1d-from-yahoo  Examples ---------     $ python collector.py normalize_data --source_dir ~/.qlib/stock_data/source --normalize_dir ~/.qlib/stock_data/normalize --region cn --interval 1d     $ python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source_cn_1min --normalize_dir ~/.qlib/stock_data/normalize_cn_1min --region CN --interval 1min', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 1358, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",51.0474169254303,717
"When specifying a custom tracking URI for an experiment launch, how does the manager propagate this URI to the underlying Mlflow client instance?","When specifying a custom tracking URI for an experiment launch, the manager propagates this URI to the underlying Mlflow client instance by setting the `tracking_uri` parameter when instantiating the `MlflowClient`. This is done in the `MLflowExpManager.client` method, which passes the current URI (obtained from `self.uri`) to the `MlflowClient` constructor. The custom URI provided to `start_exp` is passed into the new Mlflow client, and this URI is also stored in the `self._uri` attribute of the `MLflowExperiment` instance, which uses it when instantiating its own client.","When you call `start_exp(..., uri=""your_uri"")`, `ExpManager.start_exp` sets `self._active_exp_uri` to `""your_uri""`.  From then on:

• `ExpManager.uri` (used by both the manager and any experiments it creates) returns this active URI.  
• `MLflowExpManager.client` instantiates  
    `mlflow.tracking.MlflowClient(tracking_uri=self.uri)`  
so the custom URI is passed straight into the new Mlflow client.  
• Likewise, when creating an `MLflowExperiment`, its constructor does  
    `self._client = MlflowClient(tracking_uri=self._uri)`  
using the same URI.","['CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.uri\nGet the default tracking URI or current URI.  Returns ------- The tracking URI string.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.exp.MLflowExperiment has field or attribute or property qlib.qlib.workflow.exp.uri\nUse mlflow to implement Experiment.', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.default_uri\nGet the default tracking URI from qlib.config.C\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'METHOD qlib.qlib.workflow.expm.ExpManager.default_uri has parameter or accepts argument qlib.qlib.workflow.expm.ExpManager.default_uri.value\nGet the default tracking URI from qlib.config.C', 'CLASS qlib.qlib.workflow.expm.MLflowExpManager inherits from or is a subclass of qlib.qlib.workflow.expm.ExpManager\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.\nUse mlflow to implement ExpManager.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 15}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 15, 'n_files_pr': 7, 'pr': 1364, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",32.22281360626221,718
What is the maximum allowed elapsed time for creating an MlflowClient in this test on non-Linux platforms?,"The maximum allowed elapsed time for creating an MlflowClient in the test MLflowTest.test_creating_client on non-Linux platforms is 20 milliseconds, which is equivalent to 2e-2 seconds.","In tests/dependency_tests/test_mlflow.py (MLflowTest.test_creating_client), on non-Linux platforms the elapsed time must be under 2e-2 seconds (i.e. 20 ms).","['CLASS qlib.qlib.rl.order_execution.reward.PPOReward has method qlib.qlib.rl.order_execution.reward.PPOReward.__init__\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'CLASS qlib.qlib.workflow.exp.MLflowExperiment has field or attribute or property qlib.qlib.workflow.exp.max_results\nUse mlflow to implement Experiment.', 'CLASS qlib.qlib.rl.order_execution.reward.PPOReward has method qlib.qlib.rl.order_execution.reward.PPOReward.reward\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'CLASS qlib.qlib.workflow.exp.MLflowExperiment has field or attribute or property qlib.qlib.workflow.exp.rtype\nUse mlflow to implement Experiment.', 'MODULE qlib.qlib.rl.order_execution.reward contains CLASS qlib.qlib.rl.order_execution.reward.PPOReward\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1365, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",42.093027114868164,719
Which run info attribute is assigned to the recorder's artifact URI when constructed with a previous MLflow run?,The run info attribute assigned to the recorder's artifact URI when constructed with a previous MLflow run in qlib is `artifact_uri` from `mlflow_run.info`. This value is used to set the attribute `self._artifact_uri` in the `MLflowRecorder.__init__` method.,"In MLflowRecorder.__init__ (qlib/workflow/recorder.py), `self._artifact_uri` is set from `mlflow_run.info.artifact_uri`.","['CLASS qlib.qlib.workflow.expm.MLflowExpManager inherits from or is a subclass of qlib.qlib.workflow.expm.ExpManager\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.\nUse mlflow to implement ExpManager.', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.get_artifact_uri\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.expm.ExpManager has field or attribute or property qlib.qlib.workflow.expm.experiment_name\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has field or attribute or property qlib.qlib.workflow.recorder.uri\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'CLASS qlib.qlib.workflow.expm.ExpManager has field or attribute or property qlib.qlib.workflow.expm.ExpManager.active_experiment\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1367, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.471002340316772,720
How does the initialization process apply the user-defined logging level across all module loggers?,"The initialization process in qlib applies the user-defined logging level across all module loggers through the following steps: The `init()` function in `qlib/__init__.py` loads the config via `C.set(...)`. It then calls `get_module_logger.setLevel(C.logging_level)` after loading the config. `get_module_logger` is an instance of `_QLibLoggerManager`, which is defined in `qlib/log.py`. The `setLevel(level)` method of `_QLibLoggerManager` iterates over each logger in `self._loggers.values()` and calls `logger.setLevel(level)`. Every module logger stored in `_QLibLoggerManager._loggers` gets its level updated to `C.logging_level` when `setLevel` is called. This ensures that all module loggers have the same logging level as specified in the config, which is applied during the initialization process.","In qlib/__init__.py’s init(), after loading the config via C.set(...), it does:

  get_module_logger.setLevel(C.logging_level)

Here, get_module_logger is actually an instance of _QLibLoggerManager (defined in qlib/log.py). Its setLevel(level) method simply does:

  for logger in self._loggers.values():
      logger.setLevel(level)

so every module logger that’s been created (and stored in _QLibLoggerManager._loggers) gets its level updated to the user‐defined C.logging_level.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.utils.__init__ contains GLOBAL_VARIABLE qlib.qlib.utils.__init__.log', 'CLASS xgboost.demo.dask.forward_logging.ForwardLoggingMonitor has method xgboost.demo.dask.forward_logging.ForwardLoggingMonitor.__init__\nPrint the evaluation result at each iteration. The default monitor in the native interface logs the result to the Dask scheduler process. This class can be used to forward the logging to the client process. Important: see the `client` parameter for more info.  Parameters ---------- client :     Distributed client. This must be the top-level client. The class uses     :py:meth:`distributed.Client.forward_logging` in conjunction with the Python     :py:mod:`logging` module to forward the evaluation results to the client     process. It has undefined behaviour if called in a nested task. As a result,     client-side logging is not enabled by default.', 'MODULE qlib.qlib.__init__ contains GLOBAL_VARIABLE qlib.qlib.__init__.LOG', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 9, 'n_files_pr': 7, 'pr': 1368, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.533366441726685,721
How do the processes for retrieving historical constituents and newly added constituents differ in constructing dates for the external API calls?,"The processes for retrieving historical constituents and newly added constituents differ in constructing dates for the external API calls in qlib. For retrieving historical constituents, the `get_history_companies` function builds a pd.date_range with start=bench_start_date, end=now, and freq=""7D"", which constructs dates at weekly intervals starting from the bench_start_date to the current date. On the other hand, for retrieving newly added constituents, the `get_new_companies` function formats today once with today.strftime(""%Y-%m-%d"") and then calls get_data_from_baostock on the formatted date, which constructs a single date at the current date.","In CSI500Index (scripts/data_collector/cn_index/collector.py):

• get_history_companies()  
  – Builds a pd.date_range(start=bench_start_date, end=now, freq=“7D”)  
  – Iterates over each date (as a Python date) calling get_data_from_baostock(date)  

• get_new_companies()  
  – Takes only today = pd.Timestamp.now().normalize()  
  – Formats it once with today.strftime(""%Y-%m-%d"")  
  – Calls get_data_from_baostock on that single date  

So historical pulls a weekly series of dates, whereas “new” pulls just the current date.","[""MODULE qlib.qlib.contrib.online.utils contains FUNCTION qlib.qlib.contrib.online.utils.prepare\n1. Get the dates that need to do trading till today for user {user_id}     dates[0] indicate the latest trading date of User{user_id},     if User{user_id} haven't do trading before, than dates[0] presents the init date of User{user_id}. 2. Set the exchange with exchange_config file      Parameter         um : UserManager()         today : pd.Timestamp()         user_id : str     :return         dates : list of pd.Timestamp         trade_exchange : Exchange()"", 'CLASS qlib.qlib.rl.data.native.HandlerProcessedDataProvider has field or attribute or property qlib.qlib.rl.data.native.date', ""FUNCTION qlib.qlib.contrib.online.utils.prepare has parameter or accepts argument qlib.qlib.contrib.online.utils.prepare.today\n1. Get the dates that need to do trading till today for user {user_id}     dates[0] indicate the latest trading date of User{user_id},     if User{user_id} haven't do trading before, than dates[0] presents the init date of User{user_id}. 2. Set the exchange with exchange_config file      Parameter         um : UserManager()         today : pd.Timestamp()         user_id : str     :return         dates : list of pd.Timestamp         trade_exchange : Exchange()"", 'CLASS qlib.scripts.dump_pit.DumpPitData has field or attribute or property qlib.scripts.dump_pit.DumpPitData.date_column_name', ""FUNCTION qlib.qlib.contrib.online.utils.prepare has parameter or accepts argument qlib.qlib.contrib.online.utils.prepare.exchange_config\n1. Get the dates that need to do trading till today for user {user_id}     dates[0] indicate the latest trading date of User{user_id},     if User{user_id} haven't do trading before, than dates[0] presents the init date of User{user_id}. 2. Set the exchange with exchange_config file      Parameter         um : UserManager()         today : pd.Timestamp()         user_id : str     :return         dates : list of pd.Timestamp         trade_exchange : Exchange()"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md\n# iBOVESPA History Companies Collection\n\n## Requirements\n\n- Install the libs from the file `requirements.txt`\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n- `requirements.txt` file was generated using python3.8\n\n## For the ibovespa (IBOV) index, we have:\n\n<hr/>\n\n### Method `get_new_companies`\n\n#### <b>Index start date</b>\n\n- The ibovespa index started on 2 January 1968 ([wiki](https://en.wikipedia.org/wiki/%C3%8Dndice_Bovespa)).  In order to use this start date in our `bench_start_date(self)` method, two conditions must be satisfied:\n    1) APIs used to download brazilian stocks (B3) historical prices must keep track of such historic data since 2 January 1968\n\n    2) Some website or API must provide, from that date, the historic index composition. In other words, the companies used to build the index .\n\n    As a consequence, the method `bench_start_date(self)` inside `collector.py` was implemented using `pd.Timestamp(""2003-01-03"")` due to two reasons\n\n    1) The earliest ibov composition that have been found was from the first quarter of 2003. More informations about such composition can be seen on the sections below.\n\n    2) Yahoo finance, one of the libraries used to download symbols historic prices, keeps track from this date forward.\n\n- Within the `get_new_companies` method, a logic was implemented to get, for each ibovespa component stock, the start date that yahoo finance keeps track of.\n\n#### <b>Code Logic</b>\n\nThe code does a web scrapping into the B3\'s [website](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br), which keeps track of the ibovespa stocks composition on the current day. \n\nOther approaches, such as `request` and `Beautiful Soup` could have been used. However, the website shows the table with the stocks with some delay, since it uses a script inside of it to obtain such compositions.\nAlternatively, `selenium` was used to download this stocks\' composition in order to overcome this problem.\n\nFuthermore, the data downloaded from the selenium script  was preprocessed so it could be saved into the `csv` format stablished by `scripts/data_collector/index.py`.\n\n<hr/>\n\n### Method `get_changes` \n\nNo suitable data source that keeps track of ibovespa\'s history stocks composition has been found. Except from this [repository](https://github.com/igor17400/IBOV-HCI) which provide such information have been used, however it only provides the data from the 1st quarter of 2003 to 3rd quarter of 2021.\n\nWith that reference, the index\'s composition can be compared quarter by quarter and year by year and then generate a file that keeps track of which stocks have been removed and which have been added each quarter and year.\n\n<hr/>\n\n### Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method save_new_companies\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 1373, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.159499645233154,722
Which constructor argument determines the number of rounds without improvement before a sub-model stops training?,"The constructor argument that determines the number of rounds without improvement before a sub-model stops training in qlib is `early_stopping_rounds`. This argument is used to specify the number of rounds allowed without improvement before a sub-model stops training, and it is utilized in the `train_submodel` function via `lgb.early_stopping`.","The constructor argument early_stopping_rounds (in DEnsembleModel.__init__, qlib/contrib/model/double_ensemble.py) sets how many rounds without improvement are allowed before a sub-model stops training (used in train_submodel via lgb.early_stopping).","['CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss inherits from or is a subclass of keras.keras.callbacks.Callback\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.__init__ has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.__init__.early_stopping_rounds', 'MODULE keras.guides.writing_your_own_callbacks contains CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.train_submodel.df_train', 'CLASS keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss has method keras.guides.writing_your_own_callbacks.EarlyStoppingAtMinLoss.on_train_end\nStop training when the loss is at its min, i.e. the loss stops decreasing.  Arguments:     patience: Number of epochs to wait after min has been hit. After this     number of no improvement, training stops.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf).', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\n### Saving Progress Models\nIf you want to save model every two round, simply set save_period=2. You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.\n\n#### Continue from Existing Model\nIf you want to continue boosting from existing model, say 0002.model, use\n```\n../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model\n```\nxgboost will load from 0002.model continue boosting for 2 rounds, and save output to continue.model. However, beware that the training and evaluation data specified in mushroom.conf should not change when you use this function.\n#### Use Multi-Threading\nWhen you are working with a large dataset, you may want to take advantage of parallelism. If your compiler supports OpenMP, xgboost is naturally multi-threaded, to set number of parallel running add ```nthread``` parameter to your configuration.\nEg. ```nthread=10```\n\nSet nthread to be the number of your real cpu (On Unix, this can be found using ```lscpu```)\nSome systems will have ```Thread(s) per core = 2```, for example, a 4 core cpu with 8 threads, in such case set ```nthread=4``` and not 8.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1375, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",26.57767152786255,723
Which pandas option is temporarily changed to suppress chained assignment warnings during the z-score normalization?,The pandas option `mode.chained_assignment` is temporarily changed to None via `pd.option_context` to suppress chained assignment warnings during z-score normalization in qlib.,"In CSZScoreNorm.__call__ (qlib/data/dataset/processor.py) the pandas option `""mode.chained_assignment""` is set to `None` via `pd.option_context` to suppress those warnings.","['MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.ZScoreNorm\nZScore Normalization', 'MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.robust_zscore\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.CSZScoreNorm\nCross Sectional ZScore Normalization', 'CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.fit\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1386, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",41.30660152435303,724
How does download_index_data merge previously saved data with newly fetched data before writing the CSV?,"The `download_index_data` function merges previously saved data with newly fetched data by reading the old DataFrame from the file path `_path` using `pd.read_csv(_path)`, and then merging it with the new DataFrame (`df`) using `pd.concat([_old_df, df], sort=False)`. The merged DataFrame is then written back to the file path `_path` using `df.to_csv(_path, index=False)`. This ensures that any new data fetched by the function is appended to the existing data in the CSV file.","In YahooCollectorCN1d.download_index_data (scripts/data_collector/yahoo/collector.py), after fetching the new DataFrame `df` it does:

1. `_path = self.save_dir / f""sh{_index_code}.csv""`
2. If `_path.exists()`, it reads the old file: `_old_df = pd.read_csv(_path)`
3. Merges them via `df = pd.concat([_old_df, df], sort=False)`
4. Writes the combined `df.to_csv(_path, index=False)`","['MODULE qlib.scripts.data_collector.contrib.fill_cn_1min_data.fill_cn_1min_data contains FUNCTION qlib.scripts.data_collector.contrib.fill_cn_1min_data.fill_cn_1min_data.fill_1min_using_1d\nUse 1d data to fill in the missing symbols relative to 1min  Parameters ---------- data_1min_dir: str     1min data dir qlib_data_1d_dir: str     1d qlib data(bin data) dir, from: https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format max_workers: int     ThreadPoolExecutor(max_workers), by default 16 date_field_name: str     date field name, by default date symbol_field_name: str     symbol field name, by default symbol', 'METHOD qlib.scripts.dump_bin.DumpDataUpdate.__init__ has parameter or accepts argument qlib.scripts.dump_bin.DumpDataUpdate.__init__.include_fields\nParameters ---------- csv_path: str     stock data path or directory qlib_dir: str     qlib(dump) data director backup_dir: str, default None     if backup_dir is not None, backup qlib_dir to backup_dir freq: str, default ""day""     transaction frequency max_workers: int, default None     number of threads date_field_name: str, default ""date""     the name of the date field in the csv file_suffix: str, default "".csv""     file suffix symbol_field_name: str, default ""symbol""     symbol field name include_fields: tuple     dump fields exclude_fields: tuple     fields not dumped limit_nums: int     Use when debugging, default None', 'FUNCTION qlib.scripts.data_collector.contrib.fill_cn_1min_data.fill_cn_1min_data.fill_1min_using_1d has parameter or accepts argument qlib.scripts.data_collector.contrib.fill_cn_1min_data.fill_cn_1min_data.fill_1min_using_1d.data_1min_dir\nUse 1d data to fill in the missing symbols relative to 1min  Parameters ---------- data_1min_dir: str     1min data dir qlib_data_1d_dir: str     1d qlib data(bin data) dir, from: https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format max_workers: int     ThreadPoolExecutor(max_workers), by default 16 date_field_name: str     date field name, by default date symbol_field_name: str     symbol field name, by default symbol', 'METHOD qlib.qlib.utils.index_data.SingleData.add has parameter or accepts argument qlib.qlib.utils.index_data.SingleData.add.fill_value', 'FUNCTION qlib.scripts.data_collector.contrib.fill_cn_1min_data.fill_cn_1min_data.fill_1min_using_1d has parameter or accepts argument qlib.scripts.data_collector.contrib.fill_cn_1min_data.fill_cn_1min_data.fill_1min_using_1d.qlib_data_1d_dir\nUse 1d data to fill in the missing symbols relative to 1min  Parameters ---------- data_1min_dir: str     1min data dir qlib_data_1d_dir: str     1d qlib data(bin data) dir, from: https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format max_workers: int     ThreadPoolExecutor(max_workers), by default 16 date_field_name: str     date field name, by default date symbol_field_name: str     symbol field name, by default symbol', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1388, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.48047614097595,725
How does SigAnaRecord fall back to its SignalRecord dependency when loading an artifact fails?,"When SigAnaRecord encounters a LoadObjectError with parents=true, it casts itself to SignalRecord and falls back to SignalRecord’s artifact_path during recursive loading. This behavior is due to the class_casting context in RecordTemp's load method, which attempts to resolve dependencies by recursively calling self.load(name, parents=True).","SigAnaRecord doesn’t override load – it inherits RecordTemp.load (qlib/workflow/record_temp.py). That method does roughly:

```python
try:
    return self.recorder.load_object(self.get_path(name))
except LoadObjectError:
    if parents and self.depend_cls is not None:
        with class_casting(self, self.depend_cls):
            return self.load(name, parents=True)
    raise
```

Since SigAnaRecord.depend_cls = SignalRecord, on a LoadObjectError it does a class_casting(self, SignalRecord) and recursively calls load again, so it falls back to SignalRecord’s artifact_path.","['MODULE cognee.cognee.tasks.code.enrich_dependency_graph_checker contains FUNCTION cognee.cognee.tasks.code.enrich_dependency_graph_checker.main\nExecute the main logic of the dependency graph processor.  This function sets up argument parsing to retrieve the repository path, checks the existence of the specified path, and processes the repository to produce a dependency graph. If the repository path does not exist, it logs an error message and terminates without further execution.', 'CLASS qlib.qlib.workflow.record_temp.SigAnaRecord has method qlib.qlib.workflow.record_temp.SigAnaRecord.__init__\nThis is the Signal Analysis Record class that generates the analysis results such as IC and IR. This class inherits the ``RecordTemp`` class.', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.input_signature\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', 'METHOD qlib.qlib.workflow.record_temp.SigAnaRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.SigAnaRecord.__init__.recorder', 'METHOD keras.keras.src.models.model.Model.export has parameter or accepts argument keras.keras.src.models.model.Model.export.filepath\nExport the model as an artifact for inference.  Args:     filepath: `str` or `pathlib.Path` object. The path to save the         artifact.     format: `str`. The export format. Supported values:         `""tf_saved_model""` and `""onnx""`.  Defaults to         `""tf_saved_model""`.     verbose: `bool`. Whether to print a message during export. Defaults         to `None`, which uses the default value set by different         backends and formats.     input_signature: Optional. Specifies the shape and dtype of the         model inputs. Can be a structure of `keras.InputSpec`,         `tf.TensorSpec`, `backend.KerasTensor`, or backend tensor. If         not provided, it will be automatically computed. Defaults to         `None`.     **kwargs: Additional keyword arguments:         - Specific to the JAX backend and `format=""tf_saved_model""`:             - `is_static`: Optional `bool`. Indicates whether `fn` is                 static. Set to `False` if `fn` involves state updates                 (e.g., RNG seeds and counters).             - `jax2tf_kwargs`: Optional `dict`. Arguments for                 `jax2tf.convert`. See the documentation for                 [`jax2tf.convert`](                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).                 If `native_serialization` and `polymorphic_shapes` are                 not provided, they will be automatically computed.  **Note:** This feature is currently supported only with TensorFlow, JAX and Torch backends.  **Note:** Be aware that the exported artifact may contain information from the local file system when using `format=""onnx""`, `verbose=True` and Torch backend.  Examples:  Here\'s how to export a TensorFlow SavedModel for inference.  ```python # Export the model as a TensorFlow SavedModel artifact model.export(""path/to/location"", format=""tf_saved_model"")  # Load the artifact in a different process/environment reloaded_artifact = tf.saved_model.load(""path/to/location"") predictions = reloaded_artifact.serve(input_data) ```  Here\'s how to export an ONNX for inference.  ```python # Export the model as a ONNX artifact model.export(""path/to/location"", format=""onnx"")  # Load the artifact in a different process/environment ort_session = onnxruntime.InferenceSession(""path/to/location"") ort_inputs = {     k.name: v for k, v in zip(ort_session.get_inputs(), input_data) } predictions = ort_session.run(None, ort_inputs) ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### CI\n* [CI] Rotate access keys for uploading MacOS artifacts from Travis CI (#7253)\n* Reduce Travis environment setup time. (#6912)\n* Restore R cache on github action. (#6985)\n* [CI] Remove stray build artifact to avoid error in artifact packaging (#6994)\n* [CI] Move appveyor tests to action (#6986)\n* Remove appveyor badge. [skip ci] (#7035)\n* [CI] Configure RAPIDS, dask, modin (#7033)\n* Test on s390x. (#7038)\n* [CI] Upgrade to CMake 3.14 (#7060)\n* [CI] Update R cache. (#7102)\n* [CI] Pin libomp to 11.1.0  (#7107)\n* [CI] Upgrade build image to CentOS 7 + GCC 8; require CUDA 10.1 and later (#7141)\n* [dask] Work around segfault in prediction. (#7112)\n* [dask] Remove the workaround for segfault. (#7146)\n* [CI] Fix hanging Python setup in Windows CI (#7186)\n* [CI] Clean up in beginning of each task in Win CI (#7189)\n* Fix travis. (#7237)\n\n### Acknowledgement\n* **Contributors**: Adam Pocock (@Craigacp), Jeff H (@JeffHCross), Johan Hansson (@JohanWork), Jose Manuel Llorens (@JoseLlorensRipolles), Benjamin Szőke (@Livius90), @ReeceGoding, @ShvetsKS, Robert Zabel (@ZabelTech), Ali (@ali5h), Andrew Ziem (@az0), Andy Adinets (@canonizer), @david-cortes, Daniel Saxton (@dsaxton), Emil Sadek (@esadek), @farfarawayzyt, Gil Forsyth (@gforsyth), @giladmaya, @graue70, Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), José Morales (@jmoralez), Kai Fricke (@krfricke), Christian Lorentzen (@lorentzenchr), Mads R. B. Kristensen (@madsbk), Anton Kostin (@masguit42), Martin Petříček (@mpetricek-corp), @naveenkb, Taewoo Kim (@oOTWK), Viktor Szathmáry (@phraktle), Robert Maynard (@robertmaynard), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), Paul Taylor (@trxcllnt), @vslaykovsky, Bobby Wang (@wbo4958),\n* **Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Jose Manuel Llorens (@JoseLlorensRipolles), Kodi Arfer (@Kodiologist), Benjamin Szőke (@Livius90), Mark Guryanov (@MarkGuryanov), Rory Mitchell (@RAMitchell), @ReeceGoding, @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Ziem (@az0), @candalfigomoro, Andy Adinets (@canonizer), Dante Gama Dessavre (@dantegd), @david-cortes, Daniel Saxton (@dsaxton), @farfarawayzyt, Gil Forsyth (@gforsyth), Harutaka Kawamura (@harupy), Philip Hyunsu Cho (@hcho3), @jakirkham, James Lamb (@jameslamb), José Morales (@jmoralez), James Bourbeau (@jrbourbeau), Christian Lorentzen (@lorentzenchr), Martin Petříček (@mpetricek-corp), Nikolay Petrov (@napetrov), @naveenkb, Viktor Szathmáry (@phraktle), Robin Teuwens (@rteuwens), Yuan Tang (@terrytangyuan), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), @vkuzmin-uber, Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n\n## v1.4.2 (2021.05.13)\nThis is a patch release for Python package with following fixes:\n\n* Handle the latest version of cupy.ndarray in inplace_predict. (#6933)\n* Ensure output array from predict_leaf is (n_samples, ) when there's only 1 tree. 1.4.0 outputs (n_samples, 1). (#6889)\n* Fix empty dataset handling with multi-class AUC. (#6947)\n* Handle object type from pandas in inplace_predict. (#6927)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 16}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 16, 'n_files_pr': 6, 'pr': 1389, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",35.24975562095642,726
Which helper function is used to generate default rangebreaks for datetime axes across the report's time-series plots?,The helper function `guess_plotly_rangebreaks` is used to generate default rangebreaks for datetime axes across the report's time-series plots in qlib. This function takes a pandas DatetimeIndex as an argument and calculates the required rangebreaks to remove gaps in the datetime index. It is located in the `qlib.qlib.contrib.report.utils` module.,"Across the time‐series plots (e.g. in _group_return, _pred_autocorr, _pred_turnover, ic_figure, score_ic_graph), the default `rangebreaks` are generated by the helper function guess_plotly_rangebreaks.","['MODULE qlib.qlib.contrib.report.utils contains FUNCTION qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks\nThis function `guesses` the rangebreaks required to remove gaps in datetime index. It basically calculates the difference between a `continuous` datetime index and index given.  For more details on `rangebreaks` params in plotly, see https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks  Parameters ---------- dt_index: pd.DatetimeIndex The datetimes of the data.  Returns ------- the `rangebreaks` to be passed into plotly axis.', 'FUNCTION qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks has parameter or accepts argument qlib.qlib.contrib.report.utils.guess_plotly_rangebreaks.dt_index\nThis function `guesses` the rangebreaks required to remove gaps in datetime index. It basically calculates the difference between a `continuous` datetime index and index given.  For more details on `rangebreaks` params in plotly, see https://plotly.com/python/reference/layout/xaxis/#layout-xaxis-rangebreaks  Parameters ---------- dt_index: pd.DatetimeIndex The datetimes of the data.  Returns ------- the `rangebreaks` to be passed into plotly axis.', 'MODULE qlib.qlib.contrib.report.analysis_model.analysis_model_performance contains FUNCTION qlib.qlib.contrib.report.analysis_model.analysis_model_performance.ic_figure\nIC figure  :param ic_df: ic DataFrame :param show_nature_day: whether to display the abscissa of non-trading day :param \\*\\*kwargs: contains some parameters to control plot style in plotly. Currently, supports    - `rangebreaks`: https://plotly.com/python/time-series/#Hiding-Weekends-and-Holidays :return: plotly.graph_objs.Figure', 'CLASS qlib.qlib.backtest.decision.IdxTradeRange has method qlib.qlib.backtest.decision.IdxTradeRange.clip_time_range', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.TSDataSampler.idx_df\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 8, 'n_files_pr': 5, 'pr': 1390, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",32.52200269699097,727
Under what conditions does Exchange.__init__ warn that the provided limit_threshold may be unreasonable for China and Taiwan regions?,"The warning that the provided limit_threshold may be unreasonable for China and Taiwan regions is generated under the following conditions:

* When `limit_threshold` is not None.
* When `self.limit_type` equals `Exchange.LT_FLT`.
* When `abs(float(limit_threshold)) > 0.1`.
* When `C.region` is `REG_CN` or `REG_TW`. In other words, the warning will be generated when the limit threshold is greater than 10% and the region is China or Taiwan.","In qlib/backtest/exchange.py (Exchange.__init__), after you compute  
  self.limit_type = self._get_limit_type(limit_threshold)  
you get the “may not be set to a reasonable value” warning whenever:

  1. limit_threshold is not None  
  2. self.limit_type == Exchange.LT_FLT (i.e. it’s being treated as a float)  
  3. abs(float(limit_threshold)) > 0.1  
  4. C.region is REG_CN or REG_TW","['FUNCTION qlib.qlib.contrib.evaluate.long_short_backtest has parameter or accepts argument qlib.qlib.contrib.evaluate.long_short_backtest.close_cost\nA backtest for long-short strategy  :param pred:        The trading signal produced on day `T`. :param topk:       The short topk securities and long topk securities. :param deal_price:  The price to deal the trading. :param shift:       Whether to shift prediction by one day.  The trading day will be T+1 if shift==1. :param open_cost:   open transaction cost. :param close_cost:  close transaction cost. :param trade_unit:  100 for China A. :param limit_threshold: limit move 0.1 (10%) for example, long and short with same limit. :param min_cost:    min transaction cost. :param subscribe_fields: subscribe fields. :param extract_codes:  bool.                    will we pass the codes extracted from the pred to the exchange.                    NOTE: This will be faster with offline qlib. :return:            The result of backtest, it is represented by a dict.                     { ""long"": long_returns(excess),                     ""short"": short_returns(excess),                     ""long_short"": long_short_returns}', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.Exchange.limit_threshold', 'FUNCTION qlib.qlib.contrib.evaluate.long_short_backtest has parameter or accepts argument qlib.qlib.contrib.evaluate.long_short_backtest.subscribe_fields\nA backtest for long-short strategy  :param pred:        The trading signal produced on day `T`. :param topk:       The short topk securities and long topk securities. :param deal_price:  The price to deal the trading. :param shift:       Whether to shift prediction by one day.  The trading day will be T+1 if shift==1. :param open_cost:   open transaction cost. :param close_cost:  close transaction cost. :param trade_unit:  100 for China A. :param limit_threshold: limit move 0.1 (10%) for example, long and short with same limit. :param min_cost:    min transaction cost. :param subscribe_fields: subscribe fields. :param extract_codes:  bool.                    will we pass the codes extracted from the pred to the exchange.                    NOTE: This will be faster with offline qlib. :return:            The result of backtest, it is represented by a dict.                     { ""long"": long_returns(excess),                     ""short"": short_returns(excess),                     ""long_short"": long_short_returns}', 'CLASS qlib.qlib.backtest.exchange.Exchange has field or attribute or property qlib.qlib.backtest.exchange.limit_threshold', 'FUNCTION qlib.qlib.contrib.evaluate.long_short_backtest has parameter or accepts argument qlib.qlib.contrib.evaluate.long_short_backtest.open_cost\nA backtest for long-short strategy  :param pred:        The trading signal produced on day `T`. :param topk:       The short topk securities and long topk securities. :param deal_price:  The price to deal the trading. :param shift:       Whether to shift prediction by one day.  The trading day will be T+1 if shift==1. :param open_cost:   open transaction cost. :param close_cost:  close transaction cost. :param trade_unit:  100 for China A. :param limit_threshold: limit move 0.1 (10%) for example, long and short with same limit. :param min_cost:    min transaction cost. :param subscribe_fields: subscribe fields. :param extract_codes:  bool.                    will we pass the codes extracted from the pred to the exchange.                    NOTE: This will be faster with offline qlib. :return:            The result of backtest, it is represented by a dict.                     { ""long"": long_returns(excess),                     ""short"": short_returns(excess),                     ""long_short"": long_short_returns}', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1391, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.026371002197266,728
What shared approach do the min-max and z-score processors use to skip normalization for constant-value features?,"Both MinMaxNorm and ZScoreNorm processors skip normalization for constant-value features by overwriting scaling parameters to an identity transform, which leaves these features unchanged during the normalization process.","Both MinMaxNorm.fit and ZScoreNorm.fit detect constant‐value columns (`min==max` or `std==0`) and then overwrite their scaling params to an identity transform (MinMaxNorm sets those columns’ min=0, max=1; ZScoreNorm sets mean=0, std=1). That way their shared normalize() call `(x–min)/(max–min)` or `(x–mean)/std` leaves constant features unchanged.","['CLASS qlib.qlib.data.base.Expression has method qlib.qlib.data.base.Expression.load\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index\nExpression base class  Expression is designed to handle the calculation of data with the format below data with two dimension for each instrument,  - feature - time:  it  could be observation time or period time.      - period time is designed for Point-in-time database.  For example, the period time maybe 2014Q4, its value can observed for multiple times(different value may be observed at different time due to amendment).', 'CLASS qlib.qlib.data.dataset.processor.MinMaxNorm has method qlib.qlib.data.dataset.processor.MinMaxNorm.normalize', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.instrument\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.ZScoreNorm.ignore\nZScore Normalization', 'METHOD qlib.qlib.data.base.Expression.load has parameter or accepts argument qlib.qlib.data.base.Expression.load.start_index\nload  feature This function is responsible for loading feature/expression based on the expression engine.  The concrete implementation will be separated into two parts:  1) caching data, handle errors.      - This part is shared by all the expressions and implemented in Expression 2) processing and calculating data based on the specific expression.      - This part is different in each expression and implemented in each expression  Expression Engine is shared by different data. Different data will have different extra information for `args`.  Parameters ---------- instrument : str     instrument code. start_index : str     feature start index [in calendar]. end_index : str     feature end  index  [in calendar].  *args may contain following information: 1) if it is used in basic expression engine data, it contains following arguments     freq: str         feature frequency.  2) if is used in PIT data, it contains following arguments     cur_pit:         it is designed for the point-in-time data.     period: int         This is used for query specific period.         The period is represented with int in Qlib. (e.g. 202001 may represent the first quarter in 2020)  Returns ---------- pd.Series     feature series: The index of the series is the calendar index', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 4, 'pr': 1398, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.2212016582489,729
How does the subprocess execution wrapper initialize the shared configuration before running the target function?,"The subprocess execution wrapper in qlib initializes the shared configuration before running the target function by calling `C.register_from_C(self.qlib_config)` in the `_func_mod` method of the `call_in_subproc` wrapper. This method copies the passed-in configuration into the global C via `set_conf_from_C`, applies any logging config using `set_log_with_config`, and then calls `C.register()`. The target function is then executed in a fresh subprocess.","In qlib/utils/paral.py the call_in_subproc wrapper does its config init in its _func_mod method. Right before calling your target function it does:

```python
if self.qlib_config is not None:
    C.register_from_C(self.qlib_config)
```

That in turn (in qlib/config.py::Config.register_from_C) copies your passed‐in config into the global C via set_conf_from_C, applies any logging_config with set_log_with_config, and finally calls C.register() — then your function runs in the fresh subprocess.","['CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.done\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'METHOD qlib.qlib.utils.paral.call_in_subproc.__init__ has parameter or accepts argument qlib.qlib.utils.paral.call_in_subproc.__init__.func\nParameters ---------- func : Callable     the function to be wrapped  qlib_config : QlibConfig     Qlib config for initialization in subprocess  Returns ------- Callable', 'MODULE qlib.qlib.rl.order_execution.simulator_qlib contains CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'METHOD qlib.qlib.utils.paral.call_in_subproc.__init__ has parameter or accepts argument qlib.qlib.utils.paral.call_in_subproc.__init__.qlib_config\nParameters ---------- func : Callable     the function to be wrapped  qlib_config : QlibConfig     Qlib config for initialization in subprocess  Returns ------- Callable', 'CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.twap_price\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 1402, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",31.98630380630493,730
How does the parameter that controls forbidding all trades at price limits influence the direction argument used in tradability checks during sell and buy order generation?,"The `forbid_all_trade_at_limit` argument influences the direction argument used in tradability checks for sell and buy orders by adjusting it to 'hold' when trades are forbidden at price limits, allowing selling at limit-up and buying at limit-down when trades are allowed.","In TopkDropoutStrategy.generate_trade_decision (qlib/contrib/strategy/signal_strategy.py), the forbid_all_trade_at_limit flag controls what you pass as the direction arg to is_stock_tradable:

• if forbid_all_trade_at_limit=True, you call  
  direction=None  
  → no side is exempted, so hitting a limit blocks both buys and sells.  
• if forbid_all_trade_at_limit=False, you pass  
  direction=OrderDir.SELL (for sell orders) or  
  direction=OrderDir.BUY (for buy orders)  
  → allows selling at limit‐up and buying at limit‐down.","['METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.hold_thresh\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.backtest.exchange.Exchange.check_stock_limit has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.check_stock_limit.direction\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.forbid_all_trade_at_limit\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'METHOD qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.__init__ has parameter or accepts argument qlib.qlib.contrib.strategy.signal_strategy.TopkDropoutStrategy.only_tradable\nParameters ----------- topk : int     the number of stocks in the portfolio. n_drop : int     number of stocks to be replaced in each trading date. method_sell : str     dropout method_sell, random/bottom. method_buy : str     dropout method_buy, random/top. hold_thresh : int     minimum holding days     before sell stock , will check current.get_stock_count(order.stock_id) >= self.hold_thresh. only_tradable : bool     will the strategy only consider the tradable stock when buying and selling.      if only_tradable:          strategy will make decision with the tradable state of the stock info and avoid buy and sell them.      else:          strategy will make buy sell decision without checking the tradable state of the stock. forbid_all_trade_at_limit : bool     if forbid all trades when limit_up or limit_down reached.      if forbid_all_trade_at_limit:          strategy will not do any trade when price reaches limit up/down, even not sell at limit up nor buy at         limit down, though allowed in reality.      else:          strategy will sell at limit up and buy ad limit down.', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.check_stock_limit\nParameters ---------- stock_id : str start_time: pd.Timestamp end_time: pd.Timestamp direction : int, optional     trade direction, by default None     - if direction is None, check if tradable for buying and selling.     - if direction == Order.BUY, check the if tradable for buying     - if direction == Order.SELL, check the sell limit for selling.  Returns ------- True: the trading of the stock is limited (maybe hit the highest/lowest price), hence the stock is not tradable False: the trading of the stock is not limited, hence the stock may be tradable', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 9, 'n_files_pr': 2, 'pr': 1407, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",46.13952565193176,731
How does the recorder handle and report a non-zero exit status from one of its subprocess calls when logging uncommitted code?,"When logging uncommitted code, the recorder catches the CalledProcessError exception raised by subprocess.check_output and emits an info-level log via logger.info. The emitted info-level log includes the current working directory and the failed command.","In MLflowRecorder._log_uncommitted_code (qlib/workflow/recorder.py) each git call is wrapped in a try/except around subprocess.check_output. If check_output raises CalledProcessError (i.e. non-zero exit), it’s caught and an info-level log is emitted via logger.info, reporting the current working directory and the failed command.","['CLASS qlib.qlib.workflow.recorder.MLflowRecorder inherits from or is a subclass of qlib.qlib.workflow.recorder.Recorder\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'METHOD qlib.qlib.workflow.recorder.MLflowRecorder.end_run has parameter or accepts argument qlib.qlib.workflow.recorder.MLflowRecorder.end_run.status', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.get_local_dir\nThis function will return the directory path of this recorder.\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'MODULE qlib.qlib.workflow.recorder contains GLOBAL_VARIABLE qlib.qlib.workflow.recorder.logger', 'CLASS qlib.qlib.workflow.recorder.MLflowRecorder has method qlib.qlib.workflow.recorder.MLflowRecorder.__init__\nUse mlflow to implement a Recorder.  Due to the fact that mlflow will only log artifact from a file or directory, we decide to use file manager to help maintain the objects in the project.  Instead of using mlflow directly, we use another interface wrapping mlflow to log experiments. Though it takes extra efforts, but it brings users benefits due to following reasons. - It will be more convenient to change the experiment logging backend without changing any code in upper level - We can provide more convenience to automatically do some extra things and make interface easier. For examples:     - Automatically logging the uncommitted code     - Automatically logging part of environment variables     - User can control several different runs by just creating different Recorder (in mlflow, you always have to switch artifact_uri and pass in run ids frequently)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1409, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.407926559448242,732
How does the on-policy pipeline reuse the same simulator factory for both training and backtesting?,"The on-policy pipeline in qlib reuses the same simulator factory for both training and backtesting by passing the `_simulator_factory_simple` closure to both the `train()` call as `simulator_fn` and the `backtest()` call as `simulator_fn`. This allows the pipeline to leverage the same simulation logic for both training and evaluation phases, ensuring consistency and accuracy in model development and testing. The use of a single simulator factory also facilitates easier maintenance and modification of the simulation logic across different stages of the pipeline.","In train_onpolicy.py (train_and_test), there is exactly one simulator‐factory defined:

• def _simulator_factory_simple(order: Order) → SingleAssetOrderExecutionSimple

That same closure is passed into both the train() call and the backtest() call as simulator_fn. Thus training and backtesting both use the identical factory, i.e. the same simulator instantiation logic.","['CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has method qlib.qlib.rl.utils.env_wrapper.EnvWrapper.reset\nTry to get a state from state queue, and init the simulator with this state. If the queue is exhausted, generate an invalid (nan) observation.\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'MODULE qlib.qlib.rl.trainer.api contains FUNCTION qlib.qlib.rl.trainer.api.backtest\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper has method qlib.qlib.rl.utils.env_wrapper.EnvWrapper.__init__\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.policy\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'MODULE qlib.qlib.rl.utils.env_wrapper contains CLASS qlib.qlib.rl.utils.env_wrapper.EnvWrapper\nQlib-based RL environment, subclassing ``gym.Env``. A wrapper of components, including simulator, state-interpreter, action-interpreter, reward.  This is what the framework of simulator - interpreter - policy looks like in RL training. All the components other than policy needs to be assembled into a single object called ""environment"". The ""environment"" are replicated into multiple workers, and (at least in tianshou\'s implementation), one single policy (agent) plays against a batch of environments.  Parameters ---------- simulator_fn     A callable that is the simulator factory.     When ``seed_iterator`` is present, the factory should take one argument,     that is the seed (aka initial state).     Otherwise, it should take zero argument. state_interpreter     State-observation converter. action_interpreter     Policy-simulator action converter. seed_iterator     An iterable of seed. With the help of :class:`qlib.rl.utils.DataQueue`,     environment workers in different processes can share one ``seed_iterator``. reward_fn     A callable that accepts the StateType and returns a float (at least in single-agent case). aux_info_collector     Collect auxiliary information. Could be useful in MARL. logger     Log collector that collects the logs. The collected logs are sent back to main process,     via the return value of ``env.step()``.  Attributes ---------- status : EnvWrapperStatus     Status indicator. All terms are in *RL language*.     It can be used if users care about data on the RL side.     Can be none when no trajectory is available.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 22}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 22, 'n_files_pr': 9, 'pr': 1415, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",42.44233441352844,733
How is the order execution backtest pipeline structured from job scheduling through state updating to reward computation?,"The order execution backtest pipeline in qlib is structured as follows: **Job Scheduling**: The job scheduling piece reads a global order file, extracts the stock list, and fires off per-stock jobs in parallel using joblib. Each sub-job calls either single_with_simulator or single_with_collect_data_loop. **State Updating**: Each sub-job drives an SAOEState through tick updates, which map execution results to per-minute volumes, fetch market price and volume from the exchange, append to history_exec and history_steps, decrease the position, advance cur_time. **Reward Computation**: At the terminal step or when the position hits zero, the reward function computes VWAP and TWAP from history_exec and backtest_data, and returns -1, 0, or +1 based on the ratio of VWAP to TWAP.","The end‐to‐end backtest is implemented in three pieces:

1. Job scheduling (qlib/rl/contrib/backtest.py:backtest)  
   – Read a global order file, extract the stock list, and fire off per‐stock jobs in Parallel (joblib) calling either single_with_simulator or single_with_collect_data_loop.  

2. State updating (qlib/rl/order_execution/strategy.py:SAOEStateAdapter.update)  
   – Each sub‐job drives an SAOEState through tick updates:  
     • Map execution results to per-minute volumes, fetch market price/volume from the exchange,  
     • Append to history_exec and history_steps,  
     • Decrease position and advance cur_time.  

3. Reward computation (qlib/rl/order_execution/reward.py:PPOReward.reward)  
   – At terminal step (or when position hits zero), compute VWAP vs TWAP from history_exec and backtest_data, then return –1/0/+1 based on their ratio.","['MODULE qlib.qlib.rl.trainer.api contains FUNCTION qlib.qlib.rl.trainer.api.backtest\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'CLASS qlib.qlib.rl.order_execution.reward.PPOReward inherits from or is a subclass of qlib.rl.reward.Reward.Reward[qlib.rl.order_execution.state.SAOEState.SAOEState]\nReward proposed by paper ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"".  Parameters ---------- max_step     Maximum number of steps. start_time_index     First time index that allowed to trade. end_time_index     Last time index that allowed to trade.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.reward\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.initial_states\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'FUNCTION qlib.qlib.rl.trainer.api.backtest has parameter or accepts argument qlib.qlib.rl.trainer.api.backtest.state_interpreter\nBacktest with the parallelism provided by RL framework.  Experimental API. Parameters might change shortly.  Parameters ---------- simulator_fn     Callable receiving initial seed, returning a simulator. state_interpreter     Interprets the state of simulators. action_interpreter     Interprets the policy actions. initial_states     Initial states to iterate over. Every state will be run exactly once. policy     Policy to test against. logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. reward     Optional reward function. For backtest, this is for testing the rewards     and logging them only. finite_env_type     Type of finite env implementation. concurrency     Parallel workers.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=""center""> \n<img src=""workflow.png""/>\n</p>\n\nAt step <img src=""https://latex.codecogs.com/png.latex?s"" title=""s"" />, with training data <img src=""https://latex.codecogs.com/png.latex?x_s,y_s"" title=""x_s,y_s"" />, the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> chooses a suitable task <img src=""https://latex.codecogs.com/png.latex?T_{i_s}"" title=""T_{i_s}"" /> (green solid lines) to update the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> (blue solid lines). After <img src=""https://latex.codecogs.com/png.latex?S"" title=""S"" /> steps, we evaluate the model <img src=""https://latex.codecogs.com/png.latex?f"" title=""f"" /> on the validation set and update the scheduler <img src=""https://latex.codecogs.com/png.latex?\\varphi"" title=""\\varphi"" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 16}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 16, 'n_files_pr': 24, 'pr': 1447, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.50322127342224,734
How do the two data retrieval methods ensure consistent dtypes on the returned pandas Series?,"The two data retrieval methods ensure consistent dtypes on the returned pandas Series by using the following approaches: In `LocalPITProvider.period_feature`, raw values are read into a NumPy array with dtype equal to `C.pit_record_type[","In qlib/data/data.py:

- In LocalPITProvider.period_feature, the raw values are read into a NumPy array with dtype = C.pit_record_type[""value""], and then returned via  
  `pd.Series(value, index=…, dtype=VALUE_DTYPE)`  
  so the Series is guaranteed to have exactly that dtype.

- In LocalExpressionProvider.expression, after loading the Series it does  
  ```
  try:
      series = series.astype(np.float32)
  except (ValueError, TypeError):
      pass
  ```  
  forcing (where possible) all columns to np.float32 for consistency.","['CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.update_current_position\nUpdate current to make rtn consistent with earning at the end of bar, and update holding bar count of stock\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'CLASS qlib.qlib.data.data.LocalPITProvider has method qlib.qlib.data.data.LocalPITProvider.period_feature', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.data.data.LocalPITProvider.period_feature has parameter or accepts argument qlib.qlib.data.data.LocalPITProvider.period_feature.end_index', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1449, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",36.079421520233154,735
How do the experiment and recorder deletion routines validate input and convert API failures into user-friendly errors?,"The experiment and recorder deletion routines in qlib validate input by checking if either an `_id` or a `_name` is passed, and then wrap the MLflow client delete call in a try/except block to catch `MlflowException`. If such an exception occurs, it is re-raised as a `ValueError` with a clear, user-facing message indicating that the experiment or recorder ID or name is incorrect. This ensures that API failures are converted into user-friendly errors, making it easier for users to identify and fix any issues related to experiment or recorder deletion.","Both MLflowExpManager.delete_exp (qlib/workflow/expm.py) and MLflowExperiment.delete_recorder (qlib/workflow/exp.py) first use an assert to ensure you passed either an _id or a _name. They then wrap the MLflow client delete call in a try/except catching MlflowException, and re-raise it as a ValueError with a clear, user-facing message telling you to check that the experiment/recorder id or name is correct.","['MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.time_to_slc_point\nTime slicing in Qlib or Pandas is a frequently-used action. However, user often input all kinds of data format to represent time. This function will help user to convert these inputs into a uniform format which is friendly to time slicing.  Parameters ---------- t : Union[None, str, pd.Timestamp]     original time  Returns ------- Union[None, pd.Timestamp]:', 'MODULE qlib.qlib.utils.exceptions contains CLASS qlib.qlib.utils.exceptions.RecorderInitializationError\nError type for re-initialization when starting an experiment', 'FUNCTION qlib.qlib.utils.__init__.time_to_slc_point has parameter or accepts argument qlib.qlib.utils.__init__.time_to_slc_point.t\nTime slicing in Qlib or Pandas is a frequently-used action. However, user often input all kinds of data format to represent time. This function will help user to convert these inputs into a uniform format which is friendly to time slicing.  Parameters ---------- t : Union[None, str, pd.Timestamp]     original time  Returns ------- Union[None, pd.Timestamp]:', 'CLASS qlib.qlib.utils.exceptions.RecorderInitializationError inherits from or is a subclass of qlib.qlib.utils.exceptions.QlibException\nError type for re-initialization when starting an experiment', 'CLASS qlib.qlib.workflow.expm.ExpManager has method qlib.qlib.workflow.expm.ExpManager.start_exp\nStart an experiment. This method includes first get_or_create an experiment, and then set it to be active.  Maintaining `_active_exp_uri` is included in start_exp, remaining implementation should be included in _end_exp in subclass  Parameters ---------- experiment_id : str     id of the active experiment. experiment_name : str     name of the active experiment. recorder_id : str     id of the recorder to be started. recorder_name : str     name of the recorder to be started. uri : str     the current tracking URI. resume : boolean     whether to resume the experiment and recorder.  Returns ------- An active experiment.\nThis is the `ExpManager` class for managing experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The `ExpManager` is expected to be a singleton (btw, we can have multiple `Experiment`s with different uri. user can get different experiments from different uri, and then compare records of them). Global Config (i.e. `C`)  is also a singleton.  So we try to align them together.  They share the same variable, which is called **default uri**. Please refer to `ExpManager.default_uri` for details of variable sharing.  When the user starts an experiment, the user may want to set the uri to a specific uri (it will override **default uri** during this period), and then unset the **specific uri** and fallback to the **default uri**.    `ExpManager._active_exp_uri` is that **specific uri**.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crypto/README.md\n# Collect Crypto Data\n\n> *Please pay **ATTENTION** that the data is collected from [Coingecko](https://www.coingecko.com/en/api) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage of the dataset\n> *Crypto dataset only support Data retrieval function but not support backtest function due to the lack of OHLC data.*\n\n## Collector Data\n\n\n### Crypto Data\n\n#### 1d from Coingecko\n\n```bash\n\n# download from https://api.coingecko.com/api/v3/\npython collector.py download_data --source_dir ~/.qlib/crypto_data/source/1d --start 2015-01-01 --end 2021-11-30 --delay 1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/crypto_data/source/1d --normalize_dir ~/.qlib/crypto_data/source/1d_nor --interval 1d --date_field_name date\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/crypto_data/source/1d_nor --qlib_dir ~/.qlib/qlib_data/crypto_data --freq day --date_field_name date --include_fields prices,total_volumes,market_caps\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/crypto_data"")\ndf = D.features(D.instruments(market=""all""), [""$prices"", ""$total_volumes"",""$market_caps""], freq=""day"")\n```\n\n\n### Help\n```bash\npython collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- delay: 1']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 5, 'pr': 1463, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",36.18327975273132,736
How does the scheduler.step call differ when a validation segment is available versus when it's not?,"The scheduler.step call differs when a validation segment is available versus when it's not in the following way:

When a validation segment is available, the metrics argument is passed to the scheduler.step call. This means that the scheduler will use the validation loss as a metric to determine whether to step or not.

When there is no validation segment, the metrics argument is not passed to the scheduler.step call. In this case, the scheduler will not use any metrics to determine whether to step or not.

This behavior is consistent with the implementation of the EarlyStopping callback in qlib, which checks for improvement in a monitored metric (by default, the reward) during validation and stops training if no improvement is found.","In qlib/contrib/model/pytorch_nn.py inside DNNModelPytorch.fit:

• When you have a “valid” segment, on each eval‐step you do  
  auto_filter_kwargs(self.scheduler.step)(metrics=cur_loss_val, epoch=step)  
  (i.e. pass the validation loss as metrics).

• When you’re in retraining mode (no “valid”), you simply call  
  self.scheduler.step(epoch=step)  
  (no metrics arg).","[""CLASS qlib.qlib.rl.trainer.callbacks.EarlyStopping inherits from or is a subclass of qlib.qlib.rl.trainer.callbacks.Callback\nBase class of all callbacks.\nStop training when a monitored metric has stopped improving.  The earlystopping callback will be triggered each time validation ends. It will examine the metrics produced in validation, and get the metric with name ``monitor` (``monitor`` is ``reward`` by default), to check whether it's no longer increasing / decreasing. It takes ``min_delta`` and ``patience`` if applicable. If it's found to be not increasing / decreasing any more. ``trainer.should_stop`` will be set to true, and the training terminates.  Implementation reference: https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L1744-L1893"", 'METHOD qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.__init__ has parameter or accepts argument qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.__init__.scheduler', ""CLASS qlib.qlib.rl.trainer.callbacks.EarlyStopping has method qlib.qlib.rl.trainer.callbacks.EarlyStopping.__init__\nStop training when a monitored metric has stopped improving.  The earlystopping callback will be triggered each time validation ends. It will examine the metrics produced in validation, and get the metric with name ``monitor` (``monitor`` is ``reward`` by default), to check whether it's no longer increasing / decreasing. It takes ``min_delta`` and ``patience`` if applicable. If it's found to be not increasing / decreasing any more. ``trainer.should_stop`` will be set to true, and the training terminates.  Implementation reference: https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L1744-L1893"", 'METHOD qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.predict has parameter or accepts argument qlib.qlib.contrib.model.pytorch_nn.DNNModelPytorch.predict.segment', ""CLASS qlib.qlib.rl.trainer.callbacks.EarlyStopping has method qlib.qlib.rl.trainer.callbacks.EarlyStopping.on_validate_end\nStop training when a monitored metric has stopped improving.  The earlystopping callback will be triggered each time validation ends. It will examine the metrics produced in validation, and get the metric with name ``monitor` (``monitor`` is ``reward`` by default), to check whether it's no longer increasing / decreasing. It takes ``min_delta`` and ``patience`` if applicable. If it's found to be not increasing / decreasing any more. ``trainer.should_stop`` will be set to true, and the training terminates.  Implementation reference: https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L1744-L1893"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 5, 'pr': 1470, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.02375030517578,737
What mechanism removes overlapping information from the model similarity matrix before constructing the meta-dataset?,"The mechanism that removes overlapping information from the model similarity matrix before constructing the meta-dataset in qlib is the `mask_overlap` function, which uses pandas Series.mask to blank out overlapping entries in the IC matrix. This function is part of the `MetaDatasetDS._prepare_meta_ipt` method and computes a truncated end using `get_date_by_shift` and `self.trunc_days`. It then applies this mask to each column of the IC matrix to remove overlapping information.",Overlaps are zeroed out in MetaDatasetDS._prepare_meta_ipt via the inner mask_overlap function. It computes a “truncated” end (using get_date_by_shift and self.trunc_days) and then calls pandas Series.mask((index >= start)&(index <= trunc_end)) on each column of the IC matrix to blank any overlapping entries.,"['CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset inherits from or is a subclass of utils.serial.Serializable\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'METHOD qlib.qlib.contrib.meta.data_selection.model.MetaModelDS.inference has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.model.MetaModelDS.inference.meta_dataset', 'MODULE qlib.qlib.model.meta.dataset contains CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'MODULE qlib.qlib.contrib.meta.data_selection.dataset contains CLASS qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS', 'CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has field or attribute or property qlib.qlib.model.meta.dataset.segments\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md\n# ADD\n* Paper: [ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting](https://arxiv.org/abs/2012.06289).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/CatBoost/README.md\n# CatBoost\n* Code: [https://github.com/catboost/catboost](https://github.com/catboost/catboost)\n* Paper: CatBoost: unbiased boosting with categorical features. [https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 14}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 14, 'n_files_pr': 14, 'pr': 1472, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",35.4307758808136,738
Which two subclasses of BaseIntradayProcessedData are used to load handler-based versus legacy pickle-formatted intraday data?,The two subclasses of `BaseIntradayProcessedData` used to load handler-based versus legacy pickle-formatted intraday data in qlib are **HandlerIntradayProcessedData** and **PickleIntradayProcessedData**.,"The handler‐based loader is  `HandlerIntradayProcessedData` (via `load_handler_intraday_processed_data` in `qlib/rl/data/native.py`), whereas the legacy pickle‐formatted loader is  `PickleIntradayProcessedData` (via `load_pickle_intraday_processed_data` in `qlib/rl/data/pickle_styled.py`).","['CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.__init__\nParameters ---------- handler_config : dict     handler_config will be used to describe the handlers      .. code-block::          <handler_config> := {             ""group_name1"": <handler>             ""group_name2"": <handler>         }         or         <handler_config> := <handler>         <handler> := DataHandler Instance | DataHandler Config  fetch_kwargs : dict     fetch_kwargs will be used to describe the different arguments of fetch method, such as col_set, squeeze, data_key, etc.  is_group: bool     is_group will be used to describe whether the key of handler_config is group\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn\'t provide such interface & hook.', 'CLASS qlib.qlib.rl.data.native.HandlerIntradayProcessedData inherits from or is a subclass of base.BaseIntradayProcessedData\nSubclass of IntradayProcessedData. Used to handle handler (bin format) style data.', ""CLASS qlib.qlib.data.dataset.loader.DataLoaderDH has method qlib.qlib.data.dataset.loader.DataLoaderDH.load\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook."", 'CLASS qlib.qlib.rl.data.native.HandlerIntradayProcessedData has method qlib.qlib.rl.data.native.HandlerIntradayProcessedData.__repr__\nSubclass of IntradayProcessedData. Used to handle handler (bin format) style data.', ""MODULE qlib.qlib.data.dataset.loader contains CLASS qlib.qlib.data.dataset.loader.DataLoaderDH\nDataLoaderDH DataLoader based on (D)ata (H)andler It is designed to load multiple data from data handler - If you just want to load data from single datahandler, you can write them in single data handler  TODO: What make this module not that easy to use.  - For online scenario      - The underlayer data handler should be configured. But data loader doesn't provide such interface & hook."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 30}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 30, 'n_files_pr': 16, 'pr': 1480, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",30.276193857192993,739
How does the training routine enable random action sampling for DQN policies when gathering experiences?,"The training routine enables random action sampling for DQN policies when gathering experiences through the `exploration_noise=True` flag, which is passed to the Collector constructor in `TrainingVessel.train`. This flag calls the `DQN policy's action method with exploration enabled`, specifically using an ε-greedy sampler. The ε-greedy sampler is implemented in the `DQNPolicy.forward` and `DQNPolicy.act` methods in `qlib/rl/policy/dqn.py`, where random action sampling logic lives.","The key is the `exploration_noise=True` flag you pass into the `Collector` in `TrainingVessel.train`:

```python
collector = Collector(
    self.policy, vector_env, VectorReplayBuffer(...), exploration_noise=True
)
```

That flag tells `Collector.collect()` (in `qlib/rl/collector/collector.py`) to call your DQN policy’s action method with exploration enabled (i.e. its ε-greedy sampler). The actual random action sampling logic lives in the DQN policy (e.g. `DQNPolicy.forward` / `DQNPolicy.act` in `qlib/rl/policy/dqn.py`).","['CLASS qlib.qlib.rl.interpreter.ActionInterpreter has method qlib.qlib.rl.interpreter.ActionInterpreter.interpret\nConvert the policy action to simulator action.  Parameters ---------- simulator_state     Retrieved with ``simulator.get_state()``. action     Raw action given by policy.  Returns ------- The action needed by simulator,\nAction Interpreter that interpret rl agent action into qlib orders', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.get_policy_state_dict\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', 'METHOD qlib.qlib.strategy.base.RLIntStrategy.__init__ has parameter or accepts argument qlib.qlib.strategy.base.RLIntStrategy.__init__.policy\nParameters ---------- state_interpreter : Union[dict, StateInterpreter]     interpreter that interprets the qlib execute result into rl env state action_interpreter : Union[dict, ActionInterpreter]     interpreter that interprets the rl agent action into qlib order list start_time : Union[str, pd.Timestamp], optional     start time of trading, by default None end_time : Union[str, pd.Timestamp], optional     end time of trading, by default None', 'CLASS qlib.qlib.rl.trainer.trainer.Trainer has method qlib.qlib.rl.trainer.trainer.Trainer.load_state_dict\nLoad all states into current trainer.\nUtility to train a policy on a particular task.  Different from traditional DL trainer, the iteration of this trainer is ""collect"", rather than ""epoch"", or ""mini-batch"". In each collect, :class:`Collector` collects a number of policy-env interactions, and accumulates them into a replay buffer. This buffer is used as the ""data"" to train the policy. At the end of each collect, the policy is *updated* several times.  The API has some resemblence with `PyTorch Lightning <https://pytorch-lightning.readthedocs.io/>`__, but it\'s essentially different because this trainer is built for RL applications, and thus most configurations are under RL context. We are still looking for ways to incorporate existing trainer libraries, because it looks like big efforts to build a trainer as powerful as those libraries, and also, that\'s not our primary goal.  It\'s essentially different `tianshou\'s built-in trainers <https://tianshou.readthedocs.io/en/master/api/tianshou.trainer.html>`__, as it\'s far much more complicated than that.  Parameters ---------- max_iters     Maximum iterations before stopping. val_every_n_iters     Perform validation every n iterations (i.e., training collects). logger     Logger to record the backtest results. Logger must be present because     without logger, all information will be lost. finite_env_type     Type of finite env implementation. concurrency     Parallel workers. fast_dev_run     Create a subset for debugging.     How this is implemented depends on the implementation of training vessel.     For :class:`~qlib.rl.vessel.TrainingVessel`, if greater than zero,     a random subset sized ``fast_dev_run`` will be used     instead of ``train_initial_states`` and ``val_initial_states``.', ""CLASS qlib.qlib.rl.interpreter.ActionInterpreter inherits from or is a subclass of qlib.qlib.rl.interpreter.Interpreter\nInterpreter is a media between states produced by simulators and states needed by RL policies. Interpreters are two-way:  1. From simulator state to policy state (aka observation), see :class:`StateInterpreter`. 2. From policy action to action accepted by simulator, see :class:`ActionInterpreter`.  Inherit one of the two sub-classes to define your own interpreter. This super-class is only used for isinstance check.  Interpreters are recommended to be stateless, meaning that storing temporary information with ``self.xxx`` in interpreter is anti-pattern. In future, we might support register some interpreter-related states by calling ``self.env.register_state()``, but it's not planned for first iteration.\nAction Interpreter that interpret rl agent action into qlib orders"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=""#main-challenges--solutions-in-quant-research"">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=""circle""><a href=""#forecasting-finding-valuable-signalspatterns"">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=""disc""><a href=""#quant-model-paper-zoo""><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=""circle""><a href=""#run-a-single-model"">Run a Single Model</a></li>\n                    <li type=""circle""><a href=""#run-multiple-models"">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=""circle""><a href=""#adapting-to-market-dynamics"">Adapting to Market Dynamics</a></li>\n          <li type=""circle""><a href=""#reinforcement-learning-modeling-continuous-decisions"">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=""align: center"">\n<img src=""docs/_static/img/framework-abstract.jpg"" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib\'s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: ""An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization"", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: ""Universal Trading for Order Execution with Oracle Policy Distillation"", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  √        |  √           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  √        |  √           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib\'s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It\'s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1481, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",36.29452085494995,740
How does the data processing pipeline differ between independent and append modes?,"**Data Processing Pipeline Comparison between Independent and Append Modes**

In qlib, the data processing pipeline differs between independent and append modes as follows:

*   **Independent Mode (PTYPE_I)**: shared is assigned to `_shared_df`. Then, `_infer` is set to `_shared_df` and passed to `infer_processors`, and `_learn` is set to `_shared_df` and then passed to `learn_processors`. This means that in Independent mode, the data for inference and learning are both taken from the same source.

*   **Append Mode (PTYPE_A)**: shared is assigned to `_shared_df`. Then, `_infer` is set to `_shared_df` and passed to `infer_processors`, and `_learn` is set to `_infer` and then passed to `learn_processors`. This means that in Append mode, the data for learning is taken from the result of the inference process.

In summary, the key difference between Independent and Append modes lies in how the data for learning is obtained.","In DataHandlerLP.process_data (qlib/data/dataset/handler.py) the two modes differ only in how the “learn” branch is seeded:

• Independent (PTYPE_I):  
  – shared ⇒ _shared_df  
  – _infer = _shared_df → infer_processors  
  – _learn = _shared_df → learn_processors  

• Append (PTYPE_A):  
  – shared ⇒ _shared_df  
  – _infer = _shared_df → infer_processors  
  – _learn = _infer    → learn_processors  

In other words, in “independent” mode the learn‐ and infer‐pipelines both start from the shared output, whereas in “append” mode the learn‐pipeline is applied on top of the already inverted infer‐pipeline.","['CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.get_update_data\nreturn the updated data based on the given dataset  The difference between `get_update_data` and `update` - `update_date` only include some data specific feature - `update` include some general routine steps(e.g. prepare dataset, checking)\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321', 'CLASS qlib.qlib.contrib.data.handler.Alpha158 inherits from or is a subclass of data.dataset.handler.DataHandlerLP', 'CLASS qlib.qlib.data.dataset.__init__.DatasetH inherits from or is a subclass of qlib.qlib.data.dataset.__init__.Dataset\nPreparing data for model training and inferencing.\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'MODULE qlib.qlib.data.dataset.handler contains GLOBAL_VARIABLE qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', 'MODULE qlib.qlib.data.dataset.__init__ contains CLASS qlib.qlib.data.dataset.__init__.DatasetH\nDataset with Data(H)andler  User should try to put the data preprocessing functions into handler. Only following data processing functions should be placed in Dataset:  - The processing is related to specific model.  - The processing is related to data split.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1485, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.44534492492676,741
What exception is raised when BASE_CONFIG_PATH cannot be found in either the current working directory or relative to the main config file?,"When `BASE_CONFIG_PATH` cannot be found in either the current working directory or relative to the main config file, a `FileNotFoundError` is raised.","In qlib/workflow/cli.py’s `workflow()` function, if `BASE_CONFIG_PATH` isn’t found in either the CWD or relative to the main config, a `FileNotFoundError` is raised.","['MODULE cognee.cognee.infrastructure.files.utils.guess_file_type contains CLASS cognee.cognee.infrastructure.files.utils.guess_file_type.FileTypeException\nRepresents an exception for file type errors.  This exception is raised when an invalid file type is encountered. It includes a custom message to describe the error.  Parameters: -----------      - message (str): The message describing the exception error.', 'CLASS cognee.cognee.base_config.BaseConfig has field or attribute or property cognee.cognee.base_config.BaseConfig.system_root_directory', 'CLASS cognee.cognee.infrastructure.files.utils.guess_file_type.FileTypeException has field or attribute or property cognee.cognee.infrastructure.files.utils.guess_file_type.message\nRepresents an exception for file type errors.  This exception is raised when an invalid file type is encountered. It includes a custom message to describe the error.  Parameters: -----------      - message (str): The message describing the exception error.', 'CLASS cognee.cognee.base_config.BaseConfig has field or attribute or property cognee.cognee.base_config.BaseConfig.data_root_directory', 'CLASS cognee.cognee.infrastructure.files.utils.guess_file_type.FileTypeException has field or attribute or property cognee.cognee.infrastructure.files.utils.guess_file_type.FileTypeException.message\nRepresents an exception for file type errors.  This exception is raised when an invalid file type is encountered. It includes a custom message to describe the error.  Parameters: -----------      - message (str): The message describing the exception error.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/basic/README.md\nC-APIs\n===\n\n**XGBoost** implements a C API originally designed for various language\nbindings.  For detailed reference, please check xgboost/c_api.h.  Here is a\ndemonstration of using the API.\n\n# CMake\nIf you use **CMake** for your project, you can either install **XGBoost**\nsomewhere in your system and tell CMake to find it by calling\n`find_package(xgboost)`, or put **XGBoost** inside your project's source tree\nand call **CMake** command: `add_subdirectory(xgboost)`.  To use\n`find_package()`, put the following in your **CMakeLists.txt**:\n\n``` CMake\nfind_package(xgboost REQUIRED)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost::xgboost)\n```\n\nIf you want to put XGBoost inside your project (like git submodule), use this\ninstead:\n``` CMake\nadd_subdirectory(xgboost)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost)\n```\n\n# make\nYou can start by modifying the makefile in this directory to fit your need."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1500, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.869538068771362,742
How do the two single-threaded backtest routines differ in how they obtain their strategy configurations and set up the executor before running the simulation?,"The two single-threaded backtest routines in qlib differ in how they obtain their strategy configurations and set up the executor before running the simulation.

In `single_with_simulator`, the strategy config is taken straight from `backtest_config[","The two routines differ mainly in where they get their “strategy” and how they turn the executor config into a running simulator:

• single_with_simulator (backtest.py:single_with_simulator)  
  – Strategy config is taken straight from backtest_config[""strategies""] on each order.  
  – It calls _get_multi_level_executor_config(...) to build an executor_config dict, then hands that raw config to a new SingleAssetOrderExecution instance per order.  

• single_with_collect_data_loop (backtest.py:single_with_collect_data_loop)  
  – It first builds its own FileOrderStrategy config (wrapping the orders DataFrame and trade times).  
  – It still calls _get_multi_level_executor_config(...) once, but then passes both the FileOrderStrategy config and the executor_config into get_strategy_executor(...), which instantiates real Strategy and Executor objects that are driven by collect_data_loop over the full backtest period.","['MODULE qlib.qlib.backtest.executor contains CLASS qlib.qlib.backtest.executor.NestedExecutor\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'METHOD qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.__init__ has parameter or accepts argument qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.__init__.executor_config', 'CLASS qlib.qlib.backtest.executor.NestedExecutor has method qlib.qlib.backtest.executor.NestedExecutor.get_all_executors\nget all executors, including self and inner_executor.get_all_executors()\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'METHOD qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.__init__ has parameter or accepts argument qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.__init__.qlib_config', 'CLASS qlib.qlib.backtest.executor.NestedExecutor has field or attribute or property qlib.qlib.backtest.executor.end_time\nNested Executor with inner strategy and executor - At each time `execute` is called, it will call the inner strategy and executor to execute the `trade_decision`     in a higher frequency env.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline\'s testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it\'s best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use ""Price Advantage (PA)"" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 ± 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 ± 0.5780      |\n| PPO                         | -1.0935 ± 0.0922      |\n| TWAP                        |   ≈ 0.0 ± 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last ""half hour""). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 6, 'pr': 1503, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.64326786994934,743
How do the order ingestion and time-series sampler components normalize and sort the datetime index for fast retrieval?,"The `FileOrderStrategy` and `TSDataSampler` components in qlib normalize and sort the datetime index for fast retrieval as follows:

1. **FileOrderStrategy**: It casts the “datetime” column to pandas.Timestamp, sets the DataFrame index with [“datetime”, “instrument”], unifies the index dtype by calling `convert_index_format` with level=“datetime”, and sorts on datetime for fast lookups by calling `lazy_sort_index`. 

2. **TSDataSampler**: It asserts that the “datetime” column is at level 0, swaps the levels so that instrument becomes level 0 and datetime becomes level 1 using `swaplevel()`, sorts the MultiIndex (instrument, datetime) by calling `sort_index()`, and builds a numpy array and index maps for O(1) slicing.","In FileOrderStrategy (qlib/contrib/strategy/rule_strategy.py) we do:

• Cast the “datetime” column to pandas.Timestamp  
• set_index([“datetime”, “instrument”]) so datetime is level 0  
• call convert_index_format(…, level=""datetime"") to unify the index dtype  
• call lazy_sort_index(…) to sort on datetime for fast lookups  

In the TSDataSampler (qlib/data/dataset/__init__.py) __init__ we:

• assert get_level_index(data, ""datetime"")==0  
• swaplevel() so instrument becomes level 0 and datetime level 1  
• sort_index() on the MultiIndex (instrument, datetime)  
…then build a numpy array and index maps for O(1) slicing.","['CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has method qlib.qlib.data.dataset.__init__.TSDataSampler.get_index\nGet the pandas index of the data, it will be useful in following scenarios - Special sampler will be used (e.g. user want to sample day by day)\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.__init__ has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.__init__.step_len\nBuild a dataset which looks like torch.data.utils.Dataset.  Parameters ---------- data : pd.DataFrame     The raw tabular data whose index order is <""datetime"", ""instrument""> start :     The indexable start time end :     The indexable end time step_len : int     The length of the time-series step fillna_type : int     How will qlib handle the sample if there is on sample in a specific date.     none:         fill with np.nan     ffill:         ffill with previous sample     ffill+bfill:         ffill with previous samples first and fill with later samples second flt_data : pd.Series     a column of data(True or False) to filter data. Its index order is <""datetime"", ""instrument"">     This feature is essential because:     - We want some sample not included due to label-based filtering, but we can\'t filter them at the beginning due to the features is still important in the feature.     None:         kepp all data', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has method qlib.qlib.data.dataset.__init__.TSDataSampler.__init__\nBuild a dataset which looks like torch.data.utils.Dataset.  Parameters ---------- data : pd.DataFrame     The raw tabular data whose index order is <""datetime"", ""instrument""> start :     The indexable start time end :     The indexable end time step_len : int     The length of the time-series step fillna_type : int     How will qlib handle the sample if there is on sample in a specific date.     none:         fill with np.nan     ffill:         ffill with previous sample     ffill+bfill:         ffill with previous samples first and fill with later samples second flt_data : pd.Series     a column of data(True or False) to filter data. Its index order is <""datetime"", ""instrument"">     This feature is essential because:     - We want some sample not included due to label-based filtering, but we can\'t filter them at the beginning due to the features is still important in the feature.     None:         kepp all data\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.__init__ has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.__init__.end\nBuild a dataset which looks like torch.data.utils.Dataset.  Parameters ---------- data : pd.DataFrame     The raw tabular data whose index order is <""datetime"", ""instrument""> start :     The indexable start time end :     The indexable end time step_len : int     The length of the time-series step fillna_type : int     How will qlib handle the sample if there is on sample in a specific date.     none:         fill with np.nan     ffill:         ffill with previous sample     ffill+bfill:         ffill with previous samples first and fill with later samples second flt_data : pd.Series     a column of data(True or False) to filter data. Its index order is <""datetime"", ""instrument"">     This feature is essential because:     - We want some sample not included due to label-based filtering, but we can\'t filter them at the beginning due to the features is still important in the feature.     None:         kepp all data', 'CLASS qlib.qlib.data.dataset.__init__.TSDataSampler has field or attribute or property qlib.qlib.data.dataset.__init__.TSDataSampler.data_arr\n(T)ime-(S)eries DataSampler This is the result of TSDatasetH  It works like `torch.data.utils.Dataset`, it provides a very convenient interface for constructing time-series dataset based on tabular data. - On time step dimension, the smaller index indicates the historical data and the larger index indicates the future   data.  If user have further requirements for processing data, user could process them based on `TSDataSampler` or create more powerful subclasses.  Known Issues: - For performance issues, this Sampler will convert dataframe into arrays for better performance. This could result   in a different data type   Indices design:     TSDataSampler has a index mechanism to help users query time-series data efficiently.      The definition of related variables:         data_arr: np.ndarray             The original data. it will contains all the original data.             The querying are often for time-series of a specific stock.             By leveraging this data charactoristics to speed up querying, the multi-index of data_arr is rearranged in (instrument, datetime) order          data_index: pd.MultiIndex with index order <instrument, datetime>             it has the same shape with `idx_map`. Each elements of them are expected to be aligned.          idx_map: np.ndarray             It is the indexable data. It originates from data_arr, and then filtered by 1) `start` and `end`  2) `flt_data`                 The extra data in data_arr is useful in following cases                 1) creating meaningful time series data before `start` instead of padding them with zeros                 2) some data are excluded by `flt_data` (e.g. no <X, y> sample pair for that index). but they are still used in time-series in X              Finnally, it will look like.              array([[  0,   0],                    [  1,   0],                    [  2,   0],                    ...,                    [241, 348],                    [242, 348],                    [243, 348]], dtype=int32)              It list all indexable data(some data only used in historical time series data may not be indexabla), the values are the corresponding row and col in idx_df         idx_df: pd.DataFrame             It aims to map the <datetime, instrument> key to the original position in data_arr              For example, it may look like (NOTE: the index for a instrument time-series is continoues in memory)                  instrument SH600000 SH600008 SH600009 SH600010 SH600011 SH600015  ...                 datetime                 2017-01-03        0      242      473      717      NaN      974  ...                 2017-01-04        1      243      474      718      NaN      975  ...                 2017-01-05        2      244      475      719      NaN      976  ...                 2017-01-06        3      245      476      720      NaN      977  ...          With these two indices(idx_map, idx_df) and original data(data_arr), we can make the following queries fast (implemented in __getitem__)         (1) Get the i-th indexable sample(time-series):   (indexable sample index) -> [idx_map] -> (row col) -> [idx_df] -> (index in data_arr)         (2) Get the specific sample by <datetime, instrument>:  (<datetime, instrument>, i.e. <row, col>) -> [idx_df] -> (index in data_arr)         (3) Get the index of a time-series data:   (get the <row, col>, refer to (1), (2)) -> [idx_df] -> (all indices in data_arr for time-series)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 1507, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",60.79750370979309,744
"When no password is specified in configuration, what value does the Redis task client receive for the password parameter?","When no password is specified in the configuration, the Redis task client receives a `password` parameter of `None`. This is because the assignment of the `password` argument occurs in `qlib/utils/__init__.py`, and `C.redis_password` defaults to `None` when no password is specified.","The Redis client’s `password` arg is set to `C.redis_password` (in qlib/utils/__init__.py). Since `C.redis_password` defaults to None when you don’t specify one, the client gets `password=None`.","['METHOD cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__.password\nInitialize remote Kuzu database connection.  Args:     api_url: URL of the Kuzu REST API     username: Optional username for API authentication     password: Optional password for API authentication', 'CLASS qlib.qlib.config.QlibConfig has method qlib.qlib.config.QlibConfig.is_depend_redis', 'METHOD cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__.api_url\nInitialize remote Kuzu database connection.  Args:     api_url: URL of the Kuzu REST API     username: Optional username for API authentication     password: Optional password for API authentication', 'METHOD qlib.qlib.config.QlibConfig.is_depend_redis has parameter or accepts argument qlib.qlib.config.DEPENDENCY_REDIS_CACHE', 'METHOD cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.remote_kuzu_adapter.RemoteKuzuAdapter.__init__.username\nInitialize remote Kuzu database connection.  Args:     api_url: URL of the Kuzu REST API     username: Optional username for API authentication     password: Optional password for API authentication', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What\'s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=""https://github.com/microsoft/RD-Agent""><img src=""docs/_static/img/rdagent_logo.png"" alt=""RD_Agent"" style=""height: 2em""></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!\n\nTo learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (中文) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- 📃**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- 👾**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1508, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.28478193283081,745
What mechanism does the multivariate time series dataset use to infer the forecast horizon from label definitions when no horizon parameter is provided?,"When no horizon parameter is provided to MTSDatasetH, its constructor extracts the label expression from handler.data_loader.fields[""label""] and then calls the guess_horizon utility on the extracted label expression. The guess_horizon utility parses the label string for Ref($…, -k) patterns and returns the maximum lag k found in those patterns as the forecast horizon.","When you pass `horizon=0` to `MTSDatasetH` (qlib/contrib/data/dataset.py), its constructor pulls the label expression out of `handler.data_loader.fields[""label""]` and calls the `guess_horizon` utility on it. Internally, `guess_horizon` (tested in `tests/misc/test_utils.py`) parses the label string for `Ref($…, -k)` patterns and returns the maximum lag `k` as the forecast horizon.","['FUNCTION qlib.qlib.utils.data.guess_horizon has parameter or accepts argument qlib.qlib.utils.data.guess_horizon.label\nTry to guess the horizon by parsing label', 'CLASS qlib.examples.benchmarks.TRA.src.dataset.MTSDatasetH inherits from or is a subclass of qlib.data.dataset.DatasetH\nMemory Augmented Time Series Dataset  Args:     handler (DataHandler): data handler     segments (dict): data split segments     seq_len (int): time series sequence length     horizon (int): label horizon (to mask historical loss for TRA)     num_states (int): how many memory states to be added (for TRA)     batch_size (int): batch size (<0 means daily batch)     shuffle (bool): whether shuffle data     pin_memory (bool): whether pin data to gpu memory     drop_last (bool): whether drop last batch < batch_size', 'MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.guess_horizon\nTry to guess the horizon by parsing label', 'METHOD qlib.qlib.workflow.task.gen.MultiHorizonGenBase.__init__ has parameter or accepts argument qlib.qlib.workflow.task.gen.MultiHorizonGenBase.__init__.horizon\nThis task generator tries to generate tasks for different horizons based on an existing task  Parameters ---------- horizon : List[int]     the possible horizons of the tasks label_leak_n : int     How many future days it will take to get complete label after the day making prediction     For example:     - User make prediction on day `T`(after getting the close price on `T`)     - The label is the return of buying stock on `T + 1` and selling it on `T + 2`     - the `label_leak_n` will be 2 (e.g. two days of information is leaked to leverage this sample)', 'METHOD qlib.qlib.contrib.data.dataset.MTSDatasetH.__init__ has parameter or accepts argument qlib.qlib.contrib.data.dataset.MTSDatasetH.__init__.horizon', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 6, 'pr': 1509, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.310399293899536,746
How does the code suppress the SettingWithCopyWarning during the groupwise z-score normalization?,The `CSZScoreNorm.__call__` function in qlib/data/dataset/processor.py suppresses the SettingWithCopyWarning during group-wise z-score normalization by temporarily setting `mode.chained_assignment` to None within a `pd.option_context` block. This silences the warning without affecting the computation.,"In CSZScoreNorm.__call__ (qlib/data/dataset/processor.py) the group-wise z-score is wrapped in:

```python
with pd.option_context(""mode.chained_assignment"", None):
    …
```

This temporarily sets `mode.chained_assignment=None`, silencing the SettingWithCopyWarning.","['MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'CLASS qlib.qlib.data.dataset.processor.ZScoreNorm has field or attribute or property qlib.qlib.data.dataset.processor.ZScoreNorm.ignore\nZScore Normalization', 'MODULE qlib.qlib.utils.data contains FUNCTION qlib.qlib.utils.data.robust_zscore\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'MODULE qlib.qlib.data.dataset.processor contains CLASS qlib.qlib.data.dataset.processor.ZScoreNorm\nZScore Normalization', 'CLASS qlib.qlib.data.dataset.processor.RobustZScoreNorm has method qlib.qlib.data.dataset.processor.RobustZScoreNorm.fit\nRobust ZScore Normalization  Use robust statistics for Z-Score normalization:     mean(x) = median(x)     std(x) = MAD(x) * 1.4826  Reference:     https://en.wikipedia.org/wiki/Median_absolute_deviation.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1513, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",40.40896272659302,747
"After transposing the batch tensor, which slice is used as the label input for loss computation in both epochs?","After transposing the batch tensor, the last time-step is used as the label input for loss computation in both epochs. This can be seen from the expression `data[:, -1, -1]` which selects the last time-step.","In both train_epoch and test_epoch (qlib/contrib/model/pytorch_tcn_ts.py), the label is taken as the last time‐step, last feature slice:  
label = data[:, -1, -1]","['CLASS keras.keras.src.backend.common.variables_test.VariablePropertiesTest has method keras.keras.src.backend.common.variables_test.VariablePropertiesTest.test_deferred_assignment\nTests deferred assignment to variables.\nTests for Variable._deferred_initialize Variable._maybe_autocast', 'MODULE keras.keras.src.backend.tensorflow.numpy contains GLOBAL_VARIABLE keras.keras.src.backend.tensorflow.numpy.nan_batch_members', 'METHOD xgboost.python-package.xgboost.core.Booster.feature_names has parameter or accepts argument xgboost.python-package.xgboost.core.Booster.feature_names.features\nFeature names for this booster.  Can be directly set by input data or by assignment.', 'CLASS keras.keras.src.ops.numpy.Transpose has field or attribute or property keras.keras.src.ops.numpy.x', 'METHOD xgboost.python-package.xgboost.core.Booster.feature_types has parameter or accepts argument xgboost.python-package.xgboost.core.Booster.feature_types.features\nFeature types for this booster.  Can be directly set by input data or by assignment.  See :py:class:`DMatrix` for details.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1520, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",28.023911952972412,749
"Which variable holds the count of consecutive epochs without validation improvement, and which attribute defines the threshold for early stopping?",The variable that holds the count of consecutive epochs without validation improvement is `stop_steps` and the attribute that defines the threshold for early stopping is `self.early_stop`. This information can be found in qlib/contrib/model/pytorch_sandwich.py where Sandwich.fit is defined.,"In Sandwich.fit (qlib/contrib/model/pytorch_sandwich.py) the local variable stop_steps tracks how many epochs have gone by without a validation improvement, and the model attribute self.early_stop defines the threshold for triggering early stopping.","['CLASS qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer has method qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.fit\nFits deep neural network for given training and validation data.  Args:   train_df: DataFrame for training data   valid_df: DataFrame for validation data\nDefines Temporal Fusion Transformer.  Attributes:   name: Name of model   time_steps: Total number of input time steps per forecast date (i.e. Width     of Temporal fusion decoder N)   input_size: Total number of inputs   output_size: Total number of outputs   category_counts: Number of categories per categorical variable   n_multiprocessing_workers: Number of workers to use for parallel     computations   column_definition: List of tuples of (string, DataType, InputType) that     define each column   quantiles: Quantiles to forecast for TFT   use_cudnn: Whether to use Keras CuDNNLSTM or standard LSTM layers   hidden_layer_size: Internal state size of TFT   dropout_rate: Dropout discard rate   max_gradient_norm: Maximum norm for gradient clipping   learning_rate: Initial learning rate of ADAM optimizer   minibatch_size: Size of minibatches for training   num_epochs: Maximum number of epochs for training   early_stopping_patience: Maximum number of iterations of non-improvement     before early stopping kicks in   num_encoder_steps: Size of LSTM encoder -- i.e. number of past time steps     before forecast date to use   num_stacks: Number of self-attention layers to apply (default is 1 for basic     TFT)   num_heads: Number of heads for interpretable mulit-head attention   model: Keras model for TFT', 'METHOD qlib.qlib.contrib.model.pytorch_sandwich.Sandwich.test_epoch has parameter or accepts argument qlib.qlib.contrib.model.pytorch_sandwich.Sandwich.loss', 'CLASS xgboost.python-package.xgboost.callback.EarlyStopping has method xgboost.python-package.xgboost.callback.EarlyStopping.get_s\nget score if it\'s cross validation history.\nCallback function for early stopping  .. versionadded:: 1.3.0  Parameters ---------- rounds :     Early stopping rounds. metric_name :     Name of metric that is used for early stopping. data_name :     Name of dataset that is used for early stopping. maximize :     Whether to maximize evaluation metric.  None means auto (discouraged). save_best :     Whether training should return the best model or the last model. If set to     `True`, it will only keep the boosting rounds up to the detected best iteration,     discarding the ones that come after. This is only supported with tree methods     (not `gblinear`). Also, the `cv` function doesn\'t return a model, the parameter     is not applicable. min_delta :      .. versionadded:: 1.5.0      Minimum absolute change in score to be qualified as an improvement.  Examples --------  .. code-block:: python      es = xgboost.callback.EarlyStopping(         rounds=2,         min_delta=1e-3,         save_best=True,         maximize=False,         data_name=""validation_0"",         metric_name=""mlogloss"",     )     clf = xgboost.XGBClassifier(tree_method=""hist"", device=""cuda"", callbacks=[es])      X, y = load_digits(return_X_y=True)     clf.fit(X, y, eval_set=[(X, y)])', 'METHOD qlib.qlib.contrib.model.pytorch_sandwich.Sandwich.train_epoch has parameter or accepts argument qlib.qlib.contrib.model.pytorch_sandwich.Sandwich.loss', 'CLASS qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer has method qlib.examples.benchmarks.TFT.libs.tft_model.TemporalFusionTransformer.static_combine_and_mask\nApplies variable selection network to static inputs.  Args:   embedding: Transformed static inputs  Returns:   Tensor output for variable selection network\nDefines Temporal Fusion Transformer.  Attributes:   name: Name of model   time_steps: Total number of input time steps per forecast date (i.e. Width     of Temporal fusion decoder N)   input_size: Total number of inputs   output_size: Total number of outputs   category_counts: Number of categories per categorical variable   n_multiprocessing_workers: Number of workers to use for parallel     computations   column_definition: List of tuples of (string, DataType, InputType) that     define each column   quantiles: Quantiles to forecast for TFT   use_cudnn: Whether to use Keras CuDNNLSTM or standard LSTM layers   hidden_layer_size: Internal state size of TFT   dropout_rate: Dropout discard rate   max_gradient_norm: Maximum norm for gradient clipping   learning_rate: Initial learning rate of ADAM optimizer   minibatch_size: Size of minibatches for training   num_epochs: Maximum number of epochs for training   early_stopping_patience: Maximum number of iterations of non-improvement     before early stopping kicks in   num_encoder_steps: Size of LSTM encoder -- i.e. number of past time steps     before forecast date to use   num_stacks: Number of self-attention layers to apply (default is 1 for basic     TFT)   num_heads: Number of heads for interpretable mulit-head attention   model: Keras model for TFT', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Sandwich/README.md\n# Sandwich\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py](https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\nmake use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.version==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. ""Temporal fusion transformers for interpretable multi-horizon time series forecasting."" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1529, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.158961057662964,750
What steps do the two handler constructors take to configure the data loader and processing pipelines before calling the base initializer?,"The two handler constructors, Alpha360.__init__ and Alpha158.__init__, take the following steps to configure the data loader and processing pipelines before calling the base initializer:

1.  They call `check_transform_proc` on both `infer_processors` and `learn_processors` with `fit_start_time` and `fit_end_time` to wrap and validate them.
2.  They assemble a dictionary called `data_loader` with the following keys:
    *   `","Both `Alpha360.__init__` and `Alpha158.__init__` do the following before calling `super().__init__`:

1. Wrap and validate their processor lists via  
   ```python
   infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
   learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
   ```
2. Assemble a `data_loader` dict pointing to `""class"": ""QlibDataLoader""` with  
   – `config.feature` from `Alpha360DL.get_feature_config()` (Alpha360) or `self.get_feature_config()` (Alpha158)  
   – `config.label` from `kwargs.pop(""label"", self.get_label_config())`  
   – `filter_pipe`, `freq`, and `inst_processors`  
3. Pass `instruments`, `start_time`, `end_time`, the `data_loader`, and the processed `infer_processors`/`learn_processors` (plus `process_type` in Alpha158) into the base `DataHandlerLP` constructor.","['METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.cast has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.cast.handler\nMotivation  - A user creates a datahandler in his customized package. Then he wants to share the processed handler to   other users without introduce the package dependency and complicated data processing logic. - This class make it possible by casting the class to DataHandlerLP and only keep the processed data  Parameters ---------- handler : DataHandlerLP     A subclass of DataHandlerLP  Returns ------- DataHandlerLP:     the converted processed data', 'METHOD qlib.qlib.rl.data.native.HandlerIntradayProcessedData.__init__ has parameter or accepts argument qlib.qlib.rl.data.native.HandlerIntradayProcessedData.__init__.data_dir', 'CLASS qlib.qlib.data.dataset.handler.DataHandlerLP has method qlib.qlib.data.dataset.handler.DataHandlerLP.cast\nMotivation  - A user creates a datahandler in his customized package. Then he wants to share the processed handler to   other users without introduce the package dependency and complicated data processing logic. - This class make it possible by casting the class to DataHandlerLP and only keep the processed data  Parameters ---------- handler : DataHandlerLP     A subclass of DataHandlerLP  Returns ------- DataHandlerLP:     the converted processed data\nMotivation: - For the case that we hope using different processor workflows for learning and inference;   DataHandler with **(L)earnable (P)rocessor**  This handler will produce three pieces of data in pd.DataFrame format.  - DK_R / self._data: the raw data loaded from the loader - DK_I / self._infer: the data processed for inference - DK_L / self._learn: the data processed for learning model.  The motivation of using different processor workflows for learning and inference Here are some examples.  - The instrument universe for learning and inference may be different. - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).      - These processors only apply to the learning phase.  Tips for data handler  - To reduce the memory cost      - `drop_raw=True`: this will modify the data inplace on raw data;  - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib\'s `Dataset` like ""train"" and ""test""      - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors     - `segments` in Qlib\'s `Dataset` like ""train"" and ""test"" are simply the time segmentations when querying data(""train"" are often before ""test"" in time-series).     - For example, you can query `data._infer` processed by `infer_processors` in the ""train"" time segmentation.', 'METHOD qlib.qlib.rl.data.native.HandlerProcessedDataProvider.__init__ has parameter or accepts argument qlib.qlib.rl.data.native.HandlerProcessedDataProvider.__init__.data_dir', 'MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 6, 'pr': 1535, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.92157816886902,751
How does the 360 handler source its feature configuration differently from the 158 handler?,"The Alpha360 handler sources its feature configuration differently from the 158 handler because it directly pulls features from the `DataLoader` class via `Alpha360DL.get_feature_config()`, whereas the 158 handler calls its instance method `self.get_feature_config()`.","The Alpha360 handler doesn’t use its own `get_feature_config` – it pulls features directly from the DataLoader class via  
  Alpha360DL.get_feature_config()  
whereas Alpha158 calls its instance method  
  self.get_feature_config().","['METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.config has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.config.processor_kwargs\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.', 'MODULE qlib.qlib.contrib.data.handler contains CLASS qlib.qlib.contrib.data.handler.Alpha360', 'CLASS qlib.qlib.data.dataset.handler.DataHandler has method qlib.qlib.data.dataset.handler.DataHandler.config\nconfiguration of data. # what data to be loaded from data source  This method will be used when loading pickled handler from dataset. The data will be initialized with different time range.\nThe motivation of DataHandler:  - It provides an implementation of BaseDataHandler that we implement with:     - Handling responses with an internal loaded DataFrame     - The DataFrame is loaded by a data loader.  The steps to using a handler 1. initialized data handler  (call by `init`). 2. use the data.   The data handler try to maintain a handler with 2 level. `datetime` & `instruments`.  Any order of the index level can be supported (The order will be implied in the data). The order  <`datetime`, `instruments`> will be used when the dataframe index name is missed.  Example of the data: The multi-index of the columns is optional.  .. code-block:: text                              feature                                                            label                             $close     $volume  Ref($close, 1)  Mean($close, 3)  $high-$low  LABEL0     datetime   instrument     2010-01-04 SH600000    81.807068  17145150.0       83.737389        83.016739    2.741058  0.0032                SH600004    13.313329  11800983.0       13.313329        13.317701    0.183632  0.0042                SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289   Tips for improving the performance of datahandler - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`', 'CLASS qlib.qlib.contrib.data.handler.Alpha360 inherits from or is a subclass of data.dataset.handler.DataHandlerLP', 'FUNCTION qlib.qlib.workflow.task.utils.replace_task_handler_with_cache has parameter or accepts argument qlib.qlib.workflow.task.utils.replace_task_handler_with_cache.cache_dir\nReplace the handler in task with a cache handler. It will automatically cache the file and save it in cache_dir.  >>> import qlib >>> qlib.auto_init() >>> import datetime >>> # it is simplified task >>> task = {""dataset"": {""kwargs"":{\'handler\': {\'class\': \'Alpha158\', \'module_path\': \'qlib.contrib.data.handler\', \'kwargs\': {\'start_time\': datetime.date(2008, 1, 1), \'end_time\': datetime.date(2020, 8, 1), \'fit_start_time\': datetime.date(2008, 1, 1), \'fit_end_time\': datetime.date(2014, 12, 31), \'instruments\': \'CSI300\'}}}}} >>> new_task = replace_task_handler_with_cache(task) >>> print(new_task) {\'dataset\': {\'kwargs\': {\'handler\': \'file...Alpha158.3584f5f8b4.pkl\'}}}', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 9, 'pr': 1537, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",38.79721236228943,752
"In HFSignalRecord.__init__, what happens to any extra keyword arguments passed in—are they forwarded to the parent initializer or dropped?","In `HFSignalRecord.__init__` in qlib, any extra keyword arguments passed in are dropped. This is because the method calls `super().__init__(recorder=recorder)` without forwarding any additional keyword arguments to the parent initializer.","In HFSignalRecord.__init__ (qlib/workflow/record_temp.py), you accept **kwargs but only call  
```python
super().__init__(recorder=recorder)
```  
with no kwargs passed along. Any extra keyword args are simply dropped.","[""CLASS qlib.qlib.utils.serial.Serializable has method qlib.qlib.utils.serial.Serializable.to_pickle\nDump self to a pickle file.  path (Union[Path, str]): the path to dump  kwargs may include following keys      dump_all : bool         will the object dump all object     exclude : list         What attribute will not be dumped     include : list         What attribute will be dumped\nSerializable will change the behaviors of pickle.      The rule to tell if a attribute will be kept or dropped when dumping.     The rule with higher priorities is on the top     - in the config attribute list -> always dropped     - in the include attribute list -> always kept     - in the exclude attribute list -> always dropped     - name not starts with `_` -> kept     - name starts with `_` -> kept if `dump_all` is true else dropped  It provides a syntactic sugar for distinguish the attributes which user doesn't want. - For examples, a learnable Datahandler just wants to save the parameters without data when dumping to disk"", 'METHOD qlib.qlib.workflow.record_temp.HFSignalRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.HFSignalRecord.__init__.recorder', ""CLASS qlib.qlib.utils.serial.Serializable has method qlib.qlib.utils.serial.Serializable.config\nconfigure the serializable object  Parameters ---------- kwargs may include following keys      dump_all : bool         will the object dump all object     exclude : list         What attribute will not be dumped     include : list         What attribute will be dumped  recursive : bool     will the configuration be recursive\nSerializable will change the behaviors of pickle.      The rule to tell if a attribute will be kept or dropped when dumping.     The rule with higher priorities is on the top     - in the config attribute list -> always dropped     - in the include attribute list -> always kept     - in the exclude attribute list -> always dropped     - name not starts with `_` -> kept     - name starts with `_` -> kept if `dump_all` is true else dropped  It provides a syntactic sugar for distinguish the attributes which user doesn't want. - For examples, a learnable Datahandler just wants to save the parameters without data when dumping to disk"", 'METHOD qlib.qlib.workflow.record_temp.SigAnaRecord.__init__ has parameter or accepts argument qlib.qlib.workflow.record_temp.SigAnaRecord.__init__.ana_long_short', ""CLASS keras.keras.src.wrappers.sklearn_wrapper.SKLBase has method keras.keras.src.wrappers.sklearn_wrapper.SKLBase.__init__\nBase class for scikit-learn wrappers.  Note that there are sources of randomness in model initialization and training. Refer to [Reproducibility in Keras Models]( https://keras.io/examples/keras_recipes/reproducibility_recipes/) on how to control randomness.  Args:     model: `Model`.         An instance of `Model`, or a callable returning such an object.         Note that if input is a `Model`, it will be cloned using         `keras.models.clone_model` before being fitted, unless         `warm_start=True`.         The `Model` instance needs to be passed as already compiled.         If callable, it must accept at least `X` and `y` as keyword         arguments. Other arguments must be accepted if passed as         `model_kwargs` by the user.     warm_start: bool, defaults to `False`.         Whether to reuse the model weights from the previous fit. If `True`,         the given model won't be cloned and the weights from the previous         fit will be reused.     model_kwargs: dict, defaults to `None`.         Keyword arguments passed to `model`, if `model` is callable.     fit_kwargs: dict, defaults to `None`.         Keyword arguments passed to `model.fit`. These can also be passed         directly to the `fit` method of the scikit-learn wrapper. The         values passed directly to the `fit` method take precedence over         these.  Attributes:     model_ : `Model`         The fitted model.     history_ : dict         The history of the fit, returned by `model.fit`."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 7, 'pr': 1542, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.068241834640503,753
"What high-level steps are executed when generating a portfolio analysis record, from loading predictions through backtesting to logging and saving artifacts?","The high-level steps executed when generating a portfolio analysis record, from loading predictions through backtesting to logging and saving artifacts in qlib are:

1. Loading saved predictions via self.load(""pred.pkl"").
2. Filling “<PRED>” placeholders in executor_config.
3. Filling “<PRED>” placeholders in strategy_config.
4. Inferring backtest start and end times from the prediction index if those times are not explicitly set.
5. Calling normal_backtest with executor_config, strategy_config, and backtest_config.
6. Looping over portfolio_metric_dict to stash report_normal_*.pkl artifacts.
7. Looping over portfolio_metric_dict to stash positions_normal_*.pkl artifacts.
8. Looping over indicator_dict to stash indicators_normal_*.pkl artifacts.
9. Looping over indicator_dict to stash indicators_normal_*_obj.pkl artifacts.
10. For each freq in risk_analysis_freq, computing excess returns with and without cost using risk_analysis.
11. For each freq in risk_analysis_freq, logging flattened risk metrics via self.recorder.log_metrics.
12. For each freq in risk_analysis_freq, saving port_analysis_{freq}.pkl.
13. For each freq in risk_analysis_freq, printing summaries.
14. For each freq in indicator_analysis_freq, computing indicator analysis via indicator_analysis.
15. For each freq in indicator_analysis_freq, logging metrics.
16. For each freq in indicator_analysis_freq, saving indicator_analysis_{freq}.pkl.
17. For each freq in indicator_analysis_freq, printing the table.
18. Returning a dictionary of all artifact objects.","In PortAnaRecord._generate (qlib/workflow/record_temp.py) the high-level flow is:

1. Load the saved predictions via self.load(""pred.pkl"").  
2. Fill any “<PRED>” placeholders in executor_config and strategy_config.  
3. Infer backtest start/end times from the prediction index if not explicitly set.  
4. Call normal_backtest(...) with executor_config, strategy_config and backtest_config to get  
   • portfolio_metric_dict (reports + positions)  
   • indicator_dict (indicator series + objects)  
5. Loop over portfolio_metric_dict to stash report_normal_*.pkl and positions_normal_*.pkl artifacts.  
6. Loop over indicator_dict to stash indicators_normal_*.pkl and indicators_normal_*_obj.pkl artifacts.  
7. For each freq in risk_analysis_freq:  
   – Compute excess_return_with/without cost via risk_analysis(...)  
   – Log flattened risk‐metrics via self.recorder.log_metrics  
   – Save port_analysis_{freq}.pkl and print summaries  
8. For each freq in indicator_analysis_freq:  
   – Compute indicator_analysis(...)  
   – Log metrics, save indicator_analysis_{freq}.pkl and print the table  
9. Return the dict of all artifact objects.","['CLASS qlib.qlib.workflow.record_temp.PortAnaRecord inherits from or is a subclass of qlib.qlib.workflow.record_temp.ACRecordTemp\nAutomatically checking record template\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""CLASS qlib.qlib.backtest.report.PortfolioMetrics has method qlib.qlib.backtest.report.PortfolioMetrics.generate_portfolio_metrics_dataframe\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""MODULE qlib.qlib.backtest.report contains CLASS qlib.qlib.backtest.report.PortfolioMetrics\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 1546, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",53.84915065765381,754
What sequence of operations does invoking qlib_data trigger to download and extract a dataset?,"Invoking `qlib_data` triggers the following sequence of operations to download and extract a dataset in qlib:

1. The `_get_file_name_with_version` function builds a versioned file name using name, region, interval, and qlib.__version__.
2. `check_dataset()` is called to check if the dataset exists. If it returns 404, the file name is rebuilt with ""latest"".
3. `download_data(file_name, target_dir, delete_old=True)` is called to download the dataset. This function:

* Expands or creates the target directory.
* Prepends a timestamp to the zip file’s basename to form target_path.
* Calls `merge_remote_url(file_name)` to obtain the full URL.
* Calls `download(url, target_path)`, which performs an HTTP-GET and streams the response into target_path with a tqdm progress bar.
* Calls `_unzip(target_path, target_dir, delete_old)` to extract the dataset. If delete_old is True, it removes old subdirectories such as features and calendars, prompting the user.

All these methods are defined in qlib/tests/data.py under class GetData.","Calling GetData().qlib_data(...) does the following in order:

1. In qlib_data():
   - Build a versioned file name via the inner _get_file_name_with_version (using your `name`, `region`, `interval` and `qlib.__version__`), then call check_dataset(); if it returns 404 you rebuild the name with `""latest""`.
2. download_data(file_name, target_dir, delete_old=True):
   - Expand/mkdir target_dir.
   - Prepend a timestamp to the zip’s basename to form target_path.
   - Call merge_remote_url(file_name) to get the full URL.
   - download(url, target_path): HTTP‐GET + stream into target_path with a tqdm progress bar.
   - _unzip(target_path, target_dir, delete_old):
     • If delete_old, call _delete_qlib_data(target_dir) to remove old subdirs (`features`, `calendars`, etc.), prompting the user.
     • Open the ZIP and extract all entries into target_dir.
   - If self.delete_zip_file=True, remove the downloaded ZIP.

All these methods live in qlib/tests/data.py under class GetData.","['CLASS qlib.scripts.data_collector.baostock_5min.collector.Run has method qlib.scripts.data_collector.baostock_5min.collector.Run.normalize_data\nnormalize data  Attention --------- qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;      qlib_data_1d can be obtained like this:         $ python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3     or:         download 1d data, reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#1d-from-yahoo  Examples ---------     $ python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', 'METHOD qlib.scripts.data_collector.baostock_5min.collector.Run.normalize_data has parameter or accepts argument qlib.scripts.data_collector.baostock_5min.collector.Run.normalize_data.date_field_name\nnormalize data  Attention --------- qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;      qlib_data_1d can be obtained like this:         $ python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3     or:         download 1d data, reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#1d-from-yahoo  Examples ---------     $ python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerABC.DK_I', 'METHOD qlib.scripts.data_collector.baostock_5min.collector.Run.normalize_data has parameter or accepts argument qlib.scripts.data_collector.baostock_5min.collector.Run.normalize_data.symbol_field_name\nnormalize data  Attention --------- qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;      qlib_data_1d can be obtained like this:         $ python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3     or:         download 1d data, reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#1d-from-yahoo  Examples ---------     $ python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 7, 'pr': 1558, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",56.810648918151855,755
What outputs does the initial signal recorder generate for model predictions and which of these does the analysis recorder load?,"The initial signal recorder generates two files: 'pred.pkl' containing model prediction scores and 'label.pkl' containing raw labels when using a DatasetH. The analysis recorder loads these files, specifically calling `load(","In qlib/workflow/record_temp.py:

• SignalRecord.generate() saves  
  – “pred.pkl” (the model’s prediction scores)  
  – “label.pkl” (the raw labels, if using a DatasetH)  

• HFSignalRecord.generate() then calls  
  – load(""pred.pkl"")  
  – load(""label.pkl"")","['CLASS qlib.qlib.workflow.record_temp.MultiPassPortAnaRecord inherits from or is a subclass of qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`\nThis is the Multiple Pass Portfolio Analysis Record class that run backtest multiple times and generates the analysis results such as those of backtest. This class inherits the ``PortAnaRecord`` class.  If shuffle_init_score enabled, the prediction score of the first backtest date will be shuffled, so that initial position will be random. The shuffle_init_score will only works when the signal is used as <PRED> placeholder. The placeholder will be replaced by pred.pkl saved in recorder.  Parameters ---------- recorder : Recorder     The recorder used to save the backtest results. pass_num : int     The number of backtest passes. shuffle_init_score : bool     Whether to shuffle the prediction score of the first backtest date.', 'CLASS qlib.qlib.contrib.workflow.record_temp.MultiSegRecord has method qlib.qlib.contrib.workflow.record_temp.MultiSegRecord.generate\nThis is the multiple segments signal record class that generates the signal prediction. This class inherits the ``RecordTemp`` class.', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'CLASS qlib.qlib.workflow.recorder.Recorder has method qlib.qlib.workflow.recorder.Recorder.load_object\nLoad objects such as prediction file or model checkpoints.  Parameters ---------- name : str     name of the file to be loaded.  Returns ------- The saved object.\nThis is the `Recorder` class for logging the experiments. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)  The status of the recorder can be SCHEDULED, RUNNING, FINISHED, FAILED.', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 8, 'pr': 1568, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",44.10151529312134,756
What is the sequence of method calls from constructing the download URL to unzipping the retrieved dataset?,"The sequence of method calls from constructing the download URL to unzipping the retrieved dataset in qlib is as follows:

1. `GetData.merge_remote_url(file_name)` - This method builds the download URL.
2. `GetData.download(url, target_path)` - This method streams the ZIP file to disk.
3. `GetData._unzip(target_path, target_dir, delete_old)` - This method opens the ZIP file using `zipfile.ZipFile` and extracts each file from the ZIP archive.

Note: The `delete_old` parameter is set to `True` by default in the provided code snippet, which means that if this option is enabled, the existing data in the target directory will be deleted before unzipping the new dataset.","When you call GetData.download_data, it does the following in order:

1. GetData.merge_remote_url(file_name) – builds the download URL  
2. GetData.download(url, target_path) – streams the ZIP to disk  
3. GetData._unzip(target_path, target_dir, delete_old) – if delete_old=True, calls GetData._delete_qlib_data(target_dir), then opens the ZIP with zipfile.ZipFile and extracts each file.","['CLASS qlib.qlib.tests.data.GetData has method qlib.qlib.tests.data.GetData.download\nDownload a file from the specified url.  Parameters ---------- url: str     The url of the data. target_path: str     The location where the data is saved, including the file name.', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.load_dataset\nload dataset from multiple file formats', 'METHOD qlib.qlib.tests.data.GetData.download has parameter or accepts argument qlib.qlib.tests.data.GetData.download.target_path\nDownload a file from the specified url.  Parameters ---------- url: str     The url of the data. target_path: str     The location where the data is saved, including the file name.', 'METHOD qlib.qlib.tests.data.GetData.download has parameter or accepts argument qlib.qlib.tests.data.GetData.download.url\nDownload a file from the specified url.  Parameters ---------- url: str     The url of the data. target_path: str     The location where the data is saved, including the file name.', 'FUNCTION qlib.qlib.utils.__init__.load_dataset has parameter or accepts argument qlib.qlib.utils.__init__.load_dataset.path_or_obj\nload dataset from multiple file formats', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1577, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",39.973681688308716,757
What check ensures each subclass is only appended once in the result of find_all_classes?,"The check that ensures each subclass is only appended once in the result of `find_all_classes` in qlib is the `if` guard in the `_append_cls` function, which includes the condition `and cls not in cls_list`. This prevents duplicate subclasses from being added to the `cls_list`. The implementation of this guard ensures that only unique classes are collected and stored in the result.","In the nested `_append_cls` in qlib/utils/mod.py, the `if` guard includes

```python
… and cls not in cls_list
```

so each subclass is only added once to `cls_list`.","['MODULE qlib.qlib.utils.mod contains FUNCTION qlib.qlib.utils.mod.find_all_classes\nFind all the classes recursively that inherit from `cls` in a given module. - `cls` itself is also included      >>> from qlib.data.dataset.handler import DataHandler     >>> find_all_classes(""qlib.contrib.data.handler"", DataHandler)     [<class \'qlib.contrib.data.handler.Alpha158\'>, <class \'qlib.contrib.data.handler.Alpha158vwap\'>, <class \'qlib.contrib.data.handler.Alpha360\'>, <class \'qlib.contrib.data.handler.Alpha360vwap\'>, <class \'qlib.data.dataset.handler.DataHandlerLP\'>]  TODO: - skip import error', 'FUNCTION qlib.qlib.utils.mod.find_all_classes has parameter or accepts argument qlib.qlib.utils.mod.find_all_classes.module_path\nFind all the classes recursively that inherit from `cls` in a given module. - `cls` itself is also included      >>> from qlib.data.dataset.handler import DataHandler     >>> find_all_classes(""qlib.contrib.data.handler"", DataHandler)     [<class \'qlib.contrib.data.handler.Alpha158\'>, <class \'qlib.contrib.data.handler.Alpha158vwap\'>, <class \'qlib.contrib.data.handler.Alpha360\'>, <class \'qlib.contrib.data.handler.Alpha360vwap\'>, <class \'qlib.data.dataset.handler.DataHandlerLP\'>]  TODO: - skip import error', 'FUNCTION qlib.qlib.utils.mod.find_all_classes has parameter or accepts argument qlib.qlib.utils.mod.find_all_classes.cls\nFind all the classes recursively that inherit from `cls` in a given module. - `cls` itself is also included      >>> from qlib.data.dataset.handler import DataHandler     >>> find_all_classes(""qlib.contrib.data.handler"", DataHandler)     [<class \'qlib.contrib.data.handler.Alpha158\'>, <class \'qlib.contrib.data.handler.Alpha158vwap\'>, <class \'qlib.contrib.data.handler.Alpha360\'>, <class \'qlib.contrib.data.handler.Alpha360vwap\'>, <class \'qlib.data.dataset.handler.DataHandlerLP\'>]  TODO: - skip import error', 'MODULE qlib.qlib.__init__ contains FUNCTION qlib.qlib.__init__.get_project_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'CLASS qlib.qlib.data.pit.PRef inherits from or is a subclass of qlib.qlib.data.pit.P', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1601, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.3271381855011,758
Which methods are used to retrieve new versus historical components of US market indices?,The methods used to retrieve new versus historical components of US market indices are **`WIKIIndex._request_new_companies` for fetching new components** and **`NASDAQ100Index._request_history_companies` for fetching historical components**. These methods send a POST request to `HISTORY_COMPANIES_URL` to fetch the required information.,"In scripts/data_collector/us_index/collector.py:

• New components are fetched via WIKIIndex._request_new_companies  
• Historical components are fetched via NASDAQ100Index._request_history_companies (POST to HISTORY_COMPANIES_URL)","['CLASS keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyTest has method keras.keras.src.dtype_policies.dtype_policy_test.DTypePolicyTest.test_get_config_from_config\nTest get_config and from_config methods.\nTest `DTypePolicy`.  In the tests, we also test `DTypePolicy` for historical reasons.', 'METHOD qlib.scripts.data_collector.br_index.collector.IBOVIndex.get_changes has parameter or accepts argument qlib.scripts.data_collector.br_index.collector.quarter_dict\nAccess the index historic composition and compare it quarter by quarter and year by year in order to generate a file that keeps track of which stocks have been removed and which have been added.  The Dataframe used as reference will provided the index composition for each year an quarter: pd.DataFrame:     symbol     SH600000     SH600001     .     .     .  Parameters ---------- self: is used to represent the instance of the class.  Returns ---------- pd.DataFrame:     symbol      date        type     SH600000  2019-11-11    add     SH600001  2020-11-10    remove     dtypes:         symbol: str         date: pd.Timestamp         type: str, value from [""add"", ""remove""]', 'MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.scripts.data_collector.us_index.collector.NASDAQ100Index has method qlib.scripts.data_collector.us_index.collector.NASDAQ100Index.get_history_companies', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md\n# HIST\n* Code: [https://github.com/Wentao-Xu/HIST](https://github.com/Wentao-Xu/HIST)\n* Paper: [HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared InformationAdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/abs/2110.13716).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md\n# ADD\n* Paper: [ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting](https://arxiv.org/abs/2012.06289).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 41}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 41, 'n_files_pr': 17, 'pr': 1641, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",30.559187412261963,759
How does the calendar merge method restrict calendar dates to the range of the source DataFrame before reindexing?,"The calendar merge method in qlib restricts calendar dates to the range of the source DataFrame before reindexing by filtering calendars_df to only those dates between `df[self.date_field_name].min()` and `df[self.date_field_name].max()`. This is done after building the DataFrame named `calendars_df`.

Here's a code snippet from `DumpDataBase.data_merge_calendar` that shows how it restricts calendar dates:

```python
calendars_list = []
for market in self.markets:
    calendars_df = self.get_calendars(market)
    # Filter calendars to only those dates between df[self.date_field_name].min() and df[self.date_field_name].max()
    calendars_df = calendars_df[(calendars_df[self.date_field_name] >= calendars_df[self.date_field_name].min()) & (calendars_df[self.date_field_name] <= calendars_df[self.date_field_name].max())]
    # Set the index on calendars_df
    calendars_df.set_index(self.date_field_name, inplace=True)
    # Reindex calendars_df
    calendars_df.reindex(df.index)
    calendars_list.append(calendars_df)
```
This code first filters the calendars to only include dates within the range of the source DataFrame. It then sets the index on the filtered calendars and reindexes them based on the source DataFrame's index.",In DumpDataBase.data_merge_calendar (scripts/dump_bin.py) the code first builds a `calendars_df` then immediately filters it to only those dates between `df[self.date_field_name].min()` and `df[self.date_field_name].max()` before setting the index and calling `df.reindex(...)`.,"['CLASS qlib.qlib.data.data.CalendarProvider has method qlib.qlib.data.data.CalendarProvider.calendar\nGet calendar of certain market in given time range.  Parameters ---------- start_time : str     start of the time range. end_time : str     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day. future : bool     whether including future trading day.  Returns ---------- list     calendar list\nCalendar provider base class  Provide calendar data.', 'METHOD qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar.df', 'CLASS qlib.qlib.backtest.utils.TradeCalendarManager has method qlib.qlib.backtest.utils.TradeCalendarManager.__init__\nParameters ---------- freq : str     frequency of trading calendar, also trade time per trading step start_time : Union[str, pd.Timestamp], optional     closed start of the trading calendar, by default None     If `start_time` is None, it must be reset before trading. end_time : Union[str, pd.Timestamp], optional     closed end of the trade time range, by default None     If `end_time` is None, it must be reset before trading.\nManager for trading calendar     - BaseStrategy and BaseExecutor will use it', 'METHOD qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.data_merge_calendar.calendars_list', ""CLASS qlib.qlib.data.data.LocalDatasetProvider has method qlib.qlib.data.data.LocalDatasetProvider.__init__\nParameters ---------- align_time : bool     Will we align the time to calendar     the frequency is flexible in some dataset and can't be aligned.     For the data with fixed frequency with a shared calendar, the align data to the calendar will provides following benefits      - Align queries to the same parameters, so the cache can be shared.\nLocal dataset data provider class  Provide dataset data from local data source."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1656, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",48.354257583618164,760
What are the two different mechanisms employed in the RL logger tests to capture and verify log output?,The two different mechanisms employed in the RL logger tests to capture and verify log output are **CsvWriter** and **LogCollector**. These mechanisms allow the tests to dump metrics to `.output/result.csv` and then load them with `pd.read_csv` for assertion on columns and values. This ensures that the logger is working correctly.,"The two mechanisms are:

1. In test_simple_env_logger you use pytest’s caplog fixture (after doing logging.config.dictConfig(C.logging_config) and running through ConsoleWriter) and then regex-match lines in caplog.text.

2. In both tests (and especially in test_logger_with_env_wrapper) you use CsvWriter (and in the wrapper test a LogCollector(LogLevel.DEBUG)) to dump metrics to “.output/result.csv” and then load it with pd.read_csv to assert on its columns and values.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.log contains CLASS qlib.qlib.log.QlibLogger\nCustomized logger for Qlib.', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.rl.utils.log contains CLASS qlib.qlib.rl.utils.log.PickleWriter\nDump logs to pickle files.', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR\'s for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/csi300/csi500/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/SH000300/SH000905/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332±0.00 | 0.3044±0.00 | 0.0462±0.00 | 0.4326±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n| MLP        | Alpha158 | 0.0229±0.01 | 0.2181±0.05 | 0.0360±0.00 | 0.3409±0.02 | 0.0043±0.02       | 0.0602±0.27       | -0.2184±0.04 |\n| LightGBM   | Alpha158 | 0.0399±0.00 | 0.4065±0.00 | 0.0482±0.00 | 0.5101±0.00 | 0.1284±0.00       | 1.5650±0.00       | -0.0635±0.00 |\n| CatBoost   | Alpha158 | 0.0345±0.00 | 0.2855±0.00 | 0.0417±0.00 | 0.3740±0.00 | 0.0496±0.00       | 0.5977±0.00       | -0.1496±0.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380±0.00 | 0.3659±0.00 | 0.0442±0.00 | 0.4324±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258±0.00 | 0.2021±0.02 | 0.0426±0.00 | 0.3840±0.02 | 0.0022±0.02       | 0.0301±0.26       | -0.2064±0.02 |\n| LightGBM   | Alpha360 | 0.0400±0.00 | 0.3605±0.00 | 0.0536±0.00 | 0.5431±0.00 | 0.0505±0.00       | 0.7658±0.02       | -0.1880±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1661, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",32.55267906188965,761
What is the flow for checking existing cache entries and storing new responses when caching is enabled in the LLM client?,"The flow for checking existing cache entries and storing new responses when caching is enabled in the LLM client involves the following steps:

1.  In the `__init__` method, if `cache=True` is passed, `self.cache_dir` is set to a Cache under `DEFAULT_CACHE_DIR`.
2.  In the `generate_response` method, if `cache_enabled` is true, a key is computed via `_get_cache_key(messages)`.
3.  If there's a cache hit, `generate_response` returns the cached dict.
4.  On a cache miss, `generate_response` cleans each message.content via `_clean_input`.
5.  On a cache miss, `generate_response` calls `_generate_response_with_retry(...)` which wraps the provider’s `_generate_response` method.
6.  After receiving the response on a cache miss, `generate_response` recomputes the same cache key.
7.  After receiving the response on a cache miss, `generate_response` calls `self.cache_dir.set(key, response)`.
8.  After receiving the response on a cache miss, `generate_response` returns the freshly fetched response.","In LLMClient (graphiti_core/llm_client/client.py) the flow is:

1. In __init__, if you passed cache=True, `self.cache_dir` is set to a Cache under DEFAULT_CACHE_DIR.  
2. In `generate_response(...)`:
   a) If `cache_enabled`, compute a key via `_get_cache_key(messages)` and call `self.cache_dir.get(key)`.  
      – On hit, return the cached dict.  
   b) Otherwise (cache miss):
      - Clean each `message.content` via `_clean_input`.  
      - Call `_generate_response_with_retry(...)`, which wraps the provider’s `_generate_response`.  
      - After you get the response, recompute the same cache key and call `self.cache_dir.set(key, response)`.  
      - Return the freshly fetched response.","['METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.cache\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.llm_client.client.LLMClient has field or attribute or property graphiti.graphiti_core.llm_client.client.cache', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.cache\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.llm_client.client.LLMClient has field or attribute or property graphiti.graphiti_core.llm_client.client.LLMClient.cache_enabled', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.client\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 11, 'n_files_pr': 21, 'pr': 39, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",46.10656809806824,762
How does the bulk ingestion method differ from the single-episode ingestion path regarding edge invalidation and date extraction?,"The bulk ingestion method in graphiti differs from the single-episode ingestion path regarding edge invalidation and date extraction as follows:

*   Edge invalidation: The add_episode function collects invalidated edges, while the add_episode_bulk method does not perform edge invalidation.
*   Date extraction: The extract_attributes_from_nodes function includes date-field extraction, which is part of the single-episode ingestion path. However, this step is not performed in the bulk ingestion method.

Therefore, the bulk ingestion method does not include edge invalidation and date extraction steps, whereas these steps are included in the single-episode ingestion path.","In graphiti_core/graphiti.py the two paths diverge precisely on those steps:

• add_episode (single) runs resolve_extracted_edges, collects its invalidated_edges and invokes extract_attributes_from_nodes (which includes date‐field extraction) before writing to Neo4j.  
• add_episode_bulk skips both edge invalidation and any date‐extraction logic entirely (“Important: This method does not perform edge invalidation or date extraction steps.”).","['CLASS graphiti.graphiti_core.nodes.EpisodeType inherits from or is a subclass of enum.Enum\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.extracted_edges_bulk', 'CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_env\nCreate a configuration instance from environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/tests/evals/data/longmemeval_data/README.md\nThe `longmemeval_oracle` dataset is an open-source dataset that we are using.\nWe did not create this dataset and it can be found\nhere: https://huggingface.co/datasets/xiaowu0162/longmemeval/blob/main/longmemeval_oracle.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 5, 'pr': 40, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",35.20010852813721,763
How does the Graphiti.search method choose the edge reranking configuration when a center node identifier is passed?,"Based on the Graphiti.search method documentation, when a center node identifier is passed, the search_config is set to EDGE_HYBRID_SEARCH_NODE_DISTANCE. This means that the edge reranking configuration will use a hybrid approach that takes into account both the distance between nodes and other factors.","In Graphiti.search (graphiti_core/graphiti.py) you’ll see:  
```
search_config = (EDGE_HYBRID_SEARCH_RRF 
                   if center_node_uuid is None 
                   else EDGE_HYBRID_SEARCH_NODE_DISTANCE)
```
So as soon as you pass a center_node_uuid, it swaps out the default RRF config for EDGE_HYBRID_SEARCH_NODE_DISTANCE, and then uses that to rerank edges.","['CLASS cognee.cognee.tasks.graph.models.OntologyEdge inherits from or is a subclass of pydantic.BaseModel\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_edges', 'MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_nodes', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 45, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",28.79779839515686,764
How does extract_nodes determine which prompt to use for text episodes?,"The extract_nodes function determines which prompt to use for text episodes by checking the value of `episode.source`. If it equals `EpisodeType.text`, it calls `llm_client.generate_response` with a prompt that uses the `extract_text` function from the `prompt_library` as its argument. The `extract_text` function is used as the prompt template for text episodes.","In extract_nodes (graphiti_core/utils/maintenance/node_operations.py) there’s a source‐type dispatch on episode.source. If it equals EpisodeType.text, it calls:

```python
llm_client.generate_response(
    prompt_library.extract_nodes.extract_text(context),
    response_model=ExtractedEntities
)
```

i.e. it uses the extract_text prompt template for text episodes.","['CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains FUNCTION graphiti.graphiti_core.prompts.extract_nodes.extract_text', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains CLASS graphiti.graphiti_core.prompts.extract_nodes.Prompt', 'MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 46, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.436857223510742,765
Which attribute of the returned EntityEdge objects does the integration test extract for logging?,The integration test extracts the 'attributes' attribute of the returned EntityEdge objects for logging in graphiti.,The integration test uses the helper search_results_to_context_string (in tests/) which pulls out each EntityEdge’s context attribute and concatenates those for logging.,"['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.attributes', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.edge_data', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 48, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.6390643119812,766
What sequence of operations does Graphiti perform to generate node results for a textual query?,"The sequence of operations that Graphiti performs to generate node results for a textual query is as follows:

1.  The raw text query is passed to the `hybrid_node_search` function in the `graphiti_core/search/search_utils.py` module.
2.  Within `hybrid_node_search`, the raw text query is sent to the `node_fulltext_search` function, and the query embedding is passed to the `node_similarity_search` function.
3.  Both `node_fulltext_search` and `node_similarity_search` are invoked in parallel using a semaphore (`semaphore_gather`).
4.  The results from both searches are merged and deduplicated into a UUID map.
5.  Reciprocal rank fusion reranking is applied to the per-query and per-embedding result lists using the `rrf` function.
6.  The final ranked UUIDs are mapped back to EntityNode instances.

This sequence of operations allows Graphiti to generate node results for a textual query by combining both semantic similarity-based search and traditional full-text search, with the added benefit of reciprocal rank fusion reranking to improve the accuracy of the results.","When you issue a text‐only node search, Graphiti does roughly the following:

1. Uses your EmbedderClient (e.g. `OpenAIEmbedder`) to turn the query string into an embedding.  
2. Calls `hybrid_node_search` (in `graphiti_core/search/search_utils.py`), passing both
   – the raw text query to `node_fulltext_search(driver, q, …)`, and  
   – its embedding to `node_similarity_search(driver, e, …)`, in parallel via `semaphore_gather`.  
3. Merges and deduplicates all returned `EntityNode` objects into a UUID map.  
4. Applies RRF reranking (`rrf(...)`) over the per‐query/per‐embedding result lists.  
5. Maps the final ranked UUIDs back to `EntityNode` instances and returns them.","['CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_node_save_bulk_query', 'CLASS qlib.qlib.utils.index_data.LocIndexer has method qlib.qlib.utils.index_data.LocIndexer.__getitem__\nParameters ---------- indexing :     query for data  Raises ------ KeyError:     If the non-slice index is queried but does not exist, `KeyError` is raised.\n`Indexer` will behave like the `LocIndexer` in Pandas  Read-only operations has higher priorities than others. So this class is designed in a read-only way to shared data for queries. Modifications will results in new Index.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 8, 'pr': 49, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",43.02216696739197,767
How does the system's hybrid node search integrate fulltext and vector similarity queries before producing the final ranked list?,"The system's hybrid node search integrates full-text and vector similarity queries by executing them in parallel via `semaphore_gather`. The results from each search are collected, deduplicated, and fed into a reciprocal-rank fusion reranker (`rrf()`). Finally, the nodes are looked up in the merged UUID order produced by `rrf()`, and the final ranked list is returned. This process allows for efficient combination of different query types to produce accurate results.","In graphiti_core/search/search_utils.py the hybrid_node_search function does the following:

1. For each text query it calls node_fulltext_search(driver, q, …, 2*limit), and for each embedding it calls node_similarity_search(driver, e, …, 2*limit), all in parallel via semaphore_gather.  
2. It collects all returned EntityNode lists, builds a dict uuid→node to dedupe.  
3. It extracts the per-search lists of UUIDs and feeds them into the rrf() (reciprocal-rank fusion) reranker.  
4. Finally it looks up the nodes in rrf’s merged UUID order and returns that as the final ranked list.","['MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 13}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 13, 'n_files_pr': 7, 'pr': 51, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.90187072753906,768
Which utility operations are invoked to perform node and edge deduplication during bulk episode processing?,"**Node Deduplication Operations:**
* `create_entity_node_embeddings`
* `resolve_extracted_nodes`
* `compress_uuid_map` are invoked by `dedupe_nodes_bulk` to perform node deduplication.

**Edge Deduplication Operations:**
* `create_entity_edge_embeddings`
* `resolve_extracted_edge`
* `compress_uuid_map` are invoked by `dedupe_edges_bulk` to perform edge deduplication.","During add_episode_bulk, node and edge deduplication are handled by the bulk utilities in graphiti_core/utils/bulk_utils.py:

• dedupe_nodes_bulk (calls create_entity_node_embeddings, resolve_extracted_nodes, compress_uuid_map)  
• dedupe_edges_bulk (calls create_entity_edge_embeddings, resolve_extracted_edge, compress_uuid_map)","['CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.reopen\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'MODULE cognee.cognee.infrastructure.databases.graph.kuzu.adapter contains CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.nodes_by_episode', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.close\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 10, 'n_files_pr': 8, 'pr': 67, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.564148902893066,769
What are the main steps performed by the node distance reranker to produce a sorted list of node UUIDs based on proximity and score filtering?,"The main steps performed by the node distance reranker in graphiti to produce a sorted list of node UUIDs based on proximity and score filtering are:

1. Remove the center UUID from the candidate list.
2. Initialize a score map with the center UUID mapped to 0.0.
3. Run a Cypher query with MATCH …–[:RELATES_TO]–… to assign a distance score of 1 to directly connected nodes.
4. Assign infinity to nodes not returned by the Cypher query.
5. Sort the remaining UUIDs by ascending score.
6. If the center UUID was originally in the input, prepend it with a score of 0.1.
7. Apply the min_score filter by keeping only UUIDs where 1/score is greater than or equal to min_score.","In graphiti_core/search/search_utils.py the async node_distance_reranker does roughly the following:

1. Remove the center UUID from the candidate list.  
2. Initialize a score map with center→0.0.  
3. Run a Cypher query (MATCH …–[:RELATES_TO]–…) to get a “distance” score (always 1) for directly connected nodes.  
4. Assign ∞ to any nodes not returned by the query.  
5. Sort the remaining UUIDs by ascending score (i.e. closest first).  
6. If the center was originally in the input, prepend it back with a small score (0.1).  
7. Apply the min_score filter by keeping only UUIDs where (1/score) ≥ min_score.  

This yields a proximity-sorted, score-filtered list of node UUIDs.","[""CLASS keras.keras.src.ops.node.Node has method keras.keras.src.ops.node.Node.parent_nodes\nThe parent `Node`s.  Returns:     all the `Node`s whose output this node immediately depends on.\nA `Node` describes an operation `__call__()` event.  A Keras Function is a DAG with `Node` instances as nodes, and `KerasTensor` instances as edges. Nodes aren't `Operation` instances, because a single operation could be called multiple times, which would result in graph cycles.  A `__call__()` event involves input tensors (and other input arguments), the operation that was called, and the resulting output tensors. A `Node` will include all this information.  Since a single `Operation` could be called multiple times, the `Node` instances are stored on operations as a list. Each time an operation is called, a node is added to `op._inbound_nodes`. Each time the output of an operation is used by another operation, a node is added to `op._outbound_nodes`.  Every `KerasTensor` instance has a `KerasHistory` object attached, which tracks the `Node` that records the `__call__()` event that created the tensor. By recursively walking through `Node` instances via the `KerasHistory` metadata of `KerasTensor` instances, once can retrieve the entire DAG of a Keras Function.  Args:     operation: The Operation that was called in the `op.__call__()`         event that this node represents.     call_args: The positional arguments the operation was called with.     call_kwargs: The keyword arguments the operation was called with.     outputs: The output tensors of the `op.__call__()` call."", 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_node_uuids', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_nodes', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 72, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",40.9822838306427,770
"How do the bulk utility functions chain retrieval, extraction, and deduplication to process multiple episodes into final node and edge mappings?","The bulk utility functions in Graphiti chain retrieval, extraction, and deduplication as follows: **retrieve_previous_episodes_bulk** fetches the last N episodes for each EpisodicNode in the input list. **extract_nodes_and_edges_bulk** takes episode tuples, GraphitiClients, and edge_type_map as inputs, running extract_nodes(...) and extract_edges(...) in parallel via semaphore_gather. It returns two nested lists: extracted_nodes_bulk and extracted_edges_bulk. **dedupe_nodes_bulk** embeds all extracted EntityNode objects, finds cross-episode candidates by word overlap or embedding similarity, calls resolve_extracted_nodes(...) to pick or merge duplicates, builds a compressed_map of UUIDs, and returns nodes_by_episode, a dict mapping each episode.uuid to its deduplicated EntityNode list. **dedupe_edges_bulk** performs embedding for EntityEdge objects, finds semantic or lexical duplicates among EntityEdge objects, calls resolve_extracted_edge(...), compresses UUIDs, and returns edges_by_episode keyed by episode.uuid. These functions work together to process multiple episodes into final node and edge mappings in Graphiti.","The bulk pipeline in graphiti_core/utils/bulk_utils.py works in four stages, each feeding its output into the next:

1. retrieve_previous_episodes_bulk  
   – For each EpisodicNode in your input list, it fetches the last N episodes with semaphore_gather and returns a list of (episode, previous_episodes) tuples.

2. extract_nodes_and_edges_bulk  
   – Takes the episode tuples plus your GraphitiClients and edge_type_map, runs extract_nodes(...) and extract_edges(...) in parallel (again via semaphore_gather), and returns two nested lists: extracted_nodes_bulk and extracted_edges_bulk.

3. dedupe_nodes_bulk  
   – Embeds all extracted EntityNode objects, finds cross-episode candidates by word overlap or embedding similarity, calls resolve_extracted_nodes(...) to pick/merge duplicates, builds a compressed_map of UUIDs, and finally returns nodes_by_episode: a dict mapping each episode.uuid to its deduplicated EntityNode list.

4. dedupe_edges_bulk  
   – Mirrors the node step but for EntityEdge: embeds, finds semantic/lexical duplicates, calls resolve_extracted_edge(...), compresses UUIDs, and returns edges_by_episode keyed by episode.uuid.

In practice you chain them like:  
```python  
episode_tuples = await retrieve_previous_episodes_bulk(driver, episodes)  
nodes_bulk, edges_bulk = await extract_nodes_and_edges_bulk(clients, episode_tuples, edge_type_map, …)  
nodes_by_ep, node_map = await dedupe_nodes_bulk(clients, nodes_bulk, episode_tuples, …)  
edges_by_ep      = await dedupe_edges_bulk(clients, edges_bulk, episode_tuples, nodes_by_ep, edge_types, …)  
```","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.target_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_edge_save_bulk_query', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 6, 'pr': 73, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",46.82204294204712,771
Which episode addition workflow variant processes multiple episodes in one batch without performing edge invalidation or extracting temporal dates?,The `Graphiti.add_episode_bulk` method processes multiple episodes in one batch without performing edge invalidation or extracting temporal dates. It is defined in `graphiti_core/graphiti.py` and handles a list of RawEpisode in one batch.,"The bulk‐add workflow is the `Graphiti.add_episode_bulk` method (graphiti_core/graphiti.py). Per its docstring, it processes a list of `RawEpisode` in one batch and “does not perform edge invalidation or date extraction” (use `add_episode` for that).","['CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.existing_edges', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_edge_save_bulk_query', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 18}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 18, 'n_files_pr': 12, 'pr': 77, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.866010427474976,772
What steps are involved in the hybrid node search from executing fulltext and vector searches to combining and reranking results?,"The steps involved in the hybrid node search from executing fulltext and vector searches to combining and reranking results in graphiti are:

1.  **Executing Full-Text Searches**: The `hybrid_node_search` function concurrently fires off full-text searches for each query using the `node_fulltext_search` function.
2.  **Executing Vector-Similarity Searches**: It also concurrently fires off vector-similarity searches for each embedding using the `node_similarity_search` function.
3.  **Concurrent Search Execution**: The `semaphore_gather` function is used to perform the concurrent searches, allowing the full-text and vector searches to run in parallel.
4.  **Flattening Results**: After all searches have completed, the results are flattened by concatenating the lists of EntityNode objects returned from each search.
5.  **Building UUID Map**: A map is built from uuid to node to facilitate easy lookup of nodes by their uuids.
6.  **Building UUID Sequences**: A list of uuid sequences is created, where each sequence represents a path in the graph.
7.  **Reranking Results**: The results are reranked using the top search result's source node uuid as the center node for graph-distance-based reranking.
8.  **Returning Reranked Results**: Finally, the reranked results are returned to the caller.","In graphiti_core/search/search_utils.py’s hybrid_node_search:

1. Concurrently fire off  
   – full-text searches (node_fulltext_search) for each query  
   – vector-similarity searches (node_similarity_search) for each embedding  
   (via semaphore_gather)  
2. Flatten all returned EntityNode lists and build a uuid→node map  
3. Build a list of uuid sequences (one list per search)  
4. Rerank with reciprocal-rank fusion (rrf(result_uuids))  
5. Map the fused uuid ranking back to EntityNode objects and return them","['CLASS qlib.qlib.rl.utils.finite_env.FiniteDummyVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteVectorEnv inherits from or is a subclass of tianshou.env.BaseVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices', 'CLASS qlib.qlib.rl.utils.finite_env.FiniteSubprocVectorEnv inherits from or is a subclass of qlib.qlib.rl.utils.finite_env.FiniteVectorEnv\nTo allow the paralleled env workers consume a single DataQueue until it\'s exhausted.  See `tianshou issue #322 <https://github.com/thu-ml/tianshou/issues/322>`_.  The requirement is to make every possible seed (stored in :class:`qlib.rl.utils.DataQueue` in our case) consumed by exactly one environment. This is not possible by tianshou\'s native VectorEnv and Collector, because tianshou is unaware of this ""exactly one"" constraint, and might launch extra workers.  Consider a corner case, where concurrency is 2, but there is only one seed in DataQueue. The reset of two workers must be both called according to the logic in collect. The returned results of two workers are collected, regardless of what they are. The problem is, one of the reset result must be invalid, or repeated, because there\'s only one need in queue, and collector isn\'t aware of such situation.  Luckily, we can hack the vector env, and make a protocol between single env and vector env. The single environment (should be :class:`qlib.rl.utils.EnvWrapper` in our case) is responsible for reading from queue, and generate a special observation when the queue is exhausted. The special obs is called ""nan observation"", because simply using none causes problems in shared-memory vector env. :class:`FiniteVectorEnv` then read the observations from all workers, and select those non-nan observation. It also maintains an ``_alive_env_ids`` to track which workers should never be called again. When also the environments are exhausted, it will raise StopIteration exception.  The usage of this vector env in collector are two parts:  1. If the data queue is finite (usually when inference), collector should collect ""infinity"" number of    episodes, until the vector env exhausts by itself. 2. If the data queue is infinite (usually in training), collector can set number of episodes / steps.    In this case, data would be randomly ordered, and some repetitions wouldn\'t matter.  One extra function of this vector env is that it has a logger that explicitly collects logs from child workers. See :class:`qlib.rl.utils.LogWriter`.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 14}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 14, 'n_files_pr': 7, 'pr': 81, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",50.378174781799316,773
"Within the add_episode workflow, which utility handles embedding creation, related edge retrieval, and LLM-based deduplication and invalidation of extracted edges?","The `resolve_extracted_edges` function in `graphiti_core/utils/maintenance/edge_operations.py` handles embedding creation, related edge retrieval, and LLM-based deduplication and invalidation of extracted edges within the add_episode workflow in graphiti.","The utility is the resolve_extracted_edges function in graphiti_core/utils/maintenance/edge_operations.py. It calls create_entity_edge_embeddings (to embed edges), get_relevant_edges / get_edge_invalidation_candidates (to fetch related/invalidation candidates), and then uses LLM via resolve_extracted_edge for deduplication and invalidation.","['MODULE cognee.cognee.infrastructure.databases.exceptions.exceptions contains CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.EmbeddingException\nCustom exception for handling embedding-related errors.  This exception class is designed to indicate issues specifically related to embeddings within the application. It extends the base exception class CogneeConfigurationError allows for customization of the error message, name, and status code.', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_episodic_edge_from_record', 'MODULE cognee.cognee.modules.retrieval.graph_summary_completion_retriever contains CLASS cognee.cognee.modules.retrieval.graph_summary_completion_retriever.GraphSummaryCompletionRetriever\nRetriever for handling graph-based completion searches with summarized context.  This class inherits from the GraphCompletionRetriever and is intended to manage the retrieval of graph edges with an added functionality to summarize the retrieved information efficiently. Public methods include:  - __init__() - resolve_edges_to_text()', 'MODULE graphiti.graphiti_core.edges contains GLOBAL_VARIABLE graphiti.graphiti_core.edges.fact_embeddings', 'CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.EmbeddingException has method cognee.cognee.infrastructure.databases.exceptions.exceptions.EmbeddingException.__init__\nCustom exception for handling embedding-related errors.  This exception class is designed to indicate issues specifically related to embeddings within the application. It extends the base exception class CogneeConfigurationError allows for customization of the error message, name, and status code.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 13}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 13, 'n_files_pr': 8, 'pr': 85, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",28.632277965545654,774
Which two modules' Cypher queries were updated to include the name_embedding field in their return statements?,The two modules that have Cypher queries with 'n.name_embedding' in their RETURN clauses and were updated are: `graphiti_core/nodes.py` (EntityNode.get_by_uuid) and `graphiti_core/search/search_utils.py` (get_mentioned_nodes).,"The Cypher queries in

• graphiti_core/nodes.py (EntityNode.get_by_uuid)  
• graphiti_core/search/search_utils.py (get_mentioned_nodes)  

were both updated to add `n.name_embedding` to their RETURN clauses.","['MODULE qlib.qlib.model.trainer contains CLASS qlib.qlib.model.trainer.TrainerR\nTrainer based on (R)ecorder. It will train a list of tasks and return a list of model recorders in a linear way.  Assumption: models were defined by `task` and the results will be saved to `Recorder`.', 'MODULE graphiti.graphiti_core.nodes contains GLOBAL_VARIABLE graphiti.graphiti_core.nodes.name_embeddings', 'CLASS qlib.qlib.model.trainer.TrainerR inherits from or is a subclass of qlib.qlib.model.trainer.Trainer\nThe trainer can train a list of models. There are Trainer and DelayTrainer, which can be distinguished by when it will finish real training.\nTrainer based on (R)ecorder. It will train a list of tasks and return a list of model recorders in a linear way.  Assumption: models were defined by `task` and the results will be saved to `Recorder`.', 'MODULE graphiti.graphiti_core.llm_client.utils contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.utils.embedding', 'CLASS qlib.qlib.model.trainer.DelayTrainerR inherits from or is a subclass of qlib.qlib.model.trainer.TrainerR\nTrainer based on (R)ecorder. It will train a list of tasks and return a list of model recorders in a linear way.  Assumption: models were defined by `task` and the results will be saved to `Recorder`.\nA delayed implementation based on TrainerR, which means `train` method may only do some preparation and `end_train` method can do the real model fitting.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 87, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.8416006565094,775
How is the group ID parameter incorporated into both the message ingestion and the search/memory retrieval processes?,"The group ID parameter is incorporated into both the message ingestion and search/memory retrieval processes in graphiti through the use of the `group_id` argument in various functions, such as `add_episode`, `search`, and `get_memory`. These functions allow for the specification of a group ID, which is then used to filter or retrieve data related to that specific group. The incorporation of the group ID parameter enables efficient and targeted retrieval and manipulation of data related to specific groups, supporting features like real-time incremental updates, bi-temporal data modeling, and hybrid retrieval.","In ingest.py (add_messages_task), the incoming request.group_id is passed straight into graphiti.add_episode(…, group_id=request.group_id, …), so every message is tagged by its group.  

In retrieve.py both search() and get_memory() scope their calls to graphiti.search by passing either query.group_ids (or in get_memory, a singleton list [request.group_id]) into the group_ids argument, ensuring only that group’s episodes are returned.","['CLASS xgboost.python-package.xgboost.core.DMatrix has method xgboost.python-package.xgboost.core.DMatrix.get_group\nGet the group of the DMatrix.  Returns ------- group\nData Matrix used in XGBoost.  DMatrix is an internal data structure that is used by XGBoost, which is optimized for both memory efficiency and training speed.  You can construct DMatrix from multiple different sources of data.', 'FUNCTION cognee.cognee.modules.ingestion.identify.identify has parameter or accepts argument cognee.cognee.modules.ingestion.identify.identify.data', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_daily_bin_group has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_daily_bin_group.bench_values\nget_daily_bin_group Group the values of the stocks of benchmark into several bins in a day. Put the stocks into these bins.  :param bench_values: A series contains the value of stocks in benchmark.                      The index is the stock code. :param stock_values: A series contains the value of stocks of your portofolio                      The index is the stock code. :param group_n:      Bins will be produced  :return:             A series with the same size and index as the stock_value.                      The value in the series is the group id of the bins.                      The No.1 bin contains the biggest values.', 'FUNCTION cognee.cognee.modules.ingestion.identify.identify has parameter or accepts argument cognee.cognee.modules.ingestion.identify.identify.user', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_daily_bin_group has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_daily_bin_group.stock_values\nget_daily_bin_group Group the values of the stocks of benchmark into several bins in a day. Put the stocks into these bins.  :param bench_values: A series contains the value of stocks in benchmark.                      The index is the stock code. :param stock_values: A series contains the value of stocks of your portofolio                      The index is the stock code. :param group_n:      Bins will be produced  :return:             A series with the same size and index as the stock_value.                      The value in the series is the group id of the bins.                      The No.1 bin contains the biggest values.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 92, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",37.16572189331055,776
What steps does the graph client's save_entity_node method perform to embed and persist a new entity node?,"The steps performed by ZepGraphiti.save_entity_node in graphiti to embed and persist a new entity node are:

1. Instantiate an EntityNode with name, uuid, group_id, and summary.
2. Compute the EntityNode’s name embedding using `new_node.generate_name_embedding(self.embedder)`. 
3. Persist the EntityNode in Neo4j using `new_node.save(self.driver)`. These steps ensure that the new entity node is properly embedded and persisted in the graph database.","In ZepGraphiti.save_entity_node (server/graph_service/zep_graphiti.py) it:

1. Instantiates an EntityNode(name, uuid, group_id, summary)  
2. Calls new_node.generate_name_embedding(self.embedder) to compute its embedding  
3. Calls new_node.save(self.driver) to persist it in Neo4j  
4. Returns the saved EntityNode instance","['MODULE cognee.cognee.tasks.temporal_graph.add_entities_to_event contains FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_node_save_bulk_query', 'FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event has parameter or accepts argument cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event.event\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', 'MODULE graphiti.graphiti_core.models.nodes.node_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.nodes.node_db_queries.ENTITY_NODE_SAVE', 'FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event has parameter or accepts argument cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event.event_with_entities\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 9, 'pr': 100, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",34.232447385787964,777
What default edge type map does add_episode_bulk use if edge_types is not specified?,"The default edge type map used by `Graphiti.add_episode_bulk` in graphiti when `edge_types` is None is {('Entity','Entity'): []}. This default edge type map is defined as `edge_type_map_default`.","In Graphiti.add_episode_bulk (graphiti_core/graphiti.py), when `edge_types` is None it builds  
```python
edge_type_map_default = {('Entity','Entity'): []}
```","['CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS graphiti.graphiti_core.errors.EntityTypeValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an entity type uses protected attribute names.', 'MODULE graphiti.graphiti_core.models.edges.edge_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.edges.edge_db_queries.EPISODIC_EDGE_SAVE_BULK', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 101, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.919260501861572,778
Which exceptions are classified as server or retry errors and thus trigger exponential backoff in the LLM client's retry policy?,"The exceptions that are classified as server or retry errors and thus trigger exponential backoff in the LLM client's retry policy in graphiti are:

1. RateLimitError
2. json.decoder.JSONDecodeError
3. httpx.HTTPStatusError instances with a 5xx status code (500–599)","The retry decorator (in graphiti_core/llm_client/client.py on LLMClient._generate_response_with_retry) uses is_server_or_retry_error, which returns True for:

• RateLimitError  
• json.decoder.JSONDecodeError  
• httpx.HTTPStatusError with a 5xx status code (500–599)","['MODULE cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter contains FUNCTION cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.embedding_sleep_and_retry_sync\nAdd retry with exponential backoff for synchronous embedding functions.  Parameters: -----------      - max_retries: Maximum number of retries before giving up. (default 5)     - base_backoff: Base backoff time in seconds for retry intervals. (default 1.0)     - jitter: Jitter factor to randomize the backoff time to avoid collision. (default       0.5)  Returns: --------      A decorator that retries the wrapped function on rate limit errors, applying     exponential backoff with jitter.', 'FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error has parameter or accepts argument graphiti.graphiti_core.llm_client.client.is_server_or_retry_error.exception', 'MODULE cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter contains FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff\nCalculate the backoff time for a retry attempt with jitter.  Parameters: -----------      - attempt: The current retry attempt (0-based).     - initial_backoff: The initial backoff time in seconds. (default       DEFAULT_INITIAL_BACKOFF)     - backoff_factor: The multiplier for exponential backoff. (default       DEFAULT_BACKOFF_FACTOR)     - jitter: The jitter factor to avoid thundering herd. (default DEFAULT_JITTER)  Returns: --------      The backoff time in seconds, calculated using the exponential backoff formula     adjusted by a jitter component.', 'MODULE graphiti.graphiti_core.llm_client.client contains FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.rate_limiter.calculate_backoff.backoff_factor\nCalculate the backoff time for a retry attempt with jitter.  Parameters: -----------      - attempt: The current retry attempt (0-based).     - initial_backoff: The initial backoff time in seconds. (default       DEFAULT_INITIAL_BACKOFF)     - backoff_factor: The multiplier for exponential backoff. (default       DEFAULT_BACKOFF_FACTOR)     - jitter: The jitter factor to avoid thundering herd. (default DEFAULT_JITTER)  Returns: --------      The backoff time in seconds, calculated using the exponential backoff formula     adjusted by a jitter component.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 7, 'n_files_pr': 6, 'pr': 102, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.695438623428345,779
"After clustering nodes into communities, which function generates the membership edges and at what point in the build process are they persisted?","The function that generates the membership edges after clustering nodes into communities in graphiti is `build_community_edges`. This function is located in the `graphiti_core/utils/maintenance/edge_operations.py` file. The ""HAS_MEMBER"" edges are persisted immediately after saving all CommunityNode objects, which occurs in the `semaphore_gather` call. The `semaphore_gather` call runs `edge.save()` on each CommunityEdge, including the ""HAS_MEMBER"" edges generated by `build_community_edges`. This process happens in the Graphiti.build_communities function located in graphiti_core/graphiti.py.","The “HAS_MEMBER” edges are built by build_community_edges (in graphiti_core/utils/maintenance/edge_operations.py).  They’re then persisted in Graphiti.build_communities (graphiti_core/graphiti.py), immediately after you save all the CommunityNode objects – i.e. in the semaphore_gather call that runs edge.save() on each CommunityEdge.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_community_edge_from_record', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains FUNCTION graphiti.graphiti_core.utils.maintenance.edge_operations.build_community_edges', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 23}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 23, 'n_files_pr': 10, 'pr': 103, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",36.48376536369324,780
How does a missing entity edge lookup result in an HTTP 404 response?,"A missing entity edge lookup results in an HTTP 404 response because of the way the `ZepGraphiti.get_entity_edge` function is implemented. This function catches the `EdgeNotFoundError` exception that is raised when no matching edge is found, and rethrows it as a FastAPI's HTTPException with status_code=404. The `get_entity_edge` function acts as an intermediary between the missing entity edge lookup and the HTTP response, ensuring that any errors related to missing edges are handled and returned as a 404 HTTP response.","When you call EntityEdge.get_by_uuid (in graphiti_core/edges.py) and no matching edge is found it raises an EdgeNotFoundError. In server/graph_service/zep_graphiti.py’s ZepGraphiti.get_entity_edge this is caught and re-thrown as FastAPI’s HTTPException(status_code=404), so a missing edge lookup yields a 404.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.server.graph_service.routers.retrieve contains GLOBAL_VARIABLE graphiti.server.graph_service.routers.retrieve.entity_edge', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_entity_edge_from_record', 'CLASS graphiti.graphiti_core.errors.EntityTypeValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an entity type uses protected attribute names.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 5, 'n_files_pr': 6, 'pr': 104, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.49543070793152,781
Describe how extract_nodes leverages the extract_text prompt to generate and return EntityNode objects for text episodes.,"The extract_nodes function in graphiti leverages the extract_text prompt to generate and return EntityNode objects for text episodes as follows: The extract_nodes function builds a context dictionary that includes episode.content, entity_types, and any custom_prompt. It then calls llm_client.generate_response with prompt_library.extract_nodes.extract_text(context) and response_model set to ExtractedEntities. The LLM’s JSON reply is parsed into a list of ExtractedEntity models. A small reflexion loop may re-invoke the prompt to catch missed entities. The extract_nodes function filters out ExtractedEntity items with blank names and any excluded types. For each remaining ExtractedEntity, it instantiates an EntityNode with name equal to entity.name, labels equal to [""Entity"", entity_type_name], group_id equal to episode.group_id, summary set to an empty string, and created_at set to utc_now(). The extract_nodes function returns the list of EntityNode objects. This process allows graphiti to utilize the power of large language models (LLMs) to identify entities in text episodes and create corresponding EntityNode objects in the knowledge graph.","In graphiti_core/utils/maintenance/node_operations.py, the async function extract_nodes does the following for text‐based episodes (EpisodeType.text):

1. It builds a `context` dict (including `episode.content`, `entity_types`, any `custom_prompt`, etc.).  
2. It calls  
   llm_client.generate_response(  
     prompt_library.extract_nodes.extract_text(context),  
     response_model=ExtractedEntities  
   )  
   Here `extract_text` is defined in graphiti_core/prompts/extract_nodes.py and returns a system+user `Message` pair wrapping the raw text and entity‐type metadata.  
3. The LLM’s JSON reply is parsed into a list of `ExtractedEntity` models. A small reflexion loop may re‐invoke the prompt to catch missed entities.  
4. It filters out blank names and any excluded types, then for each remaining `ExtractedEntity` it instantiates an `EntityNode` (from your graph model) with:  
   – name = entity.name  
   – labels = [“Entity”, entity_type_name]  
   – group_id = episode.group_id, summary = “” , created_at = utc_now()  
5. Finally it returns that list of `EntityNode` objects.","['MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains FUNCTION graphiti.graphiti_core.prompts.extract_nodes.extract_text', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 106, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",43.99317669868469,782
How does the example script determine the subset of messages to process in bulk versus sequential mode?,"The example script determines the subset of messages to process in bulk versus sequential mode based on the value of the `use_bulk` flag. If `use_bulk=True`, it sets `raw_episodes` to `messages[3:7]` and calls `client.add_episode_bulk(raw_episodes, ...)`. If `use_bulk=False`, it loops over `messages[3:14]`, retrieves recent episodes for each message, and calls `client.add_episode(...) sequentially. This approach allows the script to process a subset of messages in bulk when `use_bulk=True`, reducing the number of API calls, while processing individual messages sequentially when `use_bulk=False`.","In examples/podcast/podcast_runner.py the main() function’s use_bulk flag drives which slice of the parsed messages gets processed:

• If use_bulk=True, it does  
 raw_episodes = messages[3:7]  
 → calls client.add_episode_bulk(raw_episodes, …)

• If use_bulk=False, it loops over  
 messages[3:14]  
 → for each message it retrieves recent episodes and calls client.add_episode(…) sequentially.","['MODULE xgboost.demo.guide-python.evals_result contains GLOBAL_VARIABLE xgboost.demo.guide-python.evals_result.evals_result\nThis script demonstrate how to access the eval metrics ======================================================', 'MODULE cognee.cognee.tests.unit.processing.chunks.chunk_by_paragraph_2_test contains GLOBAL_VARIABLE cognee.cognee.tests.unit.processing.chunks.chunk_by_paragraph_2_test.batch_paragraphs_vals', ""FUNCTION cognee.alembic.env.run_migrations_offline has parameter or accepts argument cognee.alembic.env.config\nRun migrations in 'offline' mode.  This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well.  By skipping the Engine creation we don't even need a DBAPI to be available.  Calls to context.execute() here emit the given string to the script output."", 'FUNCTION cognee.cognee.tests.unit.processing.chunks.chunk_by_paragraph_2_test.test_chunk_by_paragraph_chunk_numbering has parameter or accepts argument cognee.cognee.tests.unit.processing.chunks.chunk_by_paragraph_2_test.test_chunk_by_paragraph_chunk_numbering.batch_paragraphs', ""FUNCTION cognee.alembic.env.run_migrations_offline has parameter or accepts argument cognee.alembic.env.target_metadata\nRun migrations in 'offline' mode.  This configures the context with just a URL and not an Engine, though an Engine is acceptable here as well.  By skipping the Engine creation we don't even need a DBAPI to be available.  Calls to context.execute() here emit the given string to the script output."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/README.md\n# Requirements\n\nHere is the minimal hardware requirements to run the `workflow_by_code` example.\n- Memory: 16G\n- Free Disk: 5G\n\n\n# NOTE\nThe results will slightly vary on different OSs(the variance of annualized return will be less than 2%).\nThe evaluation results in the `README.md` page are from Linux OS.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': True, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 107, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",34.92766618728638,783
How does the search entrypoint normalize empty or blank group identifier lists before calling the specific edge and node search utilities?,"The search entrypoint in graphiti normalizes empty or blank group identifier lists by setting the `group_ids` variable to None when it is an empty list (`[]`) or a list containing an empty string (`['']`). This is done before calling the specific edge and node search utilities. The normalization of `group_ids` ensures that individual search utilities, such as `edge_search` and `node_search`, skip the `r.group_id IN $group_ids` clauses when `group_ids` is None.","In the `graphiti_core/search/search.py` entrypoint (the `search` function) you’ll find this line right before it calls the individual search utilities:

```python
# if group_ids is empty, set it to None
group_ids = group_ids if group_ids and group_ids != [''] else None
```

That forces any empty list (`[]`) or blank‐string list (`['']`) into `None`. The downstream `edge_search`, `node_search`, etc. then see `group_ids=None` and skip the `r.group_id IN $group_ids` clauses.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.group_filter_query', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 118, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.78163194656372,784
How do node classes convert query results into their domain objects for both single and batch lookups?,"Node classes in graphiti_core/nodes.py convert query results into their domain objects through a two-step process. The first step involves calling `driver.execute_query` to fetch raw records using either `get_by_uuid` or `get_by_uuids`. The second step maps the raw records to domain objects via helper functions such as `get_entity_node_from_record`, `get_community_node_from_record`, and `get_episodic_node_from_record. In the case of single lookups, the result is checked for emptiness; if empty, a `NodeNotFoundError` is raised, otherwise the first element is returned. For batch lookups, the full list of converted domain objects is returned.","In graphiti_core/nodes.py each Node class does essentially the same two‐step conversion:

1. Calls driver.execute_query(…) in either get_by_uuid or get_by_uuids to fetch raw records.  
2. Maps those records to domain objects via the appropriate helper:  
   • get_entity_node_from_record  
   • get_community_node_from_record  
   • get_episodic_node_from_record  

– For single lookups (get_by_uuid) they wrap the list comprehension, check for an empty result (raising NodeNotFoundError) and return nodes[0].  
– For batch lookups (get_by_uuids) they simply return the full list of converted objects.","['CLASS keras.keras.src.trainers.data_adapters.array_data_adapter.ArrayDataAdapter has method keras.keras.src.trainers.data_adapters.array_data_adapter.ArrayDataAdapter.slice_batch_indices\nConvert a Tensor of indices into a dataset of batched indices.  This step can be accomplished in several ways. The most natural is to slice the Tensor in a Dataset map. (With a condition on the upper index to handle the partial batch.) However it turns out that coercing the Tensor into a shape which is divisible by the batch size (and handling the last partial batch separately) allows for a much more favorable memory access pattern and improved performance.  Args:     indices: Tensor which determines the data order for an entire         epoch.  Returns:     A Dataset of batched indices.\nAdapter for array-like objects, e.g. TF/JAX Tensors, NumPy arrays.', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.query', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS cognee.cognee.tasks.graph.cascade_extract.utils.extract_nodes.PotentialNodes has field or attribute or property cognee.cognee.tasks.graph.cascade_extract.utils.extract_nodes.PotentialNodes.nodes\nResponse model containing a list of potential node names.', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 119, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",42.37952709197998,785
Describe how the episode addition workflow is extended to update community information when a new episode is processed.,"The episode addition workflow in Graphiti is extended to update community information when a new episode is processed by checking the `update_communities` flag after calling `add_nodes_and_edges_bulk`. If this flag is true, the workflow uses `semaphore_gather` to concurrently invoke `update_community` for each extracted node. The `update_community` call recompute or reassign the node's community using the LLM client and embedder. This process is contained within the ""# Update any communities"" block at the end of Graphiti.add_episode, which indicates that it is a community update extension.","When you call Graphiti.add_episode (or the bulk variant) with update_communities=True, right after the call to add_nodes_and_edges_bulk in graphiti_core/graphiti.py, the workflow does:

• Checks the update_communities flag  
• If true, uses semaphore_gather to concurrently invoke update_community(self.driver, self.llm_client, self.embedder, node) for each extracted node  

Each update_community call uses the LLM client & embedder to recompute or reassign that node’s community. This extension lives in the “# Update any communities” block at the end of Graphiti.add_episode.","['CLASS graphiti.graphiti_core.nodes.EpisodeType inherits from or is a subclass of enum.Enum\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.eval contains FUNCTION graphiti.graphiti_core.prompts.eval.eval_add_episode_results', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'FUNCTION graphiti.graphiti_core.prompts.eval.eval_add_episode_results has parameter or accepts argument graphiti.graphiti_core.prompts.eval.eval_add_episode_results.context', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 121, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.77813529968262,786
Trace the flow of a message's UUID from the ingestion endpoint to its retrieval as an edge result.,"The flow of a message's UUID from the ingestion endpoint to its retrieval as an edge result in Graphiti is as follows:

1.  The `add_messages_task` function in `server/graph_service/routers/ingest.py` extracts the message UUID `m.uuid` from the request.
2.  This UUID is then passed to the `add_episode` method in the Graphiti client, which constructs an Episode node by calling `Episode.save(driver)` with the supplied UUID.
3.  The `save(driver)` method runs a Cypher MERGE/CREATE query with the provided UUID.
4.  When edges are created to link an episode to a conversation or parent entity, they are instantiated as subclasses of Edge and persisted via their `save(driver)` methods in `graphiti_core/edges.py`.
5.  These edge objects embed the same UUIDs as the Episode node.
6.  A GET request to `/entity/{uuid}/edge` in `server/graph_service/routers/retrieve.py` invokes `await graphiti.get_entity_edge(uuid)`, which calls `Edge.get_by_uuid(driver, uuid)` on the Graphiti client.
7.  This method runs a Cypher MATCH query on the UUID to load the edge associated with it.
8.  The resulting Edge model is then converted into an API schema by the `get_fact_result_from_edge` function and returned to the client.","1. Ingestion  
   • In server/graph_service/routers/ingest.py (add_messages_task) we extract the message UUID (`m.uuid`) from the request and call  
     `await graphiti.add_episode(uuid=m.uuid, …)`.  
   • Inside your Graphiti client (e.g. server/graph_service/graphiti.py), `add_episode` constructs an Episode node by calling  
     `Episode.save(driver)` (graphiti_core/nodes.py), which runs a Cypher MERGE/CREATE with the supplied `uuid`.  
   • If any edges are created (e.g. linking this episode to a conversation or parent entity), they’re instantiated as subclasses of `Edge` and persisted via their `save(driver)` methods in graphiti_core/edges.py, embedding the same UUIDs.  

2. Retrieval  
   • A GET to `/entity/{uuid}/edge` in server/graph_service/routers/retrieve.py invokes  
     `await graphiti.get_entity_edge(uuid)` (ZepGraphitiDep).  
   • Under the hood this calls `Edge.get_by_uuid(driver, uuid)` (graphiti_core/edges.py), running a Cypher `MATCH` on the `uuid` to load the edge.  
   • The resulting `Edge` model is then converted to the API schema via `get_fact_result_from_edge` and returned.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.server.graph_service.dto.ingest contains CLASS graphiti.server.graph_service.dto.ingest.AddMessagesRequest', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.server.graph_service.dto.retrieve contains CLASS graphiti.server.graph_service.dto.retrieve.GetMemoryResponse', 'CLASS cognee.cognee.infrastructure.databases.vector.models.ScoredResult.ScoredResult inherits from or is a subclass of pydantic.BaseModel\nRepresents the result of a scoring operation with an identification and associated data.  Attributes:  - id (UUID): Unique identifier for the scored result. - score (float): The score associated with the result, where a lower score indicates a better outcome. - payload (Dict[str, Any]): Additional information related to the score, stored as key-value pairs in a dictionary.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 4, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 123, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",51.622963666915894,787
"When configured, how do edge_search and node_search apply the episode mentions reranker?","When configured, edge_search and node_search apply the episode mentions reranker as follows:

*   Edge search: When configured, edge_search applies the episode mentions reranker by running RRF (Relevance Ranking Framework) on the list of edge-UUIDs from three sub-searches. It then maps back to EntityEdge objects before sorting them so that edges with the highest number of episode mentions come first.
*   Node search: When configured, node_search applies the episode mentions reranker by skipping RRF and MMR (Maximum Marginal Relevance) reranking methods. Instead, it calls `episode_mentions_reranker`, which returns node UUIDs ranked by how often they are mentioned in past episodes.
The episode mentions reranker is used to rank edges and nodes based on their relevance to the search query, with a focus on episode mentions. This allows for more accurate and relevant results to be returned from the graph database.","In graphiti_core/search/search.py:

• edge_search (EdgeReranker.episode_mentions)  
  – It falls into the “rrf” branch: runs rrf(...) on the list of edge‐UUIDs from the three sub‐searches  
  – After mapping back to EntityEdge objects it then does  
    reranked_edges.sort(key=lambda e: len(e.episodes), reverse=True)  
  – (i.e. highest number of episode mentions first)

• node_search (NodeReranker.episode_mentions)  
  – It skips RRF/MMR etc. and directly calls  
    await episode_mentions_reranker(driver, search_result_uuids, min_score)  
  – (see episode_mentions_reranker in graphiti_core/search/search_utils.py), which returns node UUIDs ranked by how often they’re mentioned in past episodes.

In short: edge_search uses RRF + a post‐sort on edge.episodes, node_search delegates to the episode_mentions_reranker util.","['CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has method graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.\nGoogle Gemini Reranker Client', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_episodes', 'METHOD graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__.config\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.', 'CLASS graphiti.graphiti_core.search.search_config.EpisodeSearchConfig has field or attribute or property graphiti.graphiti_core.search.search_config.EpisodeSearchConfig.reranker', 'METHOD graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__.client\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 12}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 12, 'n_files_pr': 9, 'pr': 124, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",40.347058057785034,788
How are the OpenAI settings fields applied to configure the Graphiti LLM client during initialization?,"The OpenAI settings fields are applied to configure the Graphiti LLM client during initialization as follows: if `settings.openai_base_url` is set, it assigns that value to `client.llm_client.config.base_url`; if `settings.openai_api_key` is set, it assigns that value to `client.llm_client.config.api_key`; and if `settings.model_name` is set, it assigns that value to `client.llm_client.model`. This configuration occurs after instantiating a ZepGraphiti client in the `get_graphiti` function.","In `get_graphiti` (server/graph_service/zep_graphiti.py), after you instantiate your `ZepGraphiti` client, the OpenAI‐related fields from `Settings` are applied directly to its LLM client:

• if `settings.openai_base_url` is set → `client.llm_client.config.base_url = …`  
• if `settings.openai_api_key` is set → `client.llm_client.config.api_key = …`  
• if `settings.model_name` is set → `client.llm_client.model = …`","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""CLASS graphiti.graphiti_core.llm_client.openai_client.OpenAIClient has method graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the BaseOpenAIClient and provides OpenAI-specific implementation for creating completions.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API."", 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.neo4j\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 126, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",42.58645439147949,789
How are the cursor and limit parameters applied in the new group-based community fetch method?,"The cursor and limit parameters are applied in the new group-based community fetch method as follows: *   The `cursor` parameter is used to implement pagination, converting a provided uuid into a Cypher query string ""AND n.uuid < $uuid"" which is added to the WHERE clause of the generated Cypher query. *   The `limit` parameter is used to limit the number of results returned, converting a provided limit into a Cypher query string ""LIMIT $limit"" which is appended after the ORDER BY n.uuid DESC clause of the generated Cypher query. This allows for efficient retrieval of community data in batches, with the ability to resume from where previous queries left off using the cursor parameter.","In graphiti_core/nodes.py in CommunityNode.get_by_group_ids you can see:

• uuid_cursor (named uuid_cursor) is turned into  
   cursor_query = ""AND n.uuid < $uuid"" if you pass a cursor, adding it to the WHERE clause.  
• limit is turned into  
   limit_query = ""LIMIT $limit"" if you pass a limit, appending it after the ORDER BY n.uuid DESC.  

So you get a Cypher query like:

MATCH (n:Community)  
WHERE n.group_id IN $group_ids  
  [AND n.uuid < $uuid]  
RETURN …  
ORDER BY n.uuid DESC  
[LIMIT $limit]","['CLASS xgboost.python-package.xgboost.testing.data.PBM has method xgboost.python-package.xgboost.testing.data.PBM.sample_clicks_for_query\nSample clicks for one query based on input relevance degree and position.  Parameters ----------  labels :     relevance_degree\nSimulate click data with position bias model. There are other models available in `ULTRA <https://github.com/ULTR-Community/ULTRA.git>`_ like the cascading model.  References ---------- Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm', 'CLASS graphiti.graphiti_core.edges.CommunityEdge has field or attribute or property graphiti.graphiti_core.edges.CommunityEdge.limit_query', 'CLASS qlib.qlib.workflow.online.strategy.OnlineStrategy has method qlib.qlib.workflow.online.strategy.OnlineStrategy.prepare_tasks\nAfter the end of a routine, check whether we need to prepare and train some new tasks based on cur_time (None for latest).. Return the new tasks waiting for training.  You can find the last online models by OnlineTool.online_models.\nOnlineStrategy is working with `Online Manager <#Online Manager>`_, responding to how the tasks are generated, the models are updated and signals are prepared.', 'CLASS graphiti.graphiti_core.nodes.CommunityNode has field or attribute or property graphiti.graphiti_core.nodes.CommunityNode.limit_query', 'METHOD qlib.qlib.backtest.decision.BaseTradeDecision.get_data_cal_range_limit has parameter or accepts argument qlib.qlib.backtest.decision.BaseTradeDecision.get_data_cal_range_limit.raise_error\nget the range limit based on data calendar  NOTE: it is **total** range limit instead of a single step  The following assumptions are made 1) The frequency of the exchange in common_infra is the same as the data calendar 2) Users want the index mod by **day** (i.e. 240 min)  Parameters ---------- rtype: str     - ""full"": return the full limitation of the decision in the day     - ""step"": return the limitation of current step  raise_error: bool     True: raise error if no trade_range is set     False: return full trade calendar.      It is useful in following cases     - users want to follow the order specific trading time range when decision level trade range is not       available. Raising NotImplementedError to indicates that range limit is not available  Returns ------- Tuple[int, int]:     the range limit in data calendar  Raises ------ NotImplementedError:     If the following criteria meet     1) the decision can\'t provide a unified start and end     2) raise_error is True', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 15}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 15, 'n_files_pr': 4, 'pr': 130, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",35.765873670578,790
What exception does the delete_group method catch when no edges are found for the specified group?,The `delete_group` method catches the `GroupsEdgesNotFoundError` exception when no edges are found for the specified group.,The `delete_group` method catches the `GroupsEdgesNotFoundError` (raised by `EntityEdge.get_by_group_ids`) when no edges exist for the given group.,"['CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError has method graphiti.graphiti_core.errors.GroupsEdgesNotFoundError.__init__\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError has field or attribute or property graphiti.graphiti_core.errors.group_ids\nRaised when no edges are found for a list of group ids.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 132, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.791046380996704,791
Which JSON parsing error is now classified as retryable by the error handler?,The JSON parsing error that is now classified as retryable by the error handler in graphiti_core/llm_client/client.py is `json.decoder.JSONDecodeError`. This classification allows for automatic retries when encountering this type of error.,The handler in graphiti_core/llm_client/client.py (now in is_server_or_retry_error) treats json.decoder.JSONDecodeError as a retryable error.,"['METHOD graphiti.server.graph_service.zep_graphiti.ZepGraphiti.__init__ has parameter or accepts argument graphiti.server.graph_service.zep_graphiti.ZepGraphiti.__init__.llm_client', 'MODULE graphiti.graphiti_core.llm_client.client contains FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error', 'MODULE cognee.cognee.tasks.code.enrich_dependency_graph_checker contains FUNCTION cognee.cognee.tasks.code.enrich_dependency_graph_checker.main\nExecute the main logic of the dependency graph processor.  This function sets up argument parsing to retrieve the repository path, checks the existence of the specified path, and processes the repository to produce a dependency graph. If the repository path does not exist, it logs an error message and terminates without further execution.', 'FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error has parameter or accepts argument graphiti.graphiti_core.llm_client.client.is_server_or_retry_error.exception', 'MODULE keras.keras.src.datasets.cifar contains FUNCTION keras.keras.src.datasets.cifar.load_batch\nInternal utility for parsing CIFAR data.  Args:     fpath: path the file to parse.     label_key: key for label data in the retrieve         dictionary.  Returns:     A tuple `(data, labels)`.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 133, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.591201782226562,792
"What is the end-to-end flow for detecting and constructing communities in memory, from fetching related entities to creating community nodes and edges?","The end-to-end flow for detecting and constructing communities in memory, from fetching related entities to creating community nodes and edges in graphiti is as follows:

1. The function `get_community_clusters(driver, group_ids)` fetches all EntityNode instances for each group_id.
2. It queries each EntityNode’s RELATES_TO neighbors.
3. A projection mapping UUIDs to lists of Neighbor objects is built.
4. The function `label_propagation(projection)` runs a label-propagation algorithm on the projection, outputting clusters of UUIDs.
5. The method `EntityNode.get_by_uuids(...)` converts each UUID cluster into a list of EntityNode objects, called community_clusters.
6. The function `build_communities(driver, llm_client, group_ids)` calls `get_community_clusters(driver, group_ids)`, processes each community_clusters in parallel, and limits the parallel processing by a semaphore.
7. For each cluster, the function `build_community(llm_client, cluster)` iteratively pairwise summarizes entity summaries via `summarize_pair` until one summary remains.
8. It generates a human-readable community name via `generate_summary_description`.
9. Finally, it instantiates a CommunityNode and its CommunityEdges via `build_community_edges`.
10. The function `build_communities(driver, llm_client, group_ids)` returns a list of CommunityNode objects and a list of CommunityEdge objects for ingestion into Neo4j.","The in-memory community detection and construction lives entirely in graphiti_core/utils/maintenance/community_operations.py and proceeds as follows:

1. get_community_clusters(driver, group_ids)  
   - Fetch all EntityNode instances for each group_id  
   - For each node, query its RELATES_TO neighbors and build a `projection: Dict[uuid, List[Neighbor]]`  

2. label_propagation(projection)  
   - Run the label‐propagation algorithm on that projection to spit out clusters of UUIDs  

3. EntityNode.get_by_uuids(...)  
   - Turn each UUID cluster back into a List[EntityNode] → these are your “community_clusters”  

4. build_communities(driver, llm_client, group_ids)  
   - Calls get_community_clusters(...)  
   - For each cluster (in parallel, limited by a semaphore) calls build_community(llm_client, cluster):  
     a. Iteratively pairwise‐summarize entity summaries via `summarize_pair` until one summary remains  
     b. Generate a human‐readable community name via `generate_summary_description`  
     c. Instantiate a CommunityNode and its CommunityEdges via `build_community_edges`  

5. build_communities returns the full List[CommunityNode] and List[CommunityEdge] for ingestion into Neo4j.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_community_edge_from_record', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.nodes contains FUNCTION graphiti.graphiti_core.nodes.get_community_node_from_record', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 10, 'n_files_pr': 5, 'pr': 136, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",47.00736427307129,793
What does dedupe_node_list do when an LLM response references a UUID missing from the node map?,"When an LLM response references a UUID missing from the node map, `dedupe_node_list` logs a warning message ""Node {uuid} not found in node map"" and then drops the entry for the unknown UUID without raising an exception. This is because after logging the warning, `dedupe_node_list` executes a continue statement, which causes it to skip the current iteration and move on to the next one.","In node_operations.py’s dedupe_node_list, if the LLM comes back with a UUID that isn’t in the original node_map, it simply does: 

• logger.warning(f""Node {uuid} not found in node map"")  
• continue  

– dropping that entry (no exception) and moving on.","['FUNCTION cognee.cognee.infrastructure.engine.utils.parse_id.parse_id has parameter or accepts argument cognee.cognee.infrastructure.engine.utils.parse_id.parse_id.id\nParse the input ID and convert it to a UUID object if it is a valid string representation.  If the input is not a string or if the string cannot be converted to a UUID, return the input unchanged. This function catches exceptions that may arise from invalid string formats during conversion, but does not raise those exceptions further.  Parameters: -----------      - id (any): The input ID, which can be of any type and may be a string       representation of a UUID.  Returns: --------      The original input ID if it is not a string or cannot be converted to a UUID;     otherwise, a UUID object if the conversion is successful.', 'MODULE graphiti.graphiti_core.prompts.dedupe_nodes contains FUNCTION graphiti.graphiti_core.prompts.dedupe_nodes.node_list', 'MODULE cognee.cognee.infrastructure.engine.utils.parse_id contains FUNCTION cognee.cognee.infrastructure.engine.utils.parse_id.parse_id\nParse the input ID and convert it to a UUID object if it is a valid string representation.  If the input is not a string or if the string cannot be converted to a UUID, return the input unchanged. This function catches exceptions that may arise from invalid string formats during conversion, but does not raise those exceptions further.  Parameters: -----------      - id (any): The input ID, which can be of any type and may be a string       representation of a UUID.  Returns: --------      The original input ID if it is not a string or cannot be converted to a UUID;     otherwise, a UUID object if the conversion is successful.', 'FUNCTION graphiti.graphiti_core.prompts.dedupe_nodes.node_list has parameter or accepts argument graphiti.graphiti_core.prompts.dedupe_nodes.node_list.context', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 137, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.901647329330444,794
Which configuration attributes are passed to instantiate the default AsyncOpenAI client when no client is provided?,"The configuration attributes passed to instantiate the default AsyncOpenAI client when no client is provided in graphiti are:

* `config`: The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.
* `cache`: Whether to use caching for responses from the OpenAI API. Defaults to False.
* `base_url`: The base URL of the OpenAI API. Defaults to ""https://api.openai.com"".
* `headers`: A dictionary of headers to be sent with each request to the OpenAI API.

You can pass these attributes when instantiating the default AsyncOpenAI client in graphiti, like this:

```python
default AsyncOpenAIClient(config=LLMConfig(api_key=""YOUR_API_KEY"", model=""text-davinci-003""), cache=True, base_url=""https://api.openai.com/v1"", headers={""Authorization"": ""Bearer YOUR_API_KEY""})
```

Note that you should replace `YOUR_API_KEY` with your actual OpenAI API key.","In OpenAIClient.__init__ (graphiti_core/llm_client/openai_client.py), if you don’t pass in a client it does:

```python
self.client = AsyncOpenAI(
   api_key=config.api_key,
   base_url=config.base_url
)
```

So the two config attrs used are api_key and base_url.","['METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.client\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""CLASS graphiti.graphiti_core.llm_client.openai_client.OpenAIClient has method graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the BaseOpenAIClient and provides OpenAI-specific implementation for creating completions.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API."", 'METHOD graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__.client\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 140, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",46.100250482559204,795
How is the semaphore used within the community-building loop to enforce the maximum parallel tasks?,"The semaphore in graphiti is used within the community-building loop to enforce the maximum parallel tasks by limiting the number of concurrent `build_community` calls. The semaphore's async with block ensures that at most `MAX_COMMUNITY_BUILD_CONCURRENCY` `build_community` calls run concurrently, preventing overloading and ensuring efficient processing of communities.","In build_communities (graphiti_core/utils/maintenance/community_operations.py) we create an asyncio.Semaphore(MAX_COMMUNITY_BUILD_CONCURRENCY) and wrap each build_community call in:

```python
async def limited_build_community(cluster):
    async with semaphore:
        return await build_community(llm_client, cluster)
```

We then fire all limited_build_community tasks through semaphore_gather. The semaphore’s async with ensures no more than MAX_COMMUNITY_BUILD_CONCURRENCY build_community calls run in parallel.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.neo4j\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.community_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.community_operations.semaphore', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 142, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",29.449544668197632,796
What default values are applied for max_tokens in the Anthropic and Groq client initializers when the configuration lacks this setting?,"The default values for `max_tokens` in the Anthropic and Groq client initializers when the configuration lacks this setting are `DEFAULT_MAX_TOKENS`. This value is applied in both cases, regardless of whether a `config` argument is provided or not.","Both clients fall back to the same constant: DEFAULT_MAX_TOKENS.

- In graphiti_core/llm_client/anthropic_client.py AnthropicClient.__init__ has a max_tokens param defaulting to DEFAULT_MAX_TOKENS and, when config is None, does config.max_tokens = max_tokens.  
- In graphiti_core/llm_client/groq_client.py GroqClient.__init__ sets config.max_tokens = DEFAULT_MAX_TOKENS if you pass in no config or one with max_tokens=None.","['MODULE graphiti.graphiti_core.llm_client.anthropic_client contains CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.llm_client.groq_client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.groq_client.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.max_tokens\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'METHOD graphiti.graphiti_core.llm_client.groq_client.GroqClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.groq_client.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has method graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient.__init__\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 143, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.12631392478943,797
Which client subroutine does the graph initialization routine call to enforce database indexes and constraints?,The client subroutine called by the graph initialization routine to enforce database indexes and constraints in Graphiti is `ZepGraphiti.build_indices_and_constraints()`.,The init routine calls the client’s build_indices_and_constraints subroutine (i.e. ZepGraphiti.build_indices_and_constraints() in server/graph_service/zep_graphiti.py).,"[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.utils.maintenance.graph_data_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.graph_data_operations.index_queries', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.models.nodes.node_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.nodes.node_db_queries.ENTITY_NODE_SAVE_BULK', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 144, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.961145877838135,798
How are multiple group IDs handled in the search flow from the incoming request model to the Graphiti client?,"Multiple group IDs are handled by passing them as a list to the Graphiti client, which then uses this list to construct an IN filter in the Neo4j query. The `SearchQuery` class declares a list of strings called `group_ids`, and when multiple IDs are sent in the JSON payload, they are parsed into a Python list. This list is passed directly to the ZepGraphiti client, which builds the IN `$group_ids` filter in the Neo4j query.","Incoming `group_ids` are declared as a list of strings on the Pydantic model (server/graph_service/dto/retrieve.py: SearchQuery.group_ids), so when you send multiple IDs in your JSON payload they’re parsed into a Python list. In server/graph_service/routers/retrieve.py the router simply does

```python
await graphiti.search(
  group_ids=query.group_ids,
  …
)
```

passing that list straight through to your ZepGraphiti client (instantiated in server/graph_service/zep_graphiti.py). The underlying Graphiti.search implementation then uses the list to build an `IN $group_ids` filter in the Neo4j query.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n# Graphiti MCP Server\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nThis is an experimental Model Context Protocol (MCP) server implementation for Graphiti. The MCP server exposes\nGraphiti's key functionality through the MCP protocol, allowing AI assistants to interact with Graphiti's knowledge\ngraph capabilities.\n\n## Features\n\nThe Graphiti MCP server exposes the following key high-level functions of Graphiti:\n\n- **Episode Management**: Add, retrieve, and delete episodes (text, messages, or JSON data)\n- **Entity Management**: Search and manage entity nodes and relationships in the knowledge graph\n- **Search Capabilities**: Search for facts (edges) and node summaries using semantic and hybrid search\n- **Group Management**: Organize and manage groups of related data with group_id filtering\n- **Graph Maintenance**: Clear the graph and rebuild indices\n\n## Quick Start\n\n### Clone the Graphiti GitHub repo\n\n```bash\ngit clone https://github.com/getzep/graphiti.git\n```\n\nor\n\n```bash\ngh repo clone getzep/graphiti\n```\n\n### For Claude Desktop and other `stdio` only clients\n\n1. Note the full path to this directory.\n\n```\ncd graphiti && pwd\n```\n\n2. Install the [Graphiti prerequisites](#prerequisites).\n\n3. Configure Claude, Cursor, or other MCP client to use [Graphiti with a `stdio` transport](#integrating-with-mcp-clients). See the client documentation on where to find their MCP configuration files.\n\n### For Cursor and other `sse`-enabled clients\n\n1. Change directory to the `mcp_server` directory\n\n`cd graphiti/mcp_server`\n\n2. Start the service using Docker Compose\n\n`docker compose up`\n\n3. Point your MCP client to `http://localhost:8000/sse`\n\n## Installation\n\n### Prerequisites\n\n1. Ensure you have Python 3.10 or higher installed.\n2. A running Neo4j database (version 5.26 or later required)\n3. OpenAI API key for LLM operations\n\n### Setup\n\n1. Clone the repository and navigate to the mcp_server directory\n2. Use `uv` to create a virtual environment and install dependencies:\n\n```bash\n# Install uv if you don't have it already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a virtual environment and install dependencies in one step\nuv sync\n```\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `NEO4J_URI`: URI for the Neo4j database (default: `bolt://localhost:7687`)\n- `NEO4J_USER`: Neo4j username (default: `neo4j`)\n- `NEO4J_PASSWORD`: Neo4j password (default: `demodemo`)\n- `OPENAI_API_KEY`: OpenAI API key (required for LLM operations)\n- `OPENAI_BASE_URL`: Optional base URL for OpenAI API\n- `MODEL_NAME`: OpenAI model name to use for LLM operations.\n- `SMALL_MODEL_NAME`: OpenAI model name to use for smaller LLM operations.\n- `LLM_TEMPERATURE`: Temperature for LLM responses (0.0-2.0).\n- `AZURE_OPENAI_ENDPOINT`: Optional Azure OpenAI LLM endpoint URL"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 151, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",36.54792666435242,799
At what point in the episode retrieval flow is the clause to filter by group_id conditionally added?,"The GROUP_ID clause is conditionally added to the Cypher query right after the MATCH … WHERE e.valid_at <= $reference_time line, through the `retrieve_episodes` function in `graphiti_core/utils/maintenance/graph_data_operations.py`. This is achieved by concatenating `group_id_filter` into the Cypher query.","The `GROUP_ID` clause is injected in the helper retrieve_episodes (in graphiti_core/utils/maintenance/graph_data_operations.py). Right at the top of that function it does:

```python
group_id_filter = '\nAND e.group_id IN $group_ids' if group_ids and len(group_ids)>0 else ''
```

and then concatenates `group_id_filter` into the Cypher right after the `MATCH … WHERE e.valid_at <= $reference_time` line.","['CLASS graphiti.graphiti_core.errors.GroupIdValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a group_id contains invalid characters.', 'MODULE graphiti.examples.podcast.podcast_runner contains GLOBAL_VARIABLE graphiti.examples.podcast.podcast_runner.group_id', ""CLASS qlib.qlib.rl.utils.log.LogBuffer has method qlib.qlib.rl.utils.log.LogBuffer.episode_metrics\nRetrieve the numeric metrics of the latest episode.\nKeep all numbers in memory.  Objects that can't be aggregated like strings, tensors, images can't be stored in the buffer. To persist them, please use :class:`PickleWriter`.  Every time, Log buffer receives a new metric, the callback is triggered, which is useful when tracking metrics inside a trainer.  Parameters ---------- callback     A callback receiving three arguments:      - on_episode: Whether it's called at the end of an episode     - on_collect: Whether it's called at the end of a collect     - log_buffer: the :class:`LogBbuffer` object      No return value is expected."", 'CLASS graphiti.graphiti_core.search.search_config.EpisodeReranker inherits from or is a subclass of enum.Enum', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.validate_group_id\nValidate that a group_id contains only ASCII alphanumeric characters, dashes, and underscores.  Args:     group_id: The group_id to validate  Returns:     True if valid, False otherwise  Raises:     GroupIdValidationError: If group_id contains invalid characters', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 33}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 33, 'n_files_pr': 11, 'pr': 152, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.739872217178345,800
"In the search handler, how is the query’s max_facts value applied to limit the number of graph search results?","The query's max_facts value is applied to limit the number of graph search results by passing it directly into the num_results parameter of graphiti.search(...). This effectively caps the search results to query.max_facts edges. The graph search implementation in ZepGraphitiDep returns at most the number of edges specified by its num_results parameter, which is then used to determine the maximum number of search results.","In server/graph_service/routers/retrieve.py’s search handler, `query.max_facts` is passed straight into the `num_results` parameter of `graphiti.search(...)`. That tells the graph search implementation (in ZepGraphitiDep) to return at most that many edges, effectively capping your results to `max_facts`.","['CLASS qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager has method qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager.update_score\nUpdates the results from last optimisation run.  Args:   parameters: Hyperparameters used in optimisation.   loss: Validation loss obtained.   model: Model to serialised if required.   info: Any ancillary information to tag on to results.  Returns:   Boolean flag indicating if the model is the best seen so far.\nManages hyperparameter optimisation using random search for a single GPU.  Attributes:   param_ranges: Discrete hyperparameter range for random search.   results: Dataframe of validation results.   fixed_params: Fixed model parameters per experiment.   saved_params: Dataframe of parameters trained.   best_score: Minimum validation loss observed thus far.   optimal_name: Key to best configuration.   hyperparam_folder: Where to save optimisation outputs.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.max_facts', 'CLASS cognee.cognee.modules.retrieval.graph_summary_completion_retriever.GraphSummaryCompletionRetriever has method cognee.cognee.modules.retrieval.graph_summary_completion_retriever.GraphSummaryCompletionRetriever.__init__\nInitialize retriever with default prompt paths and search parameters.\nRetriever for handling graph-based completion searches with summarized context.  This class inherits from the GraphCompletionRetriever and is intended to manage the retrieval of graph edges with an added functionality to summarize the retrieved information efficiently. Public methods include:  - __init__() - resolve_edges_to_text()', 'CLASS graphiti.graphiti_core.edges.EpisodicEdge has field or attribute or property graphiti.graphiti_core.edges.EpisodicEdge.limit_query', 'CLASS qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager has method qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager.get_best_params\nReturns the optimal hyperparameters thus far.\nManages hyperparameter optimisation using random search for a single GPU.  Attributes:   param_ranges: Discrete hyperparameter range for random search.   results: Dataframe of validation results.   fixed_params: Fixed model parameters per experiment.   saved_params: Dataframe of parameters trained.   best_score: Minimum validation loss observed thus far.   optimal_name: Key to best configuration.   hyperparam_folder: Where to save optimisation outputs.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 153, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.881530046463013,801
What is the default value for group_ids when it is not provided in a search query?,"The default value for `group_ids` when it is not provided in a search query is `None`, as specified by the `Field(None...)` default definition in the `SearchQuery` class.","In SearchQuery (server/graph_service/dto/retrieve.py), group_ids is defined as list[str] | None = Field(None…), so if you don’t supply it, it defaults to None.","['CLASS qlib.qlib.utils.index_data.Index has method qlib.qlib.utils.index_data.Index.index\nGiven the index value, get the integer index  Parameters ---------- item :     The item to query  Returns ------- int:     The index of the item  Raises ------ KeyError:     If the query item does not exist\nThis is for indexing(rows or columns)  Read-only operations has higher priorities than others. So this class is designed in a **read-only** way to shared data for queries. Modifications will results in new Index.  NOTE: the indexing has following flaws - duplicated index value is not well supported (only the first appearance will be considered) - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'MODULE keras.keras.src.backend.jax.nn contains FUNCTION keras.keras.src.backend.jax.nn.wrap_flash_attention\nApplies a wrapped flash attention mechanism using the Splash kernel. This function prepares the appropriate attention mask (causal or custom), constructs a multi-head mask, and applies the Splash multi-head attention kernel to the provided query, key, and value tensors. It supports optional sharding and soft capping of attention logits. Args:     query: jax.Array. The query tensor of shape         (batch, num_heads, seq_len, head_dim).     key: jax.Array. The key tensor of shape         (batch, num_heads, seq_len, head_dim).     value: jax.Array. The value tensor of shape         (batch, num_heads, seq_len, head_dim).     decoder_segment_ids: Optional. Segment IDs for the decoder, used for         sharding or masking.     custom_mask: Optional[jax.Array]. A custom attention mask to apply. If         None, a causal mask is used.     attn_logits_soft_cap: Optional[float]. If provided, applies a soft cap         to the attention logits.     head_shards: int, default=1. Number of shards for the attention heads.     q_seq_shards: int, default=1. Number of shards for the query sequence         dimension. Returns:     jax.Array: The result of applying the Splash multi-head attention         kernel to the inputs. Raises:     AssertionError: If sharding along the sequence dimension is attempted         with decoder_segment_ids.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md\n# Crowd Source Data\n\n## Initiative\nPublic data source like yahoo is flawed, it might miss data for stock which is delisted and it might have data which is wrong. This can introduce survivorship bias into our training process.\n\nThe Crowd Source Data is introduced to merged data from multiple data source and cross validate against each other, so that:\n1. We will have a more complete history record.\n2. We can identify the anomaly data and apply correction when necessary.\n\n## Related Repo\nThe raw data is hosted on dolthub repo: https://www.dolthub.com/repositories/chenditc/investment_data\n\nThe processing script and sql is hosted on github repo: https://github.com/chenditc/investment_data\n\nThe packaged docker runtime is hosted on dockerhub: https://hub.docker.com/repository/docker/chenditc/investment_data\n\n## How to use it in qlib\n### Option 1: Download release bin data\nUser can download data in qlib bin format and use it directly: https://github.com/chenditc/investment_data/releases/latest\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```\n\n### Option 2: Generate qlib data from dolthub\nDolthub data will be update daily, so that if user wants to get up to date data, they can dump qlib bin using docker:\n```\ndocker run -v /<some output directory>:/output -it --rm chenditc/investment_data bash dump_qlib_bin.sh && cp ./qlib_bin.tar.gz /output/\n```\n\n## FAQ and other info\nSee: https://github.com/chenditc/investment_data/blob/main/README.md']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 154, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",44.56746578216553,802
What is the sequence of retrieval and deletion operations performed when a group is deleted?,"The sequence of retrieval and deletion operations performed when a group is deleted in Graphiti's ZepGraphiti.delete_group method are as follows:

1. Retrieval of entity edges by calling EntityEdge.get_by_group_ids.
2. Deletion of each edge by calling `edge.delete(self.driver)`. If an error occurs, it falls back to an empty list.
3. Retrieval of entity nodes by calling EntityNode.get_by_group_ids.
4. Deletion of each entity node by calling `node.delete(self.driver)`.
5. Retrieval of episodic nodes by calling EpisodicNode.get_by_group_ids.
6. Deletion of each episodic node by calling `episode.delete(self.driver)`. This sequence ensures that edges are deleted before entity nodes and entity nodes are deleted before episodic nodes, maintaining the integrity of the graph during group deletion operations.","In `server/graph_service/zep_graphiti.py` – ZepGraphiti.delete_group does:

1. Retrievals (all via `get_by_group_ids`):
   • Entity edges: EntityEdge.get_by_group_ids  
     (catches `GroupsEdgesNotFoundError`, falls back to `[]`)  
   • Entity nodes: EntityNode.get_by_group_ids  
   • Episodic nodes: EpisodicNode.get_by_group_ids  

2. Deletions (in order):
   • for each edge → `edge.delete(self.driver)`  
   • for each entity node → `node.delete(self.driver)`  
   • for each episodic node → `episode.delete(self.driver)`","['CLASS cognee.cognee.modules.retrieval.base_retriever.BaseRetriever inherits from or is a subclass of abc.ABC\nBase class for all retrieval operations.', 'MODULE graphiti.graphiti_core.utils.maintenance.graph_data_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.graph_data_operations.group_id_filter', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.password\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.driver.driver.GraphDriver has method graphiti.graphiti_core.driver.driver.GraphDriver.delete_all_indexes', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 4, 'pr': 155, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",41.082399129867554,803
"In the get_graphiti dependency provider, which Settings attributes are applied to configure the LLM client before yielding the Graphiti instance?","The Settings attributes that are applied to configure the LLM client before yielding the Graphiti instance in the get_graphiti dependency provider are:

* `openai_base_url` is applied to `client.llm_client.config.base_url`
* `openai_api_key` is applied to `client.llm_client.config.api_key`
* `model_name` is applied to `client.llm_client.model`

These settings are applied before yielding the ZepGraphiti instance, as mentioned in the provided code snippet.","In `server/graph_service/zep_graphiti.py`’s `get_graphiti` provider the following `Settings` fields (from `server/graph_service/config.py`) are applied to the LLM client before yielding the `ZepGraphiti` instance:

• `openai_base_url` → `client.llm_client.config.base_url`  
• `openai_api_key` → `client.llm_client.config.api_key`  
• `model_name`       → `client.llm_client.model`","['MODULE cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.get_llm_client contains FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.get_llm_client.get_llm_client\nGet the LLM client based on the configuration using Enums.  This function retrieves the configuration for the LLM provider and model, and initializes the appropriate LLM client adapter accordingly. It raises an LLMAPIKeyNotSetError if the LLM API key is not set for certain providers or if the provider is unsupported.  Returns: --------      An instance of the appropriate LLM client adapter based on the provider     configuration.', 'CLASS graphiti.graphiti_core.llm_client.client.LLMClient has field or attribute or property graphiti.graphiti_core.llm_client.client.config', 'FUNCTION cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.get_llm_client.get_llm_client has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.get_llm_client.get_llm_client.raise_api_key_error\nGet the LLM client based on the configuration using Enums.  This function retrieves the configuration for the LLM provider and model, and initializes the appropriate LLM client adapter accordingly. It raises an LLMAPIKeyNotSetError if the LLM API key is not set for certain providers or if the provider is unsupported.  Returns: --------      An instance of the appropriate LLM client adapter based on the provider     configuration.', 'MODULE graphiti.graphiti_core.llm_client.client contains CLASS graphiti.graphiti_core.llm_client.client.LLMClient', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.user\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 20}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 20, 'n_files_pr': 12, 'pr': 156, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",34.99902367591858,804
"Describe how a search request is processed end-to-end, including query sanitization and concurrent execution of search methods.","A search request is processed end-to-end in Graphiti as follows: The query string is trimmed and newlines are replaced to ensure it's clean for processing. The raw query is then passed through `lucene_sanitize` and built into group_id filters, enforcing a max-length constraint. Concurrently, four high-level methods (edge_search, node_search, episode_search, and community_search) are executed via semaphore_gather. Each of these methods executes individual sub-searches in parallel using semaphore_gather, merges the results, and reranks them based on the specified ranking algorithms (RRF, MMR, Cross-Encoder, or Node-Distance). The final SearchResults object is created by wrapping the four result lists together and returned as a response to the user's query.","1. The user calls Graphiti.search_(…) (which under the hood invokes graphiti_core/search/search.py::search).  
2. In search():  
   - Blank queries are short-circuited.  
   - The query string is trimmed and newlines are replaced; if no query_vector is provided it is fetched via embedder.create(input_data=[query]).  
3. When any full-text search runs (e.g. edge_fulltext_search, node_fulltext_search, community_fulltext_search), the raw query is passed through graphiti_core/search/search_utils.py::fulltext_query, which calls lucene_sanitize, builds group_id filters, and enforces max-length.  
4. search() then concurrently executes four high-level methods via semaphore_gather:  
   - edge_search (which itself does concurrent full-text, similarity and BFS searches),  
   - node_search (ditto),  
   - episode_search,  
   - community_search.  
5. Inside each *search():  
   - individual sub-searches (full-text, vector similarity, BFS) run in parallel via semaphore_gather.  
   - Results are merged, then reranked by RRF/MMR/Cross-Encoder/Node-Distance according to the config.  
6. Finally the four result lists are wrapped into a SearchResults and returned.","['CLASS cognee.cognee.modules.retrieval.natural_language_retriever.NaturalLanguageRetriever inherits from or is a subclass of cognee.modules.retrieval.base_retriever.BaseRetriever\nRetriever for handling natural language search.  Public methods include:  - get_context: Retrieves relevant context using a natural language query converted to Cypher. - get_completion: Returns a completion based on the query and context.', 'MODULE cognee.cognee.modules.search.operations.get_history contains GLOBAL_VARIABLE cognee.cognee.modules.search.operations.get_history.results_query', 'MODULE cognee.cognee.modules.retrieval.natural_language_retriever contains CLASS cognee.cognee.modules.retrieval.natural_language_retriever.NaturalLanguageRetriever\nRetriever for handling natural language search.  Public methods include:  - get_context: Retrieves relevant context using a natural language query converted to Cypher. - get_completion: Returns a completion based on the query and context.', 'MODULE cognee.cognee.modules.search.operations.get_history contains GLOBAL_VARIABLE cognee.cognee.modules.search.operations.get_history.queries_query', 'CLASS cognee.cognee.modules.retrieval.natural_language_retriever.NaturalLanguageRetriever has field or attribute or property cognee.cognee.modules.retrieval.natural_language_retriever.NaturalLanguageRetriever.max_attempts\nRetriever for handling natural language search.  Public methods include:  - get_context: Retrieves relevant context using a natural language query converted to Cypher. - get_completion: Returns a completion based on the query and context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/README.md\n# QA Evaluation\n\nRepeated runs of QA evaluation on 24-item HotpotQA subset, comparing Mem0, Graphiti, LightRAG, and Cognee (multiple retriever configs). Uses Modal for distributed benchmark execution.\n\n## Dataset\n\n- `hotpot_qa_24_corpus.json` and `hotpot_qa_24_qa_pairs.json`\n- `hotpot_qa_24_instance_filter.json` for instance filtering\n\n## Systems Evaluated\n\n- **Mem0**: OpenAI-based memory QA system\n- **Graphiti**: LangChain + Neo4j knowledge graph QA\n- **LightRAG**: Falkor's GraphRAG-SDK\n- **Cognee**: Multiple retriever configurations (GRAPH_COMPLETION, GRAPH_COMPLETION_COT, GRAPH_COMPLETION_CONTEXT_EXTENSION)\n\n## Project Structure\n\n- `src/` - Analysis scripts and QA implementations\n- `src/modal_apps/` - Modal deployment configurations\n- `src/qa/` - QA benchmark classes\n- `src/helpers/` and `src/analysis/` - Utilities\n\n**Notes:**\n- Use `PyProject.toml` for dependencies\n- Ensure Modal CLI is configured\n- Modular QA benchmark classes enable parallel execution on other platforms beyond Modal\n\n\n## Running Benchmarks (Modal)\n\nExecute repeated runs via Modal apps:\n- `modal run modal_apps/modal_qa_benchmark_<system>.py`\n\nWhere `<system>` is one of: `mem0`, `graphiti`, `lightrag`, `cognee`\n\nRaw results stored in Modal volumes under `/qa-benchmarks/<benchmark>/{answers,evaluated}`\n\n## Results Analysis\n\n- `python run_cross_benchmark_analysis.py`\n- Downloads Modal volumes, processes evaluated JSONs\n- Generates per-benchmark CSVs and cross-benchmark summary\n- Use `visualize_benchmarks.py` to create comparison charts\n\n## Results\n\n- **45 evaluation cycles** on 24 HotPotQA questions with multiple metrics (EM, F1, DeepEval Correctness, Human-like Correctness)\n- **Significant variance** observed in metrics across small runs due to LLM-as-judge inconsistencies\n- **Cognee showed consistent improvements** across all measured dimensions compared to Mem0, Lightrag, and Graphiti\n\n### Visualization Results\n\nThe following charts visualize the benchmark results and performance comparisons:\n\n#### Comprehensive Metrics Comparison\n![Comprehensive Metrics Comparison](comprehensive_metrics_comparison.png)\n\nA comprehensive comparison of all evaluated systems across multiple metrics, showing Cognee's performance relative to Mem0, Graphiti, and LightRAG.\n\n\n\n\n#### Optimized Cognee Configurations\n![Optimized Cognee Configurations](optimized_cognee_configurations.png)\n\nPerformance analysis of different Cognee retriever configurations (GRAPH_COMPLETION, GRAPH_COMPLETION_COT, GRAPH_COMPLETION_CONTEXT_EXTENSION), showing optimization results.\n\n## Notes\n\n- **Traditional QA metrics (EM/F1)** miss core value of AI memory systems - measure letter/word differences rather than information content\n- **HotPotQA benchmark mismatch** - designed for multi-hop reasoning but operates in constrained contexts vs. real-world cross-context linking\n- **DeepEval variance** - LLM-as-judge evaluation carries inconsistencies of underlying language model""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 16}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 16, 'n_files_pr': 12, 'pr': 157, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",55.42233657836914,805
"What central method does the system use to generate embeddings for facts, node names, and arbitrary text after the refactor?","After the refactor, the central method used to generate embeddings for facts, node names, and arbitrary text in graphiti is `embedder.create(input_data=[text])`. This method is called under the hood by various methods such as `EntityEdge.generate_embedding`, `EntityNode.generate_name_embedding`, and `CommunityNode.generate_name_embedding`.","All embedding calls now go through the same EmbedderClient.create(...) API (wrapped by the helper in graphiti_core/llm_client/utils.py – the generate_embedding function). Facts (EntityEdge.generate_embedding), node names (EntityNode/CommunityNode.generate_name_embedding) and arbitrary text all just call embedder.create(input_data=[text]) under the hood.","['CLASS cognee.cognee.infrastructure.databases.vector.embeddings.FastembedEmbeddingEngine.FastembedEmbeddingEngine has method cognee.cognee.infrastructure.databases.vector.embeddings.FastembedEmbeddingEngine.FastembedEmbeddingEngine.get_tokenizer\nInstantiate and return the tokenizer used for preparing text for embedding.  Returns: --------      A tokenizer object configured for the specified model and maximum token size.\nManages the embedding process using a specified model to generate text embeddings.  Public methods:  - embed_text - get_vector_size - get_tokenizer  Instance variables:  - model: The name of the embedding model. - dimensions: The dimensionality of the embeddings. - mock: A flag indicating whether to use mocking instead of the actual embedding model. - MAX_RETRIES: The maximum number of retries for embedding operations.', 'MODULE graphiti.graphiti_core.edges contains GLOBAL_VARIABLE graphiti.graphiti_core.edges.fact_embeddings', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.fact_embedding', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 24}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 24, 'n_files_pr': 18, 'pr': 159, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",28.36073350906372,806
What two updates were made regarding Lucene query escaping and episode grouping?,"Two updates were made regarding Lucene query escaping and episode grouping in graphiti:

1. A new helper function named `lucene_sanitize(query: str)` was added to the file `graphiti_core/helpers.py`, which properly escapes all Lucene-special characters, including `/`, `ackslash`, `+`, `-`, `&&` sequence, and `||` sequence.

2. The `group_episodes` function (or the `EpisodeGrouper` class) in the file `graphiti_core/episodes.py` has been revamped to group multi-part and single-part episodes by their parent title and original air date, fixing the old one-by-one grouping of episodes.","1. In graphiti_core/helpers.py we introduced the new lucene_sanitize(query: str) helper to properly escape all Lucene‐special characters (including “/” and “\” plus the full “+ - && || …” set).

2. In graphiti_core/episodes.py we revamped group_episodes (or the EpisodeGrouper class) so that multi-part and single-part episodes are now grouped by their parent title and original air date, fixing the old one-by-one grouping.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.graphiti_core.search.search_config.EpisodeReranker inherits from or is a subclass of enum.Enum', 'MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'CLASS graphiti.graphiti_core.search.search_config.EpisodeSearchMethod inherits from or is a subclass of enum.Enum', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 170, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.70737624168396,807
Which Python methods are used to construct and apply the character escape mapping in the query sanitization function?,The Python methods used to construct and apply the character escape mapping in the query sanitization function in graphiti are `str.maketrans` and `str.translate`. These methods are used in the `lucene_sanitize` function defined in the `graphiti_core/helpers.py` file to build an escape map and then apply it to the query.,In lucene_sanitize (graphiti_core/helpers.py) the escape map is built with Python’s str.maketrans and then applied to the query via str.translate.,"['FUNCTION keras.keras.src.datasets.imdb.get_word_index has parameter or accepts argument keras.keras.src.datasets.imdb.get_word_index.path\nRetrieves a dict mapping words to their index in the IMDB dataset.  Args:     path: where to cache the data (relative to `~/.keras/dataset`).  Returns:     The word index dictionary. Keys are word strings, values are their     index.  Example:  ```python # Use the default parameters to keras.datasets.imdb.load_data start_char = 1 oov_char = 2 index_from = 3 # Retrieve the training sequences. (x_train, _), _ = keras.datasets.imdb.load_data(     start_char=start_char, oov_char=oov_char, index_from=index_from ) # Retrieve the word index file mapping words to indices word_index = keras.datasets.imdb.get_word_index() # Reverse the word index to obtain a dict mapping indices to words # And add `index_from` to indices to sync with `x_train` inverted_word_index = dict(     (i + index_from, word) for (word, i) in word_index.items() ) # Update `inverted_word_index` to include `start_char` and `oov_char` inverted_word_index[start_char] = ""[START]"" inverted_word_index[oov_char] = ""[OOV]"" # Decode the first sequence in the dataset decoded_sequence = "" "".join(inverted_word_index[i] for i in x_train[0]) ```', 'FUNCTION graphiti.graphiti_core.helpers.lucene_sanitize has parameter or accepts argument graphiti.graphiti_core.helpers.lucene_sanitize.query', 'MODULE keras.keras.src.datasets.imdb contains FUNCTION keras.keras.src.datasets.imdb.get_word_index\nRetrieves a dict mapping words to their index in the IMDB dataset.  Args:     path: where to cache the data (relative to `~/.keras/dataset`).  Returns:     The word index dictionary. Keys are word strings, values are their     index.  Example:  ```python # Use the default parameters to keras.datasets.imdb.load_data start_char = 1 oov_char = 2 index_from = 3 # Retrieve the training sequences. (x_train, _), _ = keras.datasets.imdb.load_data(     start_char=start_char, oov_char=oov_char, index_from=index_from ) # Retrieve the word index file mapping words to indices word_index = keras.datasets.imdb.get_word_index() # Reverse the word index to obtain a dict mapping indices to words # And add `index_from` to indices to sync with `x_train` inverted_word_index = dict(     (i + index_from, word) for (word, i) in word_index.items() ) # Update `inverted_word_index` to include `start_char` and `oov_char` inverted_word_index[start_char] = ""[START]"" inverted_word_index[oov_char] = ""[OOV]"" # Decode the first sequence in the dataset decoded_sequence = "" "".join(inverted_word_index[i] for i in x_train[0]) ```', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.lucene_sanitize', 'METHOD cognee.cognee.infrastructure.data.chunking.LangchainChunkingEngine.LangchainChunkEngine.chunk_data_by_character has parameter or accepts argument cognee.cognee.infrastructure.data.chunking.LangchainChunkingEngine.LangchainChunkEngine.chunk_data_by_character.chunk_size\nChunk data based on character count.  Apply a character-based text splitter to divide the input data into chunks of specified size and overlap, returning the content and the chunk indices.  Parameters: -----------      - data_chunks: The data to be chunked based on character count.     - chunk_size: The maximum number of characters allowed in each chunk. (default 1500)     - chunk_overlap: The number of characters that overlap between chunks. (default 10)  Returns: --------      A tuple comprising the content of the character chunks and their indexed     representations.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 171, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.847127199172974,808
"When the graph update routine completes, what structured response is provided and what data does it encapsulate?","When the graph update routine completes, the structured response provided is an `AddEpisodeResults` object. This object bundles an episode (an `EpisodicNode` that was created or updated), a list of nodes (`EntityNode` objects), and a list of edges (`EntityEdge` objects). The `AddEpisodeResults` object encapsulates this data to provide a comprehensive update on the graph after the update routine completes.","The method returns an AddEpisodeResults (defined in graphiti_core/graphiti.py) which bundles:

• episode: the EpisodicNode that was created or updated  
• nodes: a list of all resolved EntityNode objects  
• edges: a list of all EntityEdge objects added or invalidated","['MODULE cognee.cognee.modules.retrieval.code_retriever contains CLASS cognee.cognee.modules.retrieval.code_retriever.CodeQueryInfo\nModel for representing the result of a query related to code files.  This class holds a list of filenames and the corresponding source code extracted from a query. It is used to encapsulate response data in a structured format.', 'MODULE graphiti.server.graph_service.dto.retrieve contains CLASS graphiti.server.graph_service.dto.retrieve.GetMemoryResponse', 'CLASS cognee.cognee.modules.retrieval.code_retriever.CodeQueryInfo inherits from or is a subclass of pydantic.BaseModel\nModel for representing the result of a query related to code files.  This class holds a list of filenames and the corresponding source code extracted from a query. It is used to encapsulate response data in a structured format.', 'CLASS graphiti.server.graph_service.dto.common.Result has field or attribute or property graphiti.server.graph_service.dto.common.Result.success', 'CLASS cognee.cognee.modules.retrieval.code_retriever.CodeQueryInfo has field or attribute or property cognee.cognee.modules.retrieval.code_retriever.CodeQueryInfo.filenames\nModel for representing the result of a query related to code files.  This class holds a list of filenames and the corresponding source code extracted from a query. It is used to encapsulate response data in a structured format.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 172, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.034337043762207,809
Which search pipelines invoke the MMR reranker and what embedding vectors do they pass into it?,"The search pipelines that invoke the MMR reranker are edge_search, node_search, and community_search. The edge_search pipeline passes fact_embedding for edges into maximal_marginal_relevance. The node_search pipeline passes name_embedding for nodes into maximal_marginal_relevance. The community_search pipeline also passes name_embedding for communities into maximal_marginal_relevance.","The only “first‐class” pipelines that ever call the MMR reranker are in graphiti_core/search/search.py:

• edge_search (EdgeReranker.mmr)  
  – loads each candidate’s fact_embedding via get_embeddings_for_edges(…)  
  – calls maximal_marginal_relevance(query_vector, <edge_uuid, fact_embedding>…)

• node_search (NodeReranker.mmr)  
  – loads each candidate’s name_embedding via get_embeddings_for_nodes(…)  
  – calls maximal_marginal_relevance(query_vector, <node_uuid, name_embedding>…)

• community_search (CommunityReranker.mmr)  
  – loads each candidate’s name_embedding via get_embeddings_for_communities(…)  
  – calls maximal_marginal_relevance(query_vector, <community_uuid, name_embedding>…)

In every case the two inputs to MMR are:  
1. query_vector – the embedding of your search query (passed into the pipeline)  
2. the list of candidate embeddings (fact_embedding for edges; name_embedding for nodes/communities) fetched by the get_embeddings_for_* calls.","['CLASS cognee.cognee.infrastructure.databases.vector.embeddings.LiteLLMEmbeddingEngine.LiteLLMEmbeddingEngine has method cognee.cognee.infrastructure.databases.vector.embeddings.LiteLLMEmbeddingEngine.LiteLLMEmbeddingEngine.get_vector_size\nRetrieve the dimensionality of the embedding vectors.  Returns: --------      - int: The size (dimensionality) of the embedding vectors.\nEngine for embedding text using a specific LLM model, supporting mock and actual embedding calls.  Public methods: - embed_text: Embed a list of strings into vector representations. - get_vector_size: Retrieve the size of the embedding vectors. - get_tokenizer: Load the appropriate tokenizer for the specified model.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.DEFAULT_MMR_LAMBDA', 'MODULE cognee.cognee.infrastructure.databases.vector.embeddings.LiteLLMEmbeddingEngine contains CLASS cognee.cognee.infrastructure.databases.vector.embeddings.LiteLLMEmbeddingEngine.LiteLLMEmbeddingEngine\nEngine for embedding text using a specific LLM model, supporting mock and actual embedding calls.  Public methods: - embed_text: Embed a list of strings into vector representations. - get_vector_size: Retrieve the size of the embedding vectors. - get_tokenizer: Load the appropriate tokenizer for the specified model.', 'MODULE xgboost.python-package.xgboost.sklearn contains CLASS xgboost.python-package.xgboost.sklearn.XGBRanker', 'CLASS cognee.cognee.infrastructure.databases.vector.embeddings.LiteLLMEmbeddingEngine.LiteLLMEmbeddingEngine has method cognee.cognee.infrastructure.databases.vector.embeddings.LiteLLMEmbeddingEngine.LiteLLMEmbeddingEngine.__init__\nEngine for embedding text using a specific LLM model, supporting mock and actual embedding calls.  Public methods: - embed_text: Embed a list of strings into vector representations. - get_vector_size: Retrieve the size of the embedding vectors. - get_tokenizer: Load the appropriate tokenizer for the specified model.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.4 (2015.05.11)\n\n* Distributed version of xgboost that runs on YARN, scales to billions of examples\n* Direct save/load data and model from/to S3 and HDFS\n* Feature importance visualization in R module, by Michael Benesty\n* Predict leaf index\n* Poisson regression for counts data\n* Early stopping option in training\n* Native save load support in R and python\n  - xgboost models now can be saved using save/load in R\n  - xgboost python model is now pickable\n* sklearn wrapper is supported in python module\n* Experimental External memory version\n\n\n## v0.3 (2014.09.07)\n\n* Faster tree construction module\n  - Allows subsample columns during tree construction via ```bst:col_samplebytree=ratio```\n* Support for boosting from initial predictions\n* Experimental version of LambdaRank\n* Linear booster is now parallelized, using parallel coordinated descent.\n* Add [Code Guide](src/README.md) for customizing objective function and evaluation\n* Add R module\n\n\n## v0.2x (2014.05.20)\n\n* Python module\n* Weighted samples instances\n* Initial version of pairwise rank\n\n\n## v0.1 (2014.03.26)\n\n* Initial release', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### ROC-AUC\nWe re-implemented the ROC-AUC metric in XGBoost.  The new implementation supports\nmulti-class classification and has better support for learning to rank tasks that are not\nbinary.  Also, it has a better-defined average on distributed environments with additional\nhandling for invalid datasets. (#6749, #6747, #6797)\n\n### Global configuration.\nStarting from 1.4, XGBoost's Python, R and C interfaces support a new global configuration\nmodel where users can specify some global parameters.  Currently, supported parameters are\n`verbosity` and `use_rmm`.  The latter is experimental, see rmm plugin demo and\nrelated README file for details. (#6414, #6656)\n\n### Other New features.\n* Better handling for input data types that support `__array_interface__`.  For some\n  data types including GPU inputs and `scipy.sparse.csr_matrix`, XGBoost employs\n  `__array_interface__` for processing the underlying data.  Starting from 1.4, XGBoost\n  can accept arbitrary array strides (which means column-major is supported) without\n  making data copies, potentially reducing a significant amount of memory consumption.\n  Also version 3 of `__cuda_array_interface__` is now supported.  (#6776, #6765, #6459,\n  #6675)\n* Improved parameter validation, now feeding XGBoost with parameters that contain\n  whitespace will trigger an error. (#6769)\n* For Python and R packages, file paths containing the home indicator `~` are supported.\n* As mentioned in the Python changes summary, the JSON model can now save feature\n  information of the trained booster.  The JSON schema is updated accordingly. (#6605)\n* Development of categorical data support is continued.  Newly added weighted data support\n  and `dart` booster support. (#6508, #6693)\n* As mentioned in Dask change summary, ranking now supports the `qid` parameter for\n  query groups. (#6576)\n* `DMatrix.slice` can now consume a numpy array. (#6368)\n\n### Other breaking changes\n* Aside from the feature name generation, there are 2 breaking changes:\n  - Drop saving binary format for memory snapshot. (#6513, #6640)\n  - Change default evaluation metric for binary:logitraw objective to logloss (#6647)\n\n### CPU Optimization\n* Aside from the general changes on predict function, some optimizations are applied on\n  CPU implementation. (#6683, #6550, #6696, #6700)\n* Also performance for sampling initialization in `hist` is improved. (#6410)\n\n### Notable fixes in the core library\nThese fixes do not reside in particular language bindings:\n* Fixes for gamma regression.  This includes checking for invalid input values, fixes for\n  gamma deviance metric, and better floating point guard for gamma negative log-likelihood\n  metric. (#6778, #6537, #6761)\n* Random forest with `gpu_hist` might generate low accuracy in previous versions. (#6755)\n* Fix a bug in GPU sketching when data size exceeds limit of 32-bit integer. (#6826)\n* Memory consumption fix for row-major adapters (#6779)\n* Don't estimate sketch batch size when rmm is used. (#6807) (#6830)\n* Fix in-place predict with missing value. (#6787)\n* Re-introduce double buffer in UpdatePosition, to fix perf regression in gpu_hist (#6757)\n* Pass correct split_type to GPU predictor (#6491)\n* Fix DMatrix feature names/types IO. (#6507)\n* Use view for `SparsePage` exclusively to avoid some data access races. (#6590)\n* Check for invalid data. (#6742)\n* Fix relocatable include in CMakeList (#6734) (#6737)""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 28}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 28, 'n_files_pr': 11, 'pr': 180, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.29628586769104,810
What message does the error include when no nodes are found for the provided group IDs?,"The error message when no nodes are found for the provided group IDs in graphiti is: ""no nodes found for group ids {group_ids}"".","The `GroupsNodesNotFoundError` (in `graphiti_core/errors.py`) sets its message to:  
“no nodes found for group ids {group_ids}”","['MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError\nRaised when no nodes are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError has method graphiti.graphiti_core.errors.GroupsNodesNotFoundError.__init__\nRaised when no nodes are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError has field or attribute or property graphiti.graphiti_core.errors.group_ids\nRaised when no nodes are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError has field or attribute or property graphiti.graphiti_core.errors.GroupsNodesNotFoundError.message\nRaised when no nodes are found for a list of group ids.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError\nRaised when no edges are found for a list of group ids.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 185, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.18851399421692,811
Which edge processing step is explicitly skipped by the bulk ingestion method compared to the single-episode addition?,The edge processing step that is explicitly skipped by the bulk ingestion method compared to the single-episode addition in graphiti is the associated valid/invalid date extraction step.,The bulk‐load path in graphiti_core/graphiti.py::add_episode_bulk explicitly skips the edge‐invalidation (and associated valid/invalid date extraction) step that the per‐episode “add_episode” flow runs.,"['METHOD keras.keras.src.models.model.Model.load_weights has parameter or accepts argument keras.keras.src.models.model.Model.load_weights.skip_mismatch\nLoad the weights from a single file or sharded files.  Weights are loaded based on the network\'s topology. This means the architecture should be the same as when the weights were saved. Note that layers that don\'t have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don\'t have weights.  **Partial weight loading**  If you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting `skip_mismatch=True`. In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.  **Sharding**  When loading sharded weights, it is important to specify `filepath` that ends with `*.weights.json` which is used as the configuration file. Additionally, the sharded files `*_xxxxx.weights.h5` must be in the same directory as the configuration file.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`.     skip_mismatch: Boolean, whether to skip loading of layers where         there is a mismatch in the number of weights, or a mismatch in         the shape of the weights.  Example:  ```python # Load the weights in a single file. model.load_weights(""model.weights.h5"")  # Load the weights in sharded files. model.load_weights(""model.weights.json"") ```', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.episodes', 'METHOD keras.keras.src.models.model.Model.load_weights has parameter or accepts argument keras.keras.src.models.model.Model.load_weights.filepath\nLoad the weights from a single file or sharded files.  Weights are loaded based on the network\'s topology. This means the architecture should be the same as when the weights were saved. Note that layers that don\'t have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don\'t have weights.  **Partial weight loading**  If you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting `skip_mismatch=True`. In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.  **Sharding**  When loading sharded weights, it is important to specify `filepath` that ends with `*.weights.json` which is used as the configuration file. Additionally, the sharded files `*_xxxxx.weights.h5` must be in the same directory as the configuration file.  Args:     filepath: `str` or `pathlib.Path` object. Path where the weights         will be saved.  When sharding, the filepath must end in         `.weights.json`.     skip_mismatch: Boolean, whether to skip loading of layers where         there is a mismatch in the number of weights, or a mismatch in         the shape of the weights.  Example:  ```python # Load the weights in a single file. model.load_weights(""model.weights.h5"")  # Load the weights in sharded files. model.load_weights(""model.weights.json"") ```', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.episode_tuples', 'CLASS qlib.qlib.rl.order_execution.interpreter.FullHistoryStateInterpreter has method qlib.qlib.rl.order_execution.interpreter.FullHistoryStateInterpreter.__init__\nThe observation of all the history, including today (until this moment), and yesterday.  Parameters ---------- max_step     Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps. data_ticks     Equal to the total number of records. For example, in SAOE per minute,     the total ticks is the length of day in minutes. data_dim     Number of dimensions in data. processed_data_provider     Provider of the processed data.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 35}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 35, 'n_files_pr': 10, 'pr': 187, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",27.368958234786987,812
"When edge reranking uses the shortest-path strategy, which function computes the node distances and how are those node IDs mapped back to edges?","When edge reranking uses the shortest-path strategy in graphiti, the function that computes the node distances is `node_distance_reranker` located in the file `graphiti_core/search/search_utils.py`. This function fires a Cypher UNWIND/MATCH query to retrieve each node's distance to the center. The node IDs are mapped back to edges using the `source_to_edge_uuid_map`, which is built by the `edge_search` function in the file `graphiti_core/search/search.py`. The `edge_search` function iterates over the node UUIDs returned by `node_distance_reranker` and retrieves the corresponding edges from the `source_to_edge_uuid_map`. These retrieved edges are then used to produce the reranked edge list.","Edge-reranking with the shortest-path strategy is done in node_distance_reranker (graphiti_core/search/search_utils.py). It fires a Cypher UNWIND/MATCH query to get each node’s distance to the center. In edge_search (graphiti_core/search/search.py) you first build  
  source_to_edge_uuid_map: source_node_uuid → [edge.uuid]  
and then for each node UUID returned by node_distance_reranker you pull its edges via that map to produce the reranked edge list.","['CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Node has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Node.add_attribute\nRepresents a node in a graph. Attributes:     id (str): A unique identifier for the node.     attributes (Dict[str, Any]): A dictionary of attributes associated with the node.     neighbors (List[Node]): Represents the original nodes     skeleton_edges (List[Edge]): Represents the original edges', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_range_indices', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Node has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Node.is_node_alive_in_dimension\nRepresents a node in a graph. Attributes:     id (str): A unique identifier for the node.     attributes (Dict[str, Any]): A dictionary of attributes associated with the node.     neighbors (List[Node]): Represents the original nodes     skeleton_edges (List[Edge]): Represents the original edges', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_edges', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Node has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Node.remove_skeleton_edge\nRepresents a node in a graph. Attributes:     id (str): A unique identifier for the node.     attributes (Dict[str, Any]): A dictionary of attributes associated with the node.     neighbors (List[Node]): Represents the original nodes     skeleton_edges (List[Edge]): Represents the original edges', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 7, 'n_files_pr': 4, 'pr': 190, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",37.36657738685608,813
At what point in add_episode does the episode's content get cleared when raw content storage is disabled?,"In the `add_episode` method of the `Graphiti` class, the episode's content gets cleared when `self.store_raw_episode_content` is False at the point where it sets `episode.content` to an empty string. This occurs after calling `add_nodes_and_edges_bulk`. The code clears the episode content when raw storage is disabled.","In Graphiti.add_episode (graphiti_core/graphiti.py), right after setting `episode.entity_edges` and just before calling `add_nodes_and_edges_bulk`, there’s a check:

```
if not self.store_raw_episode_content:
    episode.content = ''
```

That’s where the content is cleared when raw storage is disabled.","['MODULE graphiti.graphiti_core.telemetry.telemetry contains FUNCTION graphiti.graphiti_core.telemetry.telemetry.get_graphiti_version\nGet Graphiti version.', 'CLASS graphiti.graphiti_core.graphiti.Graphiti has field or attribute or property graphiti.graphiti_core.graphiti.store_raw_episode_content', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.previous_episodes_list', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 191, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.65279722213745,814
Under what condition does fulltext_query return an empty string instead of constructing a query?,The function fulltext_query returns an empty string instead of constructing a query when the sanitized query is “too long.” A sanitized query is considered “too long” when the total number of words in the query and group IDs exceeds `MAX_QUERY_LENGTH`.,"In fulltext_query (graphiti_core/search/search_utils.py), it returns an empty string whenever the sanitized query is “too long” – i.e. when  
```python
len(lucene_sanitize(query).split(' ')) + len(group_ids or [])
>= MAX_QUERY_LENGTH
```","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.where\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', 'FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query has parameter or accepts argument graphiti.graphiti_core.search.search_utils.MAX_QUERY_LENGTH', 'CLASS graphiti.graphiti_core.llm_client.errors.EmptyResponseError inherits from or is a subclass of graphiti.graphiti_core.llm_client.errors.Exception\nException raised when the LLM returns an empty response.', 'FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query has parameter or accepts argument graphiti.graphiti_core.search.search_utils.fulltext_query.query', 'FUNCTION keras.keras.src.ops.numpy.where has parameter or accepts argument keras.keras.src.ops.numpy.where.condition\nReturn elements chosen from `x1` or `x2` depending on `condition`.  Args:     condition: Where `True`, yield `x1`, otherwise yield `x2`.     x1: Values from which to choose when `condition` is `True`.     x2: Values from which to choose when `condition` is `False`.  Returns:     A tensor with elements from `x1` where `condition` is `True`, and     elements from `x2` where `condition` is `False`.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 192, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",29.478574991226196,815
What do the three full-text search functions return if the query exceeds the maximum length?,"If the query exceeds the maximum length, the edge_fulltext_search, node_fulltext_search, and community_fulltext_search functions will return an empty list because fulltext_query(...) returns an empty string in such cases.","All three (edge_fulltext_search, node_fulltext_search and community_fulltext_search) check if fulltext_query(...) returns an empty string, and if so immediately return an empty list (`[]`).","['MODULE cognee.cognee.tasks.chunks.chunk_by_sentence contains FUNCTION cognee.cognee.tasks.chunks.chunk_by_sentence.chunk_by_sentence\nSplits text into sentences while preserving word and paragraph boundaries.  This function processes the input string, dividing it into sentences based on word-level tokenization. Each sentence is identified with a unique UUID, and it handles scenarios where the text may end mid-sentence by tagging it with a specific type. If a maximum sentence length is specified, the function ensures that sentences do not exceed this length, raising a ValueError if an individual word surpasses it. The function utilizes an external word processing function `chunk_by_word` to determine the structure of the text.  Parameters: -----------      - data (str): The input text to be split into sentences.     - maximum_size (Optional[int]): An optional limit on the maximum size of sentences       generated. (default None)', 'FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query has parameter or accepts argument graphiti.graphiti_core.search.search_utils.MAX_QUERY_LENGTH', 'FUNCTION cognee.cognee.tasks.chunks.chunk_by_sentence.chunk_by_sentence has parameter or accepts argument cognee.cognee.tasks.chunks.chunk_by_sentence.chunk_by_sentence.maximum_size\nSplits text into sentences while preserving word and paragraph boundaries.  This function processes the input string, dividing it into sentences based on word-level tokenization. Each sentence is identified with a unique UUID, and it handles scenarios where the text may end mid-sentence by tagging it with a specific type. If a maximum sentence length is specified, the function ensures that sentences do not exceed this length, raising a ValueError if an individual word surpasses it. The function utilizes an external word processing function `chunk_by_word` to determine the structure of the text.  Parameters: -----------      - data (str): The input text to be split into sentences.     - maximum_size (Optional[int]): An optional limit on the maximum size of sentences       generated. (default None)', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.MAX_QUERY_LENGTH', 'FUNCTION cognee.cognee.tasks.chunks.chunk_by_sentence.chunk_by_sentence has parameter or accepts argument cognee.cognee.tasks.chunks.chunk_by_sentence.chunk_by_sentence.data\nSplits text into sentences while preserving word and paragraph boundaries.  This function processes the input string, dividing it into sentences based on word-level tokenization. Each sentence is identified with a unique UUID, and it handles scenarios where the text may end mid-sentence by tagging it with a specific type. If a maximum sentence length is specified, the function ensures that sentences do not exceed this length, raising a ValueError if an individual word surpasses it. The function utilizes an external word processing function `chunk_by_word` to determine the structure of the text.  Parameters: -----------      - data (str): The input text to be split into sentences.     - maximum_size (Optional[int]): An optional limit on the maximum size of sentences       generated. (default None)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 193, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",45.047717571258545,816
"After the refactor, which modules now house the extracted Cypher query templates for saving nodes and edges?","The extracted Cypher query templates for saving nodes and edges are now housed in two new modules under `graphiti_core/queries`: `node_queries.py` and `edge_queries.py`. Specifically, these modules contain the constants `EPISODIC_NODE_SAVE`, `ENTITY_NODE_SAVE`, `COMMUNITY_NODE_SAVE`, `EPISODIC_EDGE_SAVE`, `ENTITY_EDGE_SAVE`, and `COMMUNITY_EDGE_SAVE`.","The `*_SAVE` constants have been pulled out of the classes and into two new modules under `graphiti_core/queries`:

- `graphiti_core/queries/node_queries.py` – holds `EPISODIC_NODE_SAVE`, `ENTITY_NODE_SAVE`, `COMMUNITY_NODE_SAVE`, etc.  
- `graphiti_core/queries/edge_queries.py` – holds `EPISODIC_EDGE_SAVE`, `ENTITY_EDGE_SAVE`, `COMMUNITY_EDGE_SAVE`, etc.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_node_save_bulk_query', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 42}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 42, 'n_files_pr': 10, 'pr': 195, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",35.954103231430054,817
What data structure do the save methods in the edge classes assemble to supply parameters for their database execution calls?,"In graphiti, the `save` methods in the edge classes (`EntityEdge.save`, `EpisodicEdge.save`, and `CommunityEdge.save`) assemble a plain Python dictionary to supply parameters for their database execution calls. This dictionary is then handed over to `driver.execute_query`. The data structure used is a simple key-value pair dictionary, where each key represents a query parameter.","All of the `save` methods build up a plain Python dict of query‐parameters (in `EntityEdge.save` it’s the `edge_data: dict[str, Any]`, and in `EpisodicEdge.save`/`CommunityEdge.save` they pass keyword args which under the hood become a dict) and hand that dict to `driver.execute_query`.","['CLASS qlib.qlib.backtest.high_performance_ds.BaseSingleMetric has method qlib.qlib.backtest.high_performance_ds.BaseSingleMetric.__init__\nSingle data structure for each metric.  Parameters ---------- metric : Union[dict, pd.Series]     keys/index is stock_id, value is the metric value.     for example:         SH600068    NaN         SH600079    1.0         SH600266    NaN                    ...         SZ300692    NaN         SZ300719    NaN,\nThe data structure of the single metric. The following methods are used for computing metrics in one indicator.', 'MODULE graphiti.graphiti_core.models.edges.edge_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.edges.edge_db_queries.ENTITY_EDGE_SAVE', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.blocking_query\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'MODULE graphiti.graphiti_core.models.edges.edge_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.edges.edge_db_queries.ENTITY_EDGE_SAVE_BULK', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.reopen\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 40}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 40, 'n_files_pr': 5, 'pr': 197, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.052273273468018,818
"What specific data is passed to the cross-encoder client for reranking in edge_search, node_search, and community_search?","In edge_search, the cross-encoder client is called with the original query string plus a list of top-K EntityEdge.fact strings for reranking. In node_search and community_search, the cross-encoder client is called with the original query string plus a list of candidate EntityNode.name values and CommunityNode.name values respectively. This indicates that the specific data passed to the cross-encoder client varies depending on the search type in graphiti.","In all three searches the cross-encoder is called with the original query string plus a list of text snippets to score:

• In edge_search (graphiti_core/search/search.py), under EdgeReranker.cross_encoder you do  
  cross_encoder.rank( query, list(fact_to_uuid_map.keys()) )  
where fact_to_uuid_map.keys() are the top-K EntityEdge.fact strings.  

• In node_search, under NodeReranker.cross_encoder you do  
  cross_encoder.rank( query, list(name_to_uuid_map.keys()) )  
where name_to_uuid_map.keys() are the candidate EntityNode.name values.  

• In community_search, under CommunityReranker.cross_encoder you do the same as nodes but over CommunityNode.name.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient inherits from or is a subclass of client.CrossEncoderClient', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.password\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient inherits from or is a subclass of client.CrossEncoderClient\nGoogle Gemini Reranker Client', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 6, 'pr': 201, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",37.36062836647034,819
"In the search pipeline, where is the cross-encoder reranker integrated across the different search types?","The cross-encoder reranker is integrated across different search types in the Graphiti search pipeline through the `cross_encoder.rank(query, …)` calls in the `edge_search()`, `node_search()`, `community_search()`, and `episode_search()` functions within the `graphiti_core/search/search_utils.py` module. This integration enables cross-encoder reranking for various search types, including edge, node, community, and episode searches.","The cross‐encoder client you pass into Graphiti is plumbed through graphiti_core/search/search.py: search() into each of the individual search functions, and is only invoked in the “cross_encoder” reranker branches. Concretely, in graphiti_core/search/search_utils.py you’ll find in:

• edge_search():  
  elif config.reranker == EdgeReranker.cross_encoder:  
    await cross_encoder.rank(query, …)

• node_search():  
  elif config.reranker == NodeReranker.cross_encoder:  
    await cross_encoder.rank(query, …)

• community_search():  
  elif config.reranker == CommunityReranker.cross_encoder:  
    await cross_encoder.rank(query, …)

(and similarly in episode_search) these calls actually perform the cross‐encoder reranking.","['MODULE graphiti.graphiti_core.cross_encoder.client contains CLASS graphiti.graphiti_core.cross_encoder.client.CrossEncoderClient\nCrossEncoderClient is an abstract base class that defines the interface for cross-encoder models used for ranking passages based on their relevance to a query. It allows for different implementations of cross-encoder models to be used interchangeably.', 'CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient inherits from or is a subclass of client.CrossEncoderClient', 'CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient inherits from or is a subclass of client.CrossEncoderClient\nGoogle Gemini Reranker Client', 'CLASS graphiti.graphiti_core.cross_encoder.client.CrossEncoderClient inherits from or is a subclass of abc.ABC\nCrossEncoderClient is an abstract base class that defines the interface for cross-encoder models used for ranking passages based on their relevance to a query. It allows for different implementations of cross-encoder models to be used interchangeably.', 'MODULE graphiti.graphiti_core.cross_encoder.gemini_reranker_client contains CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient\nGoogle Gemini Reranker Client', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 24}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 24, 'n_files_pr': 8, 'pr': 202, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.77365756034851,820
What sequence of search steps does edge_search perform to gather and limit candidate edges before applying the reranker?,"The sequence of search steps that edge_search performs to gather and limit candidate edges before applying the reranker is as follows:

1. Over-fetching using a 2× limit via three parallel calls:
* One call is `edge_fulltext_search(driver, query, search_filter, group_ids, 2*limit)`. 
* One call is `edge_similarity_search(driver, query_vector, …, search_filter, group_ids, 2*limit, config.sim_min_score)`. 
* One call is `edge_bfs_search(driver, bfs_origin_node_uuids, config.bfs_max_depth, search_filter, 2*limit).

2. If EdgeSearchMethod.bfs is enabled and no bfs_origin_node_uuids were passed in, edge_search extracts source_node_uuids from the initial results.
3. In that scenario, edge_search performs a second `edge_bfs_search` call with a 2× limit.
4. edge_search merges and de-duplicates all returned edges into an `edge_uuid_map` before applying any reranker.","In graphiti_core/search/search.py, edge_search first over-fetches (using a 2× limit) via three parallel calls (semaphore_gather):

• edge_fulltext_search(driver, query, search_filter, group_ids, 2*limit)  
• edge_similarity_search(driver, query_vector, …, search_filter, group_ids, 2*limit, config.sim_min_score)  
• edge_bfs_search(driver, bfs_origin_node_uuids, config.bfs_max_depth, search_filter, 2*limit)  

Then, if EdgeSearchMethod.bfs is enabled but no bfs_origin_node_uuids were passed in, it extracts source_node_uuids from those initial results and does a second edge_bfs_search(..., 2*limit).  

All returned edges are merged and de-duplicated into an edge_uuid_map before any reranker is applied.","['CLASS graphiti.mcp_server.graphiti_mcp_server.Procedure inherits from or is a subclass of pydantic.BaseModel\nA Procedure informing the agent what actions to take or how to perform in certain scenarios. Procedures are typically composed of several steps.  Instructions for identifying and extracting procedures: 1. Look for sequential instructions or steps (""First do X, then do Y"") 2. Identify explicit directives or commands (""Always do X when Y happens"") 3. Pay attention to conditional statements (""If X occurs, then do Y"") 4. Extract procedures that have clear beginning and end points 5. Focus on actionable instructions rather than general information 6. Preserve the original sequence and dependencies between steps 7. Include any specified conditions or triggers for the procedure 8. Capture any stated purpose or goal of the procedure 9. Summarize complex procedures while maintaining critical details', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_edges', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.Procedure\nA Procedure informing the agent what actions to take or how to perform in certain scenarios. Procedures are typically composed of several steps.  Instructions for identifying and extracting procedures: 1. Look for sequential instructions or steps (""First do X, then do Y"") 2. Identify explicit directives or commands (""Always do X when Y happens"") 3. Pay attention to conditional statements (""If X occurs, then do Y"") 4. Extract procedures that have clear beginning and end points 5. Focus on actionable instructions rather than general information 6. Preserve the original sequence and dependencies between steps 7. Include any specified conditions or triggers for the procedure 8. Capture any stated purpose or goal of the procedure 9. Summarize complex procedures while maintaining critical details', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_nodes', 'CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has method graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.\nGoogle Gemini Reranker Client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 7, 'n_files_pr': 6, 'pr': 203, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",44.42350196838379,821
Under what condition does the search function use the provided query_vector instead of calling the embedder to generate a new one?,The search function uses the provided query_vector instead of calling the embedder to generate a new one when a non-None query_vector is passed into the search() function in graphiti_core/search/search.py.,"In graphiti_core/search/search.py’s search(), it only calls embedder.create(…) when the query_vector arg is None. If you pass a non-None query_vector into search(), it’s used directly.","[""CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.MissingQueryParameterError inherits from or is a subclass of cognee.exceptions.CogneeValidationError\nRaised when neither 'query_text' nor 'query_vector' is provided, and at least one is required to perform the operation."", 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.embedder', ""MODULE cognee.cognee.infrastructure.databases.exceptions.exceptions contains CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.MissingQueryParameterError\nRaised when neither 'query_text' nor 'query_vector' is provided, and at least one is required to perform the operation."", 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.embedding', ""CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.MissingQueryParameterError has method cognee.cognee.infrastructure.databases.exceptions.exceptions.MissingQueryParameterError.__init__\nRaised when neither 'query_text' nor 'query_vector' is provided, and at least one is required to perform the operation."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 6, 'pr': 204, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.375619888305664,822
What high-level operation replaced the individual node and edge save calls in the episode ingestion flow?,"The high-level operation that replaced individual node and edge save calls in the episode ingestion flow is a single bulk write via `add_nodes_and_edges_bulk`. This function, located in `graphiti_core/utils/bulk_utils.py`, wraps all `EpisodicNode` writes, all `EntityNode` writes, and all `EntityEdge` writes in one transaction. This change aims to improve performance by reducing the number of database transactions.","The per-node/edge `.save()` calls in add_episode were replaced by a single bulk write via add_nodes_and_edges_bulk (in graphiti_core/utils/bulk_utils.py), which wraps all EpisodicNode, EntityNode and EntityEdge writes in one TX.","['CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge.get_source_node\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains FUNCTION graphiti.graphiti_core.utils.maintenance.edge_operations.build_episodic_edges', 'MODULE cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements contains CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', 'MODULE graphiti.graphiti_core.models.edges.edge_db_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.models.edges.edge_db_queries.EPISODIC_EDGE_SAVE', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge.add_attribute\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 8, 'pr': 205, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.656320333480835,823
What does the search function return when given an empty or whitespace-only query?,"When given an empty or whitespace-only query, the search function in graphiti returns a SearchResults object with all attributes (edges, nodes, episodes, and communities) equal to an empty list.","In graphiti_core/search/search.py’s search(), if query.strip() == """" it immediately returns a SearchResults with all empty lists:

• edges = []  
• nodes = []  
• episodes = []  
• communities = []","['cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md\n## 了解我们的架构\n\n<div style=""text-align: center"">\n  <img src=""cognee_diagram_zh.JPG"" alt=""cognee概念图"" width=""100%"" />\n</div>\n\n\n\n## 演示\n\n1. 什么是AI记忆：\n\n[了解cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. 简单GraphRAG演示\n\n[简单GraphRAG演示](https://github.com/user-attachments/assets/f57fd9ea-1dc0-4904-86eb-de78519fdc32)\n\n3. cognee与Ollama\n\n[cognee与本地模型](https://github.com/user-attachments/assets/834baf9a-c371-4ecf-92dd-e144bd0eb3f6)\n\n\n## 行为准则\n\n我们致力于为我们的社区提供愉快和尊重的开源体验。有关更多信息，请参阅<a href=""https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md""><code>CODE_OF_CONDUCT</code></a>。\n\n## 💫 贡献者\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img alt=""contributors"" src=""https://contrib.rocks/image?repo=topoteretes/cognee""/>\n</a>\n\n\n## Star历史\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## 💫 Contributors\n\nThanks to our amazing contributors! 💖\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=topoteretes/cognee"" />\n</a>\n\n## 🏆 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 206, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.185109853744507,824
"When BFS is enabled but no origin nodes are provided, how do the edge and node search functions incorporate BFS results?","When BFS is enabled but no origin nodes are provided, the edge and node search functions in graphiti incorporate BFS results by collecting all UUIDs from other search results. For edges, it uses `edge.source_node_uuid` when collecting UUIDs, while for nodes, it uses `node.uuid`. The code then invokes `edge_bfs_search` with the collected UUIDs as origins and appends the BFS-expanded results to `search_results`. This process ensures that even without explicit origin nodes, BFS can still expand search results by traversing the graph.","In both edge_search and node_search (graphiti_core/search/search.py) you always fire off a BFS search call, even if bfs_origin_node_uuids=None (the first BFS returns nothing). Then there’s this block:

• If BFS is enabled in config.search_methods and bfs_origin_node_uuids is still None, you collect all UUIDs from the other search results (for edges you grab edge.source_node_uuid; for nodes you grab node.uuid), and invoke edge_bfs_search or node_bfs_search again with those as origins. The BFS-expanded results are appended to the search_results.","['FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS graphiti.graphiti_core.search.search_config.EdgeSearchConfig inherits from or is a subclass of pydantic.BaseModel', 'MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 14}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 14, 'n_files_pr': 9, 'pr': 212, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",32.97325038909912,825
Which fulltext search routines include a configurable result limit and what default constant do they use?,"The fulltext search routines in graphiti that include a configurable result limit are: `edge_fulltext_search`, `node_fulltext_search`, and `community_fulltext_search`. These functions default to using the `RELEVANT_SCHEMA_LIMIT` constant for their limit parameter.","The three “fulltext” routines in graphiti_core/search/search_utils.py—  
• edge_fulltext_search(…, limit=RELEVANT_SCHEMA_LIMIT)  
• node_fulltext_search(…, limit=RELEVANT_SCHEMA_LIMIT)  
• community_fulltext_search(…, limit=RELEVANT_SCHEMA_LIMIT)  

all take a configurable limit and default it to RELEVANT_SCHEMA_LIMIT.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.Procedure\nA Procedure informing the agent what actions to take or how to perform in certain scenarios. Procedures are typically composed of several steps.  Instructions for identifying and extracting procedures: 1. Look for sequential instructions or steps (""First do X, then do Y"") 2. Identify explicit directives or commands (""Always do X when Y happens"") 3. Pay attention to conditional statements (""If X occurs, then do Y"") 4. Extract procedures that have clear beginning and end points 5. Focus on actionable instructions rather than general information 6. Preserve the original sequence and dependencies between steps 7. Include any specified conditions or triggers for the procedure 8. Capture any stated purpose or goal of the procedure 9. Summarize complex procedures while maintaining critical details', 'FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query has parameter or accepts argument graphiti.graphiti_core.search.search_utils.MAX_QUERY_LENGTH', 'CLASS graphiti.mcp_server.graphiti_mcp_server.Procedure inherits from or is a subclass of pydantic.BaseModel\nA Procedure informing the agent what actions to take or how to perform in certain scenarios. Procedures are typically composed of several steps.  Instructions for identifying and extracting procedures: 1. Look for sequential instructions or steps (""First do X, then do Y"") 2. Identify explicit directives or commands (""Always do X when Y happens"") 3. Pay attention to conditional statements (""If X occurs, then do Y"") 4. Extract procedures that have clear beginning and end points 5. Focus on actionable instructions rather than general information 6. Preserve the original sequence and dependencies between steps 7. Include any specified conditions or triggers for the procedure 8. Capture any stated purpose or goal of the procedure 9. Summarize complex procedures while maintaining critical details', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query', 'CLASS graphiti.mcp_server.graphiti_mcp_server.Procedure has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.Procedure.description\nA Procedure informing the agent what actions to take or how to perform in certain scenarios. Procedures are typically composed of several steps.  Instructions for identifying and extracting procedures: 1. Look for sequential instructions or steps (""First do X, then do Y"") 2. Identify explicit directives or commands (""Always do X when Y happens"") 3. Pay attention to conditional statements (""If X occurs, then do Y"") 4. Extract procedures that have clear beginning and end points 5. Focus on actionable instructions rather than general information 6. Preserve the original sequence and dependencies between steps 7. Include any specified conditions or triggers for the procedure 8. Capture any stated purpose or goal of the procedure 9. Summarize complex procedures while maintaining critical details', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/old/comparative_eval/README.md\n# Comparative QA Benchmarks\n\nIndependent benchmarks for different QA/RAG systems using HotpotQA dataset.\n\n## Dataset Files\n- `hotpot_50_corpus.json` - 50 instances from HotpotQA\n- `hotpot_50_qa_pairs.json` - Corresponding question-answer pairs\n\n## Benchmarks\n\nEach benchmark can be run independently with appropriate dependencies:\n\n### Mem0\n```bash\npip install mem0ai openai\npython qa_benchmark_mem0.py\n```\n\n### LightRAG\n```bash\npip install ""lightrag-hku[api]""\npython qa_benchmark_lightrag.py\n```\n\n### Graphiti\n```bash\npip install graphiti-core\npython qa_benchmark_graphiti.py\n```\n\n## Environment\nCreate `.env` with required API keys:\n- `OPENAI_API_KEY` (all benchmarks)\n- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` (Graphiti only)\n\n## Usage\nEach benchmark inherits from `QABenchmarkRAG` base class and can be configured independently.\n\n# Results\nUpdated results will be posted soon.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 10, 'n_files_pr': 8, 'pr': 215, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",28.86414361000061,826
What retrieval strategies does the primary edge lookup initiate and how are their results merged for final ranking?,"The primary edge lookup in graphiti initiates three concurrent retrievals: `edge_fulltext_search` performs fact full-text search, `edge_similarity_search` performs fact embedding cosine search, and `edge_bfs_search` performs graph-BFS up to bfs_max_depth. All hits are merged into a uuid→EntityEdge map, deduplicated, reranked by the configured EdgeReranker (which can be rrf, mmr, cross_encoder or node_distance), and truncated to the requested limit.","In graphiti_core/search/search.py in edge_search, we kick off three concurrent retrievals:

• edge_fulltext_search (fact full-text)  
• edge_similarity_search (fact embedding cosine)  
• edge_bfs_search (graph-BFS up to bfs_max_depth)  

(If BFS is enabled but no origins were passed, a second BFS over the first batch’s source_node_uuids is run.) All hits are merged into a uuid→EntityEdge map (deduped), then reranked by the configured EdgeReranker (rrf, mmr, cross_encoder or node_distance) and finally truncated to the requested limit.","['MODULE cognee.cognee.infrastructure.context.BaseContextProvider contains CLASS cognee.cognee.infrastructure.context.BaseContextProvider.BaseContextProvider\nBase class for context retrieval strategies.  This class serves as a blueprint for creating different strategies that retrieve context based on specified entities and a query. Any subclass must implement the `get_context` method to define how context is retrieved.', 'CLASS graphiti.graphiti_core.search.search_config.EdgeSearchConfig inherits from or is a subclass of pydantic.BaseModel', 'CLASS cognee.cognee.infrastructure.context.BaseContextProvider.BaseContextProvider inherits from or is a subclass of abc.ABC\nBase class for context retrieval strategies.  This class serves as a blueprint for creating different strategies that retrieve context based on specified entities and a query. Any subclass must implement the `get_context` method to define how context is retrieved.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance', 'MODULE cognee.cognee.infrastructure.data.chunking.HaystackChunkEngine contains CLASS cognee.cognee.infrastructure.data.chunking.HaystackChunkEngine.HaystackChunkEngine\nManage chunking of source data using specified strategies and parameters.  The class provides functionality to process source data into manageable chunks according to defined strategies, chunk sizes, and overlaps. The primary instance variables include chunk_strategy, source_data, chunk_size, and chunk_overlap, which dictate how the data is chunked.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 216, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.632363319396973,827
What is the return value difference between deleting a single node by uuid and deleting nodes by group ID?,"The return value difference between deleting a single node by uuid and deleting nodes by group ID in graphiti is that the instance method `Node.delete(self, driver)` returns the raw result of the `driver.execute_query(...)` call, while the class method `Node.delete_by_group_id(cls, driver, group_id)` always returns the literal string `","In graphiti_core/nodes.py:

- The instance method `Node.delete(self, driver)` returns whatever the `driver.execute_query(...)` call yields (i.e. the raw Neo4j result).
- The class method `Node.delete_by_group_id(cls, driver, group_id)` always returns the literal string `""SUCCESS""`.","['CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter inherits from or is a subclass of cognee.infrastructure.databases.graph.graph_db_interface.GraphDBInterface\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', 'CLASS graphiti.graphiti_core.nodes.Node has field or attribute or property graphiti.graphiti_core.nodes.Node.group_id', 'CLASS cognee.cognee.infrastructure.databases.graph.neptune_driver.adapter.NeptuneGraphDB inherits from or is a subclass of cognee.infrastructure.databases.graph.graph_db_interface.GraphDBInterface\nAdapter for interacting with Amazon Neptune Analytics graph store. This class provides methods for querying, adding, deleting nodes and edges using the aws_langchain library.', 'CLASS graphiti.graphiti_core.nodes.Node has field or attribute or property graphiti.graphiti_core.nodes.Node.uuid', 'CLASS cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter has method cognee.cognee.infrastructure.databases.graph.kuzu.adapter.KuzuAdapter.close\nAdapter for Kuzu graph database operations with improved consistency and async support.  This class facilitates operations for working with the Kuzu graph database, supporting both direct database queries and a structured asynchronous interface for node and edge management. It contains methods for querying, adding, and deleting nodes and edges as well as for graph metrics and data extraction.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 21}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 21, 'n_files_pr': 4, 'pr': 217, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.979085206985474,828
What field acts as the cursor for pagination in the get_by_group_ids implementations?,"The uuid field acts as the cursor for pagination in the get_by_group_ids implementations in Graphiti. This can be seen in both edges and nodes, where the uuid field is used to represent the unique identifier for each edge and node.","All of the `get_by_group_ids` methods paginate using the `uuid` field (e.g. `e.uuid` in edges, `n.uuid` in nodes) as the cursor via the `uuid_cursor` parameter.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.cls\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.args\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.graphiti_core.edges.Edge has field or attribute or property graphiti.graphiti_core.edges.Edge.group_id', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.neo4j\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 17}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 17, 'n_files_pr': 12, 'pr': 218, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.486828565597534,829
How does add_episode fetch an existing episode when a uuid is provided?,"The `add_episode` method in the `Graphiti` class fetches an existing episode when a UUID is provided by calling the `get_by_uuid` class method of the `EpisodicNode` class. This method takes the driver and the UUID as parameters, and it returns the existing episode if found. The `get_by_uuid` method uses the provided driver to fetch the existing episode from the database based on the given UUID. If an episode is found, it returns the episode; otherwise, it returns `None`.","When you pass a UUID into add_episode it skips creating a new node and instead does:

• In graphiti_core/graphiti.py:Graphiti.add_episode  
  `episode = await EpisodicNode.get_by_uuid(self.driver, uuid)`  

So it fetches the existing episode via the class-method `EpisodicNode.get_by_uuid(driver, uuid)` (defined in graphiti_core/nodes/episodic_node.py).","['CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.episode_uuids', 'CLASS graphiti.graphiti_core.errors.EntityTypeValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an entity type uses protected attribute names.', 'MODULE graphiti.examples.podcast.podcast_runner contains GLOBAL_VARIABLE graphiti.examples.podcast.podcast_runner.episode_uuids', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.graph_driver\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 219, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",36.92640948295593,830
How does VersionWrapper.__call__ modify message content to disable unicode escaping for system messages?,"VersionWrapper.__call__ modifies message content to disable unicode escaping for system messages in graphiti by appending a special string to the content of any Message with role 'system'. This is done after calling the wrapped prompt function. The code checks whether a Message has the role 'system', and if so, it appends this special string to its content. This effectively disables unicode escaping for system messages.","In VersionWrapper.__call__ (graphiti_core/prompts/lib.py) after calling the wrapped prompt func it loops over each Message and, if message.role == 'system', does

 message.content += DO_NOT_ESCAPE_UNICODE

thus appending that marker only to system messages to disable unicode escaping.","['MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.prompt_helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.prompts.prompt_helpers.DO_NOT_ESCAPE_UNICODE', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'METHOD graphiti.graphiti_core.prompts.lib.VersionWrapper.__call__ has parameter or accepts argument graphiti.graphiti_core.prompts.lib.VersionWrapper.__call__.context', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n|---------|--------------------|\n| 0.x     | :white_check_mark: |\n\n\n## Reporting a Vulnerability\n\nPlease use GitHub's Private Vulnerability Reporting mechanism found in the Security section of this repo."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David Díaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Notable bug fixes\n\nSome noteworthy bug fixes that are not related to specific language bindings are listed in this section.\n\n- Some language environments use a different thread to perform garbage collection, which breaks the thread-local cache used in XGBoost. XGBoost 2.0 implements a new thread-safe cache using a light weight lock to replace the thread-local cache. (#8851)\n- Fix model IO by clearing the prediction cache. (#8904)\n- `inf` is checked during data construction. (#8911)\n- Preserve order of saved updaters configuration. Usually, this is not an issue unless the `updater` parameter is used instead of the `tree_method` parameter (#9355)\n- Fix GPU memory allocation issue with categorical splits. (#9529)\n- Handle escape sequence like `\\t\\n` in feature names for JSON model dump. (#9474)\n- Normalize file path for model IO and text input. This handles short paths on Windows and paths that contain `~` on Unix (#9463). In addition, all path inputs are required to be encoded in UTF-8 (#9448, #9443)\n- Fix integer overflow on H100. (#9380)\n- Fix weighted sketching on GPU with categorical features. (#9341)\n- Fix metric serialization. The bug might cause some of the metrics to be dropped during evaluation. (#9405)\n- Fixes compilation errors on MSVC x86 targets (#8823)\n- Pick up the dmlc-core fix for the CSV parser. (#8897)\n\n\n### Documentation\nAside from documents for new features, we have many smaller updates to improve user experience, from troubleshooting guides to typo fixes.\n\n- Explain CPU/GPU interop. (#8450)\n- Guide to troubleshoot NCCL errors. (#8943, #9206)\n- Add a note for rabit port selection. (#8879)\n- How to build the docs using conda (#9276)\n- Explain how to obtain reproducible results on distributed systems. (#8903)\n\n* Fixes and small updates to document and demonstration scripts. (#8626, #8436, #8995, #8907, #8923, #8926, #9358, #9232, #9201, #9469, #9462, #9458, #8543, #8597, #8401, #8784, #9213, #9098, #9008, #9223, #9333, #9434, #9435, #9415, #8773, #8752, #9291, #9549)\n\n### Python package\n* New Features and Improvements\n- Support primitive types of pyarrow-backed pandas dataframe. (#8653)\n- Warning messages emitted by XGBoost are now emitted using Python warnings. (#9387)\n- User can now format the value printed near the bars on the `plot_importance` plot (#8540)\n- XGBoost has improved half-type support (float16) with pandas, cupy, and cuDF. With GPU input, the handling is through CUDA `__half` type, and no data copy is made. (#8487, #9207, #8481)\n- Support `Series` and Python primitive types in `inplace_predict` and `QuantileDMatrix` (#8547, #8542)\n- Support all pandas' nullable integer types. (#8480)\n- Custom metric with the scikit-learn interface now supports `sample_weight`. (#8706)\n- Enable Installation of Python Package with System lib in a Virtual Environment (#9349)\n- Raise if expected workers are not alive in `xgboost.dask.train` (#9421)\n\n* Optimization\n- Cache transformed data in `QuantileDMatrix` for efficiency. (#8666, #9445)\n- Take datatable as row-major input. (#8472)\n- Remove unnecessary conversions between data structures (#8546)\n\n* Adopt modern Python packaging conventions (PEP 517, PEP 518, PEP 621)\n-  XGBoost adopted the modern Python packaging conventions. The old setup script `setup.py` is now replaced with the new configuration file `pyproject.toml`. Along with this, XGBoost now supports Python 3.11. (#9021, #9112, #9114, #9115) Consult the latest documentation for the updated instructions to build and install XGBoost.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 224, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.443586111068726,831
How are Pydantic response models integrated into the LLMClient pipeline to enforce structured JSON output across different providers?,"Pydantic response models are integrated into the LLMClient pipeline to enforce structured JSON output across different providers by using the `response_model` parameter in the `generate_response` method. This parameter is used to specify a Pydantic model that defines the structure of the expected response. When the `response_model` parameter is provided, the `LLMClient` serializes the schema and appends a prompt to the last user message. The `response_model` parameter is used across different providers, including OpenAI, Gemini, and others, each implementing their own way to use this parameter for generating structured output.","The key is that every call to LLMClient.generate_response can take an optional Pydantic response_model (a BaseModel subclass). Here’s how it gets wired through:

1. In graphiti_core/llm_client/client.py -> LLMClient.generate_response – if you pass in response_model, it does

   • `serialized_model = json.dumps(response_model.model_json_schema())`  
   • appends “Respond with a JSON object in the following format:\n\n{serialized_model}” to the last user message  
   • then calls `_generate_response_with_retry(messages, response_model, …)`

2. Each provider subclass then picks up that response_model in its own `_generate_response`:

   • Anthrop­i­cClient._generate_response (anthropic_client.py)  
     – wraps the Pydantic schema as a “tool” via `_create_tool(response_model)` and sends it in the Anthropic tools API  
     – looks for a `tool_use` or falls back to extracting JSON from text  

   • OpenAIClient (openai_client.py)  
     – if you supply response_model it calls the new beta-parse endpoint in `_create_structured_completion` with `response_format=response_model`  
     – otherwise it falls back to a generic `response_format={'type':'json_object'}`  

   • GroqClient._generate_response (groq_client.py)  
     – always uses `response_format={'type':'json_object'}` on the Groq chat API and then `json.loads` the result  

3. In all cases the raw JSON blob returned by the LLM is parsed into a `dict[str, Any]` and handed back to you. Because you seeded the prompt or tool with the exact Pydantic JSON schema, the model’s output is strongly shaped to match your BaseModel.","[""CLASS graphiti.graphiti_core.llm_client.gemini_client.GeminiClient has method graphiti.graphiti_core.llm_client.gemini_client.GeminiClient.salvage_json\nAttempt to salvage a JSON object if the raw output is truncated.  This is accomplished by looking for the last closing bracket for an array or object. If found, it will try to load the JSON object from the raw output. If the JSON object is not valid, it will return None.  Args:     raw_output (str): The raw output from the LLM.  Returns:     dict[str, typing.Any]: The salvaged JSON object.     None: If no salvage is possible.\nGeminiClient is a client class for interacting with Google's Gemini language models.  This class extends the LLMClient and provides methods to initialize the client and generate responses from the Gemini language model.  Attributes:     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.     thinking_config (types.ThinkingConfig | None): Optional thinking configuration for models that support it. Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, thinking_config: types.ThinkingConfig | None = None):         Initializes the GeminiClient with the provided configuration, cache setting, and optional thinking config.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'CLASS cognee.cognee.infrastructure.llm.structured_output_framework.baml.baml_client.stream_types.ResponseModel inherits from or is a subclass of pydantic.BaseModel', ""METHOD cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output.response_model\nGenerate a response from a user query.  This method creates structured output by sending a synchronous request to the OpenAI API using the provided parameters to generate a completion based on the user input and system prompt.  Parameters: -----------      - text_input (str): The input text provided by the user for generating a response.     - system_prompt (str): The system's prompt to guide the model's response.     - response_model (Type[BaseModel]): The expected model type for the response.  Returns: --------      - BaseModel: A structured output generated by the model, returned as an instance of       BaseModel."", 'CLASS cognee.cognee.infrastructure.llm.structured_output_framework.baml.baml_client.types.ResponseModel inherits from or is a subclass of pydantic.BaseModel', ""METHOD cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.OpenAIAdapter.create_structured_output has parameter or accepts argument cognee.cognee.infrastructure.llm.structured_output_framework.litellm_instructor.llm.openai.adapter.observe\nGenerate a response from a user query.  This method creates structured output by sending a synchronous request to the OpenAI API using the provided parameters to generate a completion based on the user input and system prompt.  Parameters: -----------      - text_input (str): The input text provided by the user for generating a response.     - system_prompt (str): The system's prompt to guide the model's response.     - response_model (Type[BaseModel]): The expected model type for the response.  Returns: --------      - BaseModel: A structured output generated by the model, returned as an instance of       BaseModel."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 38}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 38, 'n_files_pr': 19, 'pr': 225, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",41.88902044296265,832
Which prompting functions in the summarization module currently enforce the 250-word limit on summaries?,"The prompting functions in the summarization module that currently enforce the 250-word limit on summaries are *summarize_pair*, *summarize_context*, and *summary_description*. These functions are defined in the file `graphiti_core/prompts/summarize_nodes.py`.","In graphiti_core/prompts/summarize_nodes.py all three prompt-builders enforce the 250-word cap:

• summarize_pair  
• summarize_context  
• summary_description","[""MODULE cognee.cognee.tasks.chunks.chunk_by_sentence contains FUNCTION cognee.cognee.tasks.chunks.chunk_by_sentence.get_word_size\nCalculate the size of a given word in terms of tokens.  If an embedding engine's tokenizer is available, count the tokens for the provided word. If the tokenizer is not available, assume the word counts as one token.  Parameters: -----------      - word (str): The word for which the token size is to be calculated.  Returns: --------      - int: The number of tokens representing the word, typically an integer, depending       on the tokenizer's output."", 'MODULE graphiti.graphiti_core.prompts.summarize_nodes contains FUNCTION graphiti.graphiti_core.prompts.summarize_nodes.summarize_context', ""FUNCTION cognee.cognee.tasks.chunks.chunk_by_sentence.get_word_size has parameter or accepts argument cognee.cognee.tasks.chunks.chunk_by_sentence.get_word_size.word\nCalculate the size of a given word in terms of tokens.  If an embedding engine's tokenizer is available, count the tokens for the provided word. If the tokenizer is not available, assume the word counts as one token.  Parameters: -----------      - word (str): The word for which the token size is to be calculated.  Returns: --------      - int: The number of tokens representing the word, typically an integer, depending       on the tokenizer's output."", 'MODULE graphiti.graphiti_core.prompts.summarize_nodes contains FUNCTION graphiti.graphiti_core.prompts.summarize_nodes.summarize_pair', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    """"""Return x doubled.""""""\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = ""black""\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 227, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.029977083206177,833
Which exception does the updated generate_response method raise immediately without any retry attempts?,"The updated `generate_response` method raises immediately without any retry attempts for OpenAIError other than RateLimitError, specifically **InvalidRequestError**.",The new generate_response (in graphiti_core/llm_client/base_openai_client.py) only retries on RateLimitError. Any other OpenAIError—for example openai.error.InvalidRequestError—is re-raised immediately with no retry.,"['CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.llm_client.client contains FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error has parameter or accepts argument graphiti.graphiti_core.llm_client.client.is_server_or_retry_error.exception', 'CLASS graphiti.graphiti_core.errors.GroupIdValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a group_id contains invalid characters.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 229, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.916120767593384,834
What default score does node_distance_reranker assign to nodes without a computed distance?,"The node_distance_reranker function assigns a default score of float('inf') to nodes without a computed distance, effectively treating them as having an infinite distance.","In node_distance_reranker (graphiti_core/search/search_utils.py), any node UUID with no returned distance is assigned scores[uuid] = float('inf').","['CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', 'MODULE graphiti.examples.quickstart.quickstart_neo4j contains GLOBAL_VARIABLE graphiti.examples.quickstart.quickstart_neo4j.reranked_results', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_nodes', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.graph_driver\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 231, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.49144458770752,835
Which edge retrieval method returns an empty list instead of raising an error when no matching records exist?,"The edge retrieval method that returns an empty list instead of raising an error when no matching records exist in graphiti is `CommunityEdge.get_by_group_ids`. This method does not include an “if empty → raise” check, which means it will return an empty list if no matching records are found. The Episodic and Entity variants of this method raise errors when no matching records exist.","The method `CommunityEdge.get_by_group_ids` in `graphiti_core/edges.py` returns an empty list (it has no “if empty → raise” check), whereas the Episodic and Entity variants raise an error.","['CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.EdgesNotFoundError\nRaised when a list of edges is not found.', 'CLASS graphiti.graphiti_core.errors.EdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a list of edges is not found.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.EdgeNotFoundError\nRaised when an edge is not found.', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 2, 'pr': 232, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",28.625369548797607,836
Which uppercase letters does the sanitizer now escape?,"The sanitizer now escapes the uppercase letters O, R, N, T, A, and D.","In graphiti_core/helpers.py in the `lucene_sanitize` function the escape map now includes the uppercase letters: O, R, N, T, A and D.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.lucene_sanitize', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'FUNCTION graphiti.graphiti_core.helpers.lucene_sanitize has parameter or accepts argument graphiti.graphiti_core.helpers.lucene_sanitize.query', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n|---------|--------------------|\n| 0.x     | :white_check_mark: |\n\n\n## Reporting a Vulnerability\n\nPlease use GitHub's Private Vulnerability Reporting mechanism found in the Security section of this repo."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfounders@getzep.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 233, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.513315439224243,837
How does the code ensure all created and parsed datetimes are timezone-aware UTC during episode and edge processing?,"The code ensures all created and parsed datetimes are timezone-aware UTC during episode and edge processing in Graphiti by using the `ensure_utc` function to convert naive datetimes to UTC, and by using the `utc_now` function to get the current UTC datetime with timezone information. The `extract_edges` and `extract_edge_dates` functions parse ISO valid_at and invalid_at strings, which creates a naive datetime object, and then call `ensure_utc` on this object to ensure it's timezone-aware. This approach guarantees that all program-generated and parsed datetimes carry a UTC tzinfo.","All timestamps in episode and edge processing are either generated with utc_now() or normalized through ensure_utc():

• In Graphiti.add_episode (and add_episode_bulk) every new node/edge’s created_at and valid_at (for EpisodicNode) is set with utc_now().  
• Node.created_at (in graphiti_core/nodes.py) has a default_factory of utc_now(), so any Model‐created timestamp is UTC‐aware.  
• When LLM extraction returns ISO strings for valid_at/invalid_at, extract_edges (and extract_edge_dates) does:
    – datetime.fromisoformat(valid_at.replace('Z', '+00:00'))  
    – ensure_utc(...)  
  to force tzinfo=UTC on parsed datetimes.  
Thus all program‐generated and parsed datetimes carry a UTC tzinfo.","[""MODULE graphiti.graphiti_core.utils.datetime_utils contains FUNCTION graphiti.graphiti_core.utils.datetime_utils.ensure_utc\nEnsures a datetime is timezone-aware and in UTC. If the datetime is naive (no timezone), assumes it's in UTC. If the datetime has a different timezone, converts it to UTC. Returns None if input is None."", ""FUNCTION graphiti.graphiti_core.utils.datetime_utils.ensure_utc has parameter or accepts argument graphiti.graphiti_core.utils.datetime_utils.ensure_utc.dt\nEnsures a datetime is timezone-aware and in UTC. If the datetime is naive (no timezone), assumes it's in UTC. If the datetime has a different timezone, converts it to UTC. Returns None if input is None."", 'MODULE graphiti.graphiti_core.utils.datetime_utils contains FUNCTION graphiti.graphiti_core.utils.datetime_utils.utc_now\nReturns the current UTC datetime with timezone information.', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.prompts.extract_edge_dates contains GLOBAL_VARIABLE graphiti.graphiti_core.prompts.extract_edge_dates.versions', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 17}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 4, 'n_context_nodes': 17, 'n_files_pr': 12, 'pr': 234, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",39.03516340255737,838
Which zero-width unicode characters are removed during input sanitization?,"The following zero-width Unicode characters are removed during input sanitization in graphiti: U+200B (ZERO WIDTH SPACE), U+200C (ZERO WIDTH NON-JOINER), U+200D (ZERO WIDTH JOINER), U+FEFF (ZERO WIDTH NO-BREAK SPACE/BOM), and U+2060 (WORD JOINER).","In LLMClient._clean_input (graphiti_core/llm_client/client.py) the following zero-width/invisible chars are stripped:

• U+200B (ZERO WIDTH SPACE)  
• U+200C (ZERO WIDTH NON-JOINER)  
• U+200D (ZERO WIDTH JOINER)  
• U+FEFF (ZERO WIDTH NO-BREAK SPACE/BOM)  
• U+2060 (WORD JOINER)","['CLASS graphiti.graphiti_core.errors.GroupIdValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a group_id contains invalid characters.', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.lucene_sanitize', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.prompts.prompt_helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.prompts.prompt_helpers.DO_NOT_ESCAPE_UNICODE', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.validate_group_id\nValidate that a group_id contains only ASCII alphanumeric characters, dashes, and underscores.  Args:     group_id: The group_id to validate  Returns:     True if valid, False otherwise  Raises:     GroupIdValidationError: If group_id contains invalid characters', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n|---------|--------------------|\n| 0.x     | :white_check_mark: |\n\n\n## Reporting a Vulnerability\n\nPlease use GitHub's Private Vulnerability Reporting mechanism found in the Security section of this repo."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 4, 'pr': 238, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.029343366622925,839
"During response generation, how is each message’s content processed before forming the API request payload?","During response generation in graphiti, each message's content is processed by being passed through the `_clean_input` method of `OpenAIGenericClient`. This method sanitizes and normalizes text. After cleaning, each Message content is paired with its role and added to the `openai_messages` list, which becomes the API payload.","In OpenAIGenericClient._generate_response (graphiti_core/llm_client/openai_generic_client.py), each Message’s content is first passed through self._clean_input(…) (which sanitizes/normalizes the text) before it’s paired with its role and added to the openai_messages list that becomes the API payload.","['CLASS qlib.qlib.workflow.exp.Experiment has method qlib.qlib.workflow.exp.Experiment.list_recorders\nList all the existing recorders of this experiment. Please first get the experiment instance before calling this method. If user want to use the method `R.list_recorders()`, please refer to the related API document in `QlibRecorder`.  flt_kwargs : dict     filter recorders by conditions     e.g.  list_recorders(status=Recorder.STATUS_FI)  Returns ------- The return type depends on `rtype`     if `rtype` == ""dict"":         A dictionary (id -> recorder) of recorder information that being stored.     elif `rtype` == ""list"":         A list of Recorder.\nThis is the `Experiment` class for each experiment being run. The API is designed similar to mlflow. (The link: https://mlflow.org/docs/latest/python_api/mlflow.html)', 'MODULE graphiti.server.graph_service.dto.ingest contains CLASS graphiti.server.graph_service.dto.ingest.AddMessagesRequest', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.openai_messages\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'CLASS graphiti.server.graph_service.dto.common.Message has field or attribute or property graphiti.server.graph_service.dto.common.Message.content', ""MODULE graphiti.graphiti_core.llm_client.openai_generic_client contains CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 239, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.02804446220398,840
"How are date-based filters passed from the top-level search into the Cypher queries for fulltext, similarity, and BFS edge searches?","Date-based filters are passed from the top-level search into the Cypher queries for fulltext, similarity, and BFS edge searches in Graphiti through the `search_filter` object. This object is carried through various functions such as `edge_search()`, `edge_fulltext_search()`, `edge_similarity_search()`, and `edge_bfs_search()`. Within these functions, the `search_filter` object is used to construct filter queries and parameters that are then spliced into Cypher WHERE clauses. These filter queries and parameters are bound in driver.execute_query().","Date filters live in the SearchFilters you pass into Graphiti.search().  That `search_filter` is carried through into edge_search(…) and then into each of

• edge_fulltext_search  
• edge_similarity_search  
• edge_bfs_search  

where they all do:

```python
filter_query, filter_params = edge_search_filter_query_constructor(search_filter)
```

The returned filter_query (string of `AND r.valid_at <= $now …`) and filter_params (e.g. `{""now"": datetime.utcnow(), …}`) are spliced into the Cypher WHERE clauses and bound in driver.execute_query.","['CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever inherits from or is a subclass of cognee.modules.retrieval.base_retriever.BaseRetriever\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'CLASS graphiti.graphiti_core.search.search_filters.DateFilter inherits from or is a subclass of pydantic.BaseModel', 'MODULE cognee.cognee.modules.retrieval.cypher_search_retriever contains CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'MODULE graphiti.graphiti_core.search.search_filters contains CLASS graphiti.graphiti_core.search.search_filters.DateFilter', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever.user_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 8, 'n_files_pr': 4, 'pr': 240, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",34.572208881378174,841
"If the LLM returns a malformed valid_at timestamp, what does extract_edge_dates return for that date and what logging level is used for the parsing error?","If the LLM returns a malformed `valid_at` timestamp, extract_edge_dates in graphiti will return `None` for that date. The logging level used for the parsing error is WARNING.","If the LLM’s valid_at can’t be parsed, extract_edge_dates sets that date to None and emits a logger.warning in graphiti_core/utils/maintenance/temporal_operations.py.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.prompts.extract_edge_dates.EdgeDates has field or attribute or property graphiti.graphiti_core.prompts.extract_edge_dates.EdgeDates.invalid_at', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.password\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.prompts.extract_edge_dates contains CLASS graphiti.graphiti_core.prompts.extract_edge_dates.Versions', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/tests/evals/data/longmemeval_data/README.md\nThe `longmemeval_oracle` dataset is an open-source dataset that we are using.\nWe did not create this dataset and it can be found\nhere: https://huggingface.co/datasets/xiaowu0162/longmemeval/blob/main/longmemeval_oracle.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 242, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.272897005081177,842
Which similarity search implementation adds optional source_node_uuid and target_node_uuid parameters to its filtering logic?,"The similarity search implementation that adds optional source_node_uuid and target_node_uuid parameters to its filtering logic is edge_similarity_search, which is defined in graphiti_core/search/search_utils.py. This function incorporates these UUIDs into its filter logic.",The edge_similarity_search in graphiti_core/search/search_utils.py is the only similarity search that takes optional source_node_uuid and target_node_uuid and incorporates them into its filter logic.,"['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'FUNCTION graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance has parameter or accepts argument graphiti.graphiti_core.search.search_utils.uuid', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.source_node_uuids', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 243, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.832960605621338,843
"What method is used across evaluation, graph updates, search, and community building to constrain the number of coroutines run in parallel?","The `semaphore_gather` helper is used across evaluation, graph updates, search, and community building to constrain the number of coroutines run in parallel. This method is called with `await` and accepts a `max_coroutines` parameter, which bounds how many coroutines run in parallel. It helps manage concurrency by ensuring that only a specified number of tasks are executed simultaneously, preventing overloading and ensuring efficient processing.","Throughout the codebase we use the semaphore_gather helper (e.g. await semaphore_gather(…, max_coroutines=…)) to bound how many coroutines run in parallel.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_env\nCreate a configuration instance from environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.graphiti_core.graphiti.Graphiti has field or attribute or property graphiti.graphiti_core.graphiti.max_coroutines', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.main\nMain function to run the Graphiti MCP server.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 22}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 22, 'n_files_pr': 13, 'pr': 244, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.106613636016846,844
How are exceptions handled when a wrapped coroutine fails under the semaphore limit?,"When a wrapped coroutine fails under the semaphore limit in graphiti, exceptions are handled by asyncio.gather with default return_exceptions=False. This means that any exceptions raised by the coroutines will be re-raised to the caller, and any still-pending wrapped coroutines will be canceled. Additionally, since each coroutine is executed under an async with semaphore, if a coroutine raises an exception, the semaphore’s __aexit__ method will still run, releasing the slot.","In graphiti_core/helpers.py’s semaphore_gather, each coroutine is run under an async with semaphore. If one of them raises:

• The semaphore’s __aexit__ still runs, so the slot is released.  
• asyncio.gather (default return_exceptions=False) will re-raise that exception to the caller and cancel any still-pending wrapped coroutines.","['MODULE cognee.cognee.tasks.repo_processor.get_repo_file_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_repo_file_dependencies.run_coroutine\nRun a coroutine function until it completes.  This function creates a new asyncio event loop, sets it as the current loop, and executes the given coroutine function with the provided arguments. Once the coroutine completes, the loop is closed. Intended for use in environments where an existing event loop is not available or desirable.  Parameters: -----------      - coroutine_func: The coroutine function to be run.     - *args: Positional arguments to pass to the coroutine function.     - **kwargs: Keyword arguments to pass to the coroutine function.  Returns: --------      The result returned by the coroutine after completion.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'FUNCTION cognee.cognee.tasks.repo_processor.get_repo_file_dependencies.run_coroutine has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_repo_file_dependencies.run_coroutine.coroutine_func\nRun a coroutine function until it completes.  This function creates a new asyncio event loop, sets it as the current loop, and executes the given coroutine function with the provided arguments. Once the coroutine completes, the loop is closed. Intended for use in environments where an existing event loop is not available or desirable.  Parameters: -----------      - coroutine_func: The coroutine function to be run.     - *args: Positional arguments to pass to the coroutine function.     - **kwargs: Keyword arguments to pass to the coroutine function.  Returns: --------      The result returned by the coroutine after completion.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.SEMAPHORE_LIMIT', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe\'re thrilled you\'re interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we\'re committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random ""good first issue,"" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe\'ve restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with ""help wanted"" and ""good first issue."" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don\'t need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you\'re trying to accomplish. What are you working on? What\'s getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a ""Feature Request"" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn\'t code. If you\'re using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you\'ve found an issue tagged with ""good first issue"" or ""help wanted,"" or prepared an example to share, here\'s how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We\'ve included more detailed technical instructions below; be open to feedback during review.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 246, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",33.54904818534851,845
Which functions handle node and edge deduplication in the bulk episode ingestion process?,The functions that handle node and edge deduplication in the bulk episode ingestion process are `dedupe_nodes_bulk` and `dedupe_edges_bulk`. These functions are part of the graphiti_core.graphiti module.,"In the `add_episode_bulk` method (in graphiti_core/graphiti.py) node deduplication is done by `dedupe_nodes_bulk`, and edge deduplication is done by `dedupe_edges_bulk`.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.GraphOntology\nRepresents a graph-based structure of ontology consisting of nodes and edges.  The GraphOntology class contains a collection of OntologyNode instances representing the nodes of the graph and OntologyEdge instances representing the relationships between them. Public methods include the management of nodes and edges as well as any relevant graph operations. Instance variables include a list of nodes and a list of edges.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.nodes_by_episode', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 256, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",27.741117238998413,846
What steps does remove_episode perform to delete only the edges and nodes unique to a given episode UUID?,"The steps that `remove_episode` performs to delete only the edges and nodes unique to a given episode UUID in Graphiti are:

1. It loads an episode using `EpisodicNode.get_by_uuid`. 
2. It fetches the episode’s EntityEdge objects using `EntityEdge.get_by_uuids`. 
3. It filters edges_to_delete to edges whose edge.episodes[0] equals the episode’s UUID. 
4. It calls `get_mentioned_nodes(self.driver, [episode])` to find EntityNodes mentioned by the episode. 
5. For each mentioned node, it executes a Cypher count query: `(e:Episodic)-[:MENTIONS]->(n:Entity {uuid:$uuid})`. 
6. It keeps only nodes for which the Cypher count equals 1. 
7. It deletes eligible nodes and edges asynchronously using `semaphore_gather`. 
8. It deletes the EpisodicNode by calling `episode.delete`. These steps ensure that only the edges and nodes unique to the given episode UUID are deleted, preventing accidental removal of other graph elements.","In Graphiti.remove_episode (graphiti_core/graphiti.py) the method:

1. Loads the episode via EpisodicNode.get_by_uuid.
2. Fetches all its EntityEdge objects (EntityEdge.get_by_uuids).
3. Filters “edges_to_delete” to only those whose edge.episodes[0] == the episode UUID (i.e. edges first created by this episode).
4. Calls get_mentioned_nodes(self.driver, [episode]) to find all EntityNodes the episode mentions.
5. For each node, runs a Cypher count of `(e:Episodic)-[:MENTIONS]->(n:Entity {uuid:$uuid})` and keeps only those with count == 1 (nodes mentioned nowhere else).
6. Asynchronously deletes those nodes (node.delete) and edges (edge.delete) via semaphore_gather.
7. Finally deletes the EpisodicNode itself (episode.delete).

This ensures only graph elements unique to that episode are removed.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.nodes_by_episode', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 261, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",44.16499042510986,847
How are custom entity types supplied to the episode addition flow applied during node extraction and saved with their attributes and labels in the graph?,"Custom entity types supplied to the episode addition flow are applied during node extraction by passing them as a dictionary to `Graphiti.add_episode` or `Graphiti.add_episode_bulk`. This dictionary is then used to build an `entity_types_context` list, which is passed to the LLM prompt to assign an `entity_type_id` to each extracted entity. The custom attributes and labels for these entities are also saved in the graph by merging them with node attributes and labels into the Cypher save payload. This is done in the `add_nodes_and_edges_bulk_tx` function, which is called after extracting nodes from the episode data.","When you call Graphiti.add_episode (or add_episode_bulk) with an entity_types dict, that mapping is fed into:

1. extract_nodes (in graphiti_core/utils/maintenance/node_operations.py):  
   – Builds an entity_types_context list from your custom Pydantic models (using their __doc__ as descriptions) plus the default “Entity” type.  
   – Passes it to the LLM prompt so each ExtractedEntity gets an entity_type_id.  
   – Creates each EntityNode with labels = [“Entity”, your_type_name].

2. extract_attributes_from_nodes then populates node.attributes based on your model’s fields.

3. add_nodes_and_edges_bulk_tx (in graphiti_core/utils/bulk_utils.py) (or EntityNode.save for single-node writes) merges node.attributes and node.labels into the Cypher save payload so your custom attributes and labels are persisted in the graph.","['MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.episodes', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'CLASS graphiti.graphiti_core.nodes.EpisodeType inherits from or is a subclass of enum.Enum\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 25}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 4, 'n_context_nodes': 25, 'n_files_pr': 10, 'pr': 262, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",41.745784282684326,848
Which function applies the UUID mapping to update edge pointers in the triplet addition workflow?,The function that applies the UUID mapping to update edge pointers in the triplet addition workflow is `resolve_edge_pointers` from the `graphiti.graphiti_core.utils.bulk_utils` module.,"The UUID mapping is applied by the resolve_edge_pointers function (see Graphiti.add_triplet:  
updated_edge = resolve_edge_pointers([edge], uuid_map)[0]).","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'FUNCTION graphiti.graphiti_core.utils.bulk_utils.resolve_edge_pointers has parameter or accepts argument graphiti.graphiti_core.utils.bulk_utils.resolve_edge_pointers.uuid_map', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.config_path\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS graphiti.graphiti_core.edges.Edge has field or attribute or property graphiti.graphiti_core.edges.Edge.uuid', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 263, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.823909282684326,849
Which two functions generate the Cypher filter fragments that apply node_labels to node and edge searches?,"The two functions that generate the Cypher filter fragments for node_labels in node and edge searches are `node_search_filter_query_constructor` from `graphiti_core.search.search_filters` and `edge_search_filter_query_constructor` from `graphiti_core.search.search_filters`. These functions are part of the Graphiti framework, a Python library for building temporally-aware knowledge graphs designed for AI agents.","The Cypher fragments for `node_labels` come from the two filter‐constructor functions in `graphiti_core/search/search_filters.py`:

• node_search_filter_query_constructor  
• edge_search_filter_query_constructor","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search_filters contains FUNCTION graphiti.graphiti_core.search.search_filters.node_search_filter_query_constructor', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge.add_attribute\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', 'MODULE graphiti.graphiti_core.search.search_filters contains FUNCTION graphiti.graphiti_core.search.search_filters.edge_search_filter_query_constructor', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge.get_attribute\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 21}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 21, 'n_files_pr': 7, 'pr': 265, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.74763536453247,850
What Cypher pattern do both edge search functions use to match relationships between entity nodes?,The Cypher pattern used by both edge search functions in graphiti to match relationships between entity nodes is **MATCH (n:Entity)-[r:RELATES_TO]->(m:Entity)**. This pattern matches a RELATES_TO relationship from one Entity node to another.,"Both functions use the same RELATES_TO pattern:

```
MATCH (n:Entity)-[r:RELATES_TO]->(m:Entity)
```

i.e. they match a RELATES_TO relationship from one Entity node to another.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS cognee.cognee.shared.data_models.Edge has field or attribute or property cognee.cognee.shared.data_models.Edge.relationship_name\nEdge in a knowledge graph.', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_entity_edge_from_record', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 7, 'n_files_pr': 2, 'pr': 274, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.340174198150635,851
Which fields are removed from the attributes dictionary in get_entity_node_from_record to avoid duplicating core properties?,"The fields removed from the attributes dictionary in `get_entity_node_from_record` to avoid duplicating core properties are:

* uuid
* name
* group_id
* created_at","In get_entity_node_from_record (graphiti_core/nodes.py) the following keys are popped off entity_node.attributes to avoid duplicating core props:

• ""uuid""  
• ""name""  
• ""group_id""  
• ""name_embedding""  
• ""summary""  
• ""created_at""","['CLASS graphiti.graphiti_core.errors.GraphitiError inherits from or is a subclass of graphiti.graphiti_core.errors.Exception\nBase exception class for Graphiti Core.', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.attributes', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.entity_data', 'CLASS graphiti.graphiti_core.errors.GroupIdValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a group_id contains invalid characters.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 277, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.437156200408936,852
"When a generic client wrapper is passed into the reranker constructor, which property does it use to extract the underlying async client?","When a generic client wrapper is passed into the reranker constructor, it uses the `client` property from the OpenAIClient wrapper to extract the underlying async client.","The `OpenAIRerankerClient` ctor checks for an `OpenAIClient` wrapper and, in that case, pulls out the underlying async client via its `.client` property (i.e. it does `self.client = client.client`).","[""CLASS graphiti.graphiti_core.llm_client.openai_client.OpenAIClient has method graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the BaseOpenAIClient and provides OpenAI-specific implementation for creating completions.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API."", 'CLASS graphiti.graphiti_core.embedder.openai.OpenAIEmbedder has field or attribute or property graphiti.graphiti_core.embedder.openai.client\nOpenAI Embedder Client  This client supports both AsyncOpenAI and AsyncAzureOpenAI clients.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.args\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.graphiti_core.embedder.openai.OpenAIEmbedder has method graphiti.graphiti_core.embedder.openai.OpenAIEmbedder.__init__\nOpenAI Embedder Client  This client supports both AsyncOpenAI and AsyncAzureOpenAI clients.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.embedder\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 279, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.188817024230957,853
"If classify_nodes returns an entity with type None, how does extract_nodes incorporate this into the final labels of the corresponding EntityNode?","When `classify_nodes` returns an entity with type None in graphiti, the function `extract_nodes` will incorporate this into the final labels of the corresponding EntityNode by adding a label ""None"" to the list of labels. This is because if the classifier returns None for the entity_type parameter, entity_type_name will be None, and if entity_type_name is None, the labels list will be "".Entity"", ""None""].","In extract_nodes (graphiti_core/utils/maintenance/node_operations.py) each EntityNode’s labels are built as:

```python
labels = list({ 'Entity', str(entity_type_name) })
```

So if your classifier returns `None` for the entity_type, `entity_type_name` will be `None` and you end up with labels `[“Entity”, “None”]`.","['MODULE cognee.cognee.tasks.temporal_graph.add_entities_to_event contains FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification inherits from or is a subclass of pydantic.BaseModel', 'FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event has parameter or accepts argument cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event.event_with_entities\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification', 'FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event has parameter or accepts argument cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event.event\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 281, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.058632373809814,854
How does extract_nodes exclude entities with blank names before creating EntityNode objects?,"The `extract_nodes` function excludes entities with blank names before creating EntityNode objects through a list comprehension that filters out entities whose `name.strip()` evaluates to an empty string. This filtering step occurs immediately after parsing the LLM response and before the loop that creates EntityNode objects, ensuring that only entities with non-blank names are included in the graph. This behavior can be seen in the `extract_nodes` function located in `graphiti_core/utils/maintenance/node_operations.py`.","In extract_nodes (graphiti_core/utils/maintenance/node_operations.py), right after the LLM response is parsed it does:

filtered_extracted_entities = [e for e in extracted_entities if e.name.strip()]

This list comprehension drops any entity whose name.strip() is empty (i.e. blank or whitespace only) before the loop that creates EntityNode objects.","['MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.validate_excluded_entity_types\nValidate that excluded entity types are valid type names.  Args:     excluded_entity_types: List of entity type names to exclude     entity_types: Dictionary of available custom entity types  Returns:     True if valid  Raises:     ValueError: If any excluded type names are invalid', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.MissedEntities inherits from or is a subclass of pydantic.BaseModel', 'CLASS graphiti.graphiti_core.errors.EntityTypeValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an entity type uses protected attribute names.', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification', 'FUNCTION graphiti.graphiti_core.helpers.validate_excluded_entity_types has parameter or accepts argument graphiti.graphiti_core.helpers.validate_excluded_entity_types.entity_types\nValidate that excluded entity types are valid type names.  Args:     excluded_entity_types: List of entity type names to exclude     entity_types: Dictionary of available custom entity types  Returns:     True if valid  Raises:     ValueError: If any excluded type names are invalid', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 283, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",32.29762387275696,855
Which character is used to join multiple node labels in the search filter query constructor?,"The pipe character ‘|’ is used to join multiple node labels in the search filter query constructor, as seen in the `node_search_filter_query_constructor` function defined in the file `graphiti_core/search/search_filters.py`. This allows for the construction of complex search queries by combining different node labels.","In node_search_filter_query_constructor (graphiti_core/search/search_filters.py), multiple node labels are joined with the pipe character `|`.","['CLASS qlib.qlib.contrib.data.utils.sepdf.SepDataFrame has method qlib.qlib.contrib.data.utils.sepdf.SepDataFrame.__init__\ninitialize the data based on the dataframe dictionary  Parameters ---------- df_dict : Dict[str, pd.DataFrame]     dataframe dictionary join : str     how to join the data     It will reindex the dataframe based on the join key.     If join is None, the reindex step will be skipped  skip_align :     for some cases, we can improve performance by skipping aligning index\n(Sep)erate DataFrame We usually concat multiple dataframe to be processed together(Such as feature, label, weight, filter). However, they are usually be used separately at last. This will result in extra cost for concatenating and splitting data(reshaping and copying data in the memory is very expensive)  SepDataFrame tries to act like a DataFrame whose column with multiindex', 'CLASS graphiti.graphiti_core.search.search_filters.SearchFilters has field or attribute or property graphiti.graphiti_core.search.search_filters.SearchFilters.node_labels', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'FUNCTION graphiti.graphiti_core.search.search_filters.node_search_filter_query_constructor has parameter or accepts argument graphiti.graphiti_core.search.search_filters.node_search_filter_query_constructor.filters', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 4, 'pr': 284, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.795141458511353,856
How are entity model docstrings propagated from the Person class into the extraction prompt during node creation?,"Entity model docstrings are propagated from the Person class into the extraction prompt during node creation in graphiti by being injected into the LLM prompt in two places. The first injection occurs in `extract_nodes` in the file `graphiti_core/utils/maintenance/node_operations.py`. This function builds an entity_types_context by iterating over entity_types.items() and assigns type_model.__doc__ to the 'entity_type_description' field in entity_types_context. The second injection occurs in `resolve_extracted_nodes` when constructing extracted_nodes_context. In this function, 'entity_type_description' is set to the result of entity_types_dict.get(<entity_type>).__doc__ or the string 'Default Entity Type'. This means that if an entity type has a docstring defined, it will be used as the description for that entity type in the LLM prompt. Therefore, when you ask a question related to entities and their types in graphiti, the LLM will have access to the docstrings of these entities, which can help provide more accurate and informative responses.","Entity docstrings (e.g. from your `Person` model’s `__doc__`) are pulled directly off the Pydantic model and injected into the LLM prompt in two places:

1. In `extract_nodes` (graphiti_core/utils/maintenance/node_operations.py), when building  
   `entity_types_context`:  
   ```python
   for i, (type_name, type_model) in enumerate(entity_types.items()):
       … 'entity_type_description': type_model.__doc__
   ```
2. In `resolve_extracted_nodes` when constructing `extracted_nodes_context`:  
   ```python
   'entity_type_description':
       entity_types_dict.get(<entity_type>).__doc__ or 'Default Entity Type'
   ```

So your `Person.__doc__` (“A human person…”) becomes the `entity_type_description` field the LLM sees during node extraction.","['CLASS graphiti.examples.podcast.podcast_runner.IsPresidentOf inherits from or is a subclass of pydantic.BaseModel\nRelationship between a person and the entity they are a president of', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.ExtractedEntities inherits from or is a subclass of pydantic.BaseModel', 'MODULE graphiti.examples.podcast.podcast_runner contains CLASS graphiti.examples.podcast.podcast_runner.IsPresidentOf\nRelationship between a person and the entity they are a president of', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.ExtractedEntity inherits from or is a subclass of pydantic.BaseModel', 'MODULE cognee.cognee.infrastructure.databases.exceptions.exceptions contains CLASS cognee.cognee.infrastructure.databases.exceptions.exceptions.EntityAlreadyExistsError\nRepresents an error when an entity creation is attempted but the entity already exists.  This class is derived from CogneeValidationError and is used to signal a conflict in operations involving resource creation.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 287, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",42.838685274124146,857
How does the new get_by_node_uuid method retrieve edges regardless of their direction?,"The new `get_by_node_uuid` method retrieves edges regardless of their direction by using an undirected pattern in the Cypher query, which matches both incoming and outgoing RELATES_TO edges. This is achieved by omitting arrow directions in a Cypher relationship pattern, allowing it to match all relevant edges.","In EntityEdge.get_by_node_uuid (graphiti_core/edges.py) the Cypher uses an undirected pattern:

 MATCH (n:Entity {uuid: $node_uuid})–[e:RELATES_TO]–(m:Entity)

By omitting the “→”/“←” arrows it matches both incoming and outgoing RELATES_TO edges.","['CLASS graphiti.graphiti_core.errors.EdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a list of edges is not found.', 'FUNCTION graphiti.graphiti_core.utils.bulk_utils.resolve_edge_pointers has parameter or accepts argument graphiti.graphiti_core.utils.bulk_utils.resolve_edge_pointers.uuid_map', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.edge_uuid_map', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 4, 'pr': 291, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.451337099075317,858
Which utility is used to limit the number of simultaneous edge-fetching coroutines in this method?,"The utility used to limit the number of simultaneous edge-fetching coroutines in this method is `semaphore_gather`. This utility, invoked with `max_coroutines=self.max_coroutines`, caps the number of concurrent edge-fetching coroutines.",The method uses the `semaphore_gather` utility (invoked with `max_coroutines=self.max_coroutines`) to cap the number of concurrent edge-fetching coroutines.,"['CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'CLASS cognee.cognee.infrastructure.loaders.external.beautiful_soup_loader.BeautifulSoupLoader has method cognee.cognee.infrastructure.loaders.external.beautiful_soup_loader.BeautifulSoupLoader.can_handle\nCrawler for fetching and extracting web content using BeautifulSoup.  Supports asynchronous HTTP requests, Playwright for JavaScript rendering, robots.txt compliance, and rate limiting. Extracts content using CSS selectors or XPath rules.  Attributes:     concurrency: Number of concurrent requests allowed.     crawl_delay: Minimum seconds between requests to the same domain.     max_crawl_delay: Maximum crawl delay to respect from robots.txt (None = no limit).     timeout: Per-request timeout in seconds.     max_retries: Number of retries for failed requests.     retry_delay_factor: Multiplier for exponential backoff on retries.     headers: HTTP headers for requests (e.g., User-Agent).     robots_cache_ttl: Time-to-live for robots.txt cache in seconds.', 'CLASS graphiti.graphiti_core.graphiti.Graphiti has field or attribute or property graphiti.graphiti_core.graphiti.max_coroutines', 'MODULE cognee.cognee.infrastructure.loaders.external.beautiful_soup_loader contains CLASS cognee.cognee.infrastructure.loaders.external.beautiful_soup_loader.BeautifulSoupLoader\nCrawler for fetching and extracting web content using BeautifulSoup.  Supports asynchronous HTTP requests, Playwright for JavaScript rendering, robots.txt compliance, and rate limiting. Extracts content using CSS selectors or XPath rules.  Attributes:     concurrency: Number of concurrent requests allowed.     crawl_delay: Minimum seconds between requests to the same domain.     max_crawl_delay: Maximum crawl delay to respect from robots.txt (None = no limit).     timeout: Per-request timeout in seconds.     max_retries: Number of retries for failed requests.     retry_delay_factor: Multiplier for exponential backoff on retries.     headers: HTTP headers for requests (e.g., User-Agent).     robots_cache_ttl: Time-to-live for robots.txt cache in seconds.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 292, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.111119508743286,859
Which initialization parameter lets you inject an Azure OpenAI client instead of the default one?,"The initialization parameter that lets you inject an Azure OpenAI client instead of the default one is `client` in the `OpenAIRerankerClient` class, which is used to pass an instance of `AsyncAzureOpenAI`. This allows for customization and flexibility in using different Azure OpenAI clients.",Use the `client` parameter of `OpenAIEmbedder.__init__` (in `graphiti_core/embedder/openai.py`) to pass in your own `AsyncAzureOpenAI` instance instead of the default.,"['MODULE graphiti.graphiti_core.llm_client.openai_base_client contains CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'MODULE graphiti.graphiti_core.embedder.azure_openai contains CLASS graphiti.graphiti_core.embedder.azure_openai.AzureOpenAIEmbedderClient\nWrapper class for AsyncAzureOpenAI that implements the EmbedderClient interface.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has method graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.__init__\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'METHOD graphiti.graphiti_core.embedder.azure_openai.AzureOpenAIEmbedderClient.__init__ has parameter or accepts argument graphiti.graphiti_core.embedder.azure_openai.AzureOpenAIEmbedderClient.__init__.azure_client', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_base_client.max_tokens\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 293, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.969953298568726,860
What list holds the ontology definitions against which extracted entity labels are validated?,The list that holds the ontology definitions against which extracted entity labels are validated in Graphiti is `entity_types_context` inside the `extract_nodes` module. This list is used to validate extracted entity labels by comparing them with the ontology definitions stored in this list.,The ontology definitions are stored in the `entity_types_context` list inside `extract_nodes` (in `graphiti_core/utils/maintenance/node_operations.py`).,"['MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.ontology_utils.entity_types_utils contains FUNCTION graphiti.graphiti_core.utils.ontology_utils.entity_types_utils.validate_entity_types', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification has field or attribute or property graphiti.graphiti_core.prompts.extract_nodes.EntityClassification.entity_classifications', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 295, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",32.617676973342896,861
Under what condition will a node adopt its highest-ranking neighbor’s community instead of keeping its current community?,"In graphiti, under what condition will a node adopt its highest-ranking neighbor’s community instead of keeping its current community? The node will adopt its highest-ranking neighbor’s community if the candidate_rank is greater than 1.","In label_propagation (community_operations.py), a node will switch to its top‐ranked neighbor community only if that community’s total edge_count (candidate_rank) is greater than 1 (and community_candidate ≠ –1). Otherwise it keeps (or maxes) its current community.","['FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS graphiti.graphiti_core.nodes.CommunityNode inherits from or is a subclass of graphiti.graphiti_core.nodes.Node', 'MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'CLASS graphiti.graphiti_core.utils.maintenance.community_operations.Neighbor inherits from or is a subclass of pydantic.BaseModel', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfounders@getzep.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 302, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.529430627822876,862
"What is the overall flow from parsing CLI arguments to initializing the Graphiti client, including how custom entity extraction and graph destruction flags are handled?","The overall flow from parsing CLI arguments to initializing the Graphiti client, including handling of custom entity extraction and graph destruction flags is as follows:

1.  The `initialize_server()` function in `graphiti_mcp_server.py` parses CLI arguments using `argparse`. 
2.  It calls `GraphitiConfig.from_cli_and_env(args)` to create a configuration object from the parsed CLI arguments and environment variables.
3.  The `GraphitiConfig` class merges CLI overrides on top of environment variables and builds nested LLM, Embedder, and Neo4j sub-configs.
4.  The function logs whether a group ID was provided or generated and whether custom entities are enabled.
5.  It calls `initialize_graphiti()` to initialize the Graphiti client.
6.  In `initialize_graphiti()`, it creates an LLM client using `config.llm.create_client()`. 
7.  If `use_custom_entities` is true but no API key is set, it throws an error.
8.  It validates the Neo4j URI, user, and password from `config.neo4j`. 
9.  It builds an embedder client via `config.embedder.create_client()`. 
10. It instantiates a Graphiti client with the URI, credentials, LLM client, embedder, and semaphore limit.
11. If `destroy_graph` is true, it calls `await clear_data(graphiti_client.driver)` to drop the existing graph.
12. Finally, it logs the final settings: model, temperature, group ID, whether custom entities are on, and concurrency limit.

Custom entity extraction and graph destruction flags are handled as follows:
*   Custom entities are enabled if the `--use-custom-entities` flag is provided in the CLI arguments.
*   Graph destruction is performed if the `--destroy-graph` flag is true. In this case, the existing graph is dropped before initializing the new Graphiti client.","The startup flow lives in mcp_server/graphiti_mcp_server.py and goes roughly as follows:

1. initialize_server()  
   – Parses CLI args (`--group-id`, `--use-custom-entities`, `--destroy-graph`, etc.) via argparse.  
   – Calls GraphitiConfig.from_cli_and_env(args) to merge CLI overrides (group_id, use_custom_entities, destroy_graph) on top of env vars and to build the nested LLM/Embedder/Neo4j sub-configs.  
   – Logs whether a group_id was provided or generated and whether custom entities are enabled.  

2. initialize_graphiti()  
   – Uses config.llm.create_client() to build an LLM client; if `use_custom_entities` is true but no API key is set, throws an error.  
   – Validates Neo4j URI/user/password from config.neo4j.  
   – Builds an embedder client via config.embedder.create_client().  
   – Instantiates `graphiti_client = Graphiti(...)` with uri, credentials, llm_client, embedder, and a semaphore limit.  
   – If `config.destroy_graph` is true, calls `await clear_data(graphiti_client.driver)` to drop the existing graph.  
   – Calls `await graphiti_client.build_indices_and_constraints()` to set up the DB schema.  
   – Logs the final settings: model, temperature, group_id, whether custom entities are on, and concurrency limit.  

3. Back in initialize_server():  
   – Optionally sets `mcp.settings.host` from `--host`.  
   – Returns the MCPConfig built from the same CLI args.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.args', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.from_cli_and_env\nCreate LLM configuration from CLI arguments, falling back to environment variables.\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.parser', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n# Graphiti MCP Server\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nThis is an experimental Model Context Protocol (MCP) server implementation for Graphiti. The MCP server exposes\nGraphiti's key functionality through the MCP protocol, allowing AI assistants to interact with Graphiti's knowledge\ngraph capabilities.\n\n## Features\n\nThe Graphiti MCP server exposes the following key high-level functions of Graphiti:\n\n- **Episode Management**: Add, retrieve, and delete episodes (text, messages, or JSON data)\n- **Entity Management**: Search and manage entity nodes and relationships in the knowledge graph\n- **Search Capabilities**: Search for facts (edges) and node summaries using semantic and hybrid search\n- **Group Management**: Organize and manage groups of related data with group_id filtering\n- **Graph Maintenance**: Clear the graph and rebuild indices\n\n## Quick Start\n\n### Clone the Graphiti GitHub repo\n\n```bash\ngit clone https://github.com/getzep/graphiti.git\n```\n\nor\n\n```bash\ngh repo clone getzep/graphiti\n```\n\n### For Claude Desktop and other `stdio` only clients\n\n1. Note the full path to this directory.\n\n```\ncd graphiti && pwd\n```\n\n2. Install the [Graphiti prerequisites](#prerequisites).\n\n3. Configure Claude, Cursor, or other MCP client to use [Graphiti with a `stdio` transport](#integrating-with-mcp-clients). See the client documentation on where to find their MCP configuration files.\n\n### For Cursor and other `sse`-enabled clients\n\n1. Change directory to the `mcp_server` directory\n\n`cd graphiti/mcp_server`\n\n2. Start the service using Docker Compose\n\n`docker compose up`\n\n3. Point your MCP client to `http://localhost:8000/sse`\n\n## Installation\n\n### Prerequisites\n\n1. Ensure you have Python 3.10 or higher installed.\n2. A running Neo4j database (version 5.26 or later required)\n3. OpenAI API key for LLM operations\n\n### Setup\n\n1. Clone the repository and navigate to the mcp_server directory\n2. Use `uv` to create a virtual environment and install dependencies:\n\n```bash\n# Install uv if you don't have it already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a virtual environment and install dependencies in one step\nuv sync\n```\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `NEO4J_URI`: URI for the Neo4j database (default: `bolt://localhost:7687`)\n- `NEO4J_USER`: Neo4j username (default: `neo4j`)\n- `NEO4J_PASSWORD`: Neo4j password (default: `demodemo`)\n- `OPENAI_API_KEY`: OpenAI API key (required for LLM operations)\n- `OPENAI_BASE_URL`: Optional base URL for OpenAI API\n- `MODEL_NAME`: OpenAI model name to use for LLM operations.\n- `SMALL_MODEL_NAME`: OpenAI model name to use for smaller LLM operations.\n- `LLM_TEMPERATURE`: Temperature for LLM responses (0.0-2.0).\n- `AZURE_OPENAI_ENDPOINT`: Optional Azure OpenAI LLM endpoint URL""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 304, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",60.87726140022278,863
How does summarize_context handle attributes whose values cannot be found in the provided messages?,"The `summarize_context` function in Graphiti handles attributes whose values cannot be found in the provided messages by setting them to `None`. This is done through an LLM prompt that instructs the model to set properties with missing values to `None` if they cannot be extracted. The injected prompt in `graphiti_core/prompts/summarize_nodes.py` explicitly states this behavior, ensuring that any attribute the model cannot extract is assigned the value `None`.","The `summarize_context` function doesn’t do any Python‐side defaulting – it simply injects a user prompt (in `graphiti_core/prompts/summarize_nodes.py`) that tells the LLM:

“If the value of the entity property cannot be found in the current context, set the value of the property to the Python value `None`.”

So any attribute the model can’t extract ends up as `None`.","['MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.resolve_module_path\nFind the file path of a module.  Return the file path of the specified module if found, or return None if the module does not exist or cannot be located.  Parameters: -----------      - module_name: The name of the module whose file path is to be resolved.  Returns: --------      The file path of the module as a string or None if the module is not found.', 'CLASS graphiti.graphiti_core.prompts.summarize_nodes.Prompt has field or attribute or property graphiti.graphiti_core.prompts.summarize_nodes.Prompt.summarize_context', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.resolve_module_path has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.resolve_module_path.module_name\nFind the file path of a module.  Return the file path of the specified module if found, or return None if the module does not exist or cannot be located.  Parameters: -----------      - module_name: The name of the module whose file path is to be resolved.  Returns: --------      The file path of the module as a string or None if the module is not found.', 'CLASS graphiti.graphiti_core.prompts.summarize_nodes.Versions has field or attribute or property graphiti.graphiti_core.prompts.summarize_nodes.Versions.summarize_context', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.user\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 305, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.878320455551147,864
Which exception is raised when the procedure for retrieving multiple edges by UUIDs finds no matches?,The exception raised when no edges are found for the given UUIDs in graphiti is EdgesNotFoundError.,"When no edges are found for the given UUIDs, an EdgesNotFoundError (from graphiti_core/errors.py) is raised.","['CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.errors.EdgesNotFoundError has field or attribute or property graphiti.graphiti_core.errors.uuids\nRaised when a list of edges is not found.', 'CLASS graphiti.graphiti_core.errors.EdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a list of edges is not found.', 'CLASS graphiti.graphiti_core.errors.EdgeNotFoundError has field or attribute or property graphiti.graphiti_core.errors.uuid\nRaised when an edge is not found.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError\nRaised when no edges are found for a list of group ids.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 307, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",26.381036520004272,865
What does get_by_uuids return when provided with an empty UUID list?,"When `EntityEdge.get_by_uuids` is called with an empty list of UUIDs in Graphiti, it returns an empty list (`[]`).","In EntityEdge.get_by_uuids (graphiti_core/edges.py), if you pass in an empty `uuids` list it immediately returns an empty list (`[]`).","[""METHOD cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names has parameter or accepts argument cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names.data_point\nRetrieve the names of embeddable properties defined in the metadata.  If no index fields are defined in the metadata, this method will return an empty list.  Parameters: -----------      - data_point ('DataPoint'): The DataPoint instance from which to retrieve embeddable       property names.  Returns: --------      A list of property names corresponding to the index fields, or an empty list if none     exist."", 'CLASS graphiti.graphiti_core.edges.Edge has field or attribute or property graphiti.graphiti_core.edges.Edge.uuid', ""CLASS cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint has method cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names\nRetrieve the names of embeddable properties defined in the metadata.  If no index fields are defined in the metadata, this method will return an empty list.  Parameters: -----------      - data_point ('DataPoint'): The DataPoint instance from which to retrieve embeddable       property names.  Returns: --------      A list of property names corresponding to the index fields, or an empty list if none     exist.\nModel representing a data point with versioning and metadata support.  Public methods include: - get_embeddable_data - get_embeddable_properties - get_embeddable_property_names - update_version - to_json - from_json - to_pickle - from_pickle - to_dict - from_dict"", 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.uuid_entity_map', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 308, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.07655143737793,866
What node labels does the selective deletion logic match when removing nodes by group IDs?,"The delete_group_ids function matches nodes with the labels Entity, Episodic, and Community.","In clear_data (in graphiti_core/utils/maintenance/graph_data_operations.py), the delete_group_ids routine matches nodes with the labels Entity, Episodic or Community.","['CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.nodes.Node has field or attribute or property graphiti.graphiti_core.nodes.Node.group_id', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.utils.maintenance.graph_data_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.graph_data_operations.group_id_filter', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.main\nMain function to run the Graphiti MCP server.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 316, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.1507408618927,867
"At what point during the episode ingestion process are entity type names validated, and which exception is raised for protected names?","Entity type names are validated during the episode ingestion process in Graphiti when `Graphiti.add_episode` calls `validate_entity_types(entity_types)`, which is called immediately after recording the start time. The exception raised for protected names is `ProtectedEntityTypeNameException`. This method checks entity type names and throws a `ProtectedEntityTypeNameException` when given a protected name, such as 'Entity'.","Entity type names are checked right at the top of Graphiti.add_episode (the very first thing it does after recording the start time) by the call to validate_entity_types(entity_types). If you try to use a protected name (e.g. “Entity”), that helper will throw a ProtectedEntityTypeNameException.","['CLASS graphiti.graphiti_core.errors.EntityTypeValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an entity type uses protected attribute names.', 'CLASS cognee.cognee.shared.exceptions.exceptions.IngestionError has field or attribute or property cognee.cognee.shared.exceptions.exceptions.name', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.EntityTypeValidationError\nRaised when an entity type uses protected attribute names.', 'CLASS cognee.cognee.modules.ingestion.exceptions.exceptions.IngestionError has field or attribute or property cognee.cognee.modules.ingestion.exceptions.exceptions.name', 'CLASS graphiti.graphiti_core.errors.EntityTypeValidationError has method graphiti.graphiti_core.errors.EntityTypeValidationError.__init__\nRaised when an entity type uses protected attribute names.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""cognee\nhttps://github.com/topoteretes/cognee/blob/main//DCO.md\nDeveloper Certificate of Origin Version 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 5, 'pr': 320, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",35.16622066497803,868
Which retrieval methods across the edges and nodes modules sort results by UUID instead of the creation timestamp?,"The retrieval methods across the edges and nodes modules in graphiti that sort results by UUID instead of the creation timestamp are:

* `EpisodicEdge.get_by_group_ids` in `graphiti_core/edges.py`
* `EntityEdge.get_by_group_ids` in `graphiti_core/edges.py`
* `CommunityEdge.get_by_group_ids` in `graphiti_core/edges.py`
* `EpisodicNode.get_by_group_ids` in `graphiti_core/nodes.py`
* `EntityNode.get_by_group_ids` in `graphiti_core/nodes.py`
* `CommunityNode.get_by_group_ids` in `graphiti_core/nodes.py`

These methods sort the results by UUID, which is a unique identifier for each edge or node.","All of the shown `get_by_group_ids` methods sort by UUID, not timestamp:

• graphiti_core/edges.py  
  – EpisodicEdge.get_by_group_ids  
  – EntityEdge.get_by_group_ids  
  – CommunityEdge.get_by_group_ids  

• graphiti_core/nodes.py  
  – EpisodicNode.get_by_group_ids  
  – EntityNode.get_by_group_ids  
  – CommunityNode.get_by_group_ids","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.GraphOntology\nRepresents a graph-based structure of ontology consisting of nodes and edges.  The GraphOntology class contains a collection of OntologyNode instances representing the nodes of the graph and OntologyEdge instances representing the relationships between them. Public methods include the management of nodes and edges as well as any relevant graph operations. Instance variables include a list of nodes and a list of edges.', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.sorted_result_uuids', 'CLASS cognee.cognee.tasks.graph.models.GraphOntology inherits from or is a subclass of pydantic.BaseModel\nRepresents a graph-based structure of ontology consisting of nodes and edges.  The GraphOntology class contains a collection of OntologyNode instances representing the nodes of the graph and OntologyEdge instances representing the relationships between them. Public methods include the management of nodes and edges as well as any relevant graph operations. Instance variables include a list of nodes and a list of edges.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.sorted_uuids', 'CLASS cognee.cognee.tasks.graph.models.GraphOntology has field or attribute or property cognee.cognee.tasks.graph.models.GraphOntology.nodes\nRepresents a graph-based structure of ontology consisting of nodes and edges.  The GraphOntology class contains a collection of OntologyNode instances representing the nodes of the graph and OntologyEdge instances representing the relationships between them. Public methods include the management of nodes and edges as well as any relevant graph operations. Instance variables include a list of nodes and a list of edges.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 2, 'pr': 322, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",36.99264478683472,869
Which utility functions now pass the default database parameter to Neo4j driver sessions and queries to support non-default databases?,"The utility functions that now pass the default database parameter to Neo4j driver sessions and queries to support non-default databases are: `add_nodes_and_edges_bulk` in `graphiti_core/utils/bulk_utils.py`, `build_indices_and_constraints` in `graphiti_core/utils/maintenance/graph_data_operations.py`, `clear_data` in `graphiti_core/utils/maintenance/graph_data_operations.py`, and `retrieve_episodes` in `graphiti_core/utils/maintenance/graph_data_operations.py`. This allows targeting non-default databases by passing the default database parameter to these functions.","The following helpers now forward the default‐DB through to the Neo4j driver:

• graphiti_core/utils/bulk_utils.py – add_nodes_and_edges_bulk  
• graphiti_core/utils/maintenance/graph_data_operations.py – build_indices_and_constraints, clear_data and retrieve_episodes  

Each of these opens a session or calls execute_query with the default database parameter so you can target non-default databases.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.embedder\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'METHOD graphiti.graphiti_core.driver.neo4j_driver.Neo4jDriver.session has parameter or accepts argument graphiti.graphiti_core.driver.neo4j_driver.Neo4jDriver.session.database', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'METHOD cognee.cognee.infrastructure.databases.graph.neo4j_driver.adapter.Neo4jAdapter.__init__ has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.neo4j_driver.adapter.Neo4jAdapter.__init__.graph_database_url', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 329, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",41.13470125198364,870
How is the max_tokens argument forwarded during OpenAIClient initialization?,"The max_tokens argument is forwarded during OpenAIClient initialization by setting it as an attribute of the LLMConfig object, which is then passed to the OpenAIClient constructor. This can be seen in the code snippet provided below, where an LLMConfig object is created with the max_tokens attribute set to 2048 and then passed to the OpenAIClient constructor when initializing Graphiti.

```python
default
from openai import AsyncAzureOpenAI
from graphiti_core import Graphiti
from graphiti_core.llm_client import LLMConfig, OpenAIClient

# Azure OpenAI configuration - use separate endpoints for different services
api_key = ""<your-api-key>""
api_version = ""<your-api-version>""
llm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""
embedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""

# Create separate Azure OpenAI clients for different services
class LLMConfig:
    def __init__(self, small_model, model, max_tokens):
        self.small_model = small_model
        self.model = model
        self.max_tokens = max_tokens

llm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)

embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)

# Create LLM Config with your Azure deployment names and max_tokens
class OpenAIClient:
    def __init__(self, llm_config, client):
        self.llm_config = llm_config
        self.client = client

azure_llm_config = LLMConfig(
    small_model=""gpt-4.1-nano"",
    model=""gpt-4.1-mini"",
    max_tokens=2048  # Set max_tokens to 2048
)

# Initialize Graphiti with Azure OpenAI clients
graphiti = Graphiti(
    ""bolt://localhost:7687"",
    ""neo4j"",
    ""password"",
    llm_client=OpenAIClient(
        llm_config=azure_llm_config,
        client=llm_client_azure
    ),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(
            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name
        ),
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(
        llm_config=LLMConfig(
            model=azure_llm_config.small_model  # Use small model for reranking
        ),
        client=llm_client_azure
    )
```

In this code snippet, we create an LLMConfig object with the max_tokens attribute set to 2048. We then pass this config object to the OpenAIClient constructor when initializing Graphiti.","In OpenAIClient.__init__ (graphiti_core/llm_client/openai_client.py) the max_tokens parameter is simply passed straight through to the base class by calling:

```python
super().__init__(config, cache, max_tokens)
```

i.e. it’s forwarded into LLMClient’s constructor.","['METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.max_tokens\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'METHOD graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.__init__.max_tokens', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient inherits from or is a subclass of client.LLMClient\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'METHOD graphiti.graphiti_core.llm_client.azure_openai_client.AzureOpenAILLMClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.azure_openai_client.AzureOpenAILLMClient.__init__.max_tokens', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_generic_client.client\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 330, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",80.56663727760315,871
Which class methods are used to load environment variables and apply CLI overrides to build the Graphiti configuration before server startup?,"The class methods used to load environment variables and apply CLI overrides to build the Graphiti configuration before server startup are:

1. `GraphitiConfig.from_env()`: This method loads settings from environment variables.
2. `GraphitiConfig.from_cli_and_env(args)`: This method starts from `GraphitiConfig.from_env()` and applies CLI overrides before startup.
These methods can be found in the `mcp_server/graphiti_mcp_server.py` file of the Graphiti codebase.","The two methods on GraphitiConfig in mcp_server/graphiti_mcp_server.py are used:

• GraphitiConfig.from_env() – loads settings from environment variables  
• GraphitiConfig.from_cli_and_env(args) – starts from from_env(), then applies CLI overrides before startup","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.server.graph_service.main contains GLOBAL_VARIABLE graphiti.server.graph_service.main.settings', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_env\nCreate a configuration instance from environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.server.graph_service.main contains GLOBAL_VARIABLE graphiti.server.graph_service.main.app', 'METHOD graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env.cls\nCreate configuration from CLI arguments, falling back to environment variables.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 2, 'pr': 332, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.70950675010681,872
How do the methods for fetching nodes and edges by group construct their Cypher queries to implement cursor-based pagination?,"The methods for fetching nodes and edges by group in graphiti construct their Cypher queries to implement cursor-based pagination as follows:

1.  The `get_by_group_ids` function performs a two-step string build to create the Cypher query.
2.  In the first step, the `cursor_query` is set based on the presence of the `uuid_cursor` argument. If it's provided, `cursor_query` is set to `'AND e.uuid < $uuid'` for edges or `'AND n.uuid < $uuid'` for nodes. Otherwise, it remains an empty string.
3.  In the second step, the `limit_query` is set based on the presence of the `limit` argument. If it's not `None`, `limit_query` is set to `'LIMIT $limit'`. Otherwise, it remains an empty string.
4.  The two query fragments, `cursor_query` and `limit_query`, are concatenated into a Cypher query.
5.  The Cypher query includes a `MATCH` clause with a `WHERE` condition filtering by the group IDs provided in `$group_ids`.
6.  The Cypher query orders the matched entities by their UUID in descending order.

This approach allows for classic cursor-style pagination when both `$uuid` and `$limit` parameters are passed, enabling efficient retrieval of nodes and edges by group while supporting pagination.","Each `get_by_group_ids` (in graphiti_core/edges.py and graphiti_core/nodes.py) does the same two-step string build:

1.  
   cursor_query =  
       ‘AND e.uuid < $uuid’  (or ‘AND n.uuid < $uuid’ for nodes)  
     if a `uuid_cursor` was passed, otherwise empty.

2.  
   limit_query =  
       ‘LIMIT $limit’  
     if `limit` is not None, otherwise empty.

They then plug those into the Cypher:

  MATCH … WHERE … IN $group_ids  
  + cursor_query  
  RETURN …  
  ORDER BY (e|n).uuid DESC  
  + limit_query

This orders by UUID descending and, when you pass `$uuid`/`$limit`, yields classic cursor-style pagination.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.GraphOntology\nRepresents a graph-based structure of ontology consisting of nodes and edges.  The GraphOntology class contains a collection of OntologyNode instances representing the nodes of the graph and OntologyEdge instances representing the relationships between them. Public methods include the management of nodes and edges as well as any relevant graph operations. Instance variables include a list of nodes and a list of edges.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query', 'MODULE cognee.cognee.modules.retrieval.graph_completion_retriever contains CLASS cognee.cognee.modules.retrieval.graph_completion_retriever.GraphCompletionRetriever\nRetriever for handling graph-based completion searches.  This class provides methods to retrieve graph nodes and edges, resolve them into a human-readable format, and generate completions based on graph context. Public methods include: - resolve_edges_to_text - get_triplets - get_context - get_completion', 'FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query has parameter or accepts argument graphiti.graphiti_core.graph_queries.NEO4J_TO_FALKORDB_MAPPING', 'CLASS cognee.cognee.tasks.graph.models.GraphOntology inherits from or is a subclass of pydantic.BaseModel\nRepresents a graph-based structure of ontology consisting of nodes and edges.  The GraphOntology class contains a collection of OntologyNode instances representing the nodes of the graph and OntologyEdge instances representing the relationships between them. Public methods include the management of nodes and edges as well as any relevant graph operations. Instance variables include a list of nodes and a list of edges.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 6, 'n_files_pr': 3, 'pr': 335, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",49.888447284698486,873
"What is the high-level sequence of operations performed when a new episode is added to the graph, from node extraction to bulk storage?","The high-level sequence of operations performed when a new episode is added to the graph, from node extraction to bulk storage in Graphiti, is as follows:

1. **Node Extraction**: The `add_episode` function calls `extract_nodes` to produce a list of raw EntityNode objects.
2. **Node Resolution**: The `semaphore_gather` function runs `resolve_extracted_nodes` and `extract_edges` in parallel. `resolve_extracted_nodes` produces deduplicated nodes, a UUID map, and duplicates.
3. **Edge Extraction**: The `extract_edges` function produces a list of raw EntityEdge objects.
4. **Edge Resolution**: The `semaphore_gather` function runs `resolve_edge_pointers` and `extract_attributes_from_nodes` in parallel. `resolve_edge_pointers` swaps in resolved node UUIDs, while `extract_attributes_from_nodes` hydrates node attributes.
5. **Duplicate Edge Creation**: The `build_duplicate_of_edges` function is called to create duplicate edges for each edge in the graph.
6. **Episodic Edge Creation**: The `build_episodic_edges` function is called to create episodic edges between nodes in the graph.
7. **Bulk Storage**: The `add_nodes_and_edges_bulk` function performs a bulk store of the episode node, episodic edges, hydrated nodes, and entity edges with embeddings.","In Graphiti.add_episode (graphiti_core/graphiti.py) the key steps from node extraction to bulk write are:

1. call extract_nodes → list of raw EntityNode  
2. in parallel via semaphore_gather:  
   • resolve_extracted_nodes (graphiti_core/utils/maintenance/node_operations.py) → deduplicated nodes, UUID map, duplicates  
   • extract_edges (graphiti_core/graphiti.py) → raw EntityEdge list  
3. resolve_edge_pointers (graphiti_core/graphiti.py) to swap in resolved node UUIDs  
4. in parallel via semaphore_gather:  
   • resolve_extracted_edges (graphiti_core/utils/maintenance/edge_operations.py) → resolved vs. invalidated edges (with embeddings)  
   • extract_attributes_from_nodes (graphiti_core/graphiti.py) → hydrate node attributes  
5. build_duplicate_of_edges (graphiti_core/graphiti.py) & build_episodic_edges (graphiti_core/graphiti.py)  
6. add_nodes_and_edges_bulk (graphiti_core/graphiti.py) → bulk store the episode node, episodic edges, hydrated nodes & entity edges (with embeddings)","['CLASS cognee.cognee.infrastructure.files.storage.LocalFileStorage.LocalFileStorage has method cognee.cognee.infrastructure.files.storage.LocalFileStorage.LocalFileStorage.list_files\nList all files in the specified directory.  Parameters: -----------     - directory_path (str): The directory path to list files from     - recursive (bool): If True, list files recursively in subdirectories  Returns: --------     - list[str]: List of file paths relative to the storage root\nManage local file storage operations such as storing, retrieving, and managing files on the filesystem.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.nodes_by_episode', 'CLASS qlib.qlib.data.dataset.storage.BaseHandlerStorage has method qlib.qlib.data.dataset.storage.BaseHandlerStorage.fetch\nfetch data from the data storage  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index level : Union[str, int]     which index level to select the data     - if level is None, apply selector to df directly col_set : Union[str, List[str]]     - if isinstance(col_set, str):         select a set of meaningful columns.(e.g. features, columns)         if col_set == DataHandler.CS_RAW:             the raw dataset will be returned.     - if isinstance(col_set, List[str]):         select several sets of meaningful columns, the returned data has multiple level fetch_orig : bool     Return the original data instead of copy if possible.  Returns ------- pd.DataFrame     the dataframe fetched\nBase data storage for datahandler - pd.DataFrame is the default data storage format in Qlib datahandler - If users want to use custom data storage, they should define subclass inherited BaseHandlerStorage, and implement the following method', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'CLASS cognee.cognee.infrastructure.files.storage.S3FileStorage.S3FileStorage inherits from or is a subclass of storage.Storage\nManage S3 file storage operations such as storing, retrieving, and managing files on S3-compatible storage.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 24}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 24, 'n_files_pr': 19, 'pr': 338, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",49.37960338592529,874
Describe the overall search execution flow from the high-level hybrid method to the vector-based similarity query.,"The overall search execution flow from the high-level hybrid method to the vector-based similarity query in Graphiti involves the following steps: The `Graphiti.search` function is called, which picks a `SearchConfig` and hands it off to the shared search entry point. In the case of a hybrid search, the routine launches two separate searches: text/RF and vector search. The vector search leg is implemented by the `edge_similarity_search` function in `graphiti_core/search/search_utils.py`, which takes the query's embedding as input, applies filters, builds a Cypher query using `get_vector_cosine_func_query`, executes it via Neo4j driver, and maps returned records to EntityEdge objects. This flow enables Graphiti to execute complex searches that combine text-based relevance ranking with vector-based similarity matching.","The high-level flow is:

1. You call Graphiti.search (in graphiti_core/graphiti.py), which picks a SearchConfig (hybrid RRF or distance-aware) and hands off to the shared search(...) entry point.  
2. That hybrid search routine (in graphiti_core/search/…) internally launches both a text/RF search and a vector search.  
3. The vector leg is implemented by edge_similarity_search (graphiti_core/search/search_utils.py):  
   – It takes your query’s embedding (search_vector), applies any group/attribute filters,  
   – Builds a Cypher query using get_vector_cosine_func_query(…) to compute cosine scores,  
   – Runs it via driver.execute_query,  
   – Maps the returned records to EntityEdge objects.","['CLASS cognee.cognee.modules.retrieval.natural_language_retriever.NaturalLanguageRetriever inherits from or is a subclass of cognee.modules.retrieval.base_retriever.BaseRetriever\nRetriever for handling natural language search.  Public methods include:  - get_context: Retrieves relevant context using a natural language query converted to Cypher. - get_completion: Returns a completion based on the query and context.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'FUNCTION graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance has parameter or accepts argument graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance.query_vector', 'CLASS qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter has method qlib.qlib.rl.order_execution.strategy.SAOEStateAdapter.generate_metrics_after_done\nGenerate metrics once the upper level execution is done\nMaintain states of the environment. SAOEStateAdapter accepts execution results and update its internal state according to the execution results with additional information acquired from executors & exchange. For example, it gets the dealt order amount from execution results, and get the corresponding market price / volume from exchange.  Example usage::      adapter = SAOEStateAdapter(...)     adapter.update(...)     state = adapter.saoe_state', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 6, 'pr': 342, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",45.508296966552734,875
What boolean result does the evaluation prompt instruct to return when the candidate and baseline extractions are nearly identical in quality?,The evaluation prompt instructs to return `True` when the candidate and baseline extractions are nearly identical in quality.,"In eval_add_episode_results (graphiti_core/prompts/eval.py), the user prompt says “If the CANDIDATE extraction and BASELINE extraction are nearly identical in quality, return True.”","['CLASS xgboost.python-package.xgboost.core.Booster has method xgboost.python-package.xgboost.core.Booster.inplace_predict\nRun prediction in-place when possible, Unlike :py:meth:`predict` method, inplace prediction does not cache the prediction result.  Calling only ``inplace_predict`` in multiple threads is safe and lock free.  But the safety does not hold when used in conjunction with other methods. E.g. you can\'t train the booster in one thread and perform prediction in the other.  .. note::      If the device ordinal of the input data doesn\'t match the one configured for     the booster, data will be copied to the booster device.  .. code-block:: python      booster.set_param({""device"": ""cuda:0""})     booster.inplace_predict(cupy_array)      booster.set_param({""device"": ""cpu""})     booster.inplace_predict(numpy_array)  .. versionadded:: 1.1.0  Parameters ---------- data :     The input data. iteration_range :     See :py:meth:`predict` for details. predict_type :     * `value` Output model prediction values.     * `margin` Output the raw untransformed margin value. missing :     See :py:obj:`xgboost.DMatrix` for details. validate_features:     See :py:meth:`xgboost.Booster.predict` for details. base_margin:     See :py:obj:`xgboost.DMatrix` for details.      .. versionadded:: 1.4.0  strict_shape:     See :py:meth:`xgboost.Booster.predict` for details.      .. versionadded:: 1.4.0  Returns ------- prediction : numpy.ndarray/cupy.ndarray     The prediction result.  When input data is on GPU, prediction result is     stored in a cupy array.\nA Booster of XGBoost.  Booster is the model of xgboost, that contains low level routines for training, prediction and evaluation.', 'CLASS graphiti.graphiti_core.prompts.eval.EvalAddEpisodeResults has field or attribute or property graphiti.graphiti_core.prompts.eval.EvalAddEpisodeResults.candidate_is_worse', 'CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient has method graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.prompts.eval.EvalAddEpisodeResults inherits from or is a subclass of pydantic.BaseModel', 'FUNCTION keras.keras.src.backend.jax.sparse.elementwise_division has parameter or accepts argument keras.keras.src.backend.jax.sparse.elementwise_division.func\nDecorator to add support for `BCOO` sparse tensors to element-wise binary division and related operators.  This decorator is designed for operations related to the division of two two operands (e.g. `divide`). It accepts `BCOO` tensors for both the dividend and the divisor, but handles them differently based on whether they are the dividend or the divisor.  - If the divisor is sparse, it is densified and the result is dense because   the result contains Inf or Nan outside of the indices of the dividend. - If the dividend is sparse and the divisor is dense, it finds occurrences   of zeros and NaNs in the divisor. The result may therefore have more   indices than there were in the dividend to return correct values where the   divisor was zero or NaN. - If the dividend is sparse and the divisor is a scalar, it does the   division element-wise. Note that the result is incorrectly sparse if the   scalar divisor is zero.  Args:     func: The function to wrap. Returns:     Wrapped function that supports `BCOO` sparse tensors.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/old/comparative_eval/README.md\n# Comparative QA Benchmarks\n\nIndependent benchmarks for different QA/RAG systems using HotpotQA dataset.\n\n## Dataset Files\n- `hotpot_50_corpus.json` - 50 instances from HotpotQA\n- `hotpot_50_qa_pairs.json` - Corresponding question-answer pairs\n\n## Benchmarks\n\nEach benchmark can be run independently with appropriate dependencies:\n\n### Mem0\n```bash\npip install mem0ai openai\npython qa_benchmark_mem0.py\n```\n\n### LightRAG\n```bash\npip install ""lightrag-hku[api]""\npython qa_benchmark_lightrag.py\n```\n\n### Graphiti\n```bash\npip install graphiti-core\npython qa_benchmark_graphiti.py\n```\n\n## Environment\nCreate `.env` with required API keys:\n- `OPENAI_API_KEY` (all benchmarks)\n- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` (Graphiti only)\n\n## Usage\nEach benchmark inherits from `QABenchmarkRAG` base class and can be configured independently.\n\n# Results\nUpdated results will be posted soon.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 8, 'pr': 343, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.881141901016235,876
How does the integration test generate the JSON-formatted context string from search results?,"The integration test generates the JSON-formatted context string from search results by iterating over the edges, nodes, episodes, and communities in the SearchResults object. For each of these, it serializes them using `json.dumps` with indent=12 and injects the serialized data into a templated string.

Here's an excerpt from the code that shows how this is done:

```python
def search_results_to_context_string(search_results):
    facts = []
    entities = []
    episodes = []
    communities = []

    for edge in search_results.edges:
        # ... (serializing edge data)
        facts.append(edge.fact)

    for node in search_results.nodes:
        # ... (serializing node data)
        entities.append(node.name)

    for episode in search_results.episodes:
        # ... (serializing episode data)
        episodes.append(episode.content)

    for community in search_results.communities:
        # ... (serializing community data)
        communities.append(community.members)

    context_string = templated_context_string.format(
        facts=json.dumps(facts, indent=12),
        entities=json.dumps(entities, indent=12),
        episodes=json.dumps(episodes, indent=12),
        communities=json.dumps(communities, indent=12),
    )

    return context_string
```

This function takes a SearchResults object as input and returns a JSON-formatted string that contains the serialized data from the search results.","The test in tests/test_graphiti_int.py calls graphiti.search_(), gets back a SearchResults object, and then passes it to search_results_to_context_string (in graphiti_core/search/search_helpers.py). That function:

• Iterates over search_results.edges, .nodes, .episodes and .communities  
• Builds four Python lists of dicts (facts, entities, episodes, communities)  
• Serializes each with json.dumps(..., indent=12)  
• Injects them into a templated string with `<FACTS>…</FACTS>`, `<ENTITIES>…</ENTITIES>`, etc.  

The resulting multiline string is what the test logs as the “JSON‐formatted context.”","['CLASS cognee.cognee.modules.retrieval.context_providers.SummarizedTripletSearchContextProvider.SummarizedTripletSearchContextProvider inherits from or is a subclass of cognee.modules.retrieval.context_providers.TripletSearchContextProvider.TripletSearchContextProvider\nContext provider that uses summarized triplet search results.', 'MODULE cognee.cognee.tests.integration.documents.TextDocument_test contains GLOBAL_VARIABLE cognee.cognee.tests.integration.documents.TextDocument_test.GROUND_TRUTH', 'CLASS graphiti.graphiti_core.nodes.EpisodeType inherits from or is a subclass of enum.Enum\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'CLASS cognee.cognee.modules.search.types.SearchResult.CombinedSearchResult has field or attribute or property cognee.cognee.modules.search.types.SearchResult.CombinedSearchResult.context', 'MODULE cognee.cognee.modules.retrieval.context_providers.SummarizedTripletSearchContextProvider contains CLASS cognee.cognee.modules.retrieval.context_providers.SummarizedTripletSearchContextProvider.SummarizedTripletSearchContextProvider\nContext provider that uses summarized triplet search results.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 4, 'pr': 346, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",50.00759243965149,877
How do both LLM client generate_response methods ensure the multilingual extraction instruction is included in the prompt?,The LLM client `generate_response` methods in graphiti ensure the multilingual extraction instruction is included in the prompt by appending the `MULTILINGUAL_EXTRACTION_RESPONSES` constant to `messages[0].content` before sending the request. This constant contains the multilingual extraction instructions that will be added to the prompt.,"In both graphiti_core/llm_client/client.py (LLMClient.generate_response) and graphiti_core/llm_client/openai_generic_client.py (OpenAIGenericClient.generate_response) you’ll see the same line:

```python
# Add multilingual extraction instructions
messages[0].content += MULTILINGUAL_EXTRACTION_RESPONSES
```

This ensures the MULTILINGUAL_EXTRACTION_RESPONSES constant is always appended to the first user message before the request is sent.","['MODULE graphiti.graphiti_core.llm_client.anthropic_client contains CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.llm_client.client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.client.MULTILINGUAL_EXTRACTION_RESPONSES', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient inherits from or is a subclass of client.LLMClient\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains FUNCTION graphiti.graphiti_core.prompts.extract_nodes.extract_text', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.max_tokens\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 351, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.938573598861694,878
"Describe the overall flow of eval_graph, from LLM client initialization with default models to returning the final score.","The overall flow of eval_graph, from LLM client initialization with default models to returning the final score in graphiti, can be broken down into several steps: Initialization, Evaluation, Prompting, Embeddings, Search, and Return Score. The Graphiti instance is initialized with the provided credentials for the Neo4j database, and then it evaluates the graph based on the provided prompts using the default LLMConfig(model='gpt-4.1-mini'). It reads baseline_graph_results.json, parses each entry into AddEpisodeResults, generates new episodes and context, dumps candidate results to candidate_graph_results.json, iterates over users and episodes in a scoring loop, constructs context dicts for baseline, candidate, user message, and previous messages, calls llm_client.generate_response using the eval_add_episode_results prompt, increments scores, computes average scores, sums all users' average scores, divides by user count, and returns the overall average score as a float.","eval_graph (tests/evals/eval_e2e_graph_building.py) works as follows:

1. LLM client setup  
   – If no llm_client is passed in, it creates an OpenAIClient using the default LLMConfig(model='gpt-4.1-mini') (from graphiti_core/llm_client/config.py).  
   – Instantiates Graphiti with that client and your Neo4j creds.

2. Baseline loading  
   – Reads baseline_graph_results.json, parses each entry into AddEpisodeResults.

3. Candidate graph build  
   – Calls await build_graph('candidate', …, graphiti) to generate new episodes and context.  
   – Dumps the candidate results to candidate_graph_results.json.

4. Scoring loop  
   – For each user and each episode:  
     • Constructs a context dict with baseline result, candidate result, the current user message, and previous messages.  
     • Calls await llm_client.generate_response(...) using the eval_add_episode_results prompt to judge if the candidate is worse.  
     • Increments a per-user score (1 if not worse, 0 otherwise).

5. Aggregation  
   – Computes each user’s average (user_raw_score / #episodes), sums these, then divides by user count to get the final score.

6. Return  
   – Returns that overall average as a float.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.prompts.eval.EvalResponse inherits from or is a subclass of pydantic.BaseModel', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.prompts.eval.EvalAddEpisodeResults inherits from or is a subclass of pydantic.BaseModel', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 10, 'pr': 352, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",54.11776685714722,879
How does the reranker_min_score from SearchConfig flow into and filter results in each reranker implementation?,"The reranker_min_score from SearchConfig flows into and filters results in each reranker implementation as follows:

1.  The `reranker_min_score` is passed to the `_search` functions, which select a reranker based on the value of `config.reranker`.

2.  In the case of the cross-encoder reranker, it uses the score >= `reranker_min_score` to retain only UUIDs with scores greater than or equal to `reranker_min_score`.

3.  The `rrf` function in `graphiti_core/search/search_utils.py` is called with `min_score` set to `reranker_min_score`. It returns only UUIDs whose aggregated RRF-score is greater than or equal to `min_score`.

4.  The `maximal_marginal_relevance` function is called with `min_score` set to `reranker_min_score`. It filters out candidates with MMR less than `min_score`.

5.  In the case of the episode_mentions_reranker, it filters out nodes whose mention count is less than `min_score`.

6.  The `node_distance_reranker` function keeps only UUIDs for which the reciprocal of distance is greater than or equal to `min_score`. This globally filters out low-confidence reranker outputs before the final limit slice is applied.","The `reranker_min_score` in your `SearchConfig` is plumbed as follows:

1. In `search(...)` (graphiti_core/search/search.py) you pass  
   `config.reranker_min_score` → the `*_search` calls as their `reranker_min_score` arg.

2. In each `*_search` (edge, node, episode, community) you dispatch on `config.reranker` and invoke one of:

   • rrf (graphiti_core/search/search_utils.py:rrf):  
     rrf(..., min_score=reranker_min_score)  
     → returns only UUIDs whose aggregated RRF‐score ≥ min_score.

   • MMR (…maximal_marginal_relevance):  
     maximal_marginal_relevance(..., min_score=reranker_min_score)  
     → filters out candidates with MMR < min_score.

   • Cross‐encoder (…cross_encoder.rank in each search):  
     you do  
       `[uuid for (item,score) in reranked if score >= reranker_min_score]`.

   • episode_mentions_reranker (graphiti_core/search/search_utils.py:episode_mentions_reranker):  
     episode_mentions_reranker(..., min_score=reranker_min_score)  
     → filters nodes whose mention count < min_score.

   • node_distance_reranker (…node_distance_reranker):  
     node_distance_reranker(..., min_score=reranker_min_score)  
     → keeps only UUIDs for which (1/distance) ≥ min_score.

In every case the global `reranker_min_score` cuts off low‐confidence reranker outputs before applying the final slice to `limit`.","['CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has method graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.\nGoogle Gemini Reranker Client', 'CLASS graphiti.graphiti_core.search.search_config.NodeSearchConfig has field or attribute or property graphiti.graphiti_core.search.search_config.NodeSearchConfig.reranker', 'METHOD graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__.client\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.', 'METHOD graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__.config\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.', 'CLASS graphiti.graphiti_core.search.search_config.SearchConfig has field or attribute or property graphiti.graphiti_core.search.search_config.SearchConfig.reranker_min_score', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### ROC-AUC\nWe re-implemented the ROC-AUC metric in XGBoost.  The new implementation supports\nmulti-class classification and has better support for learning to rank tasks that are not\nbinary.  Also, it has a better-defined average on distributed environments with additional\nhandling for invalid datasets. (#6749, #6747, #6797)\n\n### Global configuration.\nStarting from 1.4, XGBoost's Python, R and C interfaces support a new global configuration\nmodel where users can specify some global parameters.  Currently, supported parameters are\n`verbosity` and `use_rmm`.  The latter is experimental, see rmm plugin demo and\nrelated README file for details. (#6414, #6656)\n\n### Other New features.\n* Better handling for input data types that support `__array_interface__`.  For some\n  data types including GPU inputs and `scipy.sparse.csr_matrix`, XGBoost employs\n  `__array_interface__` for processing the underlying data.  Starting from 1.4, XGBoost\n  can accept arbitrary array strides (which means column-major is supported) without\n  making data copies, potentially reducing a significant amount of memory consumption.\n  Also version 3 of `__cuda_array_interface__` is now supported.  (#6776, #6765, #6459,\n  #6675)\n* Improved parameter validation, now feeding XGBoost with parameters that contain\n  whitespace will trigger an error. (#6769)\n* For Python and R packages, file paths containing the home indicator `~` are supported.\n* As mentioned in the Python changes summary, the JSON model can now save feature\n  information of the trained booster.  The JSON schema is updated accordingly. (#6605)\n* Development of categorical data support is continued.  Newly added weighted data support\n  and `dart` booster support. (#6508, #6693)\n* As mentioned in Dask change summary, ranking now supports the `qid` parameter for\n  query groups. (#6576)\n* `DMatrix.slice` can now consume a numpy array. (#6368)\n\n### Other breaking changes\n* Aside from the feature name generation, there are 2 breaking changes:\n  - Drop saving binary format for memory snapshot. (#6513, #6640)\n  - Change default evaluation metric for binary:logitraw objective to logloss (#6647)\n\n### CPU Optimization\n* Aside from the general changes on predict function, some optimizations are applied on\n  CPU implementation. (#6683, #6550, #6696, #6700)\n* Also performance for sampling initialization in `hist` is improved. (#6410)\n\n### Notable fixes in the core library\nThese fixes do not reside in particular language bindings:\n* Fixes for gamma regression.  This includes checking for invalid input values, fixes for\n  gamma deviance metric, and better floating point guard for gamma negative log-likelihood\n  metric. (#6778, #6537, #6761)\n* Random forest with `gpu_hist` might generate low accuracy in previous versions. (#6755)\n* Fix a bug in GPU sketching when data size exceeds limit of 32-bit integer. (#6826)\n* Memory consumption fix for row-major adapters (#6779)\n* Don't estimate sketch batch size when rmm is used. (#6807) (#6830)\n* Fix in-place predict with missing value. (#6787)\n* Re-introduce double buffer in UpdatePosition, to fix perf regression in gpu_hist (#6757)\n* Pass correct split_type to GPU predictor (#6491)\n* Fix DMatrix feature names/types IO. (#6507)\n* Use view for `SparsePage` exclusively to avoid some data access races. (#6590)\n* Check for invalid data. (#6742)\n* Fix relocatable include in CMakeList (#6734) (#6737)"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/CatBoost/README.md\n# CatBoost\n* Code: [https://github.com/catboost/catboost](https://github.com/catboost/catboost)\n* Paper: CatBoost: unbiased boosting with categorical features. [https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 12}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 12, 'n_files_pr': 5, 'pr': 355, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",50.73937797546387,880
How does get_by_group_ids incorporate the uuid_cursor parameter into its Cypher query to filter results?,The `get_by_group_ids` function incorporates the `uuid_cursor` parameter into its Cypher query by adding a condition to the WHERE clause that checks if the `e.uuid` is less than the value of `uuid_cursor`. This is done by setting `cursor_query` to `,"In get_by_group_ids (graphiti_core/nodes.py), if you pass a non‐null uuid_cursor, it builds a small snippet

 cursor_query = ""AND e.uuid < $uuid""

and injects that into the WHERE clause:

 MATCH (e:Episodic)  
 WHERE e.group_id IN $group_ids   [+ AND e.uuid < $uuid]  
…  

It then binds `$uuid` to your uuid_cursor, so only records with `e.uuid < uuid_cursor` are returned.","['MODULE cognee.cognee.modules.retrieval.cypher_search_retriever contains CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'CLASS graphiti.server.graph_service.dto.retrieve.SearchQuery has field or attribute or property graphiti.server.graph_service.dto.retrieve.SearchQuery.group_ids', 'MODULE cognee.cognee.modules.retrieval.natural_language_retriever contains CLASS cognee.cognee.modules.retrieval.natural_language_retriever.NaturalLanguageRetriever\nRetriever for handling natural language search.  Public methods include:  - get_context: Retrieves relevant context using a natural language query converted to Cypher. - get_completion: Returns a completion based on the query and context.', 'CLASS graphiti.graphiti_core.nodes.Node has field or attribute or property graphiti.graphiti_core.nodes.Node.group_id', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever.user_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 357, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",29.91175889968872,881
"How does the client generate a structured JSON response with tool calling, and what fallback and retry strategies are applied when errors or validation failures occur?","The client generates a structured JSON response with tool calling by using the `_generate_response` method in the `AnthropicClient` class. This method uses structured JSON via tool-calling to generate a response from the language model.

When errors or validation failures occur, the following fallback and retry strategies are applied:
1.  If no ""tool_use"" appears in the result content, the method falls back to extracting a JSON substring from any ""text"" chunk using `_extract_json_from_text`.
2.  The method wraps calls to `_generate_response` in up to 2 retries.
3.  On each failure from a generic Exception or a Pydantic ValidationError when a response_model is supplied, the method appends a new user Message with an error_context prompt and retries.
4.  If validation still fails after max_retries (set to 2), the method logs the last exception.
5.  On persistent errors, the method re-raises the last exception after logging it.

These strategies ensure that the client can recover from transient errors and provide a structured JSON response with tool calling when possible.","The AnthropicClient in graphiti_core/llm_client/anthropic_client.py does the following:

1. Structured JSON via tool‐calling (_generate_response):
   - _create_tool(response_model) emits a single “tool” whose input_schema is either your Pydantic model’s JSON schema or a generic JSON object.
   - It calls await client.messages.create(…, tools=…, tool_choice=…) and then scans result.content for a content_item.type==""tool_use"". It parses content_item.input (dict or JSON string) and returns it.
   - If no tool_use appears, it falls back to extracting a JSON substring from any “text” chunk via _extract_json_from_text.

2. Error‐handling and retries (generate_response):
   - Wraps _generate_response in up to 2 retries (max_retries=2), incrementing retry_count on any Exception except RateLimitError or RefusalError (which are immediately re-raised).
   - On each failure (generic Exception or Pydantic ValidationError when a response_model is supplied), it appends a new user Message with an error_context prompt and retries.
   - If validation still fails after max_retries, or on other persistent errors, it logs and re-raises the last exception.","[""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.openai_messages\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'CLASS graphiti.graphiti_core.llm_client.errors.EmptyResponseError inherits from or is a subclass of graphiti.graphiti_core.llm_client.errors.Exception\nException raised when the LLM returns an empty response.', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_generic_client.client\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'MODULE graphiti.graphiti_core.llm_client.errors contains CLASS graphiti.graphiti_core.llm_client.errors.EmptyResponseError\nException raised when the LLM returns an empty response.', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.MAX_RETRIES\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 6, 'pr': 361, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",44.68669867515564,882
Which fulltext index does episode search use and in which module is it defined?,"The fulltext index used by episode search is named `episode_content`. It is defined in the `get_fulltext_indices` function, which is located in the `graphiti_core/utils/maintenance/graph_data_operations.py` module.","Episode search uses the fulltext index named `episode_content`, which is defined when you call get_fulltext_indices in the maintenance module at  
graphiti_core/utils/maintenance/graph_data_operations.py.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices has parameter or accepts argument graphiti.graphiti_core.graph_queries.get_fulltext_indices.db_type', ""MODULE qlib.qlib.rl.utils.log contains CLASS qlib.qlib.rl.utils.log.CsvWriter\nDump all episode metrics to a ``result.csv``.  This is not the correct implementation. It's only used for first iteration."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 13}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 13, 'n_files_pr': 9, 'pr': 362, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",27.921031713485718,883
How does get_entity_node_from_record avoid KeyErrors when removing internal fields from the attributes dictionary?,"The `get_entity_node_from_record` function in the `graphiti_core/nodes.py` file avoids KeyErrors when removing internal fields from the attributes dictionary by passing `None` as the default value to the `dict.pop` method. This causes missing keys to be skipped, preventing a KeyError from being raised. The `pop` call is executed for each internal field, ensuring that only existing keys are removed.","In graphiti_core/nodes.py’s get_entity_node_from_record it calls  
```
entity_node.attributes.pop('<field_name>', None)
```  
for each internal field. By passing `None` as the default value to `dict.pop`, it safely skips any missing keys without raising a KeyError.","[""CLASS cognee.cognee.tasks.temporal_awareness.graphiti_model.GraphitiNode inherits from or is a subclass of cognee.infrastructure.engine.DataPoint\nRepresent a node in a graph with optional content, name, and summary attributes.  This class extends DataPoint and includes a metadata dictionary that specifies the index fields for the node's data. The public instance variables are:  - content: an optional string representing the content of the node. - name: an optional string representing the name of the node. - summary: an optional string providing a summary of the node. - metadata: a dictionary outlining the fields used for indexing."", 'MODULE graphiti.graphiti_core.nodes contains FUNCTION graphiti.graphiti_core.nodes.get_entity_node_from_record', ""MODULE cognee.cognee.tasks.temporal_awareness.graphiti_model contains CLASS cognee.cognee.tasks.temporal_awareness.graphiti_model.GraphitiNode\nRepresent a node in a graph with optional content, name, and summary attributes.  This class extends DataPoint and includes a metadata dictionary that specifies the index fields for the node's data. The public instance variables are:  - content: an optional string representing the content of the node. - name: an optional string representing the name of the node. - summary: an optional string providing a summary of the node. - metadata: a dictionary outlining the fields used for indexing."", 'CLASS graphiti.graphiti_core.prompts.extract_nodes.MissedEntities has field or attribute or property graphiti.graphiti_core.prompts.extract_nodes.MissedEntities.missed_entities', ""CLASS cognee.cognee.tasks.temporal_awareness.graphiti_model.GraphitiNode has field or attribute or property cognee.cognee.tasks.temporal_awareness.graphiti_model.GraphitiNode.summary\nRepresent a node in a graph with optional content, name, and summary attributes.  This class extends DataPoint and includes a metadata dictionary that specifies the index fields for the node's data. The public instance variables are:  - content: an optional string representing the content of the node. - name: an optional string representing the name of the node. - summary: an optional string providing a summary of the node. - metadata: a dictionary outlining the fields used for indexing."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 363, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.748311758041382,884
What happens to attribute values that cannot be found in the provided messages?,"In graphiti, attribute values that cannot be found in the provided messages are explicitly set to None within the summarize_context function in the file graphiti_core/prompts/summarize_nodes.py.","In `summarize_context` (graphiti_core/prompts/summarize_nodes.py), any entity attribute whose value can’t be extracted from the messages is explicitly set to the Python value `None`.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.prompts.models.Message has field or attribute or property graphiti.graphiti_core.prompts.models.Message.content', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.prompts.extract_nodes.MissedEntities has field or attribute or property graphiti.graphiti_core.prompts.extract_nodes.MissedEntities.missed_entities', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.uri\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 365, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",54.069952964782715,885
How does the server choose between standard OpenAI and Azure OpenAI when constructing the LLM client during initialization?,"The server chooses between standard OpenAI and Azure OpenAI when constructing the LLM client during initialization in Graphiti by using the `llm_client` parameter in the `Graphiti` constructor. This parameter can be set to either an instance of `OpenAIClient` or `AsyncAzureOpenAI`, which are used for OpenAI and Azure OpenAI respectively. To choose between standard OpenAI and Azure OpenAI, you need to create instances of `OpenAIClient` and `AsyncAzureOpenAI` with the correct configuration and pass them to the `Graphiti` constructor. The configuration includes API keys, model names, and endpoints for LLM and embedding services. Here is an example of how to do this in Graphiti:

```python
default from openai import AsyncAzureOpenAIfrom graphiti_core import Graphitifrom graphiti_core.llm_client import LLMConfig, OpenAIClient# Azure OpenAI configuration - use separate endpoints for different servicesapi_key = ""<your-api-key>""api_version = ""<your-api-version>""llm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""embedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""# Create separate Azure OpenAI clients for different servicesllm_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=llm_endpoint
)
embedding_client_azure = AsyncAzureOpenAI(
    api_key=api_key,
    api_version=api_version,
    azure_endpoint=embedding_endpoint
)# Create LLM Config with your Azure deployment namesazure_llm_config = LLMConfig(
    small_model=""gpt-4.1-nano"",
    model=""gpt-4.1-mini"",
)
# Initialize Graphiti with Azure OpenAI clientsgraphiti = Graphiti(
    ""bolt://localhost:7687"",
    ""neo4j"",
    ""password"",
    llm_client=OpenAIClient(llm_config=azure_llm_config, client=llm_client_azure),
    embedder=OpenAIEmbedder(
        config=OpenAIEmbedderConfig(embedding_model=""text-embedding-3-small-deployment""),  # Your Azure embedding deployment name
        client=embedding_client_azure
    ),
    cross_encoder=OpenAIRerankerClient(llm_config=LLMConfig(model=azure_llm_config.small_model), client=llm_client_azure)
)```
This code creates instances of `AsyncAzureOpenAI` for LLM and embedding services, sets up the configuration for Azure OpenAI, and passes them to the `Graphiti` constructor.","The switch happens in GraphitiLLMConfig.create_client():

• GraphitiLLMConfig.from_env() will set azure_openai_endpoint if you’ve exported AZURE_OPENAI_ENDPOINT (and related vars) – otherwise it leaves it None.  
• create_client() then does:

  – if self.azure_openai_endpoint is not None → use AzureOpenAILLMClient  
    • if azure_openai_use_managed_identity → get a token provider via create_azure_credential_token_provider()  
    • else if api_key present → pass it to AsyncAzureOpenAI  
    • else → raise  
  – else → fall back to OpenAIClient (standard OpenAI, using OPENAI_API_KEY)

So presence of AZURE_OPENAI_ENDPOINT in your env (and its flags) drives the choice between Azure vs. vanilla OpenAI.","['CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has method graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.__init__\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'MODULE graphiti.graphiti_core.llm_client.openai_base_client contains CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient inherits from or is a subclass of client.LLMClient\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_base_client.config\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_base_client.cache\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 7, 'n_files_pr': 6, 'pr': 368, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",77.53408455848694,886
How does the search flow with MMR reranking transition from initial multi-method candidate collection to the final limited set of results?,"The search flow with MMR reranking in graphiti transitions from the initial multi-method candidate collection to the final limited set of results as follows:

1.  The Candidates stage produces a list of lists of entities by running full-text searches, vector-similarity searches, and BFS searches up to 2×limit.
2.  The De-dupe/map stage flattens the candidate lists into a uuid→entity map.
3.  The MMR rerank stage calls get_embeddings_for_edges (or get_embeddings_for_nodes or get_embeddings_for_communities) to fetch all candidate vectors.
4.  The maximal_marginal_relevance function is located in graphiti_core/search/search_utils.py and is invoked with the query vector, embeddings, mmr_lambda, and min_score as arguments.
5.  The MMR rerank stage returns a sorted list of UUIDs with MMR scores ≥ min_score.
6.  The Materialize & trim stage maps those UUIDs back to EntityEdge, EntityNode, or CommunityNode via the uuid map.
7.  The Materialize & trim stage returns the first limit results.

This process ensures that the final limited set of results is obtained by applying MMR reranking to the initial multi-method candidate collection.","In both edge_search (and similarly node_search/community_search) the flow is:

1.  Candidates: semaphore_gather runs full-text, vector-similarity and BFS searches (each up to 2×limit), producing a list of lists of entities.  
2.  De-dupe/map: flatten into an `uuid→entity` map.  
3.  MMR rerank:  
    • call get_embeddings_for_edges (or _nodes/_communities) to fetch all candidate vectors  
    • invoke maximal_marginal_relevance(query_vector, embeddings, mmr_lambda, min_score) in graphiti_core/search/search_utils.py  
    • get back a sorted list of UUIDs whose MMR score≥min_score  
4.  Materialize & trim: map those UUIDs back to EntityEdge/EntityNode/CommunityNode via the uuid-map, then return the first `limit` results.","['CLASS qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager has method qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager.get_next_parameters\nReturns the next set of parameters to optimise.  Args:   ranges_to_skip: Explicitly defines a set of keys to skip.\nManages hyperparameter optimisation using random search for a single GPU.  Attributes:   param_ranges: Discrete hyperparameter range for random search.   results: Dataframe of validation results.   fixed_params: Fixed model parameters per experiment.   saved_params: Dataframe of parameters trained.   best_score: Minimum validation loss observed thus far.   optimal_name: Key to best configuration.   hyperparam_folder: Where to save optimisation outputs.', 'CLASS xgboost.python-package.xgboost.testing.data.PBM has method xgboost.python-package.xgboost.testing.data.PBM.sample_clicks_for_query\nSample clicks for one query based on input relevance degree and position.  Parameters ----------  labels :     relevance_degree\nSimulate click data with position bias model. There are other models available in `ULTRA <https://github.com/ULTR-Community/ULTRA.git>`_ like the cascading model.  References ---------- Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm', 'CLASS cognee.cognee.modules.ontology.matching_strategies.MatchingStrategy has method cognee.cognee.modules.ontology.matching_strategies.MatchingStrategy.find_match\nFind the best match for a given name from a list of candidates.  Args:     name: The name to match     candidates: List of candidate names to match against  Returns:     The best matching candidate name, or None if no match found\nAbstract base class for ontology entity matching strategies.', 'MODULE xgboost.demo.guide-python.learning_to_rank contains GLOBAL_VARIABLE xgboost.demo.guide-python.learning_to_rank.parser', 'CLASS qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager has method qlib.examples.benchmarks.TFT.libs.hyperparam_opt.HyperparamOptManager.update_score\nUpdates the results from last optimisation run.  Args:   parameters: Hyperparameters used in optimisation.   loss: Validation loss obtained.   model: Model to serialised if required.   info: Any ancillary information to tag on to results.  Returns:   Boolean flag indicating if the model is the best seen so far.\nManages hyperparameter optimisation using random search for a single GPU.  Attributes:   param_ranges: Discrete hyperparameter range for random search.   results: Dataframe of validation results.   fixed_params: Fixed model parameters per experiment.   saved_params: Dataframe of parameters trained.   best_score: Minimum validation loss observed thus far.   optimal_name: Key to best configuration.   hyperparam_folder: Where to save optimisation outputs.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### GPUTreeSHAP: GPU acceleration of the TreeSHAP algorithm (#6038, #6064, #6087, #6099, #6163, #6281, #6332)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) is a game theoretic approach to explain predictions of machine learning models. It computes feature importance scores for individual examples, establishing how each feature influences a particular prediction. TreeSHAP is an optimized SHAP algorithm specifically designed for decision tree ensembles.\n* Starting with 1.3.0 release, it is now possible to leverage CUDA-capable GPUs to accelerate the TreeSHAP algorithm. Check out [the demo notebook](https://github.com/dmlc/xgboost/blob/master/demo/gpu_acceleration/shap.ipynb).\n* The CUDA implementation of the TreeSHAP algorithm is hosted at [rapidsai/GPUTreeSHAP](https://github.com/rapidsai/gputreeshap). XGBoost imports it as a Git submodule.\n\n### New style Python callback API (#6199, #6270, #6320, #6348, #6376, #6399, #6441)\n* The XGBoost Python package now offers a re-designed callback API. The new callback API lets you design various extensions of training in idomatic Python. In addition, the new callback API allows you to use early stopping with the native Dask API (`xgboost.dask`). Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.3.0/python/callbacks.html) and [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/callbacks.py).\n\n### Enable the use of `DeviceQuantileDMatrix` / `DaskDeviceQuantileDMatrix` with large data (#6201, #6229, #6234).\n* `DeviceQuantileDMatrix` can achieve memory saving by avoiding extra copies of the training data, and the saving is bigger for large data. Unfortunately, large data with more than 2^31 elements was triggering integer overflow bugs in CUB and Thrust. Tracking issue: #6228.\n* This release contains a series of work-arounds to allow the use of `DeviceQuantileDMatrix` with large data:\n  - Loop over `copy_if` (#6201)\n  - Loop over `thrust::reduce` (#6229)\n  - Implement the inclusive scan algorithm in-house, to handle large offsets (#6234)\n\n### Support slicing of tree models (#6302)\n* Accessing the best iteration of a model after the application of early stopping used to be error-prone, need to manually pass the `ntree_limit` argument to the `predict()` function.\n* Now we provide a simple interface to slice tree models by specifying a range of boosting rounds. The tree ensemble can be split into multiple sub-ensembles via the slicing interface. Check out [an example](https://xgboost.readthedocs.io/en/release_1.3.0/python/model.html).\n* In addition, the early stopping callback now supports `save_best` option. When enabled, XGBoost will save (persist) the model at the best boosting round and discard the trees that were fit subsequent to the best round.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 3, 'pr': 369, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",47.4703004360199,887
What condition guards against deleting edges when their episodes list is empty in remove_episode?,"The condition that guards against deleting edges when their episodes list is empty in Graphiti.remove_episode is the truth-check on `edge.episodes`, which ensures that the episodes list is not empty before indexing.","In Graphiti.remove_episode (graphiti_core/graphiti.py) the loop uses  
```python
if edge.episodes and edge.episodes[0] == episode.uuid:
    edges_to_delete.append(edge)
```  
The `edge.episodes` truth-check prevents deleting edges when their episodes list is empty.","['CLASS cognee.cognee.infrastructure.databases.graph.neo4j_driver.adapter.Neo4jAdapter has method cognee.cognee.infrastructure.databases.graph.neo4j_driver.adapter.Neo4jAdapter.serialize_properties\nConvert properties of a node or edge into a serializable format suitable for storage.  Parameters: -----------      - properties: A dictionary of properties to serialize, defaults to an empty       dictionary. (default dict())  Returns: --------      A dictionary with serialized property values.\nAdapter for interacting with a Neo4j graph database, implementing the GraphDBInterface. This class provides methods for querying, adding, deleting nodes and edges, as well as managing sessions and projecting graphs.', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.edges_by_episode', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.episodes', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 370, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.77036452293396,888
What does the newly added guideline instruct for handling negated facts?,"According to Guideline #10 from extract_edge_dates.py version v1, if a fact is negated (“no longer true”), you should record the time the negation became true in `valid_at`, and `invalid_at` should be left null when recording the time a fact is negated.","In extract_edge_dates.py (v1), the new Guideline #10 says that if a fact is negated (“no longer true”), you should record the time the negation became true in valid_at (and leave invalid_at null).","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.contradicted_facts', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.missing_facts', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.llm\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 378, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",32.100948095321655,889
Which node attribute is explicitly carried over to the surviving node during deduplication?,"The node attribute that is explicitly carried over to the surviving node during deduplication in graphiti is the summary field, which is overwritten with the LLM‐returned summary.","In dedupe_node_list (graphiti_core/utils/maintenance/node_operations.py), after the LLM groups duplicates, the surviving EntityNode has its summary field explicitly overwritten with the LLM‐returned summary.","['CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.prompts.dedupe_nodes.NodeDuplicate has field or attribute or property graphiti.graphiti_core.prompts.dedupe_nodes.NodeDuplicate.duplicate_idx', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.prompts.dedupe_nodes.NodeDuplicate has field or attribute or property graphiti.graphiti_core.prompts.dedupe_nodes.NodeDuplicate.duplicates', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 379, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",24.739861488342285,890
"Across the different LLM client generate_response methods, how is an unspecified max_tokens value handled?","In graphiti, an unspecified max_tokens value is handled by defaulting to the client's configuration when using AnthropicClient. This is because the `generate_response` method in anthropic_client.py starts with `if max_tokens is None: max_tokens = self.max_tokens.`. In contrast, GeminiClient passes any provided max_tokens value directly to `_generate_response`, without checking for `None`. The base LLMClient and OpenAIGenericClient also default to the client's configuration when max_tokens is not specified.","In AnthropicClient (anthropic_client.py), the base LLMClient (client.py) and OpenAIGenericClient (openai_generic_client.py), each generate_response starts with:

```python
if max_tokens is None:
    max_tokens = self.max_tokens
```

GeminiClient.generate_response (gemini_client.py) has no such check—it simply passes whatever `max_tokens` (even `None`) straight through to `_generate_response`.","['MODULE graphiti.graphiti_core.llm_client.anthropic_client contains CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.llm_client.config contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.config.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.client\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'MODULE graphiti.graphiti_core.llm_client.groq_client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.groq_client.DEFAULT_MAX_TOKENS', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has method graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient.__init__\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 4, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 382, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",34.057241916656494,891
What exception is raised when no entity node is found for the given UUID?,The exception raised when no entity node is found for the given UUID in Graphiti is `NodeNotFoundError`. This exception is specifically designed to handle situations where a node cannot be located based on its unique identifier.,"If no matching node is found, `EntityNode.get_by_uuid` raises a `NodeNotFoundError(uuid)`.","['CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError has field or attribute or property graphiti.graphiti_core.errors.uuid\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'CLASS graphiti.graphiti_core.nodes.Node has field or attribute or property graphiti.graphiti_core.nodes.Node.uuid', 'CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 383, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",25.873804569244385,892
Which helper function returns the list of fulltext index creation statements used in this process?,The helper function that returns the list of fulltext index creation statements used in this process is `get_fulltext_indices` from the `graphiti.graphiti_core.graph_queries` module.,The helper function get_fulltext_indices(provider) returns the list of fulltext index creation statements.,"['CLASS qlib.qlib.utils.index_data.LocIndexer has method qlib.qlib.utils.index_data.LocIndexer.proc_idx_l\nprocess the indices from user and output a list of `Index`\n`Indexer` will behave like the `LocIndexer` in Pandas  Read-only operations has higher priorities than others. So this class is designed in a read-only way to shared data for queries. Modifications will results in new Index.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.get_period_list\nThis method will be used in PIT database. It return all the possible values between `first` and `end`  (first and end is included)  Parameters ---------- quarterly : bool     will it return quarterly index or yearly index.  Returns ------- List[int]     the possible index between [first, last]', 'FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices has parameter or accepts argument graphiti.graphiti_core.graph_queries.get_fulltext_indices.db_type', ""MODULE cognee.cognee.infrastructure.loaders.create_loader_engine contains FUNCTION cognee.cognee.infrastructure.loaders.create_loader_engine.create_loader_engine\nCreate loader engine with given configuration.  Follows cognee's pattern for engine creation functions used in database adapters.  Args:     default_loader_priority: Priority order for loader selection  Returns:     Configured LoaderEngine instance"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 384, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.74424719810486,893
"Which embedder client falls back to processing items individually when a batch request fails, and how does it report this fallback?","The embedder client that falls back to processing items individually when a batch request fails in graphiti is GeminiEmbedder. It reports this fallback via logger.warning with the message ""Batch embedding failed for batch {i//batch_size + 1}, falling back to individual processing: {e}"" in an except block.","The GeminiEmbedder (in graphiti_core/embedder/gemini.py) is the only client that, in its create_batch method, catches batch failures and falls back to processing each item individually. It reports this via a logger.warning in the except block:

logger.warning(f""Batch embedding failed for batch {i//batch_size + 1}, falling back to individual processing: {e}"")","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_cli_and_env\nCreate configuration from CLI arguments, falling back to environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'CLASS graphiti.graphiti_core.embedder.client.EmbedderClient inherits from or is a subclass of abc.ABC', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.embedder\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.embedder.voyage.VoyageAIEmbedder inherits from or is a subclass of client.EmbedderClient\nVoyageAI Embedder Client', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 10}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 10, 'n_files_pr': 8, 'pr': 403, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.04553508758545,894
Which properties from each input node are included in the nodes payload for execute_query in get_relevant_nodes?,"The properties from each input node that are included in the `nodes` payload for `execute_query` in `get_relevant_nodes` in Graphiti are: * `uuid`: a unique identifier for the node, * `name`: the name of the node, * `name_embedding`: an embedding representation of the node's name, and * `fulltext_query`: a full-text query associated with the node. These properties are used to construct the payload passed into `driver.execute_query`.","In get_relevant_nodes (graphiti_core/search/search_utils.py), the `nodes` payload passed into driver.execute_query is built as a list of dicts, each containing:

• uuid  
• name  
• name_embedding  
• fulltext_query","['CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', 'CLASS graphiti.graphiti_core.nodes.CommunityNode has field or attribute or property graphiti.graphiti_core.nodes.CommunityNode.query', ""CLASS keras.keras.src.ops.node.Node has method keras.keras.src.ops.node.Node.output_tensors\nA `Node` describes an operation `__call__()` event.  A Keras Function is a DAG with `Node` instances as nodes, and `KerasTensor` instances as edges. Nodes aren't `Operation` instances, because a single operation could be called multiple times, which would result in graph cycles.  A `__call__()` event involves input tensors (and other input arguments), the operation that was called, and the resulting output tensors. A `Node` will include all this information.  Since a single `Operation` could be called multiple times, the `Node` instances are stored on operations as a list. Each time an operation is called, a node is added to `op._inbound_nodes`. Each time the output of an operation is used by another operation, a node is added to `op._outbound_nodes`.  Every `KerasTensor` instance has a `KerasHistory` object attached, which tracks the `Node` that records the `__call__()` event that created the tensor. By recursively walking through `Node` instances via the `KerasHistory` metadata of `KerasTensor` instances, once can retrieve the entire DAG of a Keras Function.  Args:     operation: The Operation that was called in the `op.__call__()`         event that this node represents.     call_args: The positional arguments the operation was called with.     call_kwargs: The keyword arguments the operation was called with.     outputs: The output tensors of the `op.__call__()` call."", 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.query', ""CLASS keras.keras.src.ops.node.Node has method keras.keras.src.ops.node.Node.input_tensors\nA `Node` describes an operation `__call__()` event.  A Keras Function is a DAG with `Node` instances as nodes, and `KerasTensor` instances as edges. Nodes aren't `Operation` instances, because a single operation could be called multiple times, which would result in graph cycles.  A `__call__()` event involves input tensors (and other input arguments), the operation that was called, and the resulting output tensors. A `Node` will include all this information.  Since a single `Operation` could be called multiple times, the `Node` instances are stored on operations as a list. Each time an operation is called, a node is added to `op._inbound_nodes`. Each time the output of an operation is used by another operation, a node is added to `op._outbound_nodes`.  Every `KerasTensor` instance has a `KerasHistory` object attached, which tracks the `Node` that records the `__call__()` event that created the tensor. By recursively walking through `Node` instances via the `KerasHistory` metadata of `KerasTensor` instances, once can retrieve the entire DAG of a Keras Function.  Args:     operation: The Operation that was called in the `op.__call__()`         event that this node represents.     call_args: The positional arguments the operation was called with.     call_kwargs: The keyword arguments the operation was called with.     outputs: The output tensors of the `op.__call__()` call."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 423, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",32.79867196083069,895
Under what context key is the episode’s source description passed to the LLM prompt?,"The episode's source description is passed to the LLM prompt under the key ""source_description"" in graphiti.","In extract_nodes (graphiti_core/utils/maintenance/node_operations.py) the episode’s source_description is injected into the LLM prompt context under the key  
“source_description.”","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.search.search_helpers contains FUNCTION graphiti.graphiti_core.search.search_helpers.search_results_to_context_string\nReformats a set of SearchResults into a single string to pass directly to an LLM as context', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.prompts.summarize_nodes contains FUNCTION graphiti.graphiti_core.prompts.summarize_nodes.summarize_context', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.destroy_graph\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 424, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",24.367491722106934,896
How is the unique group_id generated in the runner propagated into both the fulltext and vector search functions?,"The unique group_id generated in the runner is propagated into both the fulltext and vector search functions in Graphiti through the use of a global variable `group_ids` which is passed as an argument to various functions, including `fulltext_query` and `driver.execute_query`. The `group_ids` list contains the original UUID generated in the runner. In the `get_relevant_nodes` function, the group_id is obtained from the first node's group_id attribute and used in the Cypher MATCH clause for the vector search. It is also passed into `fulltext_query` and `driver.execute_query` calls to scope all searches to that group.","In examples/podcast/podcast_runner.py:main you do  
  group_id = str(uuid4())  
and then pass that into client.add_episode / add_episode_bulk.  

From there:  
• In graphiti_core/search/search_utils.py→node_fulltext_search, the same group_ids list is handed to  
    fulltext_query(query, group_ids)  
  and into driver.execute_query(…, group_ids=group_ids)  
  so the Lucene (full-text) index is scoped to your run.  

• In get_relevant_nodes, each EntityNode carries its group_id, which is pulled out as  
    group_id = nodes[0].group_id  
  and then:  
   – used in the Cypher MATCH `(n:Entity {group_id: $group_id})` for the vector search  
   – passed into fulltext_query(node.name, [group_id]) when re‐running the fulltext phase  
   – sent as group_id=group_id in the driver.execute_query call that stitches vector+fulltext results together.  

In both search paths the original UUID you generated in the runner is simply threaded through the fulltext_query calls and the execute_query params to scope all searches to that group.","['CLASS graphiti.graphiti_core.errors.GroupIdValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a group_id contains invalid characters.', 'FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query has parameter or accepts argument graphiti.graphiti_core.search.search_utils.fulltext_query.group_ids', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.validate_group_id\nValidate that a group_id contains only ASCII alphanumeric characters, dashes, and underscores.  Args:     group_id: The group_id to validate  Returns:     True if valid, False otherwise  Raises:     GroupIdValidationError: If group_id contains invalid characters', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.group_ids', 'FUNCTION graphiti.graphiti_core.helpers.validate_group_id has parameter or accepts argument graphiti.graphiti_core.helpers.validate_group_id.group_id\nValidate that a group_id contains only ASCII alphanumeric characters, dashes, and underscores.  Args:     group_id: The group_id to validate  Returns:     True if valid, False otherwise  Raises:     GroupIdValidationError: If group_id contains invalid characters', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 3, 'pr': 426, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",36.650688886642456,897
At what stage in extract_nodes are entities with empty or whitespace-only names removed from further processing?,"Entities with empty or whitespace-only names are removed from further processing right after the LLM/reflexion loop and before creating any EntityNode, as a result of applying a filter defined as `filtered_extracted_entities = [entity for entity in extracted_entities if entity.name.strip()]`.","In extract_nodes (graphiti_core/utils/maintenance/node_operations.py), right after the LLM/reflexion loop and before creating any EntityNode, there’s a filter:

filtered_extracted_entities = [entity for entity in extracted_entities if entity.name.strip()]

That’s where any entity whose name is empty or only whitespace is dropped.","[""METHOD cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names has parameter or accepts argument cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names.data_point\nRetrieve the names of embeddable properties defined in the metadata.  If no index fields are defined in the metadata, this method will return an empty list.  Parameters: -----------      - data_point ('DataPoint'): The DataPoint instance from which to retrieve embeddable       property names.  Returns: --------      A list of property names corresponding to the index fields, or an empty list if none     exist."", 'MODULE graphiti.graphiti_core.utils.maintenance.node_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.node_operations.extracted_entities', ""CLASS cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint has method cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names\nRetrieve the names of embeddable properties defined in the metadata.  If no index fields are defined in the metadata, this method will return an empty list.  Parameters: -----------      - data_point ('DataPoint'): The DataPoint instance from which to retrieve embeddable       property names.  Returns: --------      A list of property names corresponding to the index fields, or an empty list if none     exist.\nModel representing a data point with versioning and metadata support.  Public methods include: - get_embeddable_data - get_embeddable_properties - get_embeddable_property_names - update_version - to_json - from_json - to_pickle - from_pickle - to_dict - from_dict"", 'CLASS graphiti.graphiti_core.prompts.extract_nodes.ExtractedEntities inherits from or is a subclass of pydantic.BaseModel', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 427, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.561084747314453,898
"What is the end-to-end flow for producing a structured JSON response in the Anthropic client, from tool creation through API call to result parsing and error handling?","The end-to-end flow for producing a structured JSON response in the Anthropic client, from tool creation through API call to result parsing and error handling is as follows:

1.  **Tool Creation**: The `_create_tool` method in `AnthropicClient` creates a tool based on the provided Pydantic model or falls back to a generic JSON tool if no model is provided.
2.  **API Call**: The `_generate_response` method builds system and user messages, selects a value for `max_tokens`, and calls `await self.client.messages.create(...)` with the created tool and tool choice.
3.  **Result Parsing**: The result parsing logic iterates over `result.content`. If a content item has `type == 'tool_use'`, it parses `content_item.input` and returns it. If a content item chunk is of type `'text'`, it calls `_extract_json_from_text(text)` to extract embedded JSON.
4.  **Error Handling**: The client catches `anthropic.RateLimitError` and rethrows it as its own `RateLimitError`. It also catches `anthropic.APIError` containing “refused to respond” and throws a `RefusalError`, skipping retries. Any other exception bubbles up without being caught.

This flow ensures that the Anthropic client can handle various scenarios, including tool creation, API calls, result parsing, and error handling, to produce structured JSON responses.","The end-to-end flow in AnthropicClient (graphiti_core/llm_client/anthropic_client.py) is:

1. Tool Creation (_create_tool)  
   – If you pass a Pydantic `response_model`, it calls `response_model.model_json_schema()` to build a typed tool; otherwise it falls back to a generic JSON tool.  
   – Returns `tools` (a list with one schema) and `tool_choice` (the name of that tool).

2. API Call (_generate_response)  
   – Builds `system` + `user` messages and picks `max_tokens`.  
   – Calls `await self.client.messages.create(…)`, passing in `tools` and `tool_choice`.

3. Result Parsing  
   – Iterates `result.content`:  
     • If `content_item.type == 'tool_use'`, parses `content_item.input` (dict or JSON) and returns it.  
     • Else if you get a `'text'` chunk, calls `_extract_json_from_text(text)` (tested in test_extract_json_from_text) to pull embedded JSON.  
     • Otherwise raises a `ValueError` about unparseable content.

4. Error Handling  
   – Catches `anthropic.RateLimitError` → rethrows as your own `RateLimitError`.  
   – Catches `anthropic.APIError` containing “refused to respond” → throws `RefusalError` (skipping retries); other `APIError`s bubble up.  
   – Any other exception bubbles up as-is.

That sequence—from `_create_tool` → `messages.create()` → tool output extraction or text-based JSON extraction → structured dict return, with rate-limit and refusal mapped to custom errors—is how a structured JSON response is produced.","['MODULE graphiti.graphiti_core.llm_client.anthropic_client contains CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient.tool_args\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.client\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has method graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient.__init__\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'CLASS graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient has field or attribute or property graphiti.graphiti_core.llm_client.anthropic_client.AnthropicClient.model\nA client for the Anthropic LLM.  Args:     config: A configuration object for the LLM.     cache: Whether to cache the LLM responses.     client: An optional client instance to use.     max_tokens: The maximum number of tokens to generate.  Methods:     generate_response: Generate a response from the LLM.  Notes:     - If a LLMConfig is not provided, api_key will be pulled from the ANTHROPIC_API_KEY environment         variable, and all default values will be used for the LLMConfig.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 4, 'pr': 431, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",52.65586614608765,899
What model_size value do maintenance routines use when invoking LLM responses for node attribute extraction and edge contradiction checks?,"The model_size value used by maintenance routines in graphiti when invoking LLM responses for node attribute extraction and edge contradiction checks is ModelSize.small. This can be seen from the code in `graphiti_core/utils/maintenance/node_operations.py` and `graphiti_core/utils/maintenance/temporal_operations.py`, where the `llm_client.generate_response` function is called with `model_size=ModelSize.small`. Maintenance routines use this value consistently for both node attribute extraction and edge contradiction checks.","Both maintenance routines use the small model.  

- In graphiti_core/utils/maintenance/node_operations.py (extract_attributes_from_node) you’ll see  
  `llm_client.generate_response(…, model_size=ModelSize.small)`  

- In graphiti_core/utils/maintenance/temporal_operations.py (get_edge_contradictions) it likewise calls  
  `llm_client.generate_response(…, model_size=ModelSize.small)`","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.type_model', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.node_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.node_operations.unique_model_name', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.destroy_graph\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 19}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 19, 'n_files_pr': 15, 'pr': 432, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",32.184330701828,900
How are search results integrated with the LLM-based dedupe prompt to resolve extracted nodes and determine duplicates?,"The `resolve_extracted_nodes` function integrates search results with LLM-based dedupe prompts as follows: It calls `search()` for each extracted EntityNode, flattens the SearchResults into a single existing_nodes list, and uses this list to build two contexts: `existing_nodes_context` and `extracted_nodes_context`. The function then assembles a context dictionary and invokes the LLM client's generate_response method with prompt_library.dedupe_nodes.nodes(context) and response_model set to NodeResolutions. The returned duplicate_idx and duplicates array are parsed, and each extracted node is mapped to an existing node if any, resulting in final resolved nodes and a duplicate-pair list.","In resolve_extracted_nodes (graphiti_core/utils/maintenance/node_operations.py) we

1. call search() for each extracted EntityNode to get SearchResults, flatten them into a single existing_nodes list (or use an override).  
2. build two contexts:

   • existing_nodes_context: a list of dicts with `idx`, `name`, `entity_types` and attributes for each candidate node  
   • extracted_nodes_context: a list of dicts with `id`, `name`, `entity_type` and `entity_type_description` for each newly extracted node  

3. assemble  
   ```python
   context = {
     'existing_nodes': existing_nodes_context,
     'extracted_nodes': extracted_nodes_context,
     'episode_content': …,
     'previous_episodes': …,
   }
   ```  
4. invoke the LLM via  
   ```python
   llm_client.generate_response(
     prompt_library.dedupe_nodes.nodes(context),
     response_model=NodeResolutions
   )
   ```  
   where graphiti_core/prompts/dedupe_nodes.py defines a system+user prompt that embeds the JSON of `<EXISTING ENTITIES>` and `<NEW ENTITY>`.

5. parse the returned duplicate_idx (and duplicates array) to map each extracted node to an existing one (if any), yielding the final resolved_nodes and duplicate‐pair list.","['CLASS cognee.cognee.modules.retrieval.graph_completion_retriever.GraphCompletionRetriever has method cognee.cognee.modules.retrieval.graph_completion_retriever.GraphCompletionRetriever.__init__\nInitialize retriever with prompt paths and search parameters.\nRetriever for handling graph-based completion searches.  This class provides methods to retrieve graph nodes and edges, resolve them into a human-readable format, and generate completions based on graph context. Public methods include: - resolve_edges_to_text - get_triplets - get_context - get_completion', 'MODULE graphiti.graphiti_core.prompts.dedupe_nodes contains FUNCTION graphiti.graphiti_core.prompts.dedupe_nodes.node_list', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.prompts.dedupe_nodes contains FUNCTION graphiti.graphiti_core.prompts.dedupe_nodes.nodes', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 433, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",42.86161923408508,901
"In edge_search, which reranking strategies require fetching fact embeddings on demand?","In graphiti, the MMR reranking strategy requires fetching fact embeddings on demand. This is because it calls `get_embeddings_for_edges` in edge_search.","In edge_search (graphiti_core/search/search.py), only the MMR reranker (config.reranker == EdgeReranker.mmr) calls get_embeddings_for_edges to fetch fact embeddings on demand.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.edges contains GLOBAL_VARIABLE graphiti.graphiti_core.edges.fact_embeddings', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_edges', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.user\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 27}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 27, 'n_files_pr': 7, 'pr': 456, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",27.968524932861328,902
What is the sequence of steps the system takes from extracting an entity's attributes to deduplicating a list of nodes?,"Based on the provided codebase and documentation for Graphiti, here is a step-by-step sequence of steps that the system takes from extracting an entity's attributes to deduplicating a list of nodes:

**Extracting Entity Attributes:**
*   The `extract_attributes_from_node` function in `graphiti_core/utils/maintenance/node_operations.py` is called.
*   This function builds a minimal node context containing name, summary, labels, and existing attributes.
*   It adds any extra fields from an optional Pydantic entity_type to the node context.
*   A one-off Pydantic model for the attributes is created.
*   The function assembles `summary_context` using the node context, current episode content, and prior episodes.
*   It calls `llm_client.generate_response` with the `extract_nodes.extract_attributes` prompt.
*   The EntityNode's summary is updated from the LLM's output.
*   The EntityNode's attributes are updated from the LLM's output.

**Deduplicating a List of Nodes:**
*   The `dedupe_node_list` function in `graphiti_core/utils/maintenance/node_operations.py` is called.
*   This function builds a node map mapping UUID to EntityNode.
*   It serializes each node into `nodes_context` with UUID, name, and attributes.
*   The function calls `llm_client.generate_response` with the `dedupe_nodes.node_list` prompt.
*   It parses back a list of groups formatted as `{uuids: […], summary: “…”}`.
*   For each parsed group, it picks the first UUID's node as unique.
*   For each parsed group, it updates the chosen node's summary.
*   For each parsed group, it records any aliased UUIDs in a `uuid_map`.
*   The function returns the pruned unique nodes list and the uuid map.","The end-to-end flow lives in graphiti_core/utils/maintenance/node_operations.py:

1. **extract_attributes_from_node**  
   a. Build a minimal `node_context` (name, summary, labels, existing attributes) plus any extra fields from an optional Pydantic `entity_type`.  
   b. Create a one‐off Pydantic model for those attributes.  
   c. Assemble `summary_context` with that `node_context`, the current episode content, and prior episodes.  
   d. Call `llm_client.generate_response` with the `extract_nodes.extract_attributes` prompt.  
   e. Update the `EntityNode`’s `summary` and `attributes` from the LLM’s output.

2. **dedupe_node_list**  
   a. Build a `node_map` (uuid → EntityNode).  
   b. Serialize each node into `nodes_context` (uuid, name, ∗attributes).  
   c. Call `llm_client.generate_response` with the `dedupe_nodes.node_list` prompt.  
   d. Parse back a list of `{uuids: [...], summary: ""…""}` groups.  
   e. For each group, pick the first UUID’s node as “unique,” update its summary, and record any aliased UUIDs in a `uuid_map`.  
   f. Return the pruned `unique_nodes` list plus the `uuid_map`.","['CLASS keras.guides.making_new_layers_and_models_via_subclassing.VariationalAutoEncoder inherits from or is a subclass of keras.keras.Model\nCombines the encoder and decoder into an end-to-end model for training.', 'MODULE graphiti.graphiti_core.utils.maintenance.node_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.node_operations.extracted_entities', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.ExtractedEntities inherits from or is a subclass of pydantic.BaseModel', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 8, 'n_files_pr': 5, 'pr': 457, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",63.47028112411499,903
How does the edge module retrieve relationship data and populate fact embeddings for entity edges?,"The edge module in graphiti retrieves relationship data and populates fact embeddings for entity edges by using the `get_by_node_uuid` class method of the EntityEdge class, which fires a Cypher query to match related entities and calls `driver.execute_query()` to execute this query on the Neo4j database. The results are mapped to EntityEdge objects via the `get_entity_edge_from_record` function, and fact embeddings for each edge are loaded using the `load_fact_embedding` instance method of the EntityEdge class. If no records are found in the query results, an EdgeNotFoundError exception is raised; otherwise, the returned blob is assigned to the `fact_embedding` attribute of the EntityEdge object.","The edge module lives in graphiti_core/edges.py on the EntityEdge class:

• get_by_node_uuid(cls, driver, node_uuid)  
  – fires a Cypher  
    MATCH (n:Entity {uuid:$node_uuid})-[e:RELATES_TO]-(m:Entity)  
    plus the ENTITY_EDGE_RETURN fragment  
  – calls driver.execute_query(), then maps each record to an EntityEdge via get_entity_edge_from_record()  

• load_fact_embedding(self, driver)  
  – runs  
    MATCH (n:Entity)-[e:RELATES_TO {uuid:$uuid}]->(m:Entity)  
    RETURN e.fact_embedding  
  – if no records, raises EdgeNotFoundError  
  – otherwise assigns the returned blob to self.fact_embedding","['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.fact_embedding', 'MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_entity_edge_from_record', 'MODULE cognee.cognee.tasks.temporal_graph.add_entities_to_event contains FUNCTION cognee.cognee.tasks.temporal_graph.add_entities_to_event.add_entities_to_event\nAdds extracted entities to an Event object by populating its attributes field.  For each attribute in the provided EventWithEntities, the function ensures that the corresponding entity type exists, creates an Entity node with metadata, and links it to the event via an Edge representing the relationship. Entities are cached by type to avoid duplication.  Args:     event (Event): The target Event object to enrich with entities.     event_with_entities (EventWithEntities): An event model containing extracted         attributes with entity, type, and relationship metadata.  Returns:     None', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 458, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",38.99946427345276,904
At which stages in the resolution workflow does the function generate embeddings for edge objects?,"The function `resolve_extracted_edges` generates embeddings for edge objects at two stages in the resolution workflow: **Before any searching or resolution**, and **At the end of the resolution process**.","In resolve_extracted_edges (graphiti_core/utils/maintenance/edge_operations.py) embeddings are generated twice via create_entity_edge_embeddings:

1. Up‐front on the raw extracted_edges (before any searching or resolution).  
2. At the end, in parallel on both the resolved_edges and the invalidated_edges.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.embedder', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'MODULE graphiti.graphiti_core.edges contains GLOBAL_VARIABLE graphiti.graphiti_core.edges.fact_embeddings', ""CLASS qlib.qlib.data.dataset.handler.DataHandlerABC has method qlib.qlib.data.dataset.handler.DataHandlerABC.fetch\nInterface for data handler.  This class does not assume the internal data structure of the data handler. It only defines the interface for external users (uses DataFrame as the internal data structure).  In the future, the data handler's more detailed implementation should be refactored. Here are some guidelines:  It covers several components:  - [data loader] -> internal representation of the data -> data preprocessing -> interface adaptor for the fetch interface - The workflow to combine them all:   The workflow may be very complicated. DataHandlerLP is one of the practices, but it can't satisfy all the requirements.   So leaving the flexibility to the user to implement the workflow is a more reasonable choice."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 460, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.386393308639526,905
How does the system ensure missing embeddings for nodes and edges are generated before the bulk save operation?,"The system ensures missing embeddings for nodes and edges are generated before a bulk save operation by walking all entity_nodes and entity_edges. If a node's `name_embedding` is None, it awaits the node to generate its name embedding using the embedder. Similarly, if an edge's `fact_embedding` is None, it awaits the edge to generate its fact embedding using the embedder. This process guarantees every node and edge has an embedding before running the bulk save Cypher.","Before the bulk write (in graphiti_core/utils/bulk_utils.py→add_nodes_and_edges_bulk_tx) it walks all entity_nodes and entity_edges and does:

• if node.name_embedding is None: await node.generate_name_embedding(embedder)  
• if edge.fact_embedding is None: await edge.generate_embedding(embedder)

That guarantees every node and edge has an embedding before running the bulk save Cypher.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_entity_edge_save_bulk_query', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge inherits from or is a subclass of pydantic.BaseModel\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.edges contains GLOBAL_VARIABLE graphiti.graphiti_core.edges.fact_embeddings', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 461, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.476175785064697,906
What embedding generation steps are applied to entity nodes and edges in the bulk insert path versus the edge resolution path?,"**Bulk-Insert Path vs Edge-Resolution Path Embedding Generation Steps**

### Bulk-Insert Path

*   For each EntityNode whose `name_embedding` is None, `node.generate_name_embedding(embedder)` is awaited.
*   No edge embeddings are generated in the bulk-insert path.

### Edge-Resolution Path

*   Edge embeddings are batched via `create_entity_edge_embeddings(embedder, ...)`. This function is called once on the newly extracted edges before matching and again in parallel on the resolved edges and invalidated edges.
*   No node embeddings are generated in the edge-resolution path.","In the bulk‐insert path (in graphiti_core/utils/bulk_utils.py → add_nodes_and_edges_bulk_tx):

• For each EntityNode:  
  – if node.name_embedding is None → await node.generate_name_embedding(embedder)  
• For each EntityEdge:  
  – if edge.fact_embedding is None → await edge.generate_embedding(embedder)  

In the edge‐resolution path (in graphiti_core/utils/maintenance/edge_operations.py → resolve_extracted_edges):

• It never calls per‐edge.generate_embedding() directly. Instead it batches all edge embeddings via create_entity_edge_embeddings(embedder, …):  
  1) once on the newly extracted_edges before matching  
  2) once more (in parallel) on the finally resolved_edges and invalidated_edges  

No node embeddings are generated in the resolution path.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.fact_embedding', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.nodes.EntityNode has field or attribute or property graphiti.graphiti_core.nodes.EntityNode.name_embedding', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 462, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",37.39865827560425,907
How does semaphore_gather determine its concurrency limit if max_coroutines is not specified?,"If `max_coroutines` is not passed, semaphore_gather determines its concurrency limit by falling back to the module-level `SEMAPHORE_LIMIT`. This fallback is implemented by calling `Semaphore(max_coroutines or SEMAPHORE_LIMIT)`, which retrieves the value of `SEMAPHORE_LIMIT` from the `graphiti_core.helpers` module.","If you don’t pass `max_coroutines`, it falls back to the module‐level `SEMAPHORE_LIMIT` (via `Semaphore(max_coroutines or SEMAPHORE_LIMIT)`).","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.neo4j\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.SEMAPHORE_LIMIT', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.group_id\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 471, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.783336639404297,908
What are the main steps community_search follows to return the final list of reranked communities?,"The main steps community_search follows to return the final list of reranked communities are as follows:

1. **Invoke community_fulltext_search and community_similarity_search in parallel**: The `community_search` function invokes these two functions in parallel, retrieving two candidate lists each containing up to 2×limit items.

2. **Collect UUIDs from candidate lists**: It collects all UUIDs from the candidate lists.

3. **Build map of UUID to CommunityNode**: A map is built from UUID to CommunityNode.

4. **Select reranking strategy based on config.reranker**: The `community_search` function selects a reranking strategy based on the value of `config.reranker`.

5. **Apply reranking strategy**:

* If `config.reranker` is `CommunityReranker.rrf`, it calls `rrf(…)`.

* If `config.reranker` is `CommunityReranker.mmr`, it fetches embeddings using `get_embeddings_for_communities(…)` and then calls `maximal_marginal_relevance(…)`.

* If `config.reranker` is `CommunityReranker.cross_encoder`, it calls `cross_encoder.rank(query, names)`.

6. **Map reranked UUIDs back to CommunityNode objects**: The reranked UUIDs are mapped back to CommunityNode objects.

7. **Return the first limit communities**: Finally, the function returns the first limit communities.","community_search (in graphiti_core/search/search.py) proceeds as follows:

1. Early exit if no config is provided.  
2. In parallel, invoke  
   – community_fulltext_search(driver, query, …)  
   – community_similarity_search(driver, query_vector, …)  
   to get two candidate lists (each up to 2×limit).  
3. Collect all UUIDs and build a uuid→CommunityNode map.  
4. Depending on config.reranker:  
   • CommunityReranker.rrf → call rrf(…)  
   • CommunityReranker.mmr → fetch embeddings via get_embeddings_for_communities(…) then maximal_marginal_relevance(…)  
   • CommunityReranker.cross_encoder → cross_encoder.rank(query, names)  
5. Map the reranked UUIDs back to CommunityNode objects.  
6. Return the first limit communities.","['METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.hist_step_n\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'MODULE graphiti.graphiti_core.search.search contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search.reranked_communities', 'METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.step\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'CLASS graphiti.graphiti_core.search.search_config.CommunitySearchConfig has field or attribute or property graphiti.graphiti_core.search.search_config.CommunitySearchConfig.reranker', 'CLASS qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS has method qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## 💫 Contributors\n\nThanks to our amazing contributors! 💖\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=topoteretes/cognee"" />\n</a>\n\n## 🏆 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|', 'cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- Поддержка онтологий на основе RDF: Использует онтологии на основе RDF для более интеллектуального управления данными и улучшения семантического понимания.\n\n- Локальное развертывание и масштабируемость: Позволяет развернуть систему на собственных серверах, обеспечивая безопасность данных и соответствие требованиям конфиденциальности. Система масштабируется для обработки больших объемов данных.\n\n## Начало работы\n\nНачните легко с помощью Google Colab <a href=""https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing"">блокнота</a> или <a href=""https://github.com/topoteretes/cognee-starter"">стартового репозитория</a>\n\n## Помощь проекту\n\nВаш вклад является основой для превращения этого в настоящий проект с открытым исходным кодом. Любой вклад, который вы сделаете, будет **очень приветствоваться**. Смотрите [`CONTRIBUTING.md`](/CONTRIBUTING.md) для получения дополнительной информации.\n\n## 📦 Установка\n\nВы можете установить Cognee, используя **pip**, **poetry**, **uv** или любой другой менеджер пакетов Python.\n\n### С помощью pip\n\n```bash\npip install cognee\n```\n\n## 💻 Базовое использование\n\n### Настройка\n\n```python\nimport os\nos.environ[""LLM_API_KEY""] = ""ВАШ_OPENAI_API_KEY""\n```\n\nВы также можете установить переменные, создав файл .env, используя наш <a href=""https://github.com/topoteretes/cognee/blob/main/.env.template"">шаблон</a>.\nДля использования различных провайдеров LLM смотрите нашу <a href=""https://docs.cognee.ai"">документацию</a>.\n\n### Пример использования\n\nЭтот скрипт выполнит *стандартный* конвейер:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Добавляем текст в cognee\n    await cognee.add(""Обработка естественного языка (NLP) - это междисциплинарная область компьютерных наук и информационного поиска."")\n\n    # Генерируем граф знаний\n    await cognee.cognify()\n\n    # Делаем поиск\n    results = await cognee.search(""Расскажите мне о NLP"")\n\n    # Отображаем результаты\n    for result in results:\n        print(result)\n\n\nif __name__ == \'__main__\':\n    asyncio.run(main())\n\n```\nПример вывода:\n```\nОбработка естественного языка (NLP) — это междисциплинарная область, которая объединяет компьютерные науки и информационный поиск. Она включает в себя технологии и методы обработки человеческого языка для создания интерфейсов и обработки данных.\n```\n\nВизуализация графа:\n<a href=""https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html""><img src=""graph_visualization_ru.png"" width=""100%"" alt=""Визуализация графа""></a>\n[Открыть пример графа в браузере](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nБольше примеров в <a href=""https://docs.cognee.ai"">документации</a>.\n\n\n## Изучите нашу архитектуру\n\n<div style=""text-align: center"">\n  <img src=""https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png"" alt=""концептуальная диаграмма cognee"" width=""100%"" />\n</div>']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 478, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",49.542805671691895,909
"In node search with MMR reranking, which helper is used to fetch embeddings before calling the MMR function?",The helper function used to fetch embeddings before calling the MMR function in graphiti is `get_embeddings_for_nodes`. This function is defined in `graphiti_core/search/search_utils.py` and is used as part of the node search with MMR reranking process.,The helper `get_embeddings_for_nodes` (in graphiti_core/search/search_utils.py) is used to fetch the embeddings before calling `maximal_marginal_relevance` in the MMR reranker.,"['FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.nodes\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'MODULE graphiti.graphiti_core.search.search_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.search.search_utils.DEFAULT_MMR_LAMBDA', 'MODULE cognee.cognee.tasks.repo_processor.get_local_dependencies contains FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'FUNCTION graphiti.graphiti_core.search.search_utils.maximal_marginal_relevance has parameter or accepts argument graphiti.graphiti_core.search.search_utils.DEFAULT_MMR_LAMBDA', 'FUNCTION cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node has parameter or accepts argument cognee.cognee.tasks.repo_processor.get_local_dependencies.find_node.condition\nFind and return the first node that satisfies the given condition.  Iterate through the provided list of nodes and return the first node for which the condition callable returns True. If no such node is found, return None.  Parameters: -----------      - nodes (list[Node]): A list of Node objects to search through.     - condition (callable): A callable that takes a Node and returns a boolean       indicating if the node meets specified criteria.  Returns: --------      - Node: The first Node that matches the condition, or None if no such node exists.', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\nIn this example, we use tree booster for gradient boosting. If you would like to use linear booster for regression, you can keep all the parameters except booster and the tree booster parameters as below:\n```conf\n# General Parameters\n# choose the linear booster\nbooster = gblinear\n...\n\n# Change Tree Booster Parameters into Linear Booster Parameters\n# L2 regularization term on weights, default 0\nlambda = 0.01\n# L1 regularization term on weights, default 0\nalpha = 0.01\n# L2 regularization term on bias, default 0\nlambda_bias = 0.01\n\n# Regression Parameters\n...\n```\n\n#### Get Predictions\nAfter training, we can use the output model to get the prediction of the test data:\n```\n../../xgboost mushroom.conf task=pred model_in=0002.model\n```\nFor binary classification, the output predictions are probability confidence scores in [0,1], corresponds to the probability of the label to be positive.\n\n#### Dump Model\nThis is a preliminary feature, so only tree models support text dump. XGBoost can display the tree models in text or JSON files, and we can scan the model in an easy way:\n```\n../../xgboost mushroom.conf task=dump model_in=0002.model name_dump=dump.raw.txt\n../../xgboost mushroom.conf task=dump model_in=0002.model fmap=featmap.txt name_dump=dump.nice.txt\n```\n\nIn this demo, the tree boosters obtained will be printed in dump.raw.txt and dump.nice.txt, and the latter one is easier to understand because of usage of feature mapping featmap.txt\n\nFormat of ```featmap.txt: <featureid> <featurename> <q or i or int>\\n ```:\n  - Feature id must be from 0 to number of features, in sorted order.\n  - i means this feature is binary indicator feature\n  - q means this feature is a quantitative value, such as age, time, can be missing\n  - int means this feature is integer value (when int is hinted, the decision boundary will be integer)\n\n#### Monitoring Progress\nWhen you run training we can find there are messages displayed on screen\n```\ntree train end, 1 roots, 12 extra nodes, 0 pruned nodes ,max_depth=3\n[0]  test-error:0.016139\nboosting round 1, 0 sec elapsed\n\ntree train end, 1 roots, 10 extra nodes, 0 pruned nodes ,max_depth=3\n[1]  test-error:0.000000\n```\nThe messages for evaluation are printed into stderr, so if you want only to log the evaluation progress, simply type\n```\n../../xgboost mushroom.conf 2>log.txt\n```\nThen you can find the following content in log.txt\n```\n[0]     test-error:0.016139\n[1]     test-error:0.000000\n```\nWe can also monitor both training and test statistics, by adding following lines to configure\n```conf\neval[test] = ""agaricus.txt.test""\neval[trainname] = ""agaricus.txt.train""\n```\nRun the command again, we can find the log file becomes\n```\n[0]     test-error:0.016139     trainname-error:0.014433\n[1]     test-error:0.000000     trainname-error:0.001228\n```\nThe rule is eval[name-printed-in-log] = filename, then the file will be added to monitoring process, and evaluated each round.\n\nxgboost also supports monitoring multiple metrics, suppose we also want to monitor average log-likelihood of each prediction during training, simply add ```eval_metric=logloss``` to configure. Run again, we can find the log file becomes\n```\n[0]     test-error:0.016139     test-negllik:0.029795   trainname-error:0.014433        trainname-negllik:0.027023\n[1]     test-error:0.000000     test-negllik:0.000000   trainname-error:0.001228        trainname-negllik:0.002457\n```', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.4 (2015.05.11)\n\n* Distributed version of xgboost that runs on YARN, scales to billions of examples\n* Direct save/load data and model from/to S3 and HDFS\n* Feature importance visualization in R module, by Michael Benesty\n* Predict leaf index\n* Poisson regression for counts data\n* Early stopping option in training\n* Native save load support in R and python\n  - xgboost models now can be saved using save/load in R\n  - xgboost python model is now pickable\n* sklearn wrapper is supported in python module\n* Experimental External memory version\n\n\n## v0.3 (2014.09.07)\n\n* Faster tree construction module\n  - Allows subsample columns during tree construction via ```bst:col_samplebytree=ratio```\n* Support for boosting from initial predictions\n* Experimental version of LambdaRank\n* Linear booster is now parallelized, using parallel coordinated descent.\n* Add [Code Guide](src/README.md) for customizing objective function and evaluation\n* Add R module\n\n\n## v0.2x (2014.05.20)\n\n* Python module\n* Weighted samples instances\n* Initial version of pairwise rank\n\n\n## v0.1 (2014.03.26)\n\n* Initial release', 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker ""thundering herd"" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `""reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn\'t let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 11}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 11, 'n_files_pr': 6, 'pr': 481, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.28066921234131,910
"How does the wrapper ensure the semaphore is always released, even if the wrapped coroutine raises an exception?","The semaphore_gather._wrap_coroutine function ensures that the semaphore is always released, even if the wrapped coroutine raises an exception. This is because the async with semaphore block always calls its __aexit__ method, which releases the lock, regardless of whether the awaited coroutine raises an exception or not. The __aexit__ method is called when exiting the context manager, ensuring that the lock is released in all cases.","The `semaphore_gather._wrap_coroutine` in graphiti_core/helpers.py uses an `async with semaphore:` block. The semaphore’s async context manager always calls its `__aexit__`, releasing the lock even if `await coroutine` raises.","['MODULE cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter contains FUNCTION cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.wrapper\nRetry the execution of a function with backoff on failure due to rate limit errors.  This wrapper function will call the specified function and if it raises an exception, it will handle retries according to defined conditions. It will check the environment for a DISABLE_RETRIES flag to determine whether to retry or propagate errors immediately during tests. If the error is identified as a rate limit error, it will apply an exponential backoff strategy with jitter before retrying, up to a maximum number of retries. If the retries are exhausted, it raises the last encountered error.  Parameters: -----------      - *args: Positional arguments passed to the wrapped function.     - **kwargs: Keyword arguments passed to the wrapped function.  Returns: --------      Returns the result of the wrapped function if successful; otherwise, raises the last     error encountered after maximum retries are exhausted.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'FUNCTION cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.wrapper has parameter or accepts argument cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.wrapper.base_backoff\nRetry the execution of a function with backoff on failure due to rate limit errors.  This wrapper function will call the specified function and if it raises an exception, it will handle retries according to defined conditions. It will check the environment for a DISABLE_RETRIES flag to determine whether to retry or propagate errors immediately during tests. If the error is identified as a rate limit error, it will apply an exponential backoff strategy with jitter before retrying, up to a maximum number of retries. If the retries are exhausted, it raises the last encountered error.  Parameters: -----------      - *args: Positional arguments passed to the wrapped function.     - **kwargs: Keyword arguments passed to the wrapped function.  Returns: --------      Returns the result of the wrapped function if successful; otherwise, raises the last     error encountered after maximum retries are exhausted.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.semaphore', 'FUNCTION cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.wrapper has parameter or accepts argument cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.wrapper.jitter\nRetry the execution of a function with backoff on failure due to rate limit errors.  This wrapper function will call the specified function and if it raises an exception, it will handle retries according to defined conditions. It will check the environment for a DISABLE_RETRIES flag to determine whether to retry or propagate errors immediately during tests. If the error is identified as a rate limit error, it will apply an exponential backoff strategy with jitter before retrying, up to a maximum number of retries. If the retries are exhausted, it raises the last encountered error.  Parameters: -----------      - *args: Positional arguments passed to the wrapped function.     - **kwargs: Keyword arguments passed to the wrapped function.  Returns: --------      Returns the result of the wrapped function if successful; otherwise, raises the last     error encountered after maximum retries are exhausted.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', ""cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/zen_principles.md\n# The Zen of Python: Practical Guide\n\n## Overview\nThe Zen of Python (Tim Peters, import this) captures Python's philosophy. Use these principles as a checklist during design, coding, and reviews.\n\n## Key Principles With Guidance\n\n### 1. Beautiful is better than ugly\nPrefer descriptive names, clear structure, and consistent formatting.\n\n### 2. Explicit is better than implicit\nBe clear about behavior, imports, and types.\n```python\nfrom datetime import datetime, timedelta\n\ndef get_future_date(days_ahead: int) -> datetime:\n    return datetime.now() + timedelta(days=days_ahead)\n```\n\n### 3. Simple is better than complex\nChoose straightforward solutions first.\n\n### 4. Complex is better than complicated\nWhen complexity is needed, organize it with clear abstractions.\n\n### 5. Flat is better than nested\nUse early returns to reduce indentation.\n\n### 6. Sparse is better than dense\nGive code room to breathe with whitespace.\n\n### 7. Readability counts\nOptimize for human readers; add docstrings for nontrivial code.\n\n### 8. Special cases aren't special enough to break the rules\nStay consistent; exceptions should be rare and justified.\n\n### 9. Although practicality beats purity\nPrefer practical solutions that teams can maintain.\n\n### 10. Errors should never pass silently\nHandle exceptions explicitly; log with context.\n\n### 11. Unless explicitly silenced\nSilence only specific, acceptable errors and document why.\n\n### 12. In the face of ambiguity, refuse the temptation to guess\nRequire explicit inputs and behavior.\n\n### 13. There should be one obvious way to do it\nPrefer standard library patterns and idioms.\n\n### 14. Although that way may not be obvious at first\nLearn Python idioms; embrace clarity over novelty.\n\n### 15. Now is better than never; 16. Never is often better than right now\nIterate, but don't rush broken code.\n\n### 17/18. Hard to explain is bad; easy to explain is good\nPrefer designs you can explain simply.\n\n### 19. Namespaces are one honking great idea\nUse modules/packages to separate concerns; avoid wildcard imports.\n\n## Modern Python Tie-ins\n- Type hints reinforce explicitness\n- Context managers enforce safe resource handling\n- Dataclasses improve readability for data containers\n\n## Quick Review Checklist\n- Is it readable and explicit?\n- Is this the simplest working solution?\n- Are errors explicit and logged?\n- Are modules/namespaces used appropriately?"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 489, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.154193878173828,911
How do the exclude_layers and exclude_embeddings settings affect whether a variable is treated with AdamW updates?,"The `exclude_layers` and `exclude_embeddings` settings in Keras' Muon optimizer determine which variables receive the AdamW update. When `exclude_embeddings=True`, any variable path containing 'embedding' immediately returns True, excluding it from AdamW updates. For `exclude_layers`, any variable whose path matches one of the regexes in `exclude_layers` also returns True and is excluded from AdamW updates. Variables that don't match these criteria receive the AdamW update.","In Muon (keras/src/optimizers/muon.py) the decision of whether to apply the AdamW (i.e. weight-decayed) update lives in Muon._should_use_adamw:

• exclude_embeddings=True  
 – any variable whose path contains “embedding” will immediately return True in  _should_use_adamw, so it will be treated as a “pure” Adam parameter (no weight decay).  

• exclude_layers=[…]  
 – any variable whose .path matches one of the regexes in exclude_layers will also return True in  _should_use_adamw (and thus be excluded from the AdamW weight‐decay update).  

All other parameters (with 2- or 3-D shapes that don’t match those exclusions) return False and get the AdamW update.","['CLASS keras.keras.src.optimizers.muon.Muon has field or attribute or property keras.keras.src.optimizers.muon.adam_beta_2\nOptimizer that implements the Muon algorithm.  Note that this optimizer should not be used in the following layers:  1. Embedding layer 2. Final output fully connected layer 3. Any {0,1}-D variables  These should all be optimized using AdamW.  The Muon optimizer can use both the Muon update step or the AdamW update step based on the following:  - For any variable that isn\'t 2D, 3D or 4D, the AdamW step     will be used. This is not configurable. - If the argument `exclude_embeddings` (defaults to `True`) is set to `True`, the AdamW step will be used. - For any variablewith a name that matches an expression     listed in the argument `exclude_layers` (a list), the     AdamW step will be used. - Any other variable uses the Muon step.  Typically, you only need to pass the name of your densely-connected output layer to `exclude_layers`, e.g. `exclude_layers=[""output_dense""]`.  References:     - [Original implementation](https://github.com/KellerJordan/Muon)     - [Liu et al, 2025](https://arxiv.org/abs/2502.16982)  Args:     learning_rate: A float,         `keras.optimizers.schedules.LearningRateSchedule` instance, or         a callable that takes no arguments and returns the actual value to         use. The learning rate. Defaults to `0.001`.     adam_beta_1: A float value or a constant float tensor, or a callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 1st moment estimates. Defaults to         `0.9`.     adam_beta_2: A float value or a constant float tensor, ora callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 2nd moment estimates. Defaults to         `0.999`.     epsilon: A small constant for numerical stability. This is         ""epsilon hat"" in the Kingma and Ba paper         (in the formula just before Section 2.1),         not the epsilon in Algorithm 1 of the paper.         It be used at Adamw.Defaults to `1e-7`.     exclude_layers: List of strings, keywords of layer names to exclude.         All layers with keywords in their path will use adamw.     exclude_embeddings: Boolean value         If True, embedding layers will use adamw.     muon_a: Float, parameter a of the muon algorithm.         It is recommended to use the default value     muon_b: Float, parameter b of the muon algorithm.         It is recommended to use the default value     muon_c: Float, parameter c of the muon algorithm.         It is recommended to use the default value     adam_lr_ratio: Float, the ratio of the learning rate when             using Adam to the main learning rate.             it is recommended to set it to 0.1     momentum: Float, momentum used by internal SGD.     ns_steps: Integer, number of Newton-Schulz iterations to run.     nesterov: Boolean, whether to use Nesterov-style momentum     {{base_optimizer_keyword_args}}', 'MODULE keras.keras.src.optimizers.adamw_test contains CLASS keras.keras.src.optimizers.adamw_test.AdamWTest', 'CLASS keras.keras.src.optimizers.muon.Muon has field or attribute or property keras.keras.src.optimizers.muon.weight_decay\nOptimizer that implements the Muon algorithm.  Note that this optimizer should not be used in the following layers:  1. Embedding layer 2. Final output fully connected layer 3. Any {0,1}-D variables  These should all be optimized using AdamW.  The Muon optimizer can use both the Muon update step or the AdamW update step based on the following:  - For any variable that isn\'t 2D, 3D or 4D, the AdamW step     will be used. This is not configurable. - If the argument `exclude_embeddings` (defaults to `True`) is set to `True`, the AdamW step will be used. - For any variablewith a name that matches an expression     listed in the argument `exclude_layers` (a list), the     AdamW step will be used. - Any other variable uses the Muon step.  Typically, you only need to pass the name of your densely-connected output layer to `exclude_layers`, e.g. `exclude_layers=[""output_dense""]`.  References:     - [Original implementation](https://github.com/KellerJordan/Muon)     - [Liu et al, 2025](https://arxiv.org/abs/2502.16982)  Args:     learning_rate: A float,         `keras.optimizers.schedules.LearningRateSchedule` instance, or         a callable that takes no arguments and returns the actual value to         use. The learning rate. Defaults to `0.001`.     adam_beta_1: A float value or a constant float tensor, or a callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 1st moment estimates. Defaults to         `0.9`.     adam_beta_2: A float value or a constant float tensor, ora callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 2nd moment estimates. Defaults to         `0.999`.     epsilon: A small constant for numerical stability. This is         ""epsilon hat"" in the Kingma and Ba paper         (in the formula just before Section 2.1),         not the epsilon in Algorithm 1 of the paper.         It be used at Adamw.Defaults to `1e-7`.     exclude_layers: List of strings, keywords of layer names to exclude.         All layers with keywords in their path will use adamw.     exclude_embeddings: Boolean value         If True, embedding layers will use adamw.     muon_a: Float, parameter a of the muon algorithm.         It is recommended to use the default value     muon_b: Float, parameter b of the muon algorithm.         It is recommended to use the default value     muon_c: Float, parameter c of the muon algorithm.         It is recommended to use the default value     adam_lr_ratio: Float, the ratio of the learning rate when             using Adam to the main learning rate.             it is recommended to set it to 0.1     momentum: Float, momentum used by internal SGD.     ns_steps: Integer, number of Newton-Schulz iterations to run.     nesterov: Boolean, whether to use Nesterov-style momentum     {{base_optimizer_keyword_args}}', 'CLASS keras.keras.src.optimizers.muon.Muon has field or attribute or property keras.keras.src.optimizers.muon.exclude_layers\nOptimizer that implements the Muon algorithm.  Note that this optimizer should not be used in the following layers:  1. Embedding layer 2. Final output fully connected layer 3. Any {0,1}-D variables  These should all be optimized using AdamW.  The Muon optimizer can use both the Muon update step or the AdamW update step based on the following:  - For any variable that isn\'t 2D, 3D or 4D, the AdamW step     will be used. This is not configurable. - If the argument `exclude_embeddings` (defaults to `True`) is set to `True`, the AdamW step will be used. - For any variablewith a name that matches an expression     listed in the argument `exclude_layers` (a list), the     AdamW step will be used. - Any other variable uses the Muon step.  Typically, you only need to pass the name of your densely-connected output layer to `exclude_layers`, e.g. `exclude_layers=[""output_dense""]`.  References:     - [Original implementation](https://github.com/KellerJordan/Muon)     - [Liu et al, 2025](https://arxiv.org/abs/2502.16982)  Args:     learning_rate: A float,         `keras.optimizers.schedules.LearningRateSchedule` instance, or         a callable that takes no arguments and returns the actual value to         use. The learning rate. Defaults to `0.001`.     adam_beta_1: A float value or a constant float tensor, or a callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 1st moment estimates. Defaults to         `0.9`.     adam_beta_2: A float value or a constant float tensor, ora callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 2nd moment estimates. Defaults to         `0.999`.     epsilon: A small constant for numerical stability. This is         ""epsilon hat"" in the Kingma and Ba paper         (in the formula just before Section 2.1),         not the epsilon in Algorithm 1 of the paper.         It be used at Adamw.Defaults to `1e-7`.     exclude_layers: List of strings, keywords of layer names to exclude.         All layers with keywords in their path will use adamw.     exclude_embeddings: Boolean value         If True, embedding layers will use adamw.     muon_a: Float, parameter a of the muon algorithm.         It is recommended to use the default value     muon_b: Float, parameter b of the muon algorithm.         It is recommended to use the default value     muon_c: Float, parameter c of the muon algorithm.         It is recommended to use the default value     adam_lr_ratio: Float, the ratio of the learning rate when             using Adam to the main learning rate.             it is recommended to set it to 0.1     momentum: Float, momentum used by internal SGD.     ns_steps: Integer, number of Newton-Schulz iterations to run.     nesterov: Boolean, whether to use Nesterov-style momentum     {{base_optimizer_keyword_args}}', 'MODULE keras.keras.src.optimizers.muon contains CLASS keras.keras.src.optimizers.muon.Muon\nOptimizer that implements the Muon algorithm.  Note that this optimizer should not be used in the following layers:  1. Embedding layer 2. Final output fully connected layer 3. Any {0,1}-D variables  These should all be optimized using AdamW.  The Muon optimizer can use both the Muon update step or the AdamW update step based on the following:  - For any variable that isn\'t 2D, 3D or 4D, the AdamW step     will be used. This is not configurable. - If the argument `exclude_embeddings` (defaults to `True`) is set to `True`, the AdamW step will be used. - For any variablewith a name that matches an expression     listed in the argument `exclude_layers` (a list), the     AdamW step will be used. - Any other variable uses the Muon step.  Typically, you only need to pass the name of your densely-connected output layer to `exclude_layers`, e.g. `exclude_layers=[""output_dense""]`.  References:     - [Original implementation](https://github.com/KellerJordan/Muon)     - [Liu et al, 2025](https://arxiv.org/abs/2502.16982)  Args:     learning_rate: A float,         `keras.optimizers.schedules.LearningRateSchedule` instance, or         a callable that takes no arguments and returns the actual value to         use. The learning rate. Defaults to `0.001`.     adam_beta_1: A float value or a constant float tensor, or a callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 1st moment estimates. Defaults to         `0.9`.     adam_beta_2: A float value or a constant float tensor, ora callable         that takes no arguments and returns the actual value to use.         The exponential decay rate for the 2nd moment estimates. Defaults to         `0.999`.     epsilon: A small constant for numerical stability. This is         ""epsilon hat"" in the Kingma and Ba paper         (in the formula just before Section 2.1),         not the epsilon in Algorithm 1 of the paper.         It be used at Adamw.Defaults to `1e-7`.     exclude_layers: List of strings, keywords of layer names to exclude.         All layers with keywords in their path will use adamw.     exclude_embeddings: Boolean value         If True, embedding layers will use adamw.     muon_a: Float, parameter a of the muon algorithm.         It is recommended to use the default value     muon_b: Float, parameter b of the muon algorithm.         It is recommended to use the default value     muon_c: Float, parameter c of the muon algorithm.         It is recommended to use the default value     adam_lr_ratio: Float, the ratio of the learning rate when             using Adam to the main learning rate.             it is recommended to set it to 0.1     momentum: Float, momentum used by internal SGD.     ns_steps: Integer, number of Newton-Schulz iterations to run.     nesterov: Boolean, whether to use Nesterov-style momentum     {{base_optimizer_keyword_args}}', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n#### Resource Allocation\n\nA denial of service caused by one model can impact the overall system health. Implement safeguards like rate limits, access controls, and health monitoring.\n\n#### Model Sharing\n\nIn a multitenant design that allows sharing models, make sure that tenants and users fully understand the potential security risks involved. They must be aware that they will essentially be running code provided by other users. Unfortunately, there are no reliable methods available to detect malicious models, graphs, or checkpoints. To mitigate this risk, the recommended approach is to sandbox the model execution, effectively isolating it from the rest of the system.\n\n#### Hardware Attacks\n\nBesides the virtual environment, the hardware (GPUs or TPUs) can also be attacked. [Research](https://scholar.google.com/scholar?q=gpu+side+channel) has shown that side channel attacks on GPUs are possible, which can make data leak from other models or processes running on the same system at the same time.\n\n## Reporting a Vulnerability\n\nBeware that none of the topics under [Using Keras Securely](#using-keras-securely) are considered vulnerabilities of Keras.\n\nIf you have discovered a security vulnerability in this project, please report it\nprivately. **Do not disclose it as a public issue.** This gives us time to work with you\nto fix the issue before public exposure, reducing the chance that the exploit will be\nused before a patch is released.\n\nYou may submit the report in the following ways:\n\n- send an email to francois.chollet@gmail.com; and/or\n- send a [private vulnerability report](https://github.com/keras-team/keras/security/advisories/new)\n\nPlease provide the following information in your report:\n\n- A description of the vulnerability and its impact\n- How to reproduce the issue\n\nThis project is maintained by volunteers on a reasonable-effort basis. As such,\nplease give us 90 days to work on a fix before public exposure.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 21079, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",39.75652027130127,943
Which input signature inference path is used for a functional model with dict inputs?,"The input signature inference path used for a functional model with dictionary inputs in keras is `tree.map_structure(make_input_spec, model._inputs_struct)`. This is because the Functional-model branch in the `get_input_signature` function of `export_utils.py` uses this method to infer the input signature when the inputs are provided as dictionaries. For Functional models, `get_input_signature` does not use `_infer_input_signature_from_model`, and instead relies on `tree.map_structure(make_input_spec, model._inputs_struct)` regardless of the input type.","For a Functional model (even with dict inputs), `get_input_signature` takes the Functional‐model branch in  
`keras/src/export/export_utils.py`:  

```python
if isinstance(model, models.Functional):
  input_signature = [
    tree.map_structure(make_input_spec, model._inputs_struct)
  ]
]
```  
so it uses `tree.map_structure(make_input_spec, model._inputs_struct)` rather than `_infer_input_signature_from_model`.","['MODULE keras.keras.src.models.functional contains FUNCTION keras.keras.src.models.functional.clone_graph_nodes\nClone the `Node` between the inputs and output tensors.  This function is used to create a new functional model from any intermediate Keras tensors. The clone of the nodes mimic the behavior of reconstructing the functional graph network by re-executing all the `__call__()` methods. The cloned nodes will be appended to the layers.  Note that a new `keras.Input` will be created for any items in the `inputs`  Args: inputs: A nested structure of `KerasTensor` instances. outputs: A nested structure of `KerasTensor` instances.  Returns:     A pair of inputs and outputs, with cloned `KerasTensor` instances.     They can be used to create a new functional model.', 'CLASS keras.keras.src.models.model.Model has field or attribute or property keras.keras.src.models.model.input_signature\nA model grouping layers into an object with training/inference features.  There are three ways to instantiate a `Model`:  ## With the ""Functional API""  You start from `Input`, you chain layer calls to specify the model\'s forward pass, and finally, you create your model from inputs and outputs:  ```python inputs = keras.Input(shape=(37,)) x = keras.layers.Dense(32, activation=""relu"")(inputs) outputs = keras.layers.Dense(5, activation=""softmax"")(x) model = keras.Model(inputs=inputs, outputs=outputs) ```  Note: Only dicts, lists, and tuples of input tensors are supported. Nested inputs are not supported (e.g. lists of list or dicts of dict).  A new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.  Example:  ```python inputs = keras.Input(shape=(None, None, 3)) processed = keras.layers.RandomCrop(width=128, height=128)(inputs) conv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed) pooling = keras.layers.GlobalAveragePooling2D()(conv) feature = keras.layers.Dense(10)(pooling)  full_model = keras.Model(inputs, feature) backbone = keras.Model(processed, conv) activations = keras.Model(conv, feature) ```  Note that the `backbone` and `activations` models are not created with `keras.Input` objects, but with the tensors that originate from `keras.Input` objects. Under the hood, the layers and weights will be shared across these models, so that user can train the `full_model`, and use `backbone` or `activations` to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.  ## By subclassing the `Model` class  In that case, you should define your layers in `__init__()` and you should implement the model\'s forward pass in `call()`.  ```python class MyModel(keras.Model):     def __init__(self):         super().__init__()         self.dense1 = keras.layers.Dense(32, activation=""relu"")         self.dense2 = keras.layers.Dense(5, activation=""softmax"")      def call(self, inputs):         x = self.dense1(inputs)         return self.dense2(x)  model = MyModel() ```  If you subclass `Model`, you can optionally have a `training` argument (boolean) in `call()`, which you can use to specify a different behavior in training and inference:  ```python class MyModel(keras.Model):     def __init__(self):         super().__init__()         self.dense1 = keras.layers.Dense(32, activation=""relu"")         self.dense2 = keras.layers.Dense(5, activation=""softmax"")         self.dropout = keras.layers.Dropout(0.5)      def call(self, inputs, training=False):         x = self.dense1(inputs)         x = self.dropout(x, training=training)         return self.dense2(x)  model = MyModel() ```  Once the model is created, you can config the model with losses and metrics with `model.compile()`, train the model with `model.fit()`, or use the model to do prediction with `model.predict()`.  ## With the `Sequential` class  In addition, `keras.Sequential` is a special case of model where the model is purely a stack of single-input, single-output layers.  ```python model = keras.Sequential([     keras.Input(shape=(None, None, 3)),     keras.layers.Conv2D(filters=32, kernel_size=3), ]) ```', 'FUNCTION keras.keras.src.models.functional.clone_graph_nodes has parameter or accepts argument keras.keras.src.models.functional.clone_graph_nodes.inputs\nClone the `Node` between the inputs and output tensors.  This function is used to create a new functional model from any intermediate Keras tensors. The clone of the nodes mimic the behavior of reconstructing the functional graph network by re-executing all the `__call__()` methods. The cloned nodes will be appended to the layers.  Note that a new `keras.Input` will be created for any items in the `inputs`  Args: inputs: A nested structure of `KerasTensor` instances. outputs: A nested structure of `KerasTensor` instances.  Returns:     A pair of inputs and outputs, with cloned `KerasTensor` instances.     They can be used to create a new functional model.', 'MODULE keras.keras.src.models.functional contains FUNCTION keras.keras.src.models.functional.is_input_keras_tensor', 'FUNCTION keras.keras.src.models.functional.clone_graph_nodes has parameter or accepts argument keras.keras.src.models.functional.clone_graph_nodes.outputs\nClone the `Node` between the inputs and output tensors.  This function is used to create a new functional model from any intermediate Keras tensors. The clone of the nodes mimic the behavior of reconstructing the functional graph network by re-executing all the `__call__()` methods. The cloned nodes will be appended to the layers.  Note that a new `keras.Input` will be created for any items in the `inputs`  Args: inputs: A nested structure of `KerasTensor` instances. outputs: A nested structure of `KerasTensor` instances.  Returns:     A pair of inputs and outputs, with cloned `KerasTensor` instances.     They can be used to create a new functional model.', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 21095, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",33.885741233825684,944
What is the purpose of transposing the indices tensor before applying the updates?,"The purpose of transposing the indices tensor before applying the updates is to reshape it from (num_updates, rank) to (rank, num_updates), which allows PyTorch's advanced indexing to correctly unpack `tuple(indices)` when executing `outputs[tuple(indices)] = updates`. This enables the correct assignment of updates to the outputs tensor.","In scatter_update (keras/src/backend/torch/core.py), we do  
```python
indices = torch.transpose(indices, 0, 1)
```  
so that the tensor goes from shape (num_updates, rank) → (rank, num_updates). PyTorch’s advanced indexing then expects a tuple of 1D index tensors—one per dimension—so transposing lets us unpack `tuple(indices)` correctly when we do `outputs[tuple(indices)] = updates`.","['FUNCTION keras.keras.src.ops.core.scatter_update has parameter or accepts argument keras.keras.src.ops.core.scatter_update.indices\nUpdate inputs via updates at scattered (sparse) indices.  At a high level, this operation does `inputs[indices] = updates`. Assume `inputs` is a tensor of shape `(D0, D1, ..., Dn)`, there are 2 main usages of `scatter_update`.  1. `indices` is a 2D tensor of shape `(num_updates, n)`, where `num_updates`     is the number of updates to perform, and `updates` is a 1D tensor of     shape `(num_updates,)`. For example, if `inputs` is `zeros((4, 4, 4))`,     and we want to update `inputs[1, 2, 3]` and `inputs[0, 1, 3]` as 1, then     we can use:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2, 3], [0, 1, 3]] updates = np.array([1., 1.]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  2 `indices` is a 2D tensor of shape `(num_updates, k)`, where `num_updates`     is the number of updates to perform, and `k` (`k < n`) is the size of     each index in `indices`. `updates` is a `n - k`-D tensor of shape     `(num_updates, inputs.shape[k:])`. For example, if     `inputs = np.zeros((4, 4, 4))`, and we want to update `inputs[1, 2, :]`     and `inputs[2, 3, :]` as `[1, 1, 1, 1]`, then `indices` would have shape     `(num_updates, 2)` (`k = 2`), and `updates` would have shape     `(num_updates, 4)` (`inputs.shape[2:] = 4`). See the code below:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2], [2, 3]] updates = np.array([[1., 1., 1, 1,], [1., 1., 1, 1,]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  Args:     inputs: A tensor, the tensor to be updated.     indices: A tensor or list/tuple of shape `(N, inputs.ndim)`, specifying         indices to update. `N` is the number of indices to update, must be         equal to the first dimension of `updates`.     updates: A tensor, the new values to be put to `inputs` at `indices`.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'MODULE keras.keras.src.ops.core contains FUNCTION keras.keras.src.ops.core.scatter_update\nUpdate inputs via updates at scattered (sparse) indices.  At a high level, this operation does `inputs[indices] = updates`. Assume `inputs` is a tensor of shape `(D0, D1, ..., Dn)`, there are 2 main usages of `scatter_update`.  1. `indices` is a 2D tensor of shape `(num_updates, n)`, where `num_updates`     is the number of updates to perform, and `updates` is a 1D tensor of     shape `(num_updates,)`. For example, if `inputs` is `zeros((4, 4, 4))`,     and we want to update `inputs[1, 2, 3]` and `inputs[0, 1, 3]` as 1, then     we can use:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2, 3], [0, 1, 3]] updates = np.array([1., 1.]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  2 `indices` is a 2D tensor of shape `(num_updates, k)`, where `num_updates`     is the number of updates to perform, and `k` (`k < n`) is the size of     each index in `indices`. `updates` is a `n - k`-D tensor of shape     `(num_updates, inputs.shape[k:])`. For example, if     `inputs = np.zeros((4, 4, 4))`, and we want to update `inputs[1, 2, :]`     and `inputs[2, 3, :]` as `[1, 1, 1, 1]`, then `indices` would have shape     `(num_updates, 2)` (`k = 2`), and `updates` would have shape     `(num_updates, 4)` (`inputs.shape[2:] = 4`). See the code below:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2], [2, 3]] updates = np.array([[1., 1., 1, 1,], [1., 1., 1, 1,]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  Args:     inputs: A tensor, the tensor to be updated.     indices: A tensor or list/tuple of shape `(N, inputs.ndim)`, specifying         indices to update. `N` is the number of indices to update, must be         equal to the first dimension of `updates`.     updates: A tensor, the new values to be put to `inputs` at `indices`.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'MODULE keras.keras.src.ops.core contains FUNCTION keras.keras.src.ops.core.slice_update\nUpdate an input by slicing in a tensor of updated values.  At a high level, this operation does `inputs[start_indices: start_indices + updates.shape] = updates`. Assume inputs is a tensor of shape `(D0, D1, ..., Dn)`, `start_indices` must be a list/tuple of n integers, specifying the starting indices. `updates` must have the same rank as `inputs`, and the size of each dim must not exceed `Di - start_indices[i]`. For example, if we have 2D inputs `inputs = np.zeros((5, 5))`, and we want to update the intersection of last 2 rows and last 2 columns as 1, i.e., `inputs[3:, 3:] = np.ones((2, 2))`, then we can use the code below:  ```python inputs = np.zeros((5, 5)) start_indices = [3, 3] updates = np.ones((2, 2)) inputs = keras.ops.slice_update(inputs, start_indices, updates) ```  Args:     inputs: A tensor, the tensor to be updated.     start_indices: A list/tuple of shape `(inputs.ndim,)`, specifying         the starting indices for updating.     updates: A tensor, the new values to be put to `inputs` at `indices`.         `updates` must have the same rank as `inputs`.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'FUNCTION keras.keras.src.ops.core.scatter_update has parameter or accepts argument keras.keras.src.ops.core.scatter_update.inputs\nUpdate inputs via updates at scattered (sparse) indices.  At a high level, this operation does `inputs[indices] = updates`. Assume `inputs` is a tensor of shape `(D0, D1, ..., Dn)`, there are 2 main usages of `scatter_update`.  1. `indices` is a 2D tensor of shape `(num_updates, n)`, where `num_updates`     is the number of updates to perform, and `updates` is a 1D tensor of     shape `(num_updates,)`. For example, if `inputs` is `zeros((4, 4, 4))`,     and we want to update `inputs[1, 2, 3]` and `inputs[0, 1, 3]` as 1, then     we can use:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2, 3], [0, 1, 3]] updates = np.array([1., 1.]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  2 `indices` is a 2D tensor of shape `(num_updates, k)`, where `num_updates`     is the number of updates to perform, and `k` (`k < n`) is the size of     each index in `indices`. `updates` is a `n - k`-D tensor of shape     `(num_updates, inputs.shape[k:])`. For example, if     `inputs = np.zeros((4, 4, 4))`, and we want to update `inputs[1, 2, :]`     and `inputs[2, 3, :]` as `[1, 1, 1, 1]`, then `indices` would have shape     `(num_updates, 2)` (`k = 2`), and `updates` would have shape     `(num_updates, 4)` (`inputs.shape[2:] = 4`). See the code below:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2], [2, 3]] updates = np.array([[1., 1., 1, 1,], [1., 1., 1, 1,]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  Args:     inputs: A tensor, the tensor to be updated.     indices: A tensor or list/tuple of shape `(N, inputs.ndim)`, specifying         indices to update. `N` is the number of indices to update, must be         equal to the first dimension of `updates`.     updates: A tensor, the new values to be put to `inputs` at `indices`.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'FUNCTION keras.keras.src.ops.core.scatter_update has parameter or accepts argument keras.keras.src.ops.core.scatter_update.updates\nUpdate inputs via updates at scattered (sparse) indices.  At a high level, this operation does `inputs[indices] = updates`. Assume `inputs` is a tensor of shape `(D0, D1, ..., Dn)`, there are 2 main usages of `scatter_update`.  1. `indices` is a 2D tensor of shape `(num_updates, n)`, where `num_updates`     is the number of updates to perform, and `updates` is a 1D tensor of     shape `(num_updates,)`. For example, if `inputs` is `zeros((4, 4, 4))`,     and we want to update `inputs[1, 2, 3]` and `inputs[0, 1, 3]` as 1, then     we can use:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2, 3], [0, 1, 3]] updates = np.array([1., 1.]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  2 `indices` is a 2D tensor of shape `(num_updates, k)`, where `num_updates`     is the number of updates to perform, and `k` (`k < n`) is the size of     each index in `indices`. `updates` is a `n - k`-D tensor of shape     `(num_updates, inputs.shape[k:])`. For example, if     `inputs = np.zeros((4, 4, 4))`, and we want to update `inputs[1, 2, :]`     and `inputs[2, 3, :]` as `[1, 1, 1, 1]`, then `indices` would have shape     `(num_updates, 2)` (`k = 2`), and `updates` would have shape     `(num_updates, 4)` (`inputs.shape[2:] = 4`). See the code below:  ```python inputs = np.zeros((4, 4, 4)) indices = [[1, 2], [2, 3]] updates = np.array([[1., 1., 1, 1,], [1., 1., 1, 1,]) inputs = keras.ops.scatter_update(inputs, indices, updates) ```  Args:     inputs: A tensor, the tensor to be updated.     indices: A tensor or list/tuple of shape `(N, inputs.ndim)`, specifying         indices to update. `N` is the number of indices to update, must be         equal to the first dimension of `updates`.     updates: A tensor, the new values to be put to `inputs` at `indices`.  Returns:     A tensor, has the same shape and dtype as `inputs`.', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 21101, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",43.65009093284607,945
How is the axis argument propagated from the loss wrapper through to the backend crossentropy implementation?,"The `axis` argument is propagated from the loss wrapper through to the backend crossentropy implementation as follows: it is forwarded to the base class `LossFunctionWrapper`, which includes it in its call to the registered backend loss function. In the case of the Torch backend, this involves using the `axis` argument to drop the class dimension when checking shapes, compute log_softmax, sum the final loss over the specified axis, and one-hot encode with the specified axis. This propagation allows the `axis` argument to be used consistently across different backends and implementations in Keras.","The `axis` argument you pass into `SparseCategoricalCrossentropy` (in `keras/src/losses/losses.py`) is handed straight to its base class, `LossFunctionWrapper`, via

  super().__init__(…, axis=axis)

When you then call the loss, `LossFunctionWrapper` includes that same `axis` kwarg in its call to the backend function you registered (here `sparse_categorical_crossentropy`). In `keras/src/backend/torch/nn.py:sparse_categorical_crossentropy` you see:

  def sparse_categorical_crossentropy(…, axis=-1):

and that `axis` is used to

- drop the class dimension when checking shapes (del output_shape_without_class_dim[axis])
- compute `log_softmax(dim=axis)`
- sum the final loss over `dim=axis`
- one‐hot encode with `axis=axis`

So the flow is:

  losses.py → LossFunctionWrapper (stores axis) → backend/torch/nn.py sparse_categorical_crossentropy(axis=…)","['FUNCTION keras.keras.src.losses.losses.categorical_generalized_cross_entropy has parameter or accepts argument keras.keras.src.losses.losses.categorical_generalized_cross_entropy.q\nComputes the Generalized Cross Entropy loss.  Generalized Cross Entropy (GCE) is a noise-robust loss function that provides better robustness against noisy labels than standard cross entropy. It generalizes both cross entropy and mean absolute error through the parameter q, where values closer to 1 make the loss more robust to noisy labels.  Formula: ```python loss = (1 - p**q) / q ``` where `p` is the predicted probability for the true class and `q` is the noise parameter.  Args:     y_true: Ground truth labels. Expected to contain *integer class indices*         with shape `[batch_size]` or `[batch_size, 1]`.     y_pred: The predicted class probabilities, with shape         `[batch_size, num_classes]`.     q: Float in range `(0, 1)`. It is the noise parameter.        Controls the behavior of the loss:         - As `q` approaches 0: Behaves more like cross entropy         - As `q` approaches 1: Behaves more like mean absolute error  Returns:     GCE loss values with shape `[batch_size]`. ```  References:     - [Zhang, Sabuncu, 2018](https://arxiv.org/abs/1805.07836)       (""Generalized Cross Entropy Loss for Training         Deep Neural Networks with Noisy Labels"")', 'CLASS keras.keras.src.losses.losses.BinaryCrossentropy has field or attribute or property keras.keras.src.losses.losses.axis\nComputes the cross-entropy loss between true labels and predicted labels.  Use this cross-entropy loss for binary (0 or 1) classification applications. The loss function requires the following inputs:  - `y_true` (true label): This is either 0 or 1. - `y_pred` (predicted value): This is the model\'s prediction, i.e, a single     floating-point value which either represents a     [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]     when `from_logits=True`) or a probability (i.e, value in [0., 1.] when     `from_logits=False`).  Args:     from_logits: Whether to interpret `y_pred` as a tensor of         [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we         assume that `y_pred` is probabilities (i.e., values in [0, 1]).     label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.         When > 0, we compute the loss between the predicted labels         and a smoothed version of the true labels, where the smoothing         squeezes the labels towards 0.5. Larger values of         `label_smoothing` correspond to heavier smoothing.     axis: The axis along which to compute crossentropy (the features axis).         Defaults to `-1`.     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Examples:  **Recommended Usage:** (set `from_logits=True`)  With `compile()` API:  ```python model.compile(     loss=keras.losses.BinaryCrossentropy(from_logits=True),     ... ) ```  As a standalone function:  >>> # Example 1: (batch_size = 1, number of samples = 4) >>> y_true = np.array([0, 1, 0, 0]) >>> y_pred = np.array([-18.6, 0.51, 2.94, -12.8]) >>> bce = keras.losses.BinaryCrossentropy(from_logits=True) >>> bce(y_true, y_pred) 0.8654  >>> # Example 2: (batch_size = 2, number of samples = 4) >>> y_true = np.array([[0, 1], [0, 0]]) >>> y_pred = np.array([[-18.6, 0.51], [2.94, -12.8]]) >>> # Using default \'auto\'/\'sum_over_batch_size\' reduction type. >>> bce = keras.losses.BinaryCrossentropy(from_logits=True) >>> bce(y_true, y_pred) 0.8654 >>> # Using \'sample_weight\' attribute >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2]) 0.243 >>> # Using \'sum\' reduction` type. >>> bce = keras.losses.BinaryCrossentropy(from_logits=True, ...     reduction=""sum"") >>> bce(y_true, y_pred) 1.730 >>> # Using \'none\' reduction type. >>> bce = keras.losses.BinaryCrossentropy(from_logits=True, ...     reduction=None) >>> bce(y_true, y_pred) array([0.235, 1.496], dtype=float32)  **Default Usage:** (set `from_logits=False`)  >>> # Make the following updates to the above ""Recommended Usage"" section >>> # 1. Set `from_logits=False` >>> keras.losses.BinaryCrossentropy() # OR ...(\'from_logits=False\') >>> # 2. Update `y_pred` to use probabilities instead of logits >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]', 'FUNCTION keras.keras.src.losses.losses.categorical_generalized_cross_entropy has parameter or accepts argument keras.keras.src.losses.losses.categorical_generalized_cross_entropy.y_pred\nComputes the Generalized Cross Entropy loss.  Generalized Cross Entropy (GCE) is a noise-robust loss function that provides better robustness against noisy labels than standard cross entropy. It generalizes both cross entropy and mean absolute error through the parameter q, where values closer to 1 make the loss more robust to noisy labels.  Formula: ```python loss = (1 - p**q) / q ``` where `p` is the predicted probability for the true class and `q` is the noise parameter.  Args:     y_true: Ground truth labels. Expected to contain *integer class indices*         with shape `[batch_size]` or `[batch_size, 1]`.     y_pred: The predicted class probabilities, with shape         `[batch_size, num_classes]`.     q: Float in range `(0, 1)`. It is the noise parameter.        Controls the behavior of the loss:         - As `q` approaches 0: Behaves more like cross entropy         - As `q` approaches 1: Behaves more like mean absolute error  Returns:     GCE loss values with shape `[batch_size]`. ```  References:     - [Zhang, Sabuncu, 2018](https://arxiv.org/abs/1805.07836)       (""Generalized Cross Entropy Loss for Training         Deep Neural Networks with Noisy Labels"")', 'CLASS keras.keras.src.losses.losses.BinaryCrossentropy has field or attribute or property keras.keras.src.losses.losses.BinaryCrossentropy.axis\nComputes the cross-entropy loss between true labels and predicted labels.  Use this cross-entropy loss for binary (0 or 1) classification applications. The loss function requires the following inputs:  - `y_true` (true label): This is either 0 or 1. - `y_pred` (predicted value): This is the model\'s prediction, i.e, a single     floating-point value which either represents a     [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]     when `from_logits=True`) or a probability (i.e, value in [0., 1.] when     `from_logits=False`).  Args:     from_logits: Whether to interpret `y_pred` as a tensor of         [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we         assume that `y_pred` is probabilities (i.e., values in [0, 1]).     label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.         When > 0, we compute the loss between the predicted labels         and a smoothed version of the true labels, where the smoothing         squeezes the labels towards 0.5. Larger values of         `label_smoothing` correspond to heavier smoothing.     axis: The axis along which to compute crossentropy (the features axis).         Defaults to `-1`.     reduction: Type of reduction to apply to the loss. In almost all cases         this should be `""sum_over_batch_size""`. Supported options are         `""sum""`, `""sum_over_batch_size""`, `""mean""`,         `""mean_with_sample_weight""` or `None`. `""sum""` sums the loss,         `""sum_over_batch_size""` and `""mean""` sum the loss and divide by the         sample size, and `""mean_with_sample_weight""` sums the loss and         divides by the sum of the sample weights. `""none""` and `None`         perform no aggregation. Defaults to `""sum_over_batch_size""`.     name: Optional name for the loss instance.     dtype: The dtype of the loss\'s computations. Defaults to `None`, which         means using `keras.backend.floatx()`. `keras.backend.floatx()` is a         `""float32""` unless set to different value         (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is         provided, then the `compute_dtype` will be utilized.  Examples:  **Recommended Usage:** (set `from_logits=True`)  With `compile()` API:  ```python model.compile(     loss=keras.losses.BinaryCrossentropy(from_logits=True),     ... ) ```  As a standalone function:  >>> # Example 1: (batch_size = 1, number of samples = 4) >>> y_true = np.array([0, 1, 0, 0]) >>> y_pred = np.array([-18.6, 0.51, 2.94, -12.8]) >>> bce = keras.losses.BinaryCrossentropy(from_logits=True) >>> bce(y_true, y_pred) 0.8654  >>> # Example 2: (batch_size = 2, number of samples = 4) >>> y_true = np.array([[0, 1], [0, 0]]) >>> y_pred = np.array([[-18.6, 0.51], [2.94, -12.8]]) >>> # Using default \'auto\'/\'sum_over_batch_size\' reduction type. >>> bce = keras.losses.BinaryCrossentropy(from_logits=True) >>> bce(y_true, y_pred) 0.8654 >>> # Using \'sample_weight\' attribute >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2]) 0.243 >>> # Using \'sum\' reduction` type. >>> bce = keras.losses.BinaryCrossentropy(from_logits=True, ...     reduction=""sum"") >>> bce(y_true, y_pred) 1.730 >>> # Using \'none\' reduction type. >>> bce = keras.losses.BinaryCrossentropy(from_logits=True, ...     reduction=None) >>> bce(y_true, y_pred) array([0.235, 1.496], dtype=float32)  **Default Usage:** (set `from_logits=False`)  >>> # Make the following updates to the above ""Recommended Usage"" section >>> # 1. Set `from_logits=False` >>> keras.losses.BinaryCrossentropy() # OR ...(\'from_logits=False\') >>> # 2. Update `y_pred` to use probabilities instead of logits >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]', 'FUNCTION keras.keras.src.losses.losses.categorical_generalized_cross_entropy has parameter or accepts argument keras.keras.src.losses.losses.categorical_generalized_cross_entropy.y_true\nComputes the Generalized Cross Entropy loss.  Generalized Cross Entropy (GCE) is a noise-robust loss function that provides better robustness against noisy labels than standard cross entropy. It generalizes both cross entropy and mean absolute error through the parameter q, where values closer to 1 make the loss more robust to noisy labels.  Formula: ```python loss = (1 - p**q) / q ``` where `p` is the predicted probability for the true class and `q` is the noise parameter.  Args:     y_true: Ground truth labels. Expected to contain *integer class indices*         with shape `[batch_size]` or `[batch_size, 1]`.     y_pred: The predicted class probabilities, with shape         `[batch_size, num_classes]`.     q: Float in range `(0, 1)`. It is the noise parameter.        Controls the behavior of the loss:         - As `q` approaches 0: Behaves more like cross entropy         - As `q` approaches 1: Behaves more like mean absolute error  Returns:     GCE loss values with shape `[batch_size]`. ```  References:     - [Zhang, Sabuncu, 2018](https://arxiv.org/abs/1805.07836)       (""Generalized Cross Entropy Loss for Training         Deep Neural Networks with Noisy Labels"")', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```', 'keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `""tensorflow""`, `""jax""`, `""torch""`, `""openvino""`. Example:\n\n```\nexport KERAS_BACKEND=""jax""\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[""KERAS_BACKEND""] = ""jax""\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 21104, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",40.90502381324768,946
Which helper function does the activation getter invoke to deserialize activation configurations provided as dictionaries?,The helper function invoked by the `get()` method to deserialize activation configurations provided as dictionaries in Keras is `serialization_lib.deserialize_keras_object(identifier)`. This function is responsible for deserializing activation configs given as dictionaries.,"The `get()` helper in `keras/src/activations/__init__.py` calls  
`serialization_lib.deserialize_keras_object(identifier)`  
to deserialize activation configs given as dictionaries.","['FUNCTION keras.keras.src.activations.__init__.deserialize has parameter or accepts argument keras.keras.src.activations.__init__.deserialize.config\nReturn a Keras activation function via its config.', 'MODULE keras.keras.src.activations.__init__ contains FUNCTION keras.keras.src.activations.__init__.deserialize\nReturn a Keras activation function via its config.', 'FUNCTION keras.keras.src.activations.__init__.deserialize has parameter or accepts argument keras.keras.src.activations.__init__.deserialize.custom_objects\nReturn a Keras activation function via its config.', 'FUNCTION keras.keras.src.activations.__init__.deserialize has parameter or accepts argument keras.keras.src.activations.__init__.ALL_OBJECTS_DICT\nReturn a Keras activation function via its config.', ""FUNCTION cognee.cognee.infrastructure.databases.vector.utils.normalize_distances has parameter or accepts argument cognee.cognee.infrastructure.databases.vector.utils.normalize_distances.result_values\nNormalize distances in the provided result values.  This function takes a list of dictionaries containing distance values and normalizes these distances to a range of 0 to 1. If all distances are the same, it assigns all normalized values to 0 to avoid division by zero.  Parameters: -----------      - result_values (List[dict]): A list of dictionaries, each containing a '_distance'       key with a numeric value to be normalized.  Returns: --------      - List[float]: A list of normalized float values corresponding to the distances in       the input list."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).']","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 21117, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'deep', 'type': 'open_question'}",28.471009731292725,947
How do float8 quantized builds ensure their scale and amax history variables are updated via gradients?,"In Keras, float8 quantized builds ensure their scale and amax history variables are updated via gradients by using a combination of techniques. The model is designed to handle untrusted inputs by employing sandboxing, pre-analysis, updates, and strong isolation methods to prevent unwanted access to data from other tenants. Additionally, the `update_scale()` and `update_amax_history()` methods can be called manually during training to update these values. Keras provides a robust framework for building and training machine learning models with built-in security features to protect against common threats such as script injection and data leaks.","In both Dense._float8_build (keras/src/layers/core/dense.py) and EinsumDense._float8_build, all of the “ …_scale” and “ …_amax_history” variables are created via:  
```python
self.add_weight(
  …,
  trainable=True,
  overwrite_with_gradient=True,
  autocast=False,
)
```  
By passing overwrite_with_gradient=True (and trainable=True) into Layer.add_weight, the backend will use back-prop gradients to directly overwrite those float8 scale and amax history variables on each update.","['CLASS keras.keras.src.dtype_policies.dtype_policy.QuantizedFloat8DTypePolicy has method keras.keras.src.dtype_policies.dtype_policy.QuantizedFloat8DTypePolicy.amax_history_length\nThe length of the amax history window.  This property is used for scaling factor computation in float8 training.', 'MODULE keras.keras.src.quantizers.quantizers contains FUNCTION keras.keras.src.quantizers.quantizers.compute_float8_amax_history', 'CLASS keras.keras.src.optimizers.loss_scale_optimizer.LossScaleOptimizer has field or attribute or property keras.keras.src.optimizers.loss_scale_optimizer.dynamic_growth_steps\nAn optimizer that dynamically scales the loss to prevent underflow.  Loss scaling is a technique to prevent numeric underflow in intermediate gradients when float16 is used. To prevent underflow, the loss is multiplied (or ""scaled"") by a certain factor called the ""loss scale"", which causes intermediate gradients to be scaled by the loss scale as well. The final gradients are divided (or ""unscaled"") by the loss scale to bring them back to their original value.  `LossScaleOptimizer` wraps another optimizer and applies dynamic loss scaling to it. This loss scale is dynamically updated over time as follows: - On any train step, if a nonfinite gradient is encountered, the loss scale   is halved, and the train step is skipped. - If `dynamic_growth_steps` have occurred since the last time the loss scale   was updated, and no nonfinite gradients have occurred, the loss scale   is doubled.  Args:     inner_optimizer: The `keras.optimizers.Optimizer` instance to wrap.     initial_scale: Float. The initial loss scale. This scale will be updated         during training. It is recommended for this to be a very high         number, because a loss scale that is too high gets lowered far more         quickly than a loss scale that is too low gets raised.     dynamic_growth_steps: Int. How often to update the scale upwards. After         every `dynamic_growth_steps` steps with finite gradients, the         loss scale is doubled.     {{base_optimizer_keyword_args}}', 'MODULE keras.keras.src.quantizers.quantizers contains FUNCTION keras.keras.src.quantizers.quantizers.compute_float8_scale', 'MODULE keras.keras.src.optimizers.loss_scale_optimizer contains CLASS keras.keras.src.optimizers.loss_scale_optimizer.LossScaleOptimizer\nAn optimizer that dynamically scales the loss to prevent underflow.  Loss scaling is a technique to prevent numeric underflow in intermediate gradients when float16 is used. To prevent underflow, the loss is multiplied (or ""scaled"") by a certain factor called the ""loss scale"", which causes intermediate gradients to be scaled by the loss scale as well. The final gradients are divided (or ""unscaled"") by the loss scale to bring them back to their original value.  `LossScaleOptimizer` wraps another optimizer and applies dynamic loss scaling to it. This loss scale is dynamically updated over time as follows: - On any train step, if a nonfinite gradient is encountered, the loss scale   is halved, and the train step is skipped. - If `dynamic_growth_steps` have occurred since the last time the loss scale   was updated, and no nonfinite gradients have occurred, the loss scale   is doubled.  Args:     inner_optimizer: The `keras.optimizers.Optimizer` instance to wrap.     initial_scale: Float. The initial loss scale. This scale will be updated         during training. It is recommended for this to be a very high         number, because a loss scale that is too high gets lowered far more         quickly than a loss scale that is too low gets raised.     dynamic_growth_steps: Int. How often to update the scale upwards. After         every `dynamic_growth_steps` steps with finite gradients, the         loss scale is doubled.     {{base_optimizer_keyword_args}}', 'keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you\'ll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn\'t already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n""Setup environment"".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven\'t signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).', ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity."", ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 15}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 15, 'n_files_pr': 4, 'pr': 21248, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",42.4218385219574,948
How do the OpenVINO and Torch backends differ in applying dtype conversion when the input is a backend Variable?,"The OpenVINO and Torch backends differ in applying dtype conversion when the input is a backend Variable in keras. In the OpenVINO backend, if a new dtype is requested that does not match the current dtype, a cast is injected. In contrast, the Torch backend skips casting if `dtype=None` and always calls `.to(dtype)` when `dtype` is provided, even if `x` already has that dtype.","In keras/src/backend/openvino/core.py::convert_to_tensor, when x is a Variable you do:

• x = x.value  
• if a dtype is given and it differs from x.dtype → x = cast(x, dtype)  
• return x  

In keras/src/backend/torch/core.py::convert_to_tensor, for a Variable you do:

• if dtype is None → return x.value  
• else → x = x.value.to(to_torch_dtype(dtype)) (no check for “already that dtype”)  

So the OpenVINO backend only injects a cast when you ask for a new dtype that doesn’t match, whereas the Torch backend skips casting if dtype=None and otherwise always calls .to(dtype) (even if it’s already the same).","['MODULE keras.keras.src.backend.tensorflow.core contains FUNCTION keras.keras.src.backend.tensorflow.core.shape\nAlways return a tuple shape.  `tf.shape` will return a `tf.Tensor`, which differs from the tuple return type on the torch and jax backends. We write our own method instead which always returns a tuple, with integer values when the shape is known, and tensor values when the shape is unknown (this is tf specific, as dynamic shapes do not apply in other backends).', 'MODULE keras.keras.src.backend.openvino.core contains GLOBAL_VARIABLE keras.keras.src.backend.openvino.core.OPENVINO_DTYPES', 'FUNCTION keras.keras.src.backend.tensorflow.core.shape has parameter or accepts argument keras.keras.src.backend.tensorflow.core.shape.x\nAlways return a tuple shape.  `tf.shape` will return a `tf.Tensor`, which differs from the tuple return type on the torch and jax backends. We write our own method instead which always returns a tuple, with integer values when the shape is known, and tensor values when the shape is unknown (this is tf specific, as dynamic shapes do not apply in other backends).', 'MODULE keras.keras.src.backend.torch.core contains GLOBAL_VARIABLE keras.keras.src.backend.torch.core.TORCH_DTYPES', 'CLASS keras.keras.src.backend.common.variables.Variable has method keras.keras.src.backend.common.variables.Variable.value\nThe current value of the variable (numpy array or backend tensor).\nRepresents a backend-agnostic variable in Keras.  A `Variable` acts as a container for state. It holds a tensor value and can be updated. With the JAX backend, variables are used to implement ""functionalization"", the pattern of lifting stateful operations out of a piece of computation to turn it into a stateless function.  Args:     initializer: Initial value or callable for initialization.         If a callable is used, it should take the arguments         `shape` and `dtype`.     shape: Optional. Tuple for the variable\'s shape.         Required if `initializer` is a callable.     dtype: Optional. Data type of the variable. Defaults to the global float         dtype type (`""float32""` if never configured).     trainable: Optional. Boolean indicating if variable is trainable.         Defaults to `True`.     autocast: Optional. Boolean indicating whether the variable supports         autocasting. If `True`, the layer may first convert the variable         to the compute data type when accessed. Defaults to `True`.     aggregation: Optional string, one of `None`, `""none""`, `""mean""`,         `""sum""` or `""only_first_replica""` specifying how a distributed         variable will be aggregated. This serves as a semantic annotation,         to be taken into account by downstream backends or users. Defaults         to `""none""`.     name: Optional. A unique name for the variable. Automatically generated         if not set.  Attributes:     shape: The shape of the variable (tuple of integers).     ndim: The number of dimensions of the variable (integer).     dtype: The data type of the variable (string).     trainable: Whether the variable is trainable (boolean).     autocast: Whether the variable supports autocasting (boolean).     aggregation: How a distributed variable will be aggregated (string).     value: The current value of the variable (NumPy array or tensor).     name: The name of the variable (string).     path: The path of the variable within the Keras model or layer (string).     kwargs: Additional backend-specific keyword arguments.  Examples:  **Initializing a `Variable` with a NumPy array:**  ```python import numpy as np import keras initial_array = np.ones((3, 3)) variable_from_array = keras.Variable(initializer=initial_array) ```  **Using a Keras initializer to create a `Variable`:**  ```python from keras.src.initializers import Ones variable_from_initializer = keras.Variable(     initializer=Ones(), shape=(3, 3), dtype=""float32"" ) ```  **Updating the value of a `Variable`:**  ```python new_value = np.zeros((3, 3), dtype=""float32"") variable_from_array.assign(new_value) ```  **Marking a `Variable` as non-trainable:**  ```python non_trainable_variable = keras.Variable(     initializer=np.ones((3, 3), dtype=""float32""), trainable=False ) ```', 'keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", ""keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant’s identity.""]","{'commit': '89d953e1631b72c5b44397388d85f2a46c3f6e42', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 5, 'n_files_pr': 4, 'pr': 21452, 'repo': 'https://github.com/keras-team/keras.git', 'scope': 'broad', 'type': 'open_question'}",33.10717821121216,949
How is a string-based instruments identifier resolved into the actual list of tickers that the backtester uses to retrieve deal prices?,"The string-based instruments identifier is resolved into an actual list of tickers through the following steps: 1. `DataProvider.fetch_instruments` looks up a universe name based on the input string; 2. It returns a list of tickers associated with that universe; 3. The backtester feeds this list of tickers into `get_deal_price`, which is used to retrieve deal prices for each ticker in the list. This process allows users to specify a universe by its name as a string and have it resolved into a list of actual tickers for trading purposes.","When you pass a string (e.g. “csi300”) into your strategy, the constructor in qlib/contrib/strategy/rule_strategy.py does:

•  if isinstance(instruments, str):  
     self.instruments = D.instruments(instruments)

Here D.instruments(...) is just a thin wrapper for DataProvider.fetch_instruments (in qlib/data/provider/instrument.py), which looks up that universe name and returns the actual list of tickers that the backtester then feeds into get_deal_price.","['CLASS qlib.qlib.strategy.base.RLIntStrategy has method qlib.qlib.strategy.base.RLIntStrategy.__init__\nParameters ---------- state_interpreter : Union[dict, StateInterpreter]     interpreter that interprets the qlib execute result into rl env state action_interpreter : Union[dict, ActionInterpreter]     interpreter that interprets the rl agent action into qlib order list start_time : Union[str, pd.Timestamp], optional     start time of trading, by default None end_time : Union[str, pd.Timestamp], optional     end time of trading, by default None\n(RL)-based (Strategy) with (Int)erpreter', 'CLASS qlib.qlib.backtest.exchange.Exchange has method qlib.qlib.backtest.exchange.Exchange.__init__\n__init__ :param freq:             frequency of data :param start_time:       closed start time for backtest :param end_time:         closed end time for backtest :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50) :param deal_price:      Union[str, Tuple[str, str], List[str]]                         The `deal_price` supports following two types of input                         - <deal_price> : str                         - (<buy_price>, <sell_price>): Tuple[str] or List[str]                         <deal_price>, <buy_price> or <sell_price> := <price>                         <price> := str                         - for example \'$close\', \'$open\', \'$vwap\' (""close"" is OK. `Exchange` will help to prepend                           ""$"" to the expression) :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.                          It is useful when users want more fields to be queried :param limit_threshold: Union[Tuple[str, str], float, None]                         1) `None`: no limitation                         2) float, 0.1 for example, default None                         3) Tuple[str, str]: (<the expression for buying stock limitation>,                                              <the expression for sell stock limitation>)                                             `False` value indicates the stock is tradable                                             `True` value indicates the stock is limited and not tradable :param volume_threshold: Union[                             Dict[                                 ""all"": (""cum"" or ""current"", limit_str),                                 ""buy"": (""cum"" or ""current"", limit_str),                                 ""sell"":(""cum"" or ""current"", limit_str),                             ],                             (""cum"" or ""current"", limit_str),                          ]                         1) (""cum"" or ""current"", limit_str) denotes a single volume limit.                             - limit_str is qlib data expression which is allowed to define your own Operator.                             Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for                             high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom                             operator, you need to register it in qlib_init.                             - ""cum"" means that this is a cumulative value over time, such as cumulative market                             volume. So when it is used as a volume limit, it is necessary to subtract the dealt                             amount.                             - ""current"" means that this is a real-time value and will not accumulate over time,                             so it can be directly used as a capacity limit.                             e.g. (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""), (""current"", ""$bidV1"")                         2) ""all"" means the volume limits are both buying and selling.                         ""buy"" means the volume limits of buying. ""sell"" means the volume limits of selling.                         Different volume limits will be aggregated with min(). If volume_threshold is only                         (""cum"" or ""current"", limit_str) instead of a dict, the volume limits are for                         both by default. In other words, it is same as {""all"": (""cum"" or ""current"", limit_str)}.                         3) e.g. ""volume_threshold"": {                                     ""all"": (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""),                                     ""buy"": (""current"", ""$askV1""),                                     ""sell"": (""current"", ""$bidV1""),                                 } :param open_cost:        cost rate for open, default 0.0015 :param close_cost:       cost rate for close, default 0.0025 :param trade_unit:       trade unit, 100 for China A market.                          None for disable trade unit.                          **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must                          distinguish `not set` and `disable trade_unit` :param min_cost:         min cost, default 5 :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1. :param extra_quote:     pandas, dataframe consists of                             columns: like [\'$vwap\', \'$close\', \'$volume\', \'$factor\', \'limit_sell\', \'limit_buy\'].                                     The limit indicates that the etf is tradable on a specific day.                                     Necessary fields:                                         $close is for calculating the total value at end of each day.                                     Optional fields:                                         $volume is only necessary when we limit the trade amount or calculate                                         PA(vwap) indicator                                         $vwap is only necessary when we use the $vwap price as the deal price                                         $factor is for rounding to the trading unit                                         limit_sell will be set to False by default (False indicates we can sell                                         this target on this day).                                         limit_buy will be set to False by default (False indicates we can buy                                         this target on this day).                             index: MultipleIndex(instrument, pd.Datetime)', 'CLASS qlib.qlib.data.data.InstrumentProvider has method qlib.qlib.data.data.InstrumentProvider.list_instruments\nList the instruments based on a certain stockpool config.  Parameters ---------- instruments : dict     stockpool config. start_time : str     start of the time range. end_time : str     end of the time range. as_list : bool     return instruments as list or dict.  Returns ------- dict or list     instruments list or dictionary with time spans\nInstrument provider base class  Provide instrument data.', 'METHOD qlib.qlib.backtest.exchange.Exchange.__init__ has parameter or accepts argument qlib.qlib.backtest.exchange.Exchange.__init__.deal_price\n__init__ :param freq:             frequency of data :param start_time:       closed start time for backtest :param end_time:         closed end time for backtest :param codes:            list stock_id list or a string of instruments(i.e. all, csi500, sse50) :param deal_price:      Union[str, Tuple[str, str], List[str]]                         The `deal_price` supports following two types of input                         - <deal_price> : str                         - (<buy_price>, <sell_price>): Tuple[str] or List[str]                         <deal_price>, <buy_price> or <sell_price> := <price>                         <price> := str                         - for example \'$close\', \'$open\', \'$vwap\' (""close"" is OK. `Exchange` will help to prepend                           ""$"" to the expression) :param subscribe_fields: list, subscribe fields. This expressions will be added to the query and `self.quote`.                          It is useful when users want more fields to be queried :param limit_threshold: Union[Tuple[str, str], float, None]                         1) `None`: no limitation                         2) float, 0.1 for example, default None                         3) Tuple[str, str]: (<the expression for buying stock limitation>,                                              <the expression for sell stock limitation>)                                             `False` value indicates the stock is tradable                                             `True` value indicates the stock is limited and not tradable :param volume_threshold: Union[                             Dict[                                 ""all"": (""cum"" or ""current"", limit_str),                                 ""buy"": (""cum"" or ""current"", limit_str),                                 ""sell"":(""cum"" or ""current"", limit_str),                             ],                             (""cum"" or ""current"", limit_str),                          ]                         1) (""cum"" or ""current"", limit_str) denotes a single volume limit.                             - limit_str is qlib data expression which is allowed to define your own Operator.                             Please refer to qlib/contrib/ops/high_freq.py, here are any custom operator for                             high frequency, such as DayCumsum. !!!NOTE: if you want you use the custom                             operator, you need to register it in qlib_init.                             - ""cum"" means that this is a cumulative value over time, such as cumulative market                             volume. So when it is used as a volume limit, it is necessary to subtract the dealt                             amount.                             - ""current"" means that this is a real-time value and will not accumulate over time,                             so it can be directly used as a capacity limit.                             e.g. (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""), (""current"", ""$bidV1"")                         2) ""all"" means the volume limits are both buying and selling.                         ""buy"" means the volume limits of buying. ""sell"" means the volume limits of selling.                         Different volume limits will be aggregated with min(). If volume_threshold is only                         (""cum"" or ""current"", limit_str) instead of a dict, the volume limits are for                         both by default. In other words, it is same as {""all"": (""cum"" or ""current"", limit_str)}.                         3) e.g. ""volume_threshold"": {                                     ""all"": (""cum"", ""0.2 * DayCumsum($volume, \'9:45\', \'14:45\')""),                                     ""buy"": (""current"", ""$askV1""),                                     ""sell"": (""current"", ""$bidV1""),                                 } :param open_cost:        cost rate for open, default 0.0015 :param close_cost:       cost rate for close, default 0.0025 :param trade_unit:       trade unit, 100 for China A market.                          None for disable trade unit.                          **NOTE**: `trade_unit` is included in the `kwargs`. It is necessary because we must                          distinguish `not set` and `disable trade_unit` :param min_cost:         min cost, default 5 :param impact_cost:     market impact cost rate (a.k.a. slippage). A recommended value is 0.1. :param extra_quote:     pandas, dataframe consists of                             columns: like [\'$vwap\', \'$close\', \'$volume\', \'$factor\', \'limit_sell\', \'limit_buy\'].                                     The limit indicates that the etf is tradable on a specific day.                                     Necessary fields:                                         $close is for calculating the total value at end of each day.                                     Optional fields:                                         $volume is only necessary when we limit the trade amount or calculate                                         PA(vwap) indicator                                         $vwap is only necessary when we use the $vwap price as the deal price                                         $factor is for rounding to the trading unit                                         limit_sell will be set to False by default (False indicates we can sell                                         this target on this day).                                         limit_buy will be set to False by default (False indicates we can buy                                         this target on this day).                             index: MultipleIndex(instrument, pd.Datetime)', 'MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1677, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",35.16210436820984,1074
How are rows past the configured end date removed during the normalization step?,"The rows past the configured end date are removed during the normalization step in qlib by creating a boolean mask where the datetime-converted column is less than or equal to the end date, and then filtering the DataFrame using this mask.

Here's how it works:
1.  The code checks if `self._end_date` is not None.
2.  It converts the DataFrame column named by `self._date_field_name` to datetime using `pd.to_datetime`.
3.  It converts `self._end_date` to a pandas Timestamp with `pd.Timestamp`.
4.  It creates a boolean mask by comparing the datetime-converted column to the end date using the ≤ operator.
5.  The DataFrame is filtered by assigning `df = df[mask]`.

This process ensures that only rows where the date in the specified column is less than or equal to the configured end date are kept, and any rows with dates beyond this point are removed.","In Normalize._executor (scripts/data_collector/base.py) we simply drop any rows whose date exceeds the configured end date:

```python
if self._end_date is not None:
    mask = pd.to_datetime(df[self._date_field_name]) <= pd.Timestamp(self._end_date)
    df = df[mask]
```

This boolean mask keeps only rows with `date_field ≤ end_date`.","['CLASS qlib.scripts.data_collector.future_calendar_collector.CollectorFutureCalendar has method qlib.scripts.data_collector.future_calendar_collector.CollectorFutureCalendar.__init__\nParameters ---------- qlib_dir:     qlib data directory start_date     start date end_date     end date', 'METHOD qlib.scripts.data_collector.base.BaseCollector.normalize_end_datetime has parameter or accepts argument qlib.scripts.data_collector.base.BaseCollector.normalize_end_datetime.end_datetime', 'METHOD qlib.scripts.data_collector.future_calendar_collector.CollectorFutureCalendar.__init__ has parameter or accepts argument qlib.scripts.data_collector.future_calendar_collector.CollectorFutureCalendar.__init__.qlib_dir\nParameters ---------- qlib_dir:     qlib data directory start_date     start date end_date     end date', 'CLASS qlib.scripts.data_collector.base.BaseCollector has method qlib.scripts.data_collector.base.BaseCollector.normalize_end_datetime', 'METHOD qlib.scripts.data_collector.future_calendar_collector.CollectorFutureCalendar.__init__ has parameter or accepts argument qlib.scripts.data_collector.future_calendar_collector.CollectorFutureCalendar.__init__.start_date\nParameters ---------- qlib_dir:     qlib data directory start_date     start date end_date     end date', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1698, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",39.87530970573425,1075
What exception is raised if the input fields list is empty?,"The exception raised if the input fields list is empty in qlib is ValueError with the message ""fields cannot be empty"".","In qlib/data/data.py’s DatasetProvider.get_column_names, an empty fields list triggers a ValueError (“fields cannot be empty”).","['CLASS graphiti.graphiti_core.llm_client.errors.EmptyResponseError inherits from or is a subclass of graphiti.graphiti_core.llm_client.errors.Exception\nException raised when the LLM returns an empty response.', 'CLASS qlib.scripts.check_dump_bin.CheckBin has field or attribute or property qlib.scripts.check_dump_bin.CheckBin.qlib_fields', ""METHOD cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names has parameter or accepts argument cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_property_names.data_point\nRetrieve the names of embeddable properties defined in the metadata.  If no index fields are defined in the metadata, this method will return an empty list.  Parameters: -----------      - data_point ('DataPoint'): The DataPoint instance from which to retrieve embeddable       property names.  Returns: --------      A list of property names corresponding to the index fields, or an empty list if none     exist."", 'CLASS qlib.scripts.check_dump_bin.CheckBin has field or attribute or property qlib.scripts.check_dump_bin.CheckBin.check_fields', ""METHOD cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_properties has parameter or accepts argument cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.get_embeddable_properties.data_point\nRetrieve a list of embeddable properties from the data point.  This method returns a list of attribute values based on the index fields defined in the data point's metadata. If there are no index fields, it returns an empty list.  Parameters: -----------      - data_point ('DataPoint'): The DataPoint instance from which to retrieve embeddable       properties.  Returns: --------      A list of embeddable property values, or an empty list if none exist."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1732, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",23.74870991706848,1076
How does the data existence check avoid treating the ticker 'NA' as missing when loading the instruments list?,"The data existence check in qlib avoids treating the ticker 'NA' as missing when loading the instruments list by explicitly omitting “NA” and “NULL” from the `na_values` list, preventing pandas from treating the ticker ""NA"" as missing, which would otherwise cause issues with the data.","In exists_qlib_data (qlib/utils/__init__.py) we load instruments/all.txt with:

• keep_default_na=False  
• a custom na_values list for column 0 that explicitly omits “NA” (and “NULL”)

By dropping “NA” from the na_values list, pandas won’t treat the ticker “NA” as missing.","['MODULE xgboost.python-package.xgboost.testing.updater contains FUNCTION xgboost.python-package.xgboost.testing.updater.check_categorical_missing\nCheck categorical data with missing values.', 'METHOD qlib.qlib.data.data.ClientInstrumentProvider.list_instruments has parameter or accepts argument qlib.qlib.data.data.ClientInstrumentProvider.list_instruments.as_list', 'CLASS qlib.qlib.data.data.InstrumentProvider has method qlib.qlib.data.data.InstrumentProvider.list_instruments\nList the instruments based on a certain stockpool config.  Parameters ---------- instruments : dict     stockpool config. start_time : str     start of the time range. end_time : str     end of the time range. as_list : bool     return instruments as list or dict.  Returns ------- dict or list     instruments list or dictionary with time spans\nInstrument provider base class  Provide instrument data.', 'METHOD qlib.qlib.data.data.ClientInstrumentProvider.list_instruments has parameter or accepts argument qlib.qlib.data.data.ClientInstrumentProvider.list_instruments.instruments', 'FUNCTION xgboost.python-package.xgboost.testing.updater.check_categorical_missing has parameter or accepts argument xgboost.python-package.xgboost.testing.updater.USE_PART\nCheck categorical data with missing values.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = ""~/.qlib/qlib_data/cn_data""  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments(\'csi500\')\n  print(D.list_instruments(instruments=instruments, start_time=\'2010-01-01\', end_time=\'2017-12-31\', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = [\'SH600000\']\n  fields = [\'$close\', \'$volume\', \'Ref($close, 1)\', \'Mean($close, 3)\', \'$high-$low\']\n  print(D.features(instruments, fields, start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 2, 'pr': 1736, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.93066668510437,1077
"When the level name isn’t found among the DataFrame’s index names, how does the function determine the default level index?","When the level name isn't found among the DataFrame’s index names, `get_level_index` in qlib falls back to the hard-coded tuple (`","In get_level_index (qlib/data/dataset/utils.py), if your level name isn’t in df.index.names, it falls back to the hard-coded tuple (""datetime"",""instrument"") and returns (""datetime"",""instrument"").index(level).","['FUNCTION qlib.qlib.data.dataset.utils.get_level_index has parameter or accepts argument qlib.qlib.data.dataset.utils.get_level_index.level\nget the level index of `df` given `level`  Parameters ---------- df : pd.DataFrame     data level : Union[str, int]     index level  Returns ------- int:     The level index in the multiple index', 'MODULE qlib.qlib.data.dataset.utils contains FUNCTION qlib.qlib.data.dataset.utils.get_level_index\nget the level index of `df` given `level`  Parameters ---------- df : pd.DataFrame     data level : Union[str, int]     index level  Returns ------- int:     The level index in the multiple index', 'FUNCTION qlib.qlib.data.dataset.utils.get_level_index has parameter or accepts argument qlib.qlib.data.dataset.utils.get_level_index.df\nget the level index of `df` given `level`  Parameters ---------- df : pd.DataFrame     data level : Union[str, int]     index level  Returns ------- int:     The level index in the multiple index', 'MODULE qlib.qlib.backtest.utils contains FUNCTION qlib.qlib.backtest.utils.get_start_end_idx\nA helper function for getting the decision-level index range limitation for inner strategy - NOTE: this function is not applicable to order-level  Parameters ---------- trade_calendar : TradeCalendarManager outer_trade_decision : BaseTradeDecision     the trade decision made by outer strategy  Returns ------- Union[int, int]:     start index and end index', 'MODULE qlib.qlib.utils.__init__ contains FUNCTION qlib.qlib.utils.__init__.lazy_sort_index\nmake the df index sorted  df.sort_index() will take a lot of time even when `df.is_lexsorted() == True` This function could avoid such case  Parameters ---------- df : pd.DataFrame  Returns ------- pd.DataFrame:     sorted dataframe', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1751, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",28.747059106826782,1078
Under what condition does the stock symbol fetcher raise a ValueError indicating an incomplete list?,The stock symbol fetcher raises a ValueError indicating an incomplete list when the number of fetched symbols is less than 3900.,"In scripts/data_collector/utils.py (get_hs_stock_symbols._get_symbol), once all pages are fetched it does:

```python
if len(_symbols) < 3900:
    raise ValueError(""The complete list of stocks is not available."")
```

So it raises that ValueError whenever fewer than 3900 symbols were collected.","[""CLASS qlib.qlib.backtest.decision.BaseTradeDecision has method qlib.qlib.backtest.decision.BaseTradeDecision.get_decision\nget the **concrete decision**  (e.g. execution orders) This will be called by the inner strategy  Returns ------- List[DecisionType:     The decision result. Typically it is some orders     Example:         []:             Decision not available         [concrete_decision]:             available\nTrade decisions are made by strategy and executed by executor  Motivation:     Here are several typical scenarios for `BaseTradeDecision`      Case 1:     1. Outer strategy makes a decision. The decision is not available at the start of current interval     2. After a period of time, the decision are updated and become available     3. The inner strategy try to get the decision and start to execute the decision according to `get_range_limit`     Case 2:     1. The outer strategy's decision is available at the start of the interval     2. Same as `case 1.3`"", 'MODULE qlib.qlib.contrib.evaluate_portfolio contains FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_position_list_value', 'CLASS qlib.qlib.backtest.account.Account has method qlib.qlib.backtest.account.Account.__init__\nthe trade account of backtest.  Parameters ---------- init_cash : float, optional     initial cash, by default 1e9 position_dict : Dict[                     stock_id,                     Union[                         int,  # it is equal to {""amount"": int}                         {""amount"": int, ""price""(optional): float},                     ]                 ]     initial stocks with parameters amount and price,     if there is no price key in the dict of stocks, it will be filled by _fill_stock_value.     by default {}.\nThe correctness of the metrics of Account in nested execution depends on the shallow copy of `trade_account` in qlib/backtest/executor.py:NestedExecutor Different level of executor has different Account object when calculating metrics. But the position object is shared cross all the Account object.', 'MODULE qlib.examples.orderbook_data.create_dataset contains GLOBAL_VARIABLE qlib.examples.orderbook_data.create_dataset.ALL_STOCK_PATH', 'FUNCTION qlib.qlib.backtest.profit_attribution.get_daily_bin_group has parameter or accepts argument qlib.qlib.backtest.profit_attribution.get_daily_bin_group.bench_values\nget_daily_bin_group Group the values of the stocks of benchmark into several bins in a day. Put the stocks into these bins.  :param bench_values: A series contains the value of stocks in benchmark.                      The index is the stock code. :param stock_values: A series contains the value of stocks of your portofolio                      The index is the stock code. :param group_n:      Bins will be produced  :return:             A series with the same size and index as the stock_value.                      The value in the series is the group id of the bins.                      The No.1 bin contains the biggest values.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = ""~/.qlib/qlib_data/cn_data""  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments(\'csi500\')\n  print(D.list_instruments(instruments=instruments, start_time=\'2010-01-01\', end_time=\'2017-12-31\', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = [\'SH600000\']\n  fields = [\'$close\', \'$volume\', \'Ref($close, 1)\', \'Mean($close, 3)\', \'$high-$low\']\n  print(D.features(instruments, fields, start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 3, 'pr': 1758, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",26.003703832626343,1079
What message does the client log when it fails to connect to the server WebSocket?,The client logs the message “Cannot connect to server – check your network or server status.” when it fails to connect to the server WebSocket in qlib.,"In `Client.connect_server` (qlib/data/client.py), on a `socketio.exceptions.ConnectionError` it logs:  
“Cannot connect to server – check your network or server status”","['CLASS qlib.qlib.data.client.Client has method qlib.qlib.data.client.Client.connect_server\nConnect to server.\nA client class  Provide the connection tool functions for ClientProvider.', 'CLASS xgboost.demo.nvflare.horizontal.custom.trainer.XGBoostTrainer has method xgboost.demo.nvflare.horizontal.custom.trainer.XGBoostTrainer.__init__\nTrainer for federated XGBoost.  Args:     server_address: address for the gRPC server to connect to.     world_size: the number of sites.     server_cert_path: the path to the server certificate file.     client_key_path: the path to the client key file.     client_cert_path: the path to the client certificate file.', 'CLASS qlib.qlib.data.client.Client has method qlib.qlib.data.client.Client.disconnect\nDisconnect from server.\nA client class  Provide the connection tool functions for ClientProvider.', 'CLASS xgboost.demo.nvflare.vertical.custom.trainer.XGBoostTrainer has method xgboost.demo.nvflare.vertical.custom.trainer.XGBoostTrainer.__init__\nTrainer for federated XGBoost.  Args:     server_address: address for the gRPC server to connect to.     world_size: the number of sites.     server_cert_path: the path to the server certificate file.     client_key_path: the path to the client key file.     client_cert_path: the path to the client certificate file.', 'MODULE qlib.qlib.rl.order_execution.network contains GLOBAL_VARIABLE qlib.qlib.rl.order_execution.network.__all__', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What\'s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=""https://github.com/microsoft/RD-Agent""><img src=""docs/_static/img/rdagent_logo.png"" alt=""RD_Agent"" style=""height: 2em""></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**📢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star🌟!\n\nTo learn more, please visit our [♾️Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (中文) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- 📃**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- 👾**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | 📈Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = ""~/.qlib/qlib_data/cn_data""  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments(\'csi500\')\n  print(D.list_instruments(instruments=instruments, start_time=\'2010-01-01\', end_time=\'2017-12-31\', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = [\'SH600000\']\n  fields = [\'$close\', \'$volume\', \'Ref($close, 1)\', \'Mean($close, 3)\', \'$high-$low\']\n  print(D.features(instruments, fields, start_time=\'2010-01-01\', end_time=\'2017-12-31\', freq=\'day\').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1767, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.79202675819397,1080
Which column described in the docstring is not included in the returned DataFrame?,"The column described in the docstring that is not included in the returned DataFrame of CSI500Index.get_history_companies is the ""type"" column.","The docstring lists a “type” column, but CSI500Index.get_history_companies (scripts/data_collector/cn_index/collector.py) only returns “date” and “symbol”, so the “type” column is missing.","[""CLASS qlib.qlib.backtest.report.PortfolioMetrics has field or attribute or property qlib.qlib.backtest.report.PortfolioMetrics.turnovers\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'CLASS qlib.scripts.data_collector.index.IndexBase has field or attribute or property qlib.scripts.data_collector.index.history_companies', ""CLASS qlib.qlib.backtest.report.PortfolioMetrics has field or attribute or property qlib.qlib.backtest.report.PortfolioMetrics.total_turnovers\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'CLASS qlib.scripts.data_collector.us_index.collector.NASDAQ100Index has method qlib.scripts.data_collector.us_index.collector.NASDAQ100Index.get_history_companies', ""CLASS qlib.qlib.backtest.report.PortfolioMetrics has field or attribute or property qlib.qlib.backtest.report.PortfolioMetrics.accounts\nMotivation:     PortfolioMetrics is for supporting portfolio related metrics.  Implementation:      daily portfolio metrics of the account     contain those followings: return, cost, turnover, account, cash, bench, value     For each step(bar/day/minute), each column represents     - return: the return of the portfolio generated by strategy **without transaction fee**.     - cost: the transaction fee and slippage.     - account: the total value of assets(cash and securities are both included) in user account based on the close price of each step.     - cash: the amount of cash in user's account.     - bench: the return of the benchmark     - value: the total value of securities/stocks/instruments (cash is excluded).      update report"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md\n# iBOVESPA History Companies Collection\n\n## Requirements\n\n- Install the libs from the file `requirements.txt`\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n- `requirements.txt` file was generated using python3.8\n\n## For the ibovespa (IBOV) index, we have:\n\n<hr/>\n\n### Method `get_new_companies`\n\n#### <b>Index start date</b>\n\n- The ibovespa index started on 2 January 1968 ([wiki](https://en.wikipedia.org/wiki/%C3%8Dndice_Bovespa)).  In order to use this start date in our `bench_start_date(self)` method, two conditions must be satisfied:\n    1) APIs used to download brazilian stocks (B3) historical prices must keep track of such historic data since 2 January 1968\n\n    2) Some website or API must provide, from that date, the historic index composition. In other words, the companies used to build the index .\n\n    As a consequence, the method `bench_start_date(self)` inside `collector.py` was implemented using `pd.Timestamp(""2003-01-03"")` due to two reasons\n\n    1) The earliest ibov composition that have been found was from the first quarter of 2003. More informations about such composition can be seen on the sections below.\n\n    2) Yahoo finance, one of the libraries used to download symbols historic prices, keeps track from this date forward.\n\n- Within the `get_new_companies` method, a logic was implemented to get, for each ibovespa component stock, the start date that yahoo finance keeps track of.\n\n#### <b>Code Logic</b>\n\nThe code does a web scrapping into the B3\'s [website](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br), which keeps track of the ibovespa stocks composition on the current day. \n\nOther approaches, such as `request` and `Beautiful Soup` could have been used. However, the website shows the table with the stocks with some delay, since it uses a script inside of it to obtain such compositions.\nAlternatively, `selenium` was used to download this stocks\' composition in order to overcome this problem.\n\nFuthermore, the data downloaded from the selenium script  was preprocessed so it could be saved into the `csv` format stablished by `scripts/data_collector/index.py`.\n\n<hr/>\n\n### Method `get_changes` \n\nNo suitable data source that keeps track of ibovespa\'s history stocks composition has been found. Except from this [repository](https://github.com/igor17400/IBOV-HCI) which provide such information have been used, however it only provides the data from the 1st quarter of 2003 to 3rd quarter of 2021.\n\nWith that reference, the index\'s composition can be compared quarter by quarter and year by year and then generate a file that keeps track of which stocks have been removed and which have been added each quarter and year.\n\n<hr/>\n\n### Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method save_new_companies\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1776, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",27.547892093658447,1081
What steps are performed from constructing the download URL to extracting files and removing old data when fetching a ZIP archive?,"The steps performed from constructing the download URL to extracting files and removing old data when fetching a ZIP archive in qlib are as follows:

1. `merge_remote_url(file_name)` builds the HTTP URL by prepending 'v0/' to `file_name` if it contains no '/'.
2. The constructed URL is passed to `download(url, target_path)`, which streams the ZIP into a timestamped file under `target_dir`.
3. `_unzip(target_path, target_dir, delete_old=True)` logs a warning and calls `_delete_qlib_data(target_dir)`.
4. `_delete_qlib_data(target_dir)` lists existing subdirectories named features, instruments, calendars, features_cache, and dataset_cache, and prompts the user to confirm deletion of those subdirectories.
5. If confirmed, `_delete_qlib_data(target_dir)` deletes the listed subdirectories using `rmtree`.
6. `_unzip(target_path, target_dir, delete_old=True)` opens the ZIP file with `zipfile.ZipFile` and extracts each entry into `target_dir` with `tqdm` progress.
7. If `delete_zip_file=True` on the GetData instance, the downloaded ZIP file is deleted (unlinked).","When you call GetData.download_data(file_name, target_dir, delete_old):

1. merge_remote_url(file_name) builds the HTTP URL (prepends “v0/” if no “/” in file_name).  
2. download(url, target_path) streams the ZIP into a timestamped file under target_dir.  
3. _unzip(target_path, target_dir, delete_old=True) does:  
   a. logs a warning and calls _delete_qlib_data(target_dir), which lists any existing sub-dirs (features, instruments, calendars, features_cache, dataset_cache), prompts you to confirm, then rmtree’s them.  
   b. opens the ZIP with zipfile.ZipFile and, for each entry, extracts it into target_dir (with tqdm progress).  
4. If delete_zip_file=True on the GetData instance, it then unlinks (deletes) the downloaded ZIP.","['FUNCTION keras.keras.src.utils.file_utils.get_file has parameter or accepts argument keras.keras.src.utils.file_utils.get_file.fname\nDownloads a file from a URL if it not already in the cache.  By default the file at the url `origin` is downloaded to the cache_dir `~/.keras`, placed in the cache_subdir `datasets`, and given the filename `fname`. The final location of a file `example.txt` would therefore be `~/.keras/datasets/example.txt`. Files in `.tar`, `.tar.gz`, `.tar.bz`, and `.zip` formats can also be extracted.  Passing a hash will verify the file after download. The command line programs `shasum` and `sha256sum` can compute the hash.  Example:  ```python path_to_downloaded_file = get_file(     origin=""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",     extract=True, ) ```  Args:     fname: If the target is a single file, this is your desired         local name for the file.         If `None`, the name of the file at `origin` will be used.         If downloading and extracting a directory archive,         the provided `fname` will be used as extraction directory         name (only if it doesn\'t have an extension).     origin: Original URL of the file.     untar: Deprecated in favor of `extract` argument.         Boolean, whether the file is a tar archive that should         be extracted.     md5_hash: Deprecated in favor of `file_hash` argument.         md5 hash of the file for file integrity verification.     file_hash: The expected hash string of the file after download.         The sha256 and md5 hash algorithms are both supported.     cache_subdir: Subdirectory under the Keras cache dir where the file is         saved. If an absolute path, e.g. `""/path/to/folder""` is         specified, the file will be saved at that location.     hash_algorithm: Select the hash algorithm to verify the file.         options are `""md5\'`, `""sha256\'`, and `""auto\'`.         The default \'auto\' detects the hash algorithm in use.     extract: If `True`, extracts the archive. Only applicable to compressed         archive files like tar or zip.     archive_format: Archive format to try for extracting the file.         Options are `""auto\'`, `""tar\'`, `""zip\'`, and `None`.         `""tar""` includes tar, tar.gz, and tar.bz files.         The default `""auto""` corresponds to `[""tar"", ""zip""]`.         None or an empty list will return no matches found.     cache_dir: Location to store cached files, when None it         defaults ether `$KERAS_HOME` if the `KERAS_HOME` environment         variable is set or `~/.keras/`.     force_download: If `True`, the file will always be re-downloaded         regardless of the cache state.  Returns:     Path to the downloaded file.  **⚠️ Warning on malicious downloads ⚠️**  Downloading something from the Internet carries a risk. NEVER download a file/archive if you do not trust the source. We recommend that you specify the `file_hash` argument (if the hash of the source file is known) to make sure that the file you are getting is the one you expect.', 'MODULE qlib.qlib.utils.file contains FUNCTION qlib.qlib.utils.file.unpack_archive_with_buffer\nUnpack archive with archive buffer After the call is finished, the archive file and directory will be deleted.  Implementation process:     1. create \'tempfile\' in \'~/tmp/\' and directory     2. \'buffer\' write to \'tempfile\'     3. unpack archive file(\'tempfile\')     4. user does something with file_path(\'tempfile/\')     5. remove \'tempfile\' and \'tempfile directory\'  :param buffer: bytes :param format: archive format: one of ""zip"", ""tar"", ""gztar"", ""bztar"", or ""xztar"" :return: unpack archive directory path  Usage::      >>> # The following code is to print all the file names in \'test_unpack.tar.gz\'     >>> with open(\'test_unpack.tar.gz\') as fp:     ...     buffer = fp.read()     ...     >>> with unpack_archive_with_buffer(buffer) as temp_dir:     ...     for f_n in os.listdir(temp_dir):     ...         print(f_n)     ...', 'FUNCTION keras.keras.src.utils.file_utils.get_file has parameter or accepts argument keras.keras.src.utils.file_utils.get_file.extract\nDownloads a file from a URL if it not already in the cache.  By default the file at the url `origin` is downloaded to the cache_dir `~/.keras`, placed in the cache_subdir `datasets`, and given the filename `fname`. The final location of a file `example.txt` would therefore be `~/.keras/datasets/example.txt`. Files in `.tar`, `.tar.gz`, `.tar.bz`, and `.zip` formats can also be extracted.  Passing a hash will verify the file after download. The command line programs `shasum` and `sha256sum` can compute the hash.  Example:  ```python path_to_downloaded_file = get_file(     origin=""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",     extract=True, ) ```  Args:     fname: If the target is a single file, this is your desired         local name for the file.         If `None`, the name of the file at `origin` will be used.         If downloading and extracting a directory archive,         the provided `fname` will be used as extraction directory         name (only if it doesn\'t have an extension).     origin: Original URL of the file.     untar: Deprecated in favor of `extract` argument.         Boolean, whether the file is a tar archive that should         be extracted.     md5_hash: Deprecated in favor of `file_hash` argument.         md5 hash of the file for file integrity verification.     file_hash: The expected hash string of the file after download.         The sha256 and md5 hash algorithms are both supported.     cache_subdir: Subdirectory under the Keras cache dir where the file is         saved. If an absolute path, e.g. `""/path/to/folder""` is         specified, the file will be saved at that location.     hash_algorithm: Select the hash algorithm to verify the file.         options are `""md5\'`, `""sha256\'`, and `""auto\'`.         The default \'auto\' detects the hash algorithm in use.     extract: If `True`, extracts the archive. Only applicable to compressed         archive files like tar or zip.     archive_format: Archive format to try for extracting the file.         Options are `""auto\'`, `""tar\'`, `""zip\'`, and `None`.         `""tar""` includes tar, tar.gz, and tar.bz files.         The default `""auto""` corresponds to `[""tar"", ""zip""]`.         None or an empty list will return no matches found.     cache_dir: Location to store cached files, when None it         defaults ether `$KERAS_HOME` if the `KERAS_HOME` environment         variable is set or `~/.keras/`.     force_download: If `True`, the file will always be re-downloaded         regardless of the cache state.  Returns:     Path to the downloaded file.  **⚠️ Warning on malicious downloads ⚠️**  Downloading something from the Internet carries a risk. NEVER download a file/archive if you do not trust the source. We recommend that you specify the `file_hash` argument (if the hash of the source file is known) to make sure that the file you are getting is the one you expect.', 'FUNCTION qlib.qlib.utils.file.unpack_archive_with_buffer has parameter or accepts argument qlib.qlib.utils.file.log\nUnpack archive with archive buffer After the call is finished, the archive file and directory will be deleted.  Implementation process:     1. create \'tempfile\' in \'~/tmp/\' and directory     2. \'buffer\' write to \'tempfile\'     3. unpack archive file(\'tempfile\')     4. user does something with file_path(\'tempfile/\')     5. remove \'tempfile\' and \'tempfile directory\'  :param buffer: bytes :param format: archive format: one of ""zip"", ""tar"", ""gztar"", ""bztar"", or ""xztar"" :return: unpack archive directory path  Usage::      >>> # The following code is to print all the file names in \'test_unpack.tar.gz\'     >>> with open(\'test_unpack.tar.gz\') as fp:     ...     buffer = fp.read()     ...     >>> with unpack_archive_with_buffer(buffer) as temp_dir:     ...     for f_n in os.listdir(temp_dir):     ...         print(f_n)     ...', 'FUNCTION keras.keras.src.utils.file_utils.get_file has parameter or accepts argument keras.keras.src.utils.file_utils.get_file.file_hash\nDownloads a file from a URL if it not already in the cache.  By default the file at the url `origin` is downloaded to the cache_dir `~/.keras`, placed in the cache_subdir `datasets`, and given the filename `fname`. The final location of a file `example.txt` would therefore be `~/.keras/datasets/example.txt`. Files in `.tar`, `.tar.gz`, `.tar.bz`, and `.zip` formats can also be extracted.  Passing a hash will verify the file after download. The command line programs `shasum` and `sha256sum` can compute the hash.  Example:  ```python path_to_downloaded_file = get_file(     origin=""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",     extract=True, ) ```  Args:     fname: If the target is a single file, this is your desired         local name for the file.         If `None`, the name of the file at `origin` will be used.         If downloading and extracting a directory archive,         the provided `fname` will be used as extraction directory         name (only if it doesn\'t have an extension).     origin: Original URL of the file.     untar: Deprecated in favor of `extract` argument.         Boolean, whether the file is a tar archive that should         be extracted.     md5_hash: Deprecated in favor of `file_hash` argument.         md5 hash of the file for file integrity verification.     file_hash: The expected hash string of the file after download.         The sha256 and md5 hash algorithms are both supported.     cache_subdir: Subdirectory under the Keras cache dir where the file is         saved. If an absolute path, e.g. `""/path/to/folder""` is         specified, the file will be saved at that location.     hash_algorithm: Select the hash algorithm to verify the file.         options are `""md5\'`, `""sha256\'`, and `""auto\'`.         The default \'auto\' detects the hash algorithm in use.     extract: If `True`, extracts the archive. Only applicable to compressed         archive files like tar or zip.     archive_format: Archive format to try for extracting the file.         Options are `""auto\'`, `""tar\'`, `""zip\'`, and `None`.         `""tar""` includes tar, tar.gz, and tar.bz files.         The default `""auto""` corresponds to `[""tar"", ""zip""]`.         None or an empty list will return no matches found.     cache_dir: Location to store cached files, when None it         defaults ether `$KERAS_HOME` if the `KERAS_HOME` environment         variable is set or `~/.keras/`.     force_download: If `True`, the file will always be re-downloaded         regardless of the cache state.  Returns:     Path to the downloaded file.  **⚠️ Warning on malicious downloads ⚠️**  Downloading something from the Internet carries a risk. NEVER download a file/archive if you do not trust the source. We recommend that you specify the `file_hash` argument (if the hash of the source file is known) to make sure that the file you are getting is the one you expect.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = ""~/.qlib/qlib_data/us_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = ""~/.qlib/qlib_data/cn_data""  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 2, 'pr': 1793, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",51.46172547340393,1082
How is the placeholder index for padding NaNs adjusted to prevent negative slice indices in TSDataSampler?,"In TSDataSampler, the placeholder index for padding NaNs is adjusted by setting `nan_idx` to `len(self.data_arr) - 1`. This prevents negative slice indices when mapping out-of-range negative look-back indices to `nan_idx`. The line of code responsible for this adjustment is located in qlib/data/dataset/__init__.py, where the `__init__` method of TSDataSampler appends an all-NaN row to `self.data_arr` and sets `nan_idx` accordingly.","In TSDataSampler.__init__ (qlib/data/dataset/__init__.py) we append one all‐NaN row to `self.data_arr` and then set

 self.nan_idx = len(self.data_arr) - 1

instead of the usual −1.  Any out-of-range (negative) look-back indices are then mapped to this `nan_idx`, so you never end up slicing at a negative position.","['METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.__init__ has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.__init__.start\nBuild a dataset which looks like torch.data.utils.Dataset.  Parameters ---------- data : pd.DataFrame     The raw tabular data whose index order is <""datetime"", ""instrument""> start :     The indexable start time end :     The indexable end time step_len : int     The length of the time-series step fillna_type : int     How will qlib handle the sample if there is on sample in a specific date.     none:         fill with np.nan     ffill:         ffill with previous sample     ffill+bfill:         ffill with previous samples first and fill with later samples second flt_data : pd.Series     a column of data(True or False) to filter data. Its index order is <""datetime"", ""instrument"">     This feature is essential because:     - We want some sample not included due to label-based filtering, but we can\'t filter them at the beginning due to the features is still important in the feature.     None:         kepp all data', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index.data_index', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.__init__ has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.__init__.flt_data\nBuild a dataset which looks like torch.data.utils.Dataset.  Parameters ---------- data : pd.DataFrame     The raw tabular data whose index order is <""datetime"", ""instrument""> start :     The indexable start time end :     The indexable end time step_len : int     The length of the time-series step fillna_type : int     How will qlib handle the sample if there is on sample in a specific date.     none:         fill with np.nan     ffill:         ffill with previous sample     ffill+bfill:         ffill with previous samples first and fill with later samples second flt_data : pd.Series     a column of data(True or False) to filter data. Its index order is <""datetime"", ""instrument"">     This feature is essential because:     - We want some sample not included due to label-based filtering, but we can\'t filter them at the beginning due to the features is still important in the feature.     None:         kepp all data', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index.end', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.__init__ has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.__init__.dtype\nBuild a dataset which looks like torch.data.utils.Dataset.  Parameters ---------- data : pd.DataFrame     The raw tabular data whose index order is <""datetime"", ""instrument""> start :     The indexable start time end :     The indexable end time step_len : int     The length of the time-series step fillna_type : int     How will qlib handle the sample if there is on sample in a specific date.     none:         fill with np.nan     ffill:         ffill with previous sample     ffill+bfill:         ffill with previous samples first and fill with later samples second flt_data : pd.Series     a column of data(True or False) to filter data. Its index order is <""datetime"", ""instrument"">     This feature is essential because:     - We want some sample not included due to label-based filtering, but we can\'t filter them at the beginning due to the features is still important in the feature.     None:         kepp all data', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1803, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",57.89255380630493,1083
How does inst_calculator convert the index when it's not already in datetime format?,"When the index is not already in datetime format, `inst_calculator` converts it by using the `_calendar` object to map each integer index to a datetime. The code renames the index to ""datetime"". Here's how it works:

1. It first checks if the index is already in datetime format. If it is, it doesn't do anything.
2. If the index isn't in datetime format, it uses the `_calendar` object to map each integer index to a datetime.
3. The result of this mapping is assigned to `data.index`.
4. Finally, it renames the index to ""datetime"".","In qlib/data/data.py’s DatasetProvider.inst_calculator, if `data.index` isn’t already datetime, it does:

1. `_calendar = Cal.calendar(freq=freq)`  
2. `data.index = _calendar[data.index.values.astype(int)]`  

This uses the calendar object to map each integer index to a `datetime`, then renames the index to “datetime.”","['CLASS qlib.qlib.utils.index_data.Index has method qlib.qlib.utils.index_data.Index.index\nGiven the index value, get the integer index  Parameters ---------- item :     The item to query  Returns ------- int:     The index of the item  Raises ------ KeyError:     If the query item does not exist\nThis is for indexing(rows or columns)  Read-only operations has higher priorities than others. So this class is designed in a **read-only** way to shared data for queries. Modifications will results in new Index.  NOTE: the indexing has following flaws - duplicated index value is not well supported (only the first appearance will be considered) - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered', 'FUNCTION qlib.qlib.utils.time.time_to_day_index has parameter or accepts argument qlib.qlib.utils.time.CN_TIME', 'CLASS qlib.qlib.utils.index_data.Index has method qlib.qlib.utils.index_data.Index.tolist\nreturn the index with the format of list.\nThis is for indexing(rows or columns)  Read-only operations has higher priorities than others. So this class is designed in a **read-only** way to shared data for queries. Modifications will results in new Index.  NOTE: the indexing has following flaws - duplicated index value is not well supported (only the first appearance will be considered) - The order of the index is not considered!!!! So the slicing will not behave like pandas when indexings are ordered', 'MODULE qlib.qlib.utils.time contains FUNCTION qlib.qlib.utils.time.time_to_day_index', 'CLASS qlib.qlib.data.data.ExpressionProvider has method qlib.qlib.data.data.ExpressionProvider.expression\nGet Expression data.  The responsibility of `expression` - parse the `field` and `load` the according data. - When loading the data, it should handle the time dependency of the data. `get_expression_instance` is commonly used in this method  Parameters ---------- instrument : str     a certain instrument. field : str     a certain field of feature. start_time : str     start of the time range. end_time : str     end of the time range. freq : str     time frequency, available: year/quarter/month/week/day.  Returns ------- pd.Series     data of a certain expression      The data has two types of format      1) expression with datetime index      2) expression with integer index          - because the datetime is not as good as\nExpression provider class  Provide Expression data.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 1809, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",35.24284529685974,1084
How does label masking differ between the default loss metric and the mse option?,"The default metric uses `mask = torch.isfinite(label)` which filters out both NaN and infinite values, whereas the 'mse' branch uses `mask = ~torch.isnan(label)` which drops NaN values but keeps infinite values. This difference in masking affects how the loss is calculated.","In ALSTM.metric_fn (qlib/contrib/model/pytorch_alstm_ts.py), the default metric (`""""` or `""loss""`) uses  
• mask = torch.isfinite(label)  
to filter out both NaNs and ±∞ before calling self.loss_fn.  

The `""mse""` branch instead does  
• mask = ~torch.isnan(label)  
(so only NaNs are dropped, infinities are kept), builds a unit weight tensor, and calls self.mse(pred[mask], label[mask], weight[mask]).","['CLASS keras.keras.src.metrics.probabilistic_metrics.BinaryCrossentropy has field or attribute or property keras.keras.src.metrics.probabilistic_metrics.dtype\nComputes the crossentropy metric between the labels and predictions.  This is the crossentropy metric class to be used when there are only two label classes (0 and 1).  Args:     name: (Optional) string name of the metric instance.     dtype: (Optional) data type of the metric result.     from_logits: (Optional) Whether output is expected         to be a logits tensor. By default, we consider         that output encodes a probability distribution.     label_smoothing: (Optional) Float in `[0, 1]`.         When > 0, label values are smoothed,         meaning the confidence on label values are relaxed.         e.g. `label_smoothing=0.2` means that we will use         a value of 0.1 for label ""0"" and 0.9 for label ""1"".  Examples:  >>> m = keras.metrics.BinaryCrossentropy() >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) >>> m.result() 0.81492424  >>> m.reset_state() >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ...                sample_weight=[1, 0]) >>> m.result() 0.9162905  Usage with `compile()` API:  ```python model.compile(     optimizer=\'sgd\',     loss=\'mse\',     metrics=[keras.metrics.BinaryCrossentropy()]) ```', 'METHOD qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.get_loss has parameter or accepts argument qlib.qlib.contrib.model.double_ensemble.DEnsembleModel.get_loss.label', 'CLASS keras.keras.src.metrics.probabilistic_metrics.BinaryCrossentropy has field or attribute or property keras.keras.src.metrics.probabilistic_metrics.BinaryCrossentropy.from_logits\nComputes the crossentropy metric between the labels and predictions.  This is the crossentropy metric class to be used when there are only two label classes (0 and 1).  Args:     name: (Optional) string name of the metric instance.     dtype: (Optional) data type of the metric result.     from_logits: (Optional) Whether output is expected         to be a logits tensor. By default, we consider         that output encodes a probability distribution.     label_smoothing: (Optional) Float in `[0, 1]`.         When > 0, label values are smoothed,         meaning the confidence on label values are relaxed.         e.g. `label_smoothing=0.2` means that we will use         a value of 0.1 for label ""0"" and 0.9 for label ""1"".  Examples:  >>> m = keras.metrics.BinaryCrossentropy() >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) >>> m.result() 0.81492424  >>> m.reset_state() >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ...                sample_weight=[1, 0]) >>> m.result() 0.9162905  Usage with `compile()` API:  ```python model.compile(     optimizer=\'sgd\',     loss=\'mse\',     metrics=[keras.metrics.BinaryCrossentropy()]) ```', 'METHOD qlib.qlib.contrib.model.pytorch_transformer.TransformerModel.loss_fn has parameter or accepts argument qlib.qlib.contrib.model.pytorch_transformer.TransformerModel.loss_fn.label', 'CLASS keras.keras.src.metrics.probabilistic_metrics.BinaryCrossentropy has field or attribute or property keras.keras.src.metrics.probabilistic_metrics.name\nComputes the crossentropy metric between the labels and predictions.  This is the crossentropy metric class to be used when there are only two label classes (0 and 1).  Args:     name: (Optional) string name of the metric instance.     dtype: (Optional) data type of the metric result.     from_logits: (Optional) Whether output is expected         to be a logits tensor. By default, we consider         that output encodes a probability distribution.     label_smoothing: (Optional) Float in `[0, 1]`.         When > 0, label values are smoothed,         meaning the confidence on label values are relaxed.         e.g. `label_smoothing=0.2` means that we will use         a value of 0.1 for label ""0"" and 0.9 for label ""1"".  Examples:  >>> m = keras.metrics.BinaryCrossentropy() >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) >>> m.result() 0.81492424  >>> m.reset_state() >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], ...                sample_weight=[1, 0]) >>> m.result() 0.9162905  Usage with `compile()` API:  ```python model.compile(     optimizer=\'sgd\',     loss=\'mse\',     metrics=[keras.metrics.BinaryCrossentropy()]) ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394±0.00 | 0.2909±0.00 | 0.0448±0.00 | 0.3679±0.00 | 0.0344±0.00       | 0.4527±0.02       | -0.1004±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390±0.00 | 0.2946±0.01 | 0.0486±0.00 | 0.3836±0.01 | 0.0462±0.01       | 0.6151±0.18       | -0.0915±0.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400±0.00 | 0.3037±0.00 | 0.0499±0.00 | 0.4042±0.00 | 0.0558±0.00       | 0.7632±0.00       | -0.0659±0.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441±0.00 | 0.3301±0.02 | 0.0519±0.00 | 0.4130±0.01 | 0.0604±0.02       | 0.8295±0.34       | -0.1018±0.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497±0.00 | 0.3829±0.04 | 0.0599±0.00 | 0.4736±0.03 | 0.0626±0.02       | 0.8651±0.31       | -0.0994±0.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448±0.00 | 0.3474±0.04 | 0.0549±0.00 | 0.4366±0.03 | 0.0647±0.03       | 0.8963±0.39       | -0.0875±0.02 |\n| ADD                                       | Alpha360 | 0.0430±0.00 | 0.3188±0.04 | 0.0559±0.00 | 0.4301±0.03 | 0.0667±0.02       | 0.8992±0.34       | -0.0855±0.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493±0.00 | 0.3772±0.04 | 0.0584±0.00 | 0.4638±0.03 | 0.0720±0.02       | 0.9730±0.33       | -0.0821±0.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464±0.01 | 0.3619±0.08 | 0.0539±0.01 | 0.4287±0.06 | 0.0753±0.03       | 1.0200±0.40       | -0.0936±0.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476±0.00 | 0.3508±0.02 | 0.0598±0.00 | 0.4604±0.01 | 0.0824±0.02       | 1.1079±0.26       | -0.0894±0.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508±0.00 | 0.3931±0.04 | 0.0599±0.00 | 0.4756±0.03 | 0.0893±0.03       | 1.2256±0.36       | -0.0857±0.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485±0.00 | 0.3787±0.03 | 0.0587±0.00 | 0.4756±0.03 | 0.0920±0.03       | 1.2789±0.42       | -0.0834±0.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480±0.00 | 0.3589±0.02 | 0.0606±0.00 | 0.4773±0.01 | 0.0946±0.02       | 1.3509±0.25       | -0.0716±0.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522±0.00 | 0.3530±0.01 | 0.0667±0.00 | 0.4576±0.01 | 0.0987±0.02       | 1.3726±0.27       | -0.0681±0.01 |\n| KRNN                                      | Alpha360 | 0.0173±0.01 | 0.1210±0.06 | 0.0270±0.01 | 0.2018±0.04 | -0.0465±0.05      | -0.5415±0.62      | -0.2919±0.13 |\n| Sandwich                                  | Alpha360 | 0.0258±0.00 | 0.1924±0.04 | 0.0337±0.00 | 0.2624±0.03 | 0.0005±0.03       | 0.0001±0.33       | -0.1752±0.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1810, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",45.942190647125244,1085
What sequence of operations does the training workflow follow from initializing modules to generating result records?,"The training workflow in qlib follows the following sequence of operations from initializing modules to generating result records:

1. Initialize the model by calling `init_instance_by_config` with `task_config[","In qlib/model/trainer.py::_exe_task the workflow is:

1. rec = R.get_recorder()  
2. model = init_instance_by_config(task_config[""model""], Model)  
3. dataset = init_instance_by_config(task_config[""dataset""], Dataset)  
4. (opt.) reweighter = task_config.get(""reweighter"")  
5. auto_filter_kwargs(model.fit)(dataset, reweighter=reweighter)  
6. R.save_objects(params.pkl=model)  
7. dataset.config(dump_all=False); R.save_objects(dataset=dataset)  
8. task_config = fill_placeholder(task_config, {""<MODEL>"": model, ""<DATASET>"": dataset})  
9. for each record in task_config[""record""]:  
   • r = init_instance_by_config(record, recorder=rec, default_module=""qlib.workflow.record_temp"", try_kwargs={""model"": model, ""dataset"": dataset})  
   • r.generate()  

This initializes model/dataset, fits & saves them, then instantiates and runs each Record (e.g. SignalRecord, SigAnaRecord) to produce final output files.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.workflow.exp contains CLASS qlib.qlib.workflow.exp.MLflowExperiment\nUse mlflow to implement Experiment.', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.run', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n├── bin\n├── orders\n└── pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper ""[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)"".\n- **OPDS**: Method proposed by AAAI 2021 paper ""[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)"".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"", ""qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Quick Start\n\nThis quick start guide tries to demonstrate\n1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.\n2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.\n\nHere is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).\n\n\n## Installation\n\nThis table demonstrates the supported Python version of `Qlib`:\n|               | install with pip      | install from source  |        plot        |\n| ------------- |:---------------------:|:--------------------:|:------------------:|\n| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n\n**Note**: \n1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.\n2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`'s Python to install ``Qlib`` from source.\n\n### Install with pip\nUsers can easily install ``Qlib`` by pip according to the following command.\n\n```bash\n  pip install pyqlib\n```\n\n**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.\n\n### Install from source\nAlso, users can install the latest dev version ``Qlib`` by the source code according to the following steps:\n\n* Before installing ``Qlib`` from source, users need to install some dependencies:\n\n  ```bash\n  pip install numpy\n  pip install --upgrade cython\n  ```\n\n* Clone the repository and install ``Qlib`` as follows.\n    ```bash\n    git clone https://github.com/microsoft/qlib.git && cd qlib\n    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst\n    ```\n\n**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.\n\n**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully.""]","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 5, 'pr': 1811, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",41.213603019714355,1086
"When initializing an index with datetime64 values of differing precisions, how does the code respond?","When initializing an index with datetime64 values of differing precisions in qlib, the code raises a TypeError with the message ""All elements in idx_list must be of the same datetime64 precision"". This is because the Index.__init__ method checks whether all elements in idx_list have the same dtype as idx_list[0], and if not, it raises an error.","In Index.__init__ (qlib/utils/index_data.py), there’s a check:

  if isinstance(idx_list[0], np.datetime64) and not all(x.dtype == idx_list[0].dtype for x in idx_list):
      raise TypeError(""All elements in idx_list must be of the same datetime64 precision"")

So feeding it datetime64 values with mixed precisions will raise that TypeError.","['MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.argpartition\nPerforms an indirect partition along the given axis.  It returns an array of indices of the same shape as `x` that index data along the given axis in partitioned order.  Args:     a: Array to sort.     kth: Element index to partition by.         The k-th element will be in its final sorted position and all         smaller elements will be moved before it and all larger elements         behind it. The order of all elements in the partitions is undefined.         If provided with a sequence of k-th it will partition all of them         into their sorted position at once.     axis: Axis along which to sort. The default is -1 (the last axis).         If `None`, the flattened array is used.  Returns:     Array of indices that partition `x` along the specified `axis`.', 'CLASS qlib.qlib.contrib.data.handler.Alpha158 has field or attribute or property qlib.qlib.contrib.data.handler.fit_end_time', 'FUNCTION keras.keras.src.ops.numpy.argpartition has parameter or accepts argument keras.keras.src.ops.numpy.argpartition.kth\nPerforms an indirect partition along the given axis.  It returns an array of indices of the same shape as `x` that index data along the given axis in partitioned order.  Args:     a: Array to sort.     kth: Element index to partition by.         The k-th element will be in its final sorted position and all         smaller elements will be moved before it and all larger elements         behind it. The order of all elements in the partitions is undefined.         If provided with a sequence of k-th it will partition all of them         into their sorted position at once.     axis: Axis along which to sort. The default is -1 (the last axis).         If `None`, the flattened array is used.  Returns:     Array of indices that partition `x` along the specified `axis`.', 'CLASS qlib.qlib.contrib.data.handler.Alpha158 has field or attribute or property qlib.qlib.contrib.data.handler.fit_start_time', 'FUNCTION keras.keras.src.ops.numpy.argpartition has parameter or accepts argument keras.keras.src.ops.numpy.argpartition.axis\nPerforms an indirect partition along the given axis.  It returns an array of indices of the same shape as `x` that index data along the given axis in partitioned order.  Args:     a: Array to sort.     kth: Element index to partition by.         The k-th element will be in its final sorted position and all         smaller elements will be moved before it and all larger elements         behind it. The order of all elements in the partitions is undefined.         If provided with a sequence of k-th it will partition all of them         into their sorted position at once.     axis: Axis along which to sort. The default is -1 (the last axis).         If `None`, the flattened array is used.  Returns:     Array of indices that partition `x` along the specified `axis`.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': True, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 1813, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",32.297455072402954,1087
"In the DDGDA class, which routines are responsible for serializing the proxy forecasting data and orchestrating the meta model training?","The routines responsible for serializing the proxy forecasting data in the DDGDA class are:
*   `_dump_data_for_proxy_model`: This method serializes the proxy forecasting data to `fea_label_df.pkl` and `handler_proxy.pkl`. The routine responsible for orchestrating the meta-model training in the DDGDA class is:
*   `_train_meta_model`: This method builds the MetaDatasetDS, invokes MetaModelDS.fit, and performs logging during meta-model training.","In qlib/contrib/rolling/ddgda.py, DDGDA._dump_data_for_proxy_model handles serializing the proxy forecasting data (to fea_label_df.pkl and handler_proxy.pkl), and DDGDA._train_meta_model orchestrates the meta‐model training (building the MetaDatasetDS, invoking MetaModelDS.fit and logging).","['METHOD qlib.qlib.contrib.rolling.ddgda.DDGDA.get_task_list has parameter or accepts argument qlib.qlib.contrib.rolling.ddgda.DDGDA.hist_step_n\nLeverage meta-model for inference: - Given     - baseline tasks     - input for meta model(internal data)     - meta model (its learnt knowledge on proxy forecasting model is expected to transfer to normal forecasting model)', 'CLASS qlib.qlib.contrib.rolling.ddgda.DDGDA has method qlib.qlib.contrib.rolling.ddgda.DDGDA.get_task_list\nLeverage meta-model for inference: - Given     - baseline tasks     - input for meta model(internal data)     - meta model (its learnt knowledge on proxy forecasting model is expected to transfer to normal forecasting model)\nIt is a rolling based on DDG-DA  **NOTE** before running the example, please clean your previous results with following command - `rm -r mlruns`', 'CLASS qlib.qlib.contrib.meta.data_selection.model.MetaModelDS has method qlib.qlib.contrib.meta.data_selection.model.MetaModelDS.fit\nThe meta-learning-based data selection interacts directly with meta-dataset due to the close-form proxy measurement.  Parameters ---------- meta_dataset : MetaDatasetDS     The meta-model takes the meta-dataset for its training process.\nThe meta-model for meta-learning-based data selection.', 'CLASS qlib.qlib.contrib.rolling.ddgda.DDGDA has method qlib.qlib.contrib.rolling.ddgda.DDGDA.__init__\nParameters ---------- sim_task_model: Literal[""linear"", ""gbdt""] = ""gbdt"",     The model for calculating similarity between data. meta_1st_train_end: Optional[str]     the datetime of training end of the first meta_task alpha: float     Setting the L2 regularization for ridge     The `alpha` is only passed to MetaModelDS (it is not passed to sim_task_model currently..) loss_skip_thresh: int     The thresh to skip the loss calculation for each day. If the number of item is less than it, it will skip the loss on that day. meta_data_proc : Optional[str]     How we process the meta dataset for learning meta model. segments : Union[float, str]     if segments is a float:         The ratio of training data in the meta task dataset     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set\nIt is a rolling based on DDG-DA  **NOTE** before running the example, please clean your previous results with following command - `rm -r mlruns`', 'CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has method qlib.qlib.model.meta.dataset.MetaTaskDataset.prepare_tasks\nPrepare the data in each meta-task and ready for training.  The following code example shows how to retrieve a list of meta-tasks from the `meta_dataset`:      .. code-block:: Python          # get the train segment and the test segment, both of them are lists         train_meta_tasks, test_meta_tasks = meta_dataset.prepare_tasks([""train"", ""test""])  Parameters ---------- segments: Union[List[Text], Tuple[Text], Text]     the info to select data  Returns ------- list:     A list of the prepared data of each meta-task for training the meta-model. For multiple segments [seg1, seg2, ... , segN], the returned list will be [[tasks in seg1], [tasks in seg2], ... , [tasks in segN]].     Each task is a meta task\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 25}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 25, 'n_files_pr': 15, 'pr': 1816, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",32.26480317115784,1088
What is the overall process by which the nested loader merges outputs from its constituent data loaders?,"The overall process by which the nested loader merges outputs from its constituent data loaders in qlib is as follows:

1.  The NestedDataLoader.load method iterates over each loader in self.data_loader_l.
2.  For each loader, it calls dl.load(instruments, start_time, end_time).
3.  If dl.load raises a KeyError, it retries dl.load with instruments=None.
4.  If the current loader is the first loader, it sets df_full equal to df_current.
5.  For non-first loaders, it identifies columns in df_current that collide with df_full and drops the colliding columns from df_full.
6.  It merges df_full and df_current using pd.merge with left_index=True, right_index=True, and how=self.join.
7.  After processing all loaders, it returns df_full.

This process ensures that the outputs from each loader are properly combined and merged to produce a single output dataframe.","NestedDataLoader.load (qlib/data/dataset/loader.py) works as follows:

1. Iterate over each loader in self.data_loader_l, calling  
   dl.load(instruments, start_time, end_time)  
   (on KeyError it retries with instruments=None).  
2. If it’s the first loader, set df_full = df_current.  
3. Otherwise:
   a. Identify columns in df_current that collide with df_full.  
   b. Drop those columns from df_full.  
   c. Merge df_full and df_current with pd.merge(..., left_index=True, right_index=True, how=self.join).  
4. After all loaders are processed, return df_full with its columns sorted (df_full.sort_index(axis=1)).","['CLASS qlib.qlib.workflow.online.update.PredUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the prediction in the Recorder', 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has method qlib.qlib.data.dataset.loader.NestedDataLoader.load\nWe have multiple DataLoader, we can use this class to combine them.', 'CLASS qlib.qlib.workflow.online.update.LabelUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the label in the recorder  Assumption - The label is generated from record_temp.SignalRecord.', 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has field or attribute or property qlib.qlib.data.dataset.loader.join\nWe have multiple DataLoader, we can use this class to combine them.', 'CLASS qlib.qlib.data.dataset.storage.BaseHandlerStorage has method qlib.qlib.data.dataset.storage.BaseHandlerStorage.fetch\nfetch data from the data storage  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index level : Union[str, int]     which index level to select the data     - if level is None, apply selector to df directly col_set : Union[str, List[str]]     - if isinstance(col_set, str):         select a set of meaningful columns.(e.g. features, columns)         if col_set == DataHandler.CS_RAW:             the raw dataset will be returned.     - if isinstance(col_set, List[str]):         select several sets of meaningful columns, the returned data has multiple level fetch_orig : bool     Return the original data instead of copy if possible.  Returns ------- pd.DataFrame     the dataframe fetched\nBase data storage for datahandler - pd.DataFrame is the default data storage format in Qlib datahandler - If users want to use custom data storage, they should define subclass inherited BaseHandlerStorage, and implement the following method', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 4, 'pr': 1822, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",40.86202144622803,1089
"Describe how the nested loader assembles a final DataFrame from multiple sub-loaders, including its fallback for instrument filtering errors.","The nested loader assembles a final DataFrame from multiple sub-loaders by iterating over each sub-loader, calling its `load` method with parameters `instruments`, `start_time`, and `end_time`. If a `KeyError` occurs due to an unsupported instrument filter, it issues a warning and retries the call with `instruments` set to `None`. The results from sub-loaders are accumulated into a DataFrame named `df_full`, where each subsequent DataFrame is merged into `df_full` on the multi-index composed of `datetime` and `instrument`, using its join mode. After processing all sub-loaders, it sorts the columns of `df_full` along axis 1 and returns the combined DataFrame.","In qlib/data/dataset/loader.py, NestedDataLoader.load does the following:

1. Iterates over each sub‐loader in self.data_loader_l  
2. For each dl:
   - Calls dl.load(instruments, start_time, end_time)  
   - On KeyError (unsupported instrument filter), warns and retries with instruments=None (i.e. no filtering)  
3. Builds up df_full:
   - The first df_current becomes df_full  
   - For subsequent dfs, it drops any columns in df_current that already exist in df_full, then merges on the multi‐index (datetime, instrument) using the loader’s join mode  
4. At the end, it sorts df_full’s columns (axis=1) and returns the combined DataFrame.","[""MODULE qlib.qlib.backtest.profit_attribution contains FUNCTION qlib.qlib.backtest.profit_attribution.decompose_portofolio_weight\ndecompose_portofolio_weight  ''' :param stock_weight_df: a pandas dataframe to describe the portofolio by weight.                 every row corresponds to a  day                 every column corresponds to a stock.                 Here is an example below.                 code        SH600004  SH600006  SH600017  SH600022  SH600026  SH600037                      date                 2016-01-05  0.001543  0.001570  0.002732  0.001320  0.003000       NaN                 2016-01-06  0.001538  0.001569  0.002770  0.001417  0.002945       NaN                 .... :param stock_group_df: a pandas dataframe to describe  the stock group.                 every row corresponds to a  day                 every column corresponds to a stock.                 the value in the cell repreponds the group id.                 Here is a example by for stock_group_df for industry. The value is the industry code                 instrument  SH600000  SH600004  SH600005  SH600006  SH600007  SH600008                      datetime                 2016-01-05  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0                 2016-01-06  801780.0  801170.0  801040.0  801880.0  801180.0  801160.0                 ... :return:        Two dict will be returned.  The group_weight and the stock_weight_in_group.                 The key is the group. The value is a Series or Dataframe to describe the weight of group or weight of stock"", 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has field or attribute or property qlib.qlib.data.dataset.loader.join\nWe have multiple DataLoader, we can use this class to combine them.', 'CLASS qlib.qlib.data.dataset.storage.BaseHandlerStorage has method qlib.qlib.data.dataset.storage.BaseHandlerStorage.fetch\nfetch data from the data storage  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index level : Union[str, int]     which index level to select the data     - if level is None, apply selector to df directly col_set : Union[str, List[str]]     - if isinstance(col_set, str):         select a set of meaningful columns.(e.g. features, columns)         if col_set == DataHandler.CS_RAW:             the raw dataset will be returned.     - if isinstance(col_set, List[str]):         select several sets of meaningful columns, the returned data has multiple level fetch_orig : bool     Return the original data instead of copy if possible.  Returns ------- pd.DataFrame     the dataframe fetched\nBase data storage for datahandler - pd.DataFrame is the default data storage format in Qlib datahandler - If users want to use custom data storage, they should define subclass inherited BaseHandlerStorage, and implement the following method', 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has method qlib.qlib.data.dataset.loader.NestedDataLoader.load\nWe have multiple DataLoader, we can use this class to combine them.', 'CLASS qlib.qlib.contrib.report.data.base.FeaAnalyser has method qlib.qlib.contrib.report.data.base.FeaAnalyser.__init__\nParameters ---------- dataset : pd.DataFrame      We often have multiple columns for dataset. Each column corresponds to one sub figure.     There will be a datatime column in the index levels.     Aggretation will be used for more summarized metrics overtime.     Here is an example of data:      .. code-block::                                      return         datetime   instrument         2007-02-06 equity_tpx     0.010087                    equity_spx     0.000786', ""xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Usability Improvements, Documentation\n* [jvm-packages] add example to handle missing value other than 0 (#5677)\n* Add DMatrix usage examples to the C API demo (#5854)\n* List `DaskDeviceQuantileDMatrix` in the doc. (#5975)\n* Update Python custom objective demo. (#5981)\n* Update the JSON model schema to document more objective functions. (#5982)\n* [Python] Fix warning when `missing` field is not used. (#5969)\n* Fix typo in tracker logging (#5994)\n* Move a warning about empty dataset, so that it's shown for all objectives and metrics (#5998)\n* Fix the instructions for installing the nightly build. (#6004)\n* [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)\n* [jvm-packages] [doc] Update install doc for JVM packages (#6051)\n* Fix typo in `xgboost.callback.early_stop` docstring (#6071)\n* Add cache suffix to the files used in the external memory demo. (#6088)\n* [Doc] Document the parameter `kill_spark_context_on_worker_failure` (#6097)\n* Fix link to the demo for custom objectives (#6100)\n* Update Dask doc. (#6108)\n* Validate weights are positive values. (#6115)\n* Document the updated CMake version requirement. (#6123)\n* Add demo for `DaskDeviceQuantileDMatrix`. (#6156)\n* Cosmetic fixes in `faq.rst` (#6161)\n* Fix error message. (#6176)\n* [Doc] Add list of winning solutions in data science competitions using XGBoost (#6177)\n* Fix a comment in demo to use correct reference (#6190)\n* Update the list of winning solutions using XGBoost (#6192)\n* Consistent style for build status badge (#6203)\n* [Doc] Add info on GPU compiler (#6204)\n* Update the list of winning solutions (#6222, #6254)\n* Add link to XGBoost's Twitter handle (#6244)\n* Fix minor typos in XGBClassifier methods' docstrings (#6247)\n* Add sponsors link to FUNDING.yml (#6252)\n* Group CLI demo into subdirectory. (#6258)\n* Reduce warning messages from `gbtree`. (#6273)\n* Create a tutorial for using the C API in a C/C++ application (#6285)\n* Update plugin instructions for CMake build (#6289)\n* [doc] make Dask distributed example copy-pastable (#6345)\n* [Python] Add option to use `libxgboost.so` from the system path (#6362)\n* Fixed few grammatical mistakes in doc (#6393)\n* Fix broken link in CLI doc (#6396)\n* Improve documentation for the Dask API (#6413)\n* Revise misleading exception information: no such param of `allow_non_zero_missing` (#6418)\n* Fix CLI ranking demo. (#6439)\n* Fix broken links. (#6455)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @FelixYBW, Jack Dunn (@JackDunnNZ), Jean Lescut-Muller (@JeanLescut),  Boris Feld (@Lothiraldan), Nikhil Choudhary (@Nikhil1O1), Rory Mitchell (@RAMitchell), @ShvetsKS, Anthony D'Amato (@Totoketchup), @Wittty-Panda, neko (@akiyamaneko), Alexander Gugel (@alexanderGugel), @dependabot[bot], DIVYA CHAUHAN (@divya661), Daniel Steinberg (@dstein64), Akira Funahashi (@funasoul), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Hristo Iliev (@hiliev), Honza Sterba (@honzasterba), @hzy001, Igor Moura (@igormp), @jameskrach, James Lamb (@jameslamb), Naveed Ahmed Saleem Janvekar (@janvekarnaveed), Kyle Nicholson (@kylejn27), lacrosse91 (@lacrosse91), Christian Lorentzen (@lorentzenchr), Manikya Bardhan (@manikyabard), @nabokovas, John Quitto-Graham (@nvidia-johnq), @odidev, Qi Zhang (@qzhang90), Sergio Gavilán (@sgavil), Tanuja Kirthi Doddapaneni (@tanuja3), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), Zeno Gantner (@zenogantner), zhang_jf (@zuston)"", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [dask] Set dataframe index in predict. (#6944)\n* [dask] Fix prediction on df with latest dask. (#6969)\n* [dask] Fix dask predict on `DaskDMatrix` with `iteration_range`. (#7005)\n* [dask] Disallow importing non-dask estimators from xgboost.dask (#7133)\n\n### R package\nImprovements other than new features on R package:\n* Optimization for updating R handles in-place (#6903)\n* Removed the magrittr dependency. (#6855, #6906, #6928)\n* The R package now hides all C++ symbols to avoid conflicts. (#7245)\n* Other maintenance including code cleanups, document updates. (#6863, #6915, #6930, #6966, #6967)\n\n### JVM packages\nImprovements other than new features on JVM packages:\n* Constructors with implicit missing value are deprecated due to confusing behaviors. (#7225)\n* Reduce scala-compiler, scalatest dependency scopes (#6730)\n* Making the Java library loader emit helpful error messages on missing dependencies. (#6926)\n* JVM packages now use the Python tracker in XGBoost instead of dmlc.  The one in XGBoost\n  is shared between JVM packages and Python Dask and enjoys better maintenance (#7132)\n* Fix ""key not found: train"" error (#6842)\n* Fix model loading from stream (#7067)\n\n### General document improvements\n* Overhaul the installation documents. (#6877)\n* A few demos are added for AFT with dask (#6853), callback with dask (#6995), inference\n  in C (#7151), `process_type`. (#7135)\n* Fix PDF format of document. (#7143)\n* Clarify the behavior of `use_rmm`. (#6808)\n* Clarify prediction function. (#6813)\n* Improve tutorial on feature interactions (#7219)\n* Add small example for dask sklearn interface. (#6970)\n* Update Python intro.  (#7235)\n* Some fixes/updates (#6810, #6856, #6935, #6948, #6976, #7084, #7097, #7170, #7173, #7174, #7226, #6979, #6809, #6796, #6979)\n\n### Maintenance\n* Some refactoring around CPU hist, which lead to better performance but are listed under general maintenance tasks:\n  - Extract evaluate splits from CPU hist. (#7079)\n  - Merge lossgude and depthwise strategies for CPU hist (#7007)\n  - Simplify sparse and dense CPU hist kernels (#7029)\n  - Extract histogram builder from CPU Hist. (#7152)\n\n* Others\n  - Fix `gpu_id` with custom objective. (#7015)\n  - Fix typos in AUC. (#6795)\n  - Use constexpr in `dh::CopyIf`. (#6828)\n  - Update dmlc-core. (#6862)\n  - Bump version to 1.5.0 snapshot in master. (#6875)\n  - Relax shotgun test. (#6900)\n  - Guard against index error in prediction. (#6982)\n  - Hide symbols in CI build + hide symbols for C and CUDA (#6798)\n  - Persist data in dask test. (#7077)\n  - Fix typo in arguments of PartitionBuilder::Init (#7113)\n  - Fix typo in src/common/hist.cc BuildHistKernel (#7116)\n  - Use upstream URI in distributed quantile tests. (#7129)\n  - Include cpack (#7160)\n  - Remove synchronization in monitor. (#7164)\n  - Remove unused code. (#7175)\n  - Fix building on CUDA 11.0. (#7187)\n  - Better error message for `ncclUnhandledCudaError`. (#7190)\n  - Add noexcept to JSON objects. (#7205)\n  - Improve wording for warning (#7248)\n  - Fix typo in release script. [skip ci] (#7238)\n  - Relax shotgun test. (#6918)\n  - Relax test for decision stump in distributed environment. (#6919)\n  -\t[dask] speed up tests (#7020)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 3, 'n_files_pr': 5, 'pr': 1823, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",51.17778515815735,1090
How is the flat input tensor reshaped and permuted into the sequence format expected by the GRU before being passed through the recurrent layer?,"The input tensor is reshaped into the sequence format expected by the GRU before being passed through the recurrent layer in qlib as follows:

1. The input x to `GRUModel.forward` is flat with shape `[N, F*T]`
2. The code reshapes x using `x = x.reshape(len(x), self.d_feat, -1)`, resulting in a tensor with shape `[N, F, T]`
3. The code then permutes x using `x = x.permute(0, 2, 1)`, resulting in a tensor with shape `[N, T, F]` that is passed into `self.rnn`. This reshaping and permuting of the input tensor is done to match the expected sequence format for the GRU model in qlib.","In GRUModel.forward (qlib/contrib/model/pytorch_gru.py) the flat input x of shape [N, F*T] is first reshaped with  
```
x = x.reshape(len(x), self.d_feat, -1)  # → [N, F, T]
```  
then permuted  
```
x = x.permute(0, 2, 1)  # → [N, T, F]
```  
before being passed into self.rnn.","['CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.model.pytorch_gru.GRUModel.forward has parameter or accepts argument qlib.qlib.contrib.model.pytorch_gru.GRUModel.forward.x', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.model.pytorch_gru_ts.GRUModel.forward has parameter or accepts argument qlib.qlib.contrib.model.pytorch_gru_ts.GRUModel.forward.x', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has field or attribute or property qlib.qlib.workflow.record_temp.indicator_analysis_freq\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 7, 'pr': 1827, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",37.31763792037964,1091
How does fit filter pretrained weights to update only matching parameters in the model?,"The `fit` function in qlib filters pretrained weights to update only matching parameters in the model by using a dictionary comprehension to create a new dictionary (`pretrained_dict`) that includes only the keys from the target model's state dictionary. This is done by iterating over the items in the target model's state dictionary and including them in `pretrained_dict` if their key is present in the target model's state dictionary.

Here is an example of how this works:

```python
# Assuming self.HIST_model.state_dict() returns a dictionary with keys 'layer1.weight', 'layer2.bias', etc.
model_dict = self.HIST_model.state_dict()

# Create a new dictionary that includes only the keys from the target model's state dictionary
pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}

# Update model_dict with the contents of pretrained_dict
model_dict.update(pretrained_dict)

# Load the merged state dictionary into self.HIST_model
self.HIST_model.load_state_dict(model_dict)
```

This way, only the matching parameters get overwritten in `self.HIST_model`, which is what you want when loading pre-trained weights.","In HIST.fit (qlib/contrib/model/pytorch_hist.py), after loading the pretrained LSTMModel/GRUModel state_dict, it does:

• model_dict = self.HIST_model.state_dict()  
• pretrained_dict = {k: v for k, v in pretrained_model.state_dict().items() if k in model_dict}  
• model_dict.update(pretrained_dict)  
• self.HIST_model.load_state_dict(model_dict)  

This comprehension filters out any keys not present in the target model, so only matching parameters get overwritten.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.contrib.model.pytorch_hist.HIST.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_hist.HIST.fit.dataset', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'METHOD qlib.qlib.contrib.model.pytorch_hist.HIST.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_hist.HIST.fit.evals_result', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.uri_folder\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382±0.00 | 0.3229±0.00 | 0.0489±0.00 | 0.4649±0.00 | 0.0297±0.00       | 0.4227±0.02       | -0.1499±0.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361±0.00 | 0.3092±0.00 | 0.0499±0.00 | 0.4793±0.00 | 0.0382±0.00       | 0.1723±0.02       | -0.4876±0.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."", ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (""one-vs-rest split""). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1842, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",46.02574968338013,1092
What check causes the AsyncCaller thread to exit its loop when the main process ends?,The check that causes the AsyncCaller thread to exit its loop when the main process ends is `if not threading.main_thread().is_alive(): break` in the file qlib/utils/paral.py. This check is used to ensure that the AsyncCaller thread exits when the main process terminates.,"In AsyncCaller.run (qlib/utils/paral.py), it does:

    if not threading.main_thread().is_alive():
        break

This “main thread is dead” check causes the loop to exit when the main process ends.","['CLASS qlib.qlib.rl.trainer.callbacks.Callback has method qlib.qlib.rl.trainer.callbacks.Callback.on_train_end\nCalled when the training ends. To access all outputs produced during training, cache the data in either trainer and vessel, and post-process them in this hook.\nBase class of all callbacks.', 'METHOD qlib.qlib.utils.paral.AsyncCaller.wait has parameter or accepts argument qlib.qlib.utils.paral.AsyncCaller.wait.close', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.update_data_to_bin has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.update_data_to_bin.exists_skip\nupdate yahoo data to bin  Parameters ---------- qlib_data_1d_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data  end_date: str     end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end) check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. delay: float     time.sleep(delay), default 1 exists_skip: bool     exists skip, by default False Notes -----     If the data in qlib_data_dir is incomplete, np.nan will be populated to trading_date for the previous trading day  Examples -------     $ python collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>', 'METHOD qlib.qlib.utils.paral.AsyncCaller.wrapper has parameter or accepts argument qlib.qlib.utils.paral.AsyncCaller.wrapper.func', 'METHOD qlib.scripts.data_collector.yahoo.collector.Run.update_data_to_bin has parameter or accepts argument qlib.scripts.data_collector.yahoo.collector.Run.update_data_to_bin.qlib_data_1d_dir\nupdate yahoo data to bin  Parameters ---------- qlib_data_1d_dir: str     the qlib data to be updated for yahoo, usually from: https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data  end_date: str     end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end) check_data_length: int     check data length, if not None and greater than 0, each symbol will be considered complete if its data length is greater than or equal to this value, otherwise it will be fetched again, the maximum number of fetches being (max_collector_count). By default None. delay: float     time.sleep(delay), default 1 exists_skip: bool     exists skip, by default False Notes -----     If the data in qlib_data_dir is incomplete, np.nan will be populated to trading_date for the previous trading day  Examples -------     $ python collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR\'s for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/csi300/csi500/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i ""s/SH000300/SH000905/g""  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332±0.00 | 0.3044±0.00 | 0.0462±0.00 | 0.4326±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n| MLP        | Alpha158 | 0.0229±0.01 | 0.2181±0.05 | 0.0360±0.00 | 0.3409±0.02 | 0.0043±0.02       | 0.0602±0.27       | -0.2184±0.04 |\n| LightGBM   | Alpha158 | 0.0399±0.00 | 0.4065±0.00 | 0.0482±0.00 | 0.5101±0.00 | 0.1284±0.00       | 1.5650±0.00       | -0.0635±0.00 |\n| CatBoost   | Alpha158 | 0.0345±0.00 | 0.2855±0.00 | 0.0417±0.00 | 0.3740±0.00 | 0.0496±0.00       | 0.5977±0.00       | -0.1496±0.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380±0.00 | 0.3659±0.00 | 0.0442±0.00 | 0.4324±0.00 | 0.0382±0.00       | 0.1723±0.00       | -0.4876±0.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258±0.00 | 0.2021±0.02 | 0.0426±0.00 | 0.3840±0.02 | 0.0022±0.02       | 0.0301±0.26       | -0.2064±0.02 |\n| LightGBM   | Alpha360 | 0.0400±0.00 | 0.3605±0.00 | 0.0536±0.00 | 0.5431±0.00 | 0.0505±0.00       | 0.7658±0.02       | -0.1880±0.00 |', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1869, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.12308430671692,1093
Which configuration types are supported by the static loader and how does it load each one?,"The static loader in qlib supports three configuration types: **dict**, **str or Path**, and **pandas DataFrame**.

### Dict Configuration
For a dict configuration, the static loader loads datasets for each fields_group and path_or_obj pair using `load_dataset(path_or_obj)`, concatenates them with `pandas.concat` using axis=1 and join=self.join, and then sorts the index.

### String or Path Configuration
For a string or Path configuration ending with "".parquet"", the static loader uses `pandas.read_parquet` with engine=""pyarrow"" to load the data. For a string or Path configuration not ending with "".parquet"", it uses `pickle.load` on the file.

### Pandas DataFrame Configuration
For a pandas DataFrame configuration, the static loader assigns the DataFrame to self._data.","In StaticDataLoader._maybe_load_raw_data (qlib/data/dataset/loader.py) three config types are supported:

• dict: for each (fields_group → path_or_obj) in the dict it calls load_dataset(path_or_obj), then pd.concat(…, axis=1, join=self.join) and sorts the index.  
• str or Path: if the string ends with “.parquet” it uses pandas.read_parquet(..., engine=""pyarrow""); otherwise it does pickle.load() on the file.  
• pd.DataFrame: it just assigns the DataFrame directly to self._data.","['MODULE qlib.qlib.workflow.cli contains FUNCTION qlib.qlib.workflow.cli.workflow\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.qlib.data.dataset.loader.StaticDataLoader has field or attribute or property qlib.qlib.data.dataset.loader.config\nDataLoader that supports loading data from file or as provided.', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.logger\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'CLASS qlib.qlib.data.dataset.loader.StaticDataLoader has field or attribute or property qlib.qlib.data.dataset.loader.join\nDataLoader that supports loading data from file or as provided.', 'FUNCTION qlib.qlib.workflow.cli.workflow has parameter or accepts argument qlib.qlib.workflow.cli.workflow.experiment_name\nThis is a Qlib CLI entrance. User can run the whole Quant research workflow defined by a configure file - the code is located here ``qlib/workflow/cli.py`  User can specify a base_config file in your workflow.yml file by adding ""BASE_CONFIG_PATH"". Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields in their own workflow.yml file.  For examples:      qlib_init:         provider_uri: ""~/.qlib/qlib_data/cn_data""         region: cn     BASE_CONFIG_PATH: ""workflow_config_lightgbm_Alpha158_csi500.yaml""     market: csi300', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 9, 'pr': 1896, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",38.04333257675171,1094
What strategy does the loader use to prevent duplicate columns when merging outputs from multiple sub-loaders?,"The strategy used to prevent duplicate columns when merging outputs from multiple sub-loaders in qlib is the intersection of `df_full.columns` and `df_current.columns`, followed by dropping the overlapping columns from `df_full`. This ensures that no duplicate columns are carried into the merged output. The loader computes the intersection before each merge operation, which guarantees that only unique columns are added to the final output.","In NestedDataLoader.load (qlib/data/dataset/loader.py), before each pd.merge it computes the intersection of df_full.columns and df_current.columns, drops those overlapping columns from df_full, then merges on the index. This ensures no duplicate columns ever get carried into the merged output.","[""CLASS qlib.qlib.rl.order_execution.strategy.ProxySAOEStrategy inherits from or is a subclass of qlib.qlib.rl.order_execution.strategy.SAOEStrategy\nRL-based strategies that use SAOEState as state.\nProxy strategy that uses SAOEState. It is called a 'proxy' strategy because it does not make any decisions by itself. Instead, when the strategy is required to generate a decision, it will yield the environment's information and let the outside agents to make the decision. Please refer to `_generate_trade_decision` for more details."", 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has field or attribute or property qlib.qlib.data.dataset.loader.join\nWe have multiple DataLoader, we can use this class to combine them.', 'MODULE qlib.qlib.__init__ contains FUNCTION qlib.qlib.__init__.get_project_path\nIf users are building a project follow the following pattern. - Qlib is a sub folder in project path - There is a file named `config.yaml` in qlib.  For example:     If your project file system structure follows such a pattern          <project_path>/           - config.yaml           - ...some folders...             - qlib/      This folder will return <project_path>      NOTE: link is not supported here.   This method is often used when - user want to use a relative config path instead of hard-coding qlib config path in code  Raises ------ FileNotFoundError:     If project path is not found', 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has method qlib.qlib.data.dataset.loader.NestedDataLoader.__init__\nParameters ---------- dataloader_l : list[dict]     A list of dataloader, for exmaple      .. code-block:: python          nd = NestedDataLoader(             dataloader_l=[                 {                     ""class"": ""qlib.contrib.data.loader.Alpha158DL"",                 }, {                     ""class"": ""qlib.contrib.data.loader.Alpha360DL"",                     ""kwargs"": {                         ""config"": {                             ""label"": ( [""Ref($close, -2)/Ref($close, -1) - 1""], [""LABEL0""])                         }                     }                 }             ]         ) join :     it will pass to pd.concat when merging it.\nWe have multiple DataLoader, we can use this class to combine them.', 'CLASS qlib.qlib.data.dataset.loader.NestedDataLoader has field or attribute or property qlib.qlib.data.dataset.loader.NestedDataLoader.join\nWe have multiple DataLoader, we can use this class to combine them.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1904, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.12488031387329,1095
Which pandas groupby argument is set to avoid hierarchical indexing after grouping by datetime when normalizing before averaging?,"The pandas groupby argument set to avoid hierarchical indexing after grouping by datetime when normalizing before averaging is `group_keys=False`. This can be seen in the AverageEnsemble.__call__ method, where it is used as follows: `results.groupby(""datetime"", group_keys=False)`. This approach ensures that the resulting DataFrame does not have a hierarchical index.","In AverageEnsemble.__call__ (qlib/model/ens/ensemble.py), the call  
```python
results.groupby(""datetime"", group_keys=False)
```  
uses group_keys=False to prevent hierarchical indexing.","['FUNCTION qlib.qlib.utils.paral.datetime_groupby_apply has parameter or accepts argument qlib.qlib.utils.paral.datetime_groupby_apply.apply_func\ndatetime_groupby_apply This function will apply the `apply_func` on the datetime level index.  Parameters ---------- df :     DataFrame for processing apply_func : Union[Callable, Text]     apply_func for processing the data     if a string is given, then it is treated as naive pandas function axis :     which axis is the datetime level located level :     which level is the datetime level resample_rule :     How to resample the data to calculating parallel n_jobs :     n_jobs for joblib Returns:     pd.DataFrame', 'FUNCTION qlib.qlib.utils.paral.datetime_groupby_apply has parameter or accepts argument qlib.qlib.utils.paral.datetime_groupby_apply.level\ndatetime_groupby_apply This function will apply the `apply_func` on the datetime level index.  Parameters ---------- df :     DataFrame for processing apply_func : Union[Callable, Text]     apply_func for processing the data     if a string is given, then it is treated as naive pandas function axis :     which axis is the datetime level located level :     which level is the datetime level resample_rule :     How to resample the data to calculating parallel n_jobs :     n_jobs for joblib Returns:     pd.DataFrame', 'FUNCTION qlib.qlib.utils.paral.datetime_groupby_apply has parameter or accepts argument qlib.qlib.utils.paral.datetime_groupby_apply.n_jobs\ndatetime_groupby_apply This function will apply the `apply_func` on the datetime level index.  Parameters ---------- df :     DataFrame for processing apply_func : Union[Callable, Text]     apply_func for processing the data     if a string is given, then it is treated as naive pandas function axis :     which axis is the datetime level located level :     which level is the datetime level resample_rule :     How to resample the data to calculating parallel n_jobs :     n_jobs for joblib Returns:     pd.DataFrame', 'FUNCTION qlib.qlib.utils.paral.datetime_groupby_apply has parameter or accepts argument qlib.qlib.utils.paral.datetime_groupby_apply.df\ndatetime_groupby_apply This function will apply the `apply_func` on the datetime level index.  Parameters ---------- df :     DataFrame for processing apply_func : Union[Callable, Text]     apply_func for processing the data     if a string is given, then it is treated as naive pandas function axis :     which axis is the datetime level located level :     which level is the datetime level resample_rule :     How to resample the data to calculating parallel n_jobs :     n_jobs for joblib Returns:     pd.DataFrame', 'FUNCTION qlib.qlib.utils.paral.datetime_groupby_apply has parameter or accepts argument qlib.qlib.utils.paral.datetime_groupby_apply.axis\ndatetime_groupby_apply This function will apply the `apply_func` on the datetime level index.  Parameters ---------- df :     DataFrame for processing apply_func : Union[Callable, Text]     apply_func for processing the data     if a string is given, then it is treated as naive pandas function axis :     which axis is the datetime level located level :     which level is the datetime level resample_rule :     How to resample the data to calculating parallel n_jobs :     n_jobs for joblib Returns:     pd.DataFrame', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1913, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",30.50438690185547,1096
"When a specific fields_group is provided to the missing-value processor, how does it apply the fill_value to those columns?","When a specific `fields_group` is provided to the missing-value processor, it applies the `fill_value` to those columns by selecting only the specified columns using `df[self.fields_group]`, replacing missing values in these columns with `fill_value` using `fillna(self.fill_value)`, and then writing the filled columns back into the dataframe under the same column names. This process ensures that only the specified columns are affected, allowing for targeted handling of missing values.","In Fillna.__call__ (qlib/data/dataset/processor.py), if you pass a non-None fields_group, it does exactly:

```python
df[self.fields_group] = df[self.fields_group].fillna(self.fill_value)
```

i.e. it selects only those columns, applies `fillna(fill_value)` on them, and assigns them back.","['METHOD qlib.qlib.data.dataset.handler.DataHandler.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE\nfetch data from underlying data source  Design motivation: - providing a unified interface for underlying data. - Potential to make the interface more friendly. - User can improve performance when fetching data in this extra layer  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index     It can be categories as following      - fetch single index     - fetch a range of index          - a slice range         - pd.Index for specific indexes      Following conflicts may occur      - Does [""20200101"", ""20210101""] mean selecting this slice or these two days?          - slice have higher priorities  level : Union[str, int]     which index level to select the data  col_set : Union[str, List[str]]      - if isinstance(col_set, str):          select a set of meaningful, pd.Index columns.(e.g. features, columns)          - if col_set == CS_RAW:              the raw dataset will be returned.      - if isinstance(col_set, List[str]):          select several sets of meaningful columns, the returned data has multiple levels  proc_func: Callable      - Give a hook for processing data before fetching     - An example to explain the necessity of the hook:          - A Dataset learned some processors to process data which is related to data segmentation         - It will apply them every time when preparing data.         - The learned processor require the dataframe remains the same format when fitting and applying         - However the data format will change according to the parameters.         - So the processors should be applied to the underlayer data.  squeeze : bool     whether squeeze columns and index  Returns ------- pd.DataFrame.', 'CLASS qlib.qlib.data.dataset.processor.Fillna has field or attribute or property qlib.qlib.data.dataset.processor.fields_group\nProcess NaN', 'METHOD qlib.qlib.data.dataset.handler.DataHandler.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandler.fetch.squeeze\nfetch data from underlying data source  Design motivation: - providing a unified interface for underlying data. - Potential to make the interface more friendly. - User can improve performance when fetching data in this extra layer  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index     It can be categories as following      - fetch single index     - fetch a range of index          - a slice range         - pd.Index for specific indexes      Following conflicts may occur      - Does [""20200101"", ""20210101""] mean selecting this slice or these two days?          - slice have higher priorities  level : Union[str, int]     which index level to select the data  col_set : Union[str, List[str]]      - if isinstance(col_set, str):          select a set of meaningful, pd.Index columns.(e.g. features, columns)          - if col_set == CS_RAW:              the raw dataset will be returned.      - if isinstance(col_set, List[str]):          select several sets of meaningful columns, the returned data has multiple levels  proc_func: Callable      - Give a hook for processing data before fetching     - An example to explain the necessity of the hook:          - A Dataset learned some processors to process data which is related to data segmentation         - It will apply them every time when preparing data.         - The learned processor require the dataframe remains the same format when fitting and applying         - However the data format will change according to the parameters.         - So the processors should be applied to the underlayer data.  squeeze : bool     whether squeeze columns and index  Returns ------- pd.DataFrame.', 'CLASS qlib.qlib.data.dataset.processor.Fillna has field or attribute or property qlib.qlib.data.dataset.processor.Fillna.fields_group\nProcess NaN', 'METHOD qlib.qlib.data.dataset.handler.DataHandler.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandler.fetch.proc_func\nfetch data from underlying data source  Design motivation: - providing a unified interface for underlying data. - Potential to make the interface more friendly. - User can improve performance when fetching data in this extra layer  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index     It can be categories as following      - fetch single index     - fetch a range of index          - a slice range         - pd.Index for specific indexes      Following conflicts may occur      - Does [""20200101"", ""20210101""] mean selecting this slice or these two days?          - slice have higher priorities  level : Union[str, int]     which index level to select the data  col_set : Union[str, List[str]]      - if isinstance(col_set, str):          select a set of meaningful, pd.Index columns.(e.g. features, columns)          - if col_set == CS_RAW:              the raw dataset will be returned.      - if isinstance(col_set, List[str]):          select several sets of meaningful columns, the returned data has multiple levels  proc_func: Callable      - Give a hook for processing data before fetching     - An example to explain the necessity of the hook:          - A Dataset learned some processors to process data which is related to data segmentation         - It will apply them every time when preparing data.         - The learned processor require the dataframe remains the same format when fitting and applying         - However the data format will change according to the parameters.         - So the processors should be applied to the underlayer data.  squeeze : bool     whether squeeze columns and index  Returns ------- pd.DataFrame.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [天天基金网](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=""~/.qlib/qlib_data/cn_fund_data"")\ndf = D.features(D.instruments(market=""all""), [""$DWJZ"", ""$LJJZ""], freq=""day"")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## 免责声明\n\n本项目仅供学习研究使用，不作为任何行为的指导和建议，由此而引发任何争议和纠纷，与本项目无任何关系', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 7, 'pr': 1914, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.250330924987793,1097
How does the symbol-fetching routine determine which suffix to append to each stock code and exclude invalid codes?,"The symbol-fetching routine in qlib determines which suffix to append to each stock code and excludes invalid codes based on the following rules:

1. Codes starting with ""6"" are appended with "".ss"".
2. Codes starting with ""0"" or ""3"" are appended with "".sz"".
3. Codes not starting with ""6"", ""0"", or ""3"" are mapped to None.
4. After the list comprehension, None entries are filtered out. This is implemented in the `get_hs_stock_symbols._get_symbol` function located in the file `scripts/data_collector/utils.py`. The raw codes are post-processed with a single list comprehension.","In get_hs_stock_symbols._get_symbol (scripts/data_collector/utils.py) the raw codes are post-processed with a single list-comprehension:

  • if a code startswith “6” → append “.ss”  
  • elif it startswith (“0” or “3”) → append “.sz”  
  • else → map to None  

Immediately after it filters out all None entries, so any code not matching those prefixes is dropped.","['METHOD qlib.scripts.dump_bin.DumpDataBase.__init__ has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.__init__.file_suffix\nParameters ---------- csv_path: str     stock data path or directory qlib_dir: str     qlib(dump) data director backup_dir: str, default None     if backup_dir is not None, backup qlib_dir to backup_dir freq: str, default ""day""     transaction frequency max_workers: int, default None     number of threads date_field_name: str, default ""date""     the name of the date field in the csv file_suffix: str, default "".csv""     file suffix symbol_field_name: str, default ""symbol""     symbol field name include_fields: tuple     dump fields exclude_fields: tuple     fields not dumped limit_nums: int     Use when debugging, default None', 'FUNCTION qlib.scripts.data_collector.utils.get_hs_stock_symbols has parameter or accepts argument qlib.scripts.data_collector.utils.MINIMUM_SYMBOLS_NUM\nget SH/SZ stock symbols  Returns -------     stock symbols', 'METHOD qlib.scripts.dump_bin.DumpDataBase.__init__ has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.__init__.qlib_dir\nParameters ---------- csv_path: str     stock data path or directory qlib_dir: str     qlib(dump) data director backup_dir: str, default None     if backup_dir is not None, backup qlib_dir to backup_dir freq: str, default ""day""     transaction frequency max_workers: int, default None     number of threads date_field_name: str, default ""date""     the name of the date field in the csv file_suffix: str, default "".csv""     file suffix symbol_field_name: str, default ""symbol""     symbol field name include_fields: tuple     dump fields exclude_fields: tuple     fields not dumped limit_nums: int     Use when debugging, default None', 'FUNCTION qlib.scripts.data_collector.utils.get_in_stock_symbols has parameter or accepts argument qlib.scripts.data_collector.utils.get_in_stock_symbols.qlib_data_path\nget IN stock symbols  Returns -------     stock symbols', 'METHOD qlib.scripts.dump_bin.DumpDataBase.__init__ has parameter or accepts argument qlib.scripts.dump_bin.DumpDataBase.__init__.exclude_fields\nParameters ---------- csv_path: str     stock data path or directory qlib_dir: str     qlib(dump) data director backup_dir: str, default None     if backup_dir is not None, backup qlib_dir to backup_dir freq: str, default ""day""     transaction frequency max_workers: int, default None     number of threads date_field_name: str, default ""date""     the name of the date field in the csv file_suffix: str, default "".csv""     file suffix symbol_field_name: str, default ""symbol""     symbol field name include_fields: tuple     dump fields exclude_fields: tuple     fields not dumped limit_nums: int     Use when debugging, default None', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex ""^(600519|000725).*""\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1916, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",34.77181601524353,1098
What distinct preconditions do the two `_create_ts_slices` functions assert on the input index and parameters?,"The distinct preconditions asserted by the Qlib version of `_create_ts_slices` on the input index and parameters are:

1. `index` is an instance of `pd.MultiIndex`. 
2. `seq_len` is greater than 0. 
3. `index.is_monotonic_increasing`. 
4. After slicing, the length of slices equals the length of `index`.","The TRA version (examples/benchmarks/TRA/src/dataset.py:_create_ts_slices) only asserts:

• index.is_lexsorted()

The Qlib version (qlib/contrib/data/dataset.py:_create_ts_slices) adds several more pre- and post-conditions:

• isinstance(index, pd.MultiIndex)  
• seq_len > 0  
• index.is_monotonic_increasing  
• after slicing, len(slices) == len(index)","['CLASS qlib.qlib.model.meta.dataset.MetaTaskDataset has method qlib.qlib.model.meta.dataset.MetaTaskDataset.__init__\nThe meta-dataset maintains a list of meta-tasks when it is initialized.  The segments indicates the way to divide the data  The duty of the `__init__` function of MetaTaskDataset - initialize the tasks\nA dataset fetching the data in a meta-level.  A Meta Dataset is responsible for  - input tasks(e.g. Qlib tasks) and prepare meta tasks      - meta task contains more information than normal tasks (e.g. input data for meta model)  The learnt pattern could transfer to other meta dataset. The following cases should be supported  - A meta-model trained on meta-dataset A and then applied to meta-dataset B      - Some pattern are shared between meta-dataset A and B, so meta-input on meta-dataset A are used when meta model are applied on meta-dataset-B', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index.data_index', 'CLASS qlib.qlib.utils.index_data.SingleData has method qlib.qlib.utils.index_data.SingleData.reindex\nreindex data and fill the missing value with np.nan.  Parameters ---------- new_index : list     new index fill_value:     what value to fill if index is missing  Returns ------- SingleData     reindex data', 'METHOD qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index has parameter or accepts argument qlib.qlib.data.dataset.__init__.TSDataSampler.slice_idx_map_and_data_index.idx_map', 'METHOD qlib.qlib.backtest.utils.TradeCalendarManager.get_range_idx has parameter or accepts argument qlib.qlib.backtest.utils.TradeCalendarManager.get_range_idx.start_time\nget the range index which involve start_time~end_time  (both sides are closed)  Parameters ---------- start_time : pd.Timestamp end_time : pd.Timestamp  Returns ------- Tuple[int, int]:     the index of the range.  **the left and right are closed**', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379±0.00 | 0.2959±0.04 | 0.0464±0.00 | 0.3825±0.04 | 0.0465±0.02       | 0.5672±0.29       | -0.1282±0.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362±0.01 | 0.2789±0.06 | 0.0463±0.01 | 0.3661±0.05 | 0.0470±0.03       | 0.6992±0.47       | -0.1072±0.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349±0.00 | 0.2511±0.01 | 0.0462±0.00 | 0.3564±0.01 | 0.0497±0.01       | 0.7338±0.19       | -0.0777±0.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404±0.00 | 0.3197±0.05 | 0.0490±0.00 | 0.4047±0.04 | 0.0649±0.02       | 1.0091±0.30       | -0.0860±0.02 |\n| Linear                                   | Alpha158                            | 0.0397±0.00 | 0.3000±0.00 | 0.0472±0.00 | 0.3531±0.00 | 0.0692±0.00       | 0.9209±0.00       | -0.1509±0.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440±0.00 | 0.3535±0.05 | 0.0540±0.00 | 0.4451±0.03 | 0.0718±0.02       | 1.0835±0.35       | -0.0760±0.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481±0.00 | 0.3366±0.00 | 0.0454±0.00 | 0.3311±0.00 | 0.0765±0.00       | 0.8032±0.01       | -0.1092±0.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498±0.00 | 0.3779±0.00 | 0.0505±0.00 | 0.4131±0.00 | 0.0780±0.00       | 0.9070±0.00       | -0.1168±0.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358±0.00 | 0.2160±0.03 | 0.0116±0.01 | 0.0720±0.03 | 0.0847±0.02       | 0.8131±0.19       | -0.1824±0.03 |\n| MLP                                      | Alpha158                            | 0.0376±0.00 | 0.2846±0.02 | 0.0429±0.00 | 0.3220±0.01 | 0.0895±0.02       | 1.1408±0.23       | -0.1103±0.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448±0.00 | 0.3660±0.00 | 0.0469±0.00 | 0.3877±0.00 | 0.0901±0.00       | 1.0164±0.00       | -0.1038±0.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521±0.00 | 0.4223±0.01 | 0.0502±0.00 | 0.4117±0.01 | 0.1158±0.01       | 1.3432±0.11       | -0.0920±0.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114±0.00 | 0.0716±0.03 | 0.0327±0.00 | 0.2248±0.02 | -0.0270±0.03      | -0.3378±0.37      | -0.1653±0.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099±0.00 | 0.0593±0.00 | 0.0290±0.00 | 0.1887±0.00 | -0.0369±0.00      | -0.3892±0.00      | -0.2145±0.00 |\n| MLP                                       | Alpha360 | 0.0273±0.00 | 0.1870±0.02 | 0.0396±0.00 | 0.2910±0.02 | 0.0029±0.02       | 0.0274±0.23       | -0.1385±0.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404±0.00 | 0.2932±0.04 | 0.0542±0.00 | 0.4110±0.03 | 0.0246±0.02       | 0.3211±0.21       | -0.1095±0.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378±0.00 | 0.2714±0.00 | 0.0467±0.00 | 0.3659±0.00 | 0.0292±0.00       | 0.3781±0.00       | -0.0862±0.00 |', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'hard', 'found_stats': {'path': 64}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 64, 'n_files_pr': 43, 'pr': 1917, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",33.377697706222534,1099
How does the constructor decide whether to use the backend's args or kwargs to set the child task limit based on joblib version?,"The constructor of the ParallelExt class in qlib decides whether to use the backend's args or kwargs to set the child task limit based on the joblib version as follows: If the joblib version is less than 1.5.0, it assigns `self._backend_args[","In ParallelExt.__init__ (qlib/utils/parallel.py), after popping out maxtasksperchild and calling the parent ctor, it does:

– If the backend is a MultiprocessingBackend  
  • If joblib.__version__ < ""1.5.0"", it assigns  
    self._backend_args[""maxtasksperchild""] = maxtasksperchild  
  • Else (joblib ≥ 1.5.0), it assigns  
    self._backend_kwargs[""maxtasksperchild""] = maxtasksperchild","['CLASS qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution has method qlib.qlib.rl.order_execution.simulator_qlib.SingleAssetOrderExecution.__init__\nSingle-asset order execution (SAOE) simulator which is implemented based on Qlib backtest tools.  Parameters ---------- order     The seed to start an SAOE simulator is an order. executor_config     Executor configuration exchange_config     Exchange configuration qlib_config     Configuration used to initialize Qlib. If it is None, Qlib will not be initialized. cash_limit:     Cash limit.', 'CLASS qlib.qlib.utils.paral.ParallelExt inherits from or is a subclass of joblib.Parallel', 'METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.step\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', 'CLASS qlib.qlib.utils.paral.ParallelExt has method qlib.qlib.utils.paral.ParallelExt.__init__', 'METHOD qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.__init__ has parameter or accepts argument qlib.qlib.contrib.meta.data_selection.dataset.MetaDatasetDS.hist_step_n\nA dataset for meta model.  Parameters ---------- task_tpl : Union[dict, list]     Decide what tasks are used.     - dict : the task template, the prepared task is generated with `step`, `trunc_days` and `RollingGen`     - list : when list, use the list of tasks directly              the list is supposed to be sorted according timeline step : int     the rolling step trunc_days: int     days to be truncated based on the test start rolling_ext_days: int     sometimes users want to train meta models for a longer test period but with smaller rolling steps for more task samples.     the total length of test periods will be `step + rolling_ext_days`  exp_name : Union[str, InternalData]     Decide what meta_info are used for prediction.     - str: the name of the experiment to store the performance of data     - InternalData: a prepared internal data segments: Union[Dict[Text, Tuple], float]     if the segment is a Dict         the segments to divide data         both left and right are included     if segments is a float:         the float represents the percentage of data for training     if segments is a string:         it will try its best to put its data in training and ensure that the date `segments` is in the test set hist_step_n: int     length of historical steps for the meta infomation     Number of steps of the data similarity information task_mode : str     Please refer to the docs of MetaTask', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1921, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",31.751688957214355,1100
What are the main steps GeneralPTNN executes in its fit method from data loading to saving the best model?,"The main steps GeneralPTNN executes in its fit method from data loading to saving the best model are:

1. Data preparation:
* Dataset preparation: dataset.prepare is called with argument “train” or “valid”.
* Optional Reweighter application.
* Filling missing values using forward fill and backward fill for TSDatasetH data.
* Converting non-TSDatasetH data to numpy.

2. DataLoader creation:
* Features, labels, and sample weights are wrapped in a ConcatDataset.
* train_loader and valid_loader are created using torch.utils.data.DataLoader.

3. Training loop:
* Iterating from epoch 0 to n_epochs.
* In each iteration:
	+ Calling train_epoch with train_loader.
	+ Computing train_score by calling test_epoch with train_loader.
	+ Computing val_score by calling test_epoch with valid_loader.
	+ Recording scores in evals_result.
	+ Updating lr_scheduler.step with val_score.
	+ Tracking the best val_score by making a deep copy of self.dnn_model.state_dict() for early stopping.

4. Finalize:
* Reloading the best_param into self.dnn_model.
* Saving the best_param to save_path using torch.save.
* Calling torch.cuda.empty_cache() if using GPU.","In qlib/contrib/model/pytorch_general_nn.py (GeneralPTNN.fit) the high-level steps are:

1. Data preparation  
   – call dataset.prepare(“train”/“valid”)  
   – apply optional Reweighter  
   – for TSDatasetH fillna (ffill+bfill), else convert to numpy  

2. DataLoader creation  
   – wrap (features, labels) + sample weights in ConcatDataset  
   – build train_loader & valid_loader via torch.utils.data.DataLoader  

3. Training loop (0…n_epochs)  
   – train_epoch(train_loader)  
   – test_epoch(train_loader) → train_score  
   – test_epoch(valid_loader) → val_score  
   – record scores in evals_result  
   – lr_scheduler.step(val_score)  
   – track best val_score via copy.deepcopy(self.dnn_model.state_dict()), implement early stopping  

4. Finalize  
   – reload best_param into self.dnn_model  
   – torch.save(best_param, save_path)  
   – torch.cuda.empty_cache() (if using GPU)","['CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has method qlib.qlib.workflow.record_temp.PortAnaRecord.list\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.model.pytorch_general_nn.GeneralPTNN.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_general_nn.GeneralPTNN.fit.save_path', 'MODULE qlib.qlib.workflow.record_temp contains CLASS qlib.qlib.workflow.record_temp.PortAnaRecord\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'METHOD qlib.qlib.contrib.model.pytorch_general_nn.GeneralPTNN.fit has parameter or accepts argument qlib.qlib.contrib.model.pytorch_general_nn.GeneralPTNN.fit.evals_result', 'CLASS qlib.qlib.workflow.record_temp.PortAnaRecord has field or attribute or property qlib.qlib.workflow.record_temp.PortAnaRecord.strategy_config\nThis is the Portfolio Analysis Record class that generates the analysis results such as those of backtest. This class inherits the ``RecordTemp`` class.  The following files will be stored in recorder  - report_normal.pkl & positions_normal.pkl:      - The return report and detailed positions of the backtest, returned by `qlib/contrib/evaluate.py:backtest` - port_analysis.pkl : The risk analysis of your portfolio, returned by `qlib/contrib/evaluate.py:risk_analysis`', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 4, 'pr': 1928, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",46.52471733093262,1101
Which transformation of the return series is used to compute volatility in geometric accumulation mode?,"In qlib, the transformation of the return series used to compute volatility in geometric accumulation mode is std = np.log(1 + r).std(ddof=1), where r is the return series. This method computes volatility as the standard deviation of log-returns.","In the “product” branch of risk_analysis (qlib/contrib/evaluate.py), volatility is computed as the standard deviation of log‐returns, i.e.  
std = np.log(1 + r).std(ddof=1).","['MODULE qlib.qlib.contrib.evaluate contains FUNCTION qlib.qlib.contrib.evaluate.risk_analysis\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'MODULE qlib.qlib.contrib.evaluate_portfolio contains FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_volatility_from_series', 'FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.mode\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', 'FUNCTION qlib.qlib.contrib.evaluate_portfolio.get_volatility_from_series has parameter or accepts argument qlib.qlib.contrib.evaluate_portfolio.get_volatility_from_series.r', 'FUNCTION qlib.qlib.contrib.evaluate.risk_analysis has parameter or accepts argument qlib.qlib.contrib.evaluate.risk_analysis.r\nRisk Analysis NOTE: The calculation of annualized return is different from the definition of annualized return. It is implemented by design. Qlib tries to cumulate returns by summation instead of production to avoid the cumulated curve being skewed exponentially. All the calculation of annualized returns follows this principle in Qlib.  Parameters ---------- r : pandas.Series     daily return series. N: int     scaler for annualizing information_ratio (day: 252, week: 50, month: 12), at least one of `N` and `freq` should exist freq: str     analysis frequency used for calculating the scaler, at least one of `N` and `freq` should exist mode: Literal[""sum"", ""product""]     the method by which returns are accumulated:     - ""sum"": Arithmetic accumulation (linear returns).     - ""product"": Geometric accumulation (compounded returns).', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1"" />\n</div>\n\n* Temporally correlated task sets <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}"" title=""\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}"" />, in this paper, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_5"" title=""\\mathcal{T}_5"" /> and <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}"" title=""\\mathcal{T}_{10}"" /> are used in <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />, <img src=""https://latex.codecogs.com/png.latex?T_2"" title=""T_2"" />, and <img src=""https://latex.codecogs.com/png.latex?T_3"" title=""T_3"" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=""https://latex.codecogs.com/png.latex?T_k"" title=""T_k"" /> refers to forecasting return of stock <img src=""https://latex.codecogs.com/png.latex?i"" title=""i"" /> as following,\n<div align=center>\n<img src=""https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1"" title=""r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1"" />\n</div>\n\n* In Qlib baseline, <img src=""https://latex.codecogs.com/png.latex?\\mathcal{T}_3"" title=""\\mathcal{T}_3"" />, is used in  <img src=""https://latex.codecogs.com/png.latex?T_1"" title=""T_1"" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md\n# ADD\n* Paper: [ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting](https://arxiv.org/abs/2012.06289).']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 1938, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",29.792221069335938,1102
Which exception is raised when the nfs-common package is not installed on Unix-like systems?,"The exception raised when the nfs-common package is not installed on Unix-like systems in qlib is `OSError` with the message “nfs-common is not found…”. This is because the `_mount_nfs_uri` function checks for the presence of the `nfs-common` package using the command `dpkg -l | grep nfs-common`, and raises an `OSError` if it produces no output.","In `_mount_nfs_uri` (qlib/__init__.py), if `dpkg -l | grep nfs-common` returns nothing, it raises an `OSError` (“nfs-common is not found…”).","['MODULE cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions contains CLASS cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsResourceNotFoundError\nException raised when a Neptune Analytics resource is not found.', 'MODULE qlib.qlib.utils.exceptions contains CLASS qlib.qlib.utils.exceptions.QlibException', 'CLASS cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsResourceNotFoundError has method cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsResourceNotFoundError.__init__\nException raised when a Neptune Analytics resource is not found.', 'MODULE qlib.qlib.utils.exceptions contains CLASS qlib.qlib.utils.exceptions.ExpAlreadyExistError\nExperiment already exists', 'CLASS cognee.cognee.infrastructure.databases.graph.neptune_driver.exceptions.NeptuneAnalyticsResourceNotFoundError inherits from or is a subclass of cognee.exceptions.CogneeValidationError\nException raised when a Neptune Analytics resource is not found.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 1941, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",26.91930913925171,1103
What is the flow of a fetch call from DataHandlerLP.fetch through to the underlying storage fetch method?,"The flow of a fetch call from DataHandlerLP.fetch through to the underlying storage fetch method in qlib is as follows:

1.  DataHandlerLP.fetch calls `_get_df_by_key(data_key)` to retrieve the data storage.
2.  If the data storage is an instance of `BaseHandlerStorage`, it calls the `fetch` method on the storage object with the `selector`, `level`, `col_set`, and `fetch_orig` arguments.

    a.  In this case, if the data storage is an instance of `NaiveDFStorage` or `HashingStockStorage`, it will call `fetch_df_by_col` and `fetch_df_by_index` methods to perform the actual fetching.
3.  If the data storage is a pandas.DataFrame, DataHandler._fetch_data calls `fetch_df_by_col(df, col_set)` and `fetch_df_by_index(df, selector, level, fetch_orig)`.

    a.  In this case, the actual fetching is performed by these two methods.","The call chain is roughly:

1. qlib/data/dataset/handler.py – DataHandlerLP.fetch(selector, level, col_set, data_key,…)
   • calls  
2. DataHandler._fetch_data(data_storage=self._get_df_by_key(data_key), selector, level, col_set, …)  
   – in the same file  

3. Inside _fetch_data:  
   a) If data_storage is a pandas.DataFrame  
      • fetch_df_by_col(df, col_set) → fetch_df_by_index(df, selector, level, fetch_orig)  
   b) Else if isinstance(data_storage, BaseHandlerStorage)  
      • data_storage.fetch(selector, level, col_set, fetch_orig)  

4. qlib/data/dataset/storage.py – BaseHandlerStorage.fetch (abstract)  
   • implemented by:  
   – NaiveDFStorage.fetch → fetch_df_by_col + fetch_df_by_index  
   – HashingStockStorage.fetch → per‐stock slice + fetch_df_by_col + fetch_df_by_index → concat  

And the final pd.DataFrame is returned back up to your original DataHandlerLP.fetch call.","[""CLASS qlib.qlib.data.dataset.storage.HashingStockStorage inherits from or is a subclass of qlib.qlib.data.dataset.storage.BaseHandlerStorage\nBase data storage for datahandler - pd.DataFrame is the default data storage format in Qlib datahandler - If users want to use custom data storage, they should define subclass inherited BaseHandlerStorage, and implement the following method\nHashing data storage for datahanlder - The default data storage pandas.DataFrame is too slow when randomly accessing one stock's data - HashingStockStorage hashes the multiple stocks' data(pandas.DataFrame) by the key `stock_id`. - HashingStockStorage hashes the pandas.DataFrame into a dict, whose key is the stock_id(str) and value this stock data(panda.DataFrame), it has the following format:     {         stock1_id: stock1_data,         stock2_id: stock2_data,         ...         stockn_id: stockn_data,     } - By the `fetch` method, users can access any stock data with much lower time cost than default data storage"", 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerLP.fetch.squeeze\nfetch data from underlying data source  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index. level : Union[str, int]     which index level to select the data. col_set : str     select a set of meaningful columns.(e.g. features, columns). data_key : str     the data to fetch:  DK_*. proc_func: Callable     please refer to the doc of DataHandler.fetch  Returns ------- pd.DataFrame:', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerABC.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DataHandlerABC.DK_I', 'METHOD qlib.qlib.data.dataset.handler.DataHandlerLP.fetch has parameter or accepts argument qlib.qlib.data.dataset.handler.DATA_KEY_TYPE\nfetch data from underlying data source  Parameters ---------- selector : Union[pd.Timestamp, slice, str]     describe how to select data by index. level : Union[str, int]     which index level to select the data. col_set : str     select a set of meaningful columns.(e.g. features, columns). data_key : str     the data to fetch:  DK_*. proc_func: Callable     please refer to the doc of DataHandler.fetch  Returns ------- pd.DataFrame:', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer', ""qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 18}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 18, 'n_files_pr': 4, 'pr': 1958, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'broad', 'type': 'open_question'}",43.50108027458191,1104
Which pylint warning is disabled in the base data handler's constructor?,The pylint warning W0246 is disabled in the __init__ method of DataHandlerABC in qlib.,"In `DataHandlerABC.__init__` (qlib/data/dataset/handler.py), the pylint warning `W0246` is disabled.","['CLASS qlib.qlib.workflow.online.update.PredUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the prediction in the Recorder', 'MODULE qlib.qlib.data.dataset.handler contains GLOBAL_VARIABLE qlib.qlib.data.dataset.handler.DATA_KEY_TYPE', 'CLASS qlib.qlib.workflow.online.update.LabelUpdater inherits from or is a subclass of qlib.qlib.workflow.online.update.DSBasedUpdater\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321\nUpdate the label in the recorder  Assumption - The label is generated from record_temp.SignalRecord.', 'CLASS qlib.qlib.contrib.data.handler.Alpha158 inherits from or is a subclass of data.dataset.handler.DataHandlerLP', ""CLASS qlib.qlib.workflow.online.update.DSBasedUpdater has method qlib.qlib.workflow.online.update.DSBasedUpdater.__init__\nInit PredUpdater.  Expected behavior in following cases:  - if `to_date` is greater than the max date in the calendar, the data will be updated to the latest date - if there are data before `from_date` or after `to_date`, only the data between `from_date` and `to_date` are affected.  Args:     record : Recorder     to_date :         update to prediction to the `to_date`          if to_date is None:              data will updated to the latest date.     from_date :         the update will start from `from_date`          if from_date is None:              the updating will occur on the next tick after the latest data in historical data     hist_ref : int         Sometimes, the dataset will have historical depends.         Leave the problem to users to set the length of historical dependency         If user doesn't specify this parameter, Updater will try to load dataset to automatically determine the hist_ref          .. note::              the start_time is not included in the `hist_ref`; So the `hist_ref` will be `step_len - 1` in most cases      loader_cls : type         the class to load the model and dataset\nDataset-Based Updater  - Providing updating feature for Updating data based on Qlib Dataset  Assumption  - Based on Qlib dataset - The data to be updated is a multi-level index pd.DataFrame. For example label, prediction.      .. code-block::                                   LABEL0         datetime   instrument         2021-05-10 SH600000    0.006965                    SH600004    0.003407         ...                         ...         2021-05-28 SZ300498    0.015748                    SZ300676   -0.001321"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n❗ Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can\'t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the ""qlib"" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```']","{'commit': '78b77e302b9cab90100d05c6c534e2ed13980860', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 1960, 'repo': 'https://github.com/microsoft/qlib.git', 'scope': 'deep', 'type': 'open_question'}",25.946335554122925,1105
"In the triplet insertion flow, how are deduplication tasks for entities and facts orchestrated before persisting the data?","In the triplet insertion flow, entity and fact de-duplication tasks are orchestrated before persisting data in Graphiti as follows:

1.  The `Graphiti.add_triplet` method is defined in `graphiti_core/graphiti.py`. This method takes care of adding triplets to the knowledge graph.
2.  Before adding a triplet, the `resolve_extracted_nodes` function is called from `graphiti_core/utils/maintenance/node_operations.py`. This function runs a hybrid search and uses the dedupe nodes LLM prompt to map each extracted EntityNode to an existing or new node.
3.  The `resolve_extracted_nodes` function returns two values: `resolved_nodes` and a `uuid_map` for edge pointer resolution.
4.  The `resolve_edge_pointers` function is then called, which uses the `uuid_map` to rewire the new edges to the deduped nodes.
5.  Both node and edge resolves use `semaphore_gather` to parallelize searches and LLM calls.

This process ensures that entities and facts are properly de-duplicated before being added to the knowledge graph, maintaining data consistency and integrity in Graphiti.","Entity and fact de-duplication in the triplet flow is entirely handled before the bulk write in Graphiti.add_triplet (graphiti_core/graphiti.py):

1. resolve_extracted_nodes (graphiti_core/utils/maintenance/node_operations.py)  
   – runs hybrid search + the dedupe_nodes.nodes LLM prompt to map each extracted EntityNode to an existing or new node  
   – returns resolved_nodes and a uuid_map for edge pointer resolution  

2. resolve_edge_pointers uses that uuid_map to rewire the new edge to the deduped nodes  

3. Fetch relevant/invalidation candidates via get_relevant_edges and get_edge_invalidation_candidates  

4. resolve_extracted_edge (graphiti_core/utils/maintenance/edge_operations.py)  
   – calls the dedupe_edges.resolve_edge LLM prompt to identify duplicate facts, contradictions and fact types  
   – returns a single resolved_edge plus lists of invalidated and duplicate edges  

5. add_nodes_and_edges_bulk persists the deduped resolved_nodes and [resolved_edge + invalidated_edges]  

Both node and edge resolves use semaphore_gather to parallelize searches and LLM calls.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.prompts.dedupe_nodes.NodeDuplicate inherits from or is a subclass of pydantic.BaseModel', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.duplicate_fact_ids', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.store_raw_episode_content\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 15}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 15, 'n_files_pr': 9, 'pr': 490, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",48.151419162750244,1106
What field names are used to specify the model in the LLM client config versus the embedder client config?,"The field names used to specify the model in the LLM client config versus embedder client config in Graphiti are:

* In the LLM client config: `model`
* In the embedder client config: `embedding_model`

These field names are used in the `LLMConfig` and `OpenAIEmbedderConfig` classes, respectively.","In GraphitiLLMConfig.create_client you pass your model in LLMConfig.model (and optionally small_model), e.g.:

• LLMConfig(api_key=…, model=self.model, small_model=self.small_model, …)

In GraphitiEmbedderConfig.create_client you use:

• AzureOpenAIEmbedderClient(…, model=self.model)  
• OpenAIEmbedderConfig(api_key=…, embedding_model=self.model)

So the OpenAIEmbedderConfig field is named embedding_model, whereas LLMConfig uses model.","['METHOD graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__.config\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.embedder.client.EmbedderConfig inherits from or is a subclass of pydantic.BaseModel', ""CLASS graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient has method graphiti.graphiti_core.llm_client.openai_generic_client.OpenAIGenericClient.__init__\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.\nOpenAIClient is a client class for interacting with OpenAI's language models.  This class extends the LLMClient and provides methods to initialize the client, get an embedder, and generate responses from the language model.  Attributes:     client (AsyncOpenAI): The OpenAI client used to interact with the API.     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.  Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, client: typing.Any = None):         Initializes the OpenAIClient with the provided configuration, cache setting, and client.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'CLASS graphiti.graphiti_core.embedder.voyage.VoyageAIEmbedderConfig inherits from or is a subclass of client.EmbedderConfig', 'METHOD graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__.client\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 1, 'pr': 493, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.476807832717896,1107
How does the code ensure additional edge attributes are included when saving and retrieving EntityEdge objects?,"The code ensures additional edge attributes are included when saving and retrieving EntityEdge objects by using `edge_data.update(self.attributes or {})` in the `EntityEdge.save` method, which updates the `edge_data` dictionary with custom key-value pairs from the `attributes`. Additionally, `get_entity_edge_from_record(record)` preserves extra attributes through save and load operations. Pydantic's serialization capabilities are also used to serialize dates and exclude embeddings when retrieving EntityEdge objects.","When saving, EntityEdge.save (graphiti_core/edges.py) builds its payload dict and then does

• edge_data.update(self.attributes or {})

so any custom key/​value in the edge’s .attributes gets written as relationship properties.  

When loading (in all “get…” methods and the full-text/similarity/search utils) each Cypher query returns

• properties(r) AS attributes

and get_entity_edge_from_record(record) feeds record['attributes'] into the EntityEdge.attributes field (then pops out the reserved keys). This round-trips any extra attributes you added.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.attributes', 'FUNCTION graphiti.mcp_server.graphiti_mcp_server.format_fact_result has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.format_fact_result.edge\nFormat an entity edge into a readable result.  Since EntityEdge is a Pydantic BaseModel, we can use its built-in serialization capabilities.  Args:     edge: The EntityEdge to format  Returns:     A dictionary representation of the edge with serialized dates and excluded embeddings', 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.edge_data', 'CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 22}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 22, 'n_files_pr': 8, 'pr': 501, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.88709545135498,1108
How is the effective group_id determined for add_memory when no group_id argument is provided?,"The effective group ID is determined by `config.group_id`, which defaults to ""default"" if there is no CLI or environment variable override. If a `group_id` argument is provided to the `add_memory` function, it will be assigned to `effective_group_id`. Otherwise, `effective_group_id` falls back to `config.group_id`. This means that when no `group_id` is passed to `add_memory`, the effective group ID will be `config.group_id`, which defaults to ""default"".","In add_memory (mcp_server/graphiti_mcp_server.py), you’ll see:

•  
effective_group_id = group_id if group_id is not None else config.group_id  
•  
group_id_str = str(effective_group_id) if effective_group_id is not None else ''  

So when you don’t pass a group_id, it falls back to the global config.group_id (as set by GraphitiConfig.from_cli_and_env, defaulting to “default” if no CLI/env override), then casts it to a string.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.effective_group_ids', 'MODULE graphiti.graphiti_core.telemetry.telemetry contains FUNCTION graphiti.graphiti_core.telemetry.telemetry.get_graphiti_version\nGet Graphiti version.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.effective_group_id', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.main\nMain function to run the Graphiti MCP server.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 14}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 14, 'n_files_pr': 5, 'pr': 512, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",32.23594284057617,1109
Which error is raised when an embedding in the batch response contains no values?,"The error raised when an embedding in the batch response contains no values is ValueError with the message ""Empty embedding values returned"". This error is raised by GeminiEmbedder.create_batch if `embedding.values` is empty in the batch response.","In `GeminiEmbedder.create_batch` (graphiti_core/embedder/gemini.py), if `embedding.values` is empty in the batch response it raises a  
ValueError(""Empty embedding values returned"").","['CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.embedder.client contains GLOBAL_VARIABLE graphiti.graphiti_core.embedder.client.EMBEDDING_DIM', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'MODULE graphiti.graphiti_core.nodes contains GLOBAL_VARIABLE graphiti.graphiti_core.nodes.name_embeddings', 'CLASS graphiti.graphiti_core.errors.EdgeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when an edge is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 515, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.830439805984497,1110
How is the small model name selected and applied to the LLM client configuration?,"The small model name is selected in the `GraphitiLLMConfig` class of the graphiti codebase. The selection process involves reading the `SMALL_MODEL_NAME` environment variable using the `from_env()` method, which defaults to `SMALL_LLM_MODEL` if unset or blank. Additionally, the `--small-model` command line argument can override this default value through the `from_cli_and_env()` method. The selected small model name is then passed into the LLMConfig in the `create_client()` method and handed off to either an OpenAIClient or AzureOpenAILLMClient.","In mcp_server/graphiti_mcp_server.py the small‐model name is chosen in GraphitiLLMConfig:

• from_env() reads os.environ['SMALL_MODEL_NAME'], defaults to SMALL_LLM_MODEL if unset or blank.  
• from_cli_and_env() will override config.small_model with args.small_model when you pass --small-model.  
• create_client() then passes self.small_model into the LLMConfig that gets handed off to OpenAIClient or AzureOpenAILLMClient.","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.from_cli_and_env\nCreate LLM configuration from CLI arguments, falling back to environment variables.\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.small_model\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig.create_client\nCreate an LLM client based on this configuration.  Returns:     LLMClient instance\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', ""CLASS graphiti.graphiti_core.llm_client.config.LLMConfig has field or attribute or property graphiti.graphiti_core.llm_client.config.small_model\nConfiguration class for the Language Learning Model (LLM).  This class encapsulates the necessary parameters to interact with an LLM API, such as OpenAI's GPT models. It stores the API key, model name, and base URL for making requests to the LLM service."", 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiLLMConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for the LLM client.  Centralizes all LLM-specific configuration parameters including API keys and model selection.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 5, 'n_files_pr': 2, 'pr': 516, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",36.10348963737488,1111
Which key holds the episode's content in the context passed to the attribute extraction prompt?,The key that holds the episode's content in the context passed to the attribute extraction prompt is **episode_content**.,"In `resolve_extracted_edge` (graphiti_core/utils/maintenance/edge_operations.py), the attribute‐extraction context uses the key `""episode_content""` to hold the episode’s content.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.password\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.edges.EntityEdge has field or attribute or property graphiti.graphiti_core.edges.EntityEdge.episodes', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.graphiti.AddEpisodeResults has field or attribute or property graphiti.graphiti_core.graphiti.AddEpisodeResults.episode', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.uri\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'easy', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 531, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.407871961593628,1112
How are edge type restrictions applied across the different edge search operations?,"Edge type restrictions are applied across different edge search operations in Graphiti by filtering edges based on their types before performing any date or node-label filters. This is achieved through the use of the `SearchFilters.edge_types` variable, which injects a clause into the Cypher WHERE clause that only returns edges with names matching the values in `$edge_types`. This approach ensures that edge type restrictions are consistently applied across various edge search operations, including `edge_fulltext_search`, `edge_similarity_search`, and `edge_bfs_search.`","All of the edge searches (full‐text, vector similarity and BFS) call the same helper:

• graphiti_core/search/search_filters.py → edge_search_filter_query_constructor()

If you set SearchFilters.edge_types, it injects

  AND r.name IN $edge_types

into the Cypher WHERE clause and binds $edge_types. That clause is prepended to every edge search (in edge_fulltext_search, edge_similarity_search and edge_bfs_search) before any date or node‐label filters.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'CLASS graphiti.graphiti_core.edges.Edge inherits from or is a subclass of abc.ABC', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.target_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.extracted_edge_types', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.relationship_type\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 2, 'n_context_nodes': 9, 'n_files_pr': 5, 'pr': 537, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",35.43863129615784,1113
What happens when an extracted edge references a source or target index outside the available nodes list?,"When an extracted edge references a source or target index outside the available nodes list in graphiti, it will trigger a logger.warning and skip creating any EntityEdge for that edge. This means that such edges will not be included in the knowledge graph.","In extract_edges (graphiti_core/utils/maintenance/edge_operations.py), any edge where  
`source_entity_id` or `target_entity_id` is not in 0…len(nodes)–1 triggers a  
logger.warning and is skipped (the loop does a continue), so no EntityEdge is created for it.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.extracted_edge_types', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.edge_operations.source_node_idx', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 539, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",28.780895233154297,1114
How do node label filters differ between the node-level and edge-level search filter query constructors?,"The node label filters differ between the node-level and edge-level search filter query constructors because the edge_search_filter_query_constructor applies the same labels to both ends of the relationship, whereas the node_search_filter_query_constructor only appends "" AND n:Label1|Label2"" to the Cypher query. This means that when searching for edges, the query will look for nodes with the specified labels at both the start and end of the edge, whereas when searching for nodes, the query will only look for nodes with the specified labels.","In node_search_filter_query_constructor (graphiti_core/search/search_filters.py) a node‐label filter simply appends:

  “ AND n:Label1|Label2”

to the Cypher.

In edge_search_filter_query_constructor, by contrast, the same labels are applied to both ends of the relationship:

  “ AND n:Label1|Label2 AND m:Label1|Label2”

(i.e. it filters on both the start node `n` and end node `m`).","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE graphiti.graphiti_core.search.search_filters contains FUNCTION graphiti.graphiti_core.search.search_filters.node_search_filter_query_constructor', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge.is_edge_alive_in_dimension\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', 'MODULE graphiti.graphiti_core.search.search_filters contains FUNCTION graphiti.graphiti_core.search.search_filters.edge_search_filter_query_constructor', 'CLASS cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge has method cognee.cognee.modules.graph.cognee_graph.CogneeGraphElements.Edge.add_attribute\nRepresents an edge in a graph, connecting two nodes. Attributes:     node1 (Node): The starting node of the edge.     node2 (Node): The ending node of the edge.     attributes (Dict[str, Any]): A dictionary of attributes associated with the edge.     directed (bool): A flag indicating whether the edge is directed or undirected.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 540, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",32.54143786430359,1115
How does the function prevent a trailing 'AND' in each group of date filter expressions?,"The function `edge_search_filter_query_constructor` prevents a trailing 'AND' in each group of date filter expressions by checking whether the current index `j` is not equal to the last index `len(and_filters) - 1`. If it's not the last index, it appends ' AND ' to `and_filter_query`, thus avoiding a trailing 'AND' in each group.","In edge_search_filter_query_constructor (graphiti_core/search/search_filters.py), each date‐filter block builds an and_filters list and then stitches it together with:

```
for j, f in enumerate(and_filters):
    and_filter_query += f
    if j != len(and_filters) - 1:
        and_filter_query += ' AND '
```

By only appending “ AND ” when j isn’t the last index, it avoids a trailing AND in each group.","['CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'CLASS graphiti.graphiti_core.search.search_filters.DateFilter has field or attribute or property graphiti.graphiti_core.search.search_filters.DateFilter.date', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', 'CLASS graphiti.graphiti_core.search.search_filters.DateFilter inherits from or is a subclass of pydantic.BaseModel', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', 'qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `""""`\n       - `exclude_fields`: fields not dumped, by default `""""""\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 541, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.254103899002075,1116
What is the sequence of operations from parsing a podcast message to generating deduplicated nodes and extracted edges?,"The sequence of operations from parsing a podcast message to generating deduplicated nodes and extracted edges in Graphiti is as follows:

1.  The main function in `examples/podcast/podcast_runner.py` calls `parse_podcast_messages()`. 
2.  `parse_podcast_messages()` returns a list of Message objects.
3.  The main function wraps each Message object in a RawEpisode.
4.  The main function invokes `Graphiti.add_episode or Graphiti.add_episode_bulk` with each RawEpisode.
5.  `Graphiti.add_episode and Graphiti.add_episode_bulk` create an EpisodicNode from a RawEpisode.
6.  `Graphiti.add_episode and Graphiti.add_episode_bulk` extract raw EntityNode objects.
7.  `Graphiti.add_episode and Graphiti.add_episode_bulk` call `resolve_extracted_nodes()` from `graphiti_core/utils/maintenance/node_operations.py`. 
8.  `resolve_extracted_nodes()` runs a hybrid search over each extracted EntityNode.
9.  `resolve_extracted_nodes()` builds an LLM prompt via `graphiti_core/prompts/dedupe_nodes.py` by calling nodes().
10. The LLM returns entity_resolutions, which yields resolved_nodes and a uuid_map.
11. `Graphiti.add_episode and Graphiti.add_episode_bulk` call `extract_edges()` from `graphiti_core/utils/maintenance/edge_operations.py`. 
12. `extract_edges()` prepares a context consisting of episode.content, resolved_nodes, previous episodes, and edge_type_map.
13. `extract_edges()` prompts via `graphiti_core/prompts/extract_edges.py` by calling edge(), using a reflection loop.
14. `extract_edges()` converts LLM output into EntityEdge objects with valid_at and invalid_at timestamps and source and target UUIDs.
15. Graphiti persists the deduplicated nodes into Neo4j.
16. Graphiti persists the extracted edges into Neo4j.","Here’s the end-to-end flow:

1. In examples/podcast/podcast_runner.py:main  
   – call parse_podcast_messages() → list of Message objects  
   – wrap each Message in a RawEpisode and invoke Graphiti.add_episode(…) or add_episode_bulk(…)

2. Inside Graphiti.add_episode / add_episode_bulk  
   a. create an EpisodicNode from your RawEpisode  
   b. **Extract raw EntityNode objects** (not shown in context)  
   c. call graphiti_core/utils/maintenance/node_operations.py::resolve_extracted_nodes()  
      – runs hybrid search over each extracted node  
      – builds LLM prompt via graphiti_core/prompts/dedupe_nodes.py::nodes()  
      – LLM returns entity_resolutions → you get resolved_nodes (and a uuid_map)  
   d. call graphiti_core/utils/maintenance/edge_operations.py::extract_edges()  
      – prepares context (episode.content, resolved_nodes, previous episodes, edge_type_map)  
      – prompts via graphiti_core/prompts/extract_edges.py::edge() (with reflexion loop)  
      – converts LLM output into EntityEdge objects with valid_at/invalid_at, source/target UUIDs  

3. finally, Graphiti persists the deduplicated nodes and extracted edges into Neo4j.","['MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.examples.podcast.transcript_parser contains FUNCTION graphiti.examples.podcast.transcript_parser.parse_podcast_messages', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.examples.podcast.transcript_parser contains FUNCTION graphiti.examples.podcast.transcript_parser.parse_conversation_file', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 7, 'n_files_pr': 6, 'pr': 564, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",63.31747364997864,1117
How does initialize_server apply the provided host argument to the MCP server configuration?,"The provided host argument is applied to the MCP server configuration by setting `mcp.settings.host = args.host` in the `initialize_server` function. This assignment takes place when `args.host` is set, and it applies the host setting to the running MCP server.","In initialize_server (mcp_server/graphiti_mcp_server.py) the `--host` CLI flag (or `MCP_SERVER_HOST` env var) is parsed into `args.host`, and if set you’ll see:

•  logger.info(…)

•  `mcp.settings.host = args.host`

before returning the final MCPConfig via `MCPConfig.from_cli(args)`. This direct assignment is what applies your host to the running MCP server.","['FUNCTION graphiti.mcp_server.graphiti_mcp_server.main has parameter or accepts argument graphiti.mcp_server.graphiti_mcp_server.logger\nMain function to run the Graphiti MCP server.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.client', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains FUNCTION graphiti.mcp_server.graphiti_mcp_server.main\nMain function to run the Graphiti MCP server.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.args', 'CLASS graphiti.mcp_server.graphiti_mcp_server.MCPConfig has method graphiti.mcp_server.graphiti_mcp_server.MCPConfig.from_cli\nCreate MCP configuration from CLI arguments.\nConfiguration for MCP server.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n#### Neo4j Configuration\n\nThe Docker Compose setup includes a Neo4j container with the following default configuration:\n\n- Username: `neo4j`\n- Password: `demodemo`\n- URI: `bolt://neo4j:7687` (from within the Docker network)\n- Memory settings optimized for development use\n\n#### Running with Docker Compose\n\nA Graphiti MCP container is available at: `zepai/knowledge-graph-mcp`. The latest build of this container is used by the Compose setup below.\n\nStart the services using Docker Compose:\n\n```bash\ndocker compose up\n```\n\nOr if you\'re using an older version of Docker Compose:\n\n```bash\ndocker-compose up\n```\n\nThis will start both the Neo4j database and the Graphiti MCP server. The Docker setup:\n\n- Uses `uv` for package management and running the server\n- Installs dependencies from the `pyproject.toml` file\n- Connects to the Neo4j container using the environment variables\n- Exposes the server on port 8000 for HTTP-based SSE transport\n- Includes a healthcheck for Neo4j to ensure it\'s fully operational before starting the MCP server\n\n## Integrating with MCP Clients\n\n### Configuration\n\nTo use the Graphiti MCP server with an MCP-compatible client, configure it to connect to the server:\n\n> [!IMPORTANT]\n> You will need the Python package manager, `uv` installed. Please refer to the [`uv` install instructions](https://docs.astral.sh/uv/getting-started/installation/).\n>\n> Ensure that you set the full path to the `uv` binary and your Graphiti project folder.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""transport"": ""stdio"",\n      ""command"": ""/Users/<user>/.local/bin/uv"",\n      ""args"": [\n        ""run"",\n        ""--isolated"",\n        ""--directory"",\n        ""/Users/<user>>/dev/zep/graphiti/mcp_server"",\n        ""--project"",\n        ""."",\n        ""graphiti_mcp_server.py"",\n        ""--transport"",\n        ""stdio""\n      ],\n      ""env"": {\n        ""NEO4J_URI"": ""bolt://localhost:7687"",\n        ""NEO4J_USER"": ""neo4j"",\n        ""NEO4J_PASSWORD"": ""password"",\n        ""OPENAI_API_KEY"": ""sk-XXXXXXXX"",\n        ""MODEL_NAME"": ""gpt-4.1-mini""\n      }\n    }\n  }\n}\n```\n\nFor SSE transport (HTTP-based), you can use this configuration:\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""transport"": ""sse"",\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe Graphiti MCP server exposes the following tools:\n\n- `add_episode`: Add an episode to the knowledge graph (supports text, JSON, and message formats)\n- `search_nodes`: Search the knowledge graph for relevant node summaries\n- `search_facts`: Search the knowledge graph for relevant facts (edges between entities)\n- `delete_entity_edge`: Delete an entity edge from the knowledge graph\n- `delete_episode`: Delete an episode from the knowledge graph\n- `get_entity_edge`: Get an entity edge by its UUID\n- `get_episodes`: Get the most recent episodes for a specific group\n- `clear_graph`: Clear all data from the knowledge graph and rebuild indices\n- `get_status`: Get the status of the Graphiti MCP server and Neo4j connection']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 574, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",29.933976650238037,1118
"In the rank method's boolean evaluation, which string transformations are applied to the model's token response before comparing it to 'true'?","In the rank method's boolean evaluation, each processed choice’s token is run through strip(), split(' ')[0], and lower() before comparing it to 'true'.","In OpenAIRerankerClient.rank (graphiti_core/cross_encoder/openai_reranker_client.py), each choice’s token is run through:

• strip()  
• split(' ')[0]  
• lower()  

before comparing to “true”.","['CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient has method graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'MODULE graphiti.graphiti_core.cross_encoder.gemini_reranker_client contains GLOBAL_VARIABLE graphiti.graphiti_core.cross_encoder.gemini_reranker_client.DEFAULT_MODEL', 'CLASS keras.keras.src.legacy.preprocessing.image.ImageDataGenerator has method keras.keras.src.legacy.preprocessing.image.ImageDataGenerator.fit\nFits the data generator to some sample data.  This computes the internal data stats related to the data-dependent transformations, based on an array of sample data.  Only required if `featurewise_center` or `featurewise_std_normalization` or `zca_whitening` are set to `True`.  When `rescale` is set to a value, rescaling is applied to sample data before computing the internal data stats.  Args:     x: Sample data. Should have rank 4.      In case of grayscale data,      the channels axis should have value 1, in case      of RGB data, it should have value 3, and in case      of RGBA data, it should have value 4.     augment: Boolean (default: False).         Whether to fit on randomly augmented samples.     rounds: Int (default: 1).         If using data augmentation (`augment=True`),         this is how many augmentation passes over the data to use.     seed: Int (default: None). Random seed.\nDEPRECATED.', 'MODULE graphiti.graphiti_core.search.search_helpers contains FUNCTION graphiti.graphiti_core.search.search_helpers.search_results_to_context_string\nReformats a set of SearchResults into a single string to pass directly to an LLM as context', 'METHOD graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__.client\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 579, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",27.845853328704834,1119
How does the server initialization select and configure LLM and embedding clients when Azure settings are provided versus default OpenAI?,"When Azure settings are provided versus default OpenAI in graphiti, the server initialization selects and configures LLM and embedding clients as follows:

- If Azure settings are provided, the server initialization uses the `AsyncAzureOpenAI` client to create separate clients for LLM and embedding services. The `LLMConfig` is created with the small model and model names specified in the Azure deployment.

- If default OpenAI settings are used, the server initialization creates an `OpenAIClient` with a default configuration and uses it as the LLM client.

- For embeddings, if Azure settings are provided, the server initialization uses the `AsyncAzureOpenAI` client to create an embedding client. The embedding model name is specified in the Azure deployment.

- If default OpenAI settings are used, the server initialization creates an `OpenAIEmbedder` with a default configuration and uses it as the embedder.","During startup (`initialize_graphiti` in mcp_server/graphiti_mcp_server.py) we call:

– config.llm.create_client()  
  • If azure_openai_endpoint is set:  
    – If azure_openai_use_managed_identity → use AsyncAzureOpenAI + managed identity token provider → AzureOpenAILLMClient  
    – Elif api_key → use AsyncAzureOpenAI + API key → AzureOpenAILLMClient  
    – Else → error  
  • Else (no Azure) → require OPENAI_API_KEY → return OpenAIClient  

– config.embedder.create_client()  
  • If azure_openai_endpoint is set: same managed-identity vs API-key logic → AzureOpenAIEmbedderClient  
  • Else if OPENAI_API_KEY → return OpenAIEmbedder  
  • Else → returns None  

Those clients are then passed into the Graphiti constructor.","['MODULE graphiti.graphiti_core.llm_client.openai_base_client contains CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.graphiti_core.embedder.azure_openai.AzureOpenAIEmbedderClient has method graphiti.graphiti_core.embedder.azure_openai.AzureOpenAIEmbedderClient.__init__\nWrapper class for AsyncAzureOpenAI that implements the EmbedderClient interface.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has method graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.__init__\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'METHOD graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__ has parameter or accepts argument graphiti.graphiti_core.llm_client.openai_client.OpenAIClient.__init__.config\nInitialize the OpenAIClient with the provided configuration, cache setting, and client.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     cache (bool): Whether to use caching for responses. Defaults to False.     client (Any | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.max_tokens\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 4, 'n_files_pr': 6, 'pr': 581, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",46.58992338180542,1120
"What sequence of actions is used to perform a fulltext relationship search, from provider-specific query construction through result mapping?","The sequence of actions used to perform a fulltext relationship search in graphiti is as follows:

1. The `edge_fulltext_search` function in `search_utils.py` is called with the query and group_ids parameters.
2. This function calls the `fulltext_query` function, which returns a fuzzy search string.
3. The `edge_search_filter_query_constructor` function is then called with the search filter parameter, which returns a WHERE-clause and params.
4. The `get_relationships_query` function in `graph_queries.py` is called with 'edge_name_and_fact' and db_type equal to driver.provider.
5. This function returns a provider-specific CALL db.index.fulltext.queryRelationships(...) call.
6. The pipeline embeds the filter and fuzzy query into a single Cypher string.
7. The pipeline awaits driver.execute_query with cypher, params=filter_params, query=fuzzy_query, group_ids, limit, and routing_='r'.
8. The pipeline maps each raw record via get_entity_edge_from_record defined in edges.py.
9. The function get_entity_edge_from_record returns EntityEdge instances.","The fulltext‐relationship search is implemented in graphiti_core/search/search_utils.py in edge_fulltext_search:

1. fulltext_query(query, group_ids) → a fuzzy search string  
2. edge_search_filter_query_constructor(search_filter) → WHERE-clause + params  
3. get_relationships_query('edge_name_and_fact', db_type=driver.provider) (in graphiti_core/graph_queries.py) → provider-specific `CALL db.index.fulltext.queryRelationships(...)`  
4. embed the filter and fuzzy query into a single Cypher string  
5. await driver.execute_query(cypher, params=filter_params, query=fuzzy_query, group_ids=…, limit=…, routing_='r')  
6. map each raw record via get_entity_edge_from_record (in graphiti_core/edges.py) → EntityEdge instances  

This pipeline handles provider differences (Neo4j vs FalkorDB) in the call generation step and then normalizes all results into your domain model.","['MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.RelationshipModel\nRepresents a relationship between two entities in a model.  This class holds the type of the relationship and the identifiers for the source and target entities. It includes the following public instance variables:  - type: A string indicating the type of relationship. - source: A string representing the source entity of the relationship. - target: A string representing the target entity of the relationship.', 'MODULE graphiti.graphiti_core.search.search_utils contains FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query', 'CLASS cognee.cognee.tasks.graph.models.RelationshipModel inherits from or is a subclass of pydantic.BaseModel\nRepresents a relationship between two entities in a model.  This class holds the type of the relationship and the identifiers for the source and target entities. It includes the following public instance variables:  - type: A string indicating the type of relationship. - source: A string representing the source entity of the relationship. - target: A string representing the target entity of the relationship.', 'MODULE graphiti.graphiti_core.graph_queries contains FUNCTION graphiti.graphiti_core.graph_queries.get_relationships_query', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 26}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 26, 'n_files_pr': 11, 'pr': 583, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",43.249789237976074,1121
"When adding a memory, what condition leads to spawning the background task that processes the queue for its group?","The condition that leads to spawning the background task `process_episode_queue` when adding a memory in Graphiti is when `queue_workers.get(group_id_str, False)` is `False` or missing. This means that there isn't already a running worker for the given group ID, and therefore, a new worker needs to be spawned to process the queue for its group.","In add_memory (mcp_server/graphiti_mcp_server.py) after you enqueue the episode, it does:

```python
if not queue_workers.get(group_id_str, False):
    asyncio.create_task(process_episode_queue(group_id_str))
```

So the background task is spawned only if there isn’t already a running worker for that group_id (i.e. `queue_workers[group_id]` is False or missing).","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.episode_queues', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.cls\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.utils.maintenance.temporal_operations contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.maintenance.temporal_operations.new_edge_context', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep\'s memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we\'ve demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe\'re excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=""center"">\n    <a href=""https://arxiv.org/abs/2501.13956""><img src=""images/arxiv-screenshot.png"" alt=""Zep: A Temporal Knowledge Graph Architecture for Agent Memory"" width=""700px""></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=""center"">\n    <img src=""/images/graphiti-intro-slides-stock-2.gif"" alt=""Graphiti structured + unstructured demo"" width=""700px"">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 588, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",29.975090980529785,1122
How are the explicit connection parameters from the demo script applied when instantiating the database client?,"When instantiating the FalkorDriver class, the explicit connection parameters from the demo script are applied by being passed directly to the FalkorDB instance creation within the FalkorDriver.__init__ method. If no falkor_db instance is supplied, FalkorDriver.__init__ calls FalkorDB(host=falkor_host, port=falkor_port, username=falkor_username, password=falkor_password) and assigns the created FalkorDB instance to self.client. This means that the connection parameters are used to create a new FalkorDB instance, which is then assigned to the driver's client attribute.","The demo’s main() passes its falkor_host, falkor_port, falkor_username and falkor_password directly into the FalkorDriver constructor in examples/quickstart/quickstart_falkordb.py. In graphiti_core/driver/falkordb_driver.py FalkorDriver.__init__ simply does:

• if no falkor_db instance is supplied, it calls  
  self.client = FalkorDB(host=host, port=port, username=username, password=password)  

thus forwarding your explicit parameters into the FalkorDB client.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'METHOD graphiti.graphiti_core.driver.neo4j_driver.Neo4jDriver.session has parameter or accepts argument graphiti.graphiti_core.driver.neo4j_driver.Neo4jDriver.session.database', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.user\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.graph_driver\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 2, 'n_files_pr': 2, 'pr': 589, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",46.714285135269165,1123
"Which functions are involved in identifying, filtering, and constructing IS_DUPLICATE_OF edges during episode addition?","The functions involved in identifying, filtering, and constructing IS_DUPLICATE_OF edges during episode addition in Graphiti are:

1. `resolve_extracted_nodes`: This function calls the LLM to identify node duplicates.
2. `filter_existing_duplicate_of_edges`: This function drops any IS_DUPLICATE_OF edges already in the database.
3. `build_duplicate_of_edges`: This function constructs new EntityEdge objects named “IS_DUPLICATE_OF” and takes the filtered node_pairs as input.

These functions are part of the Graphiti codebase, specifically in the `graphiti_core/utils/maintenance/node_operations.py` and `graphiti_core/utils/maintenance/edge_operations.py` modules.","During add_episode the “duplicate” workflow spans:

• resolve_extracted_nodes (graphiti_core/utils/maintenance/node_operations.py) – calls the LLM to identify node duplicates, then invokes filter_existing_duplicate_of_edges (to drop any IS_DUPLICATE_OF edges already in the DB)  
• build_duplicate_of_edges (graphiti_core/utils/maintenance/edge_operations.py) – takes the filtered node_pairs and constructs new EntityEdge objects named “IS_DUPLICATE_OF”  
• Graphiti.add_episode then merges these edges into the episode’s edge set before bulk‐inserting.","['CLASS graphiti.graphiti_core.errors.EdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a list of edges is not found.', 'MODULE graphiti.graphiti_core.utils.maintenance.edge_operations contains FUNCTION graphiti.graphiti_core.utils.maintenance.edge_operations.build_duplicate_of_edges', 'CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'MODULE graphiti.graphiti_core.edges contains FUNCTION graphiti.graphiti_core.edges.get_episodic_edge_from_record', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods’ features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 5, 'n_files_pr': 4, 'pr': 599, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",33.203957080841064,1124
"In the add_episode workflow, how are default edge type signatures determined and passed to the edge extraction step when no custom map is supplied?","In the add_episode workflow of Graphiti, default edge type signatures are determined by building an edge_type_map_default as {('Entity','Entity'): list(edge_types.keys())} when no edge_type_map is passed to Graphiti.add_episode. This map is then passed to extract_edges which inverts the provided edge_type_map into edge_type_signature_map. When no custom signature map is supplied, extract_edges assigns the signature ('Entity','Entity') to every edge type name.","In Graphiti.add_episode (graphiti_core/graphiti.py) you’ll see:

• If you don’t pass in an edge_type_map, it builds  
  edge_type_map_default = {('Entity','Entity'): list(edge_types.keys())}  
  (or `{('Entity','Entity'): []}` if you didn’t supply any edge_types).

• That default map is handed straight into extract_edges.

Then in extract_edges (graphiti_core/utils/maintenance/edge_operations.py) it inverts that map into edge_type_signature_map so every edge type name you defined ends up with the signature `('Entity','Entity')` when no custom signature map was provided.","['CLASS cognee.cognee.tasks.graph.models.OntologyEdge inherits from or is a subclass of pydantic.BaseModel\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'MODULE cognee.cognee.modules.graph.utils.retrieve_existing_edges contains GLOBAL_VARIABLE cognee.cognee.modules.graph.utils.retrieve_existing_edges.type_entity_edges', 'MODULE cognee.cognee.tasks.graph.models contains CLASS cognee.cognee.tasks.graph.models.OntologyEdge\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', 'FUNCTION keras.keras.src.utils.model_visualization.add_edge has parameter or accepts argument keras.keras.src.utils.model_visualization.add_edge.src', 'CLASS cognee.cognee.tasks.graph.models.OntologyEdge has field or attribute or property cognee.cognee.tasks.graph.models.OntologyEdge.target_id\nRepresent an edge in an ontology, connecting a source and target with a specific relationship type.  The class includes the following instance variables: - id: A unique identifier for the edge. - source_id: The identifier of the source node. - target_id: The identifier of the target node. - relationship_type: The type of relationship represented by this edge, defining how the source and target are related.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## 💫 Contributors\n\nThanks to our amazing contributors! 💖\n\n<a href=""https://github.com/topoteretes/cognee/graphs/contributors"">\n  <img src=""https://contrib.rocks/image?repo=topoteretes/cognee"" />\n</a>\n\n## 🏆 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 5, 'n_files_pr': 6, 'pr': 600, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.563401699066162,1125
Which field in the duplicate model warns not to include JSON formatting in its value?,The `name` field on the NodeDuplicate model located in graphiti_core/prompts/dedupe_nodes.py warns not to include any JSON formatting in its value. This is a warning advising against including JSON formatting in this specific field.,The `name` field on the NodeDuplicate model (in graphiti_core/prompts/dedupe_nodes.py) has the warning not to include any JSON formatting in its value.,"['MODULE cognee.cognee.cli.echo contains FUNCTION cognee.cognee.cli.echo.warning\nPrint a warning in yellow', 'CLASS graphiti.graphiti_core.prompts.dedupe_nodes.Prompt has field or attribute or property graphiti.graphiti_core.prompts.dedupe_nodes.Prompt.node_list', ""CLASS cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint has method cognee.cognee.infrastructure.engine.models.DataPoint.DataPoint.to_json\nSerialize the DataPoint instance to a JSON string format.  This method uses the model's built-in serialization functionality to convert the instance into a JSON-compatible string.  Returns: --------      - str: The JSON string representation of the DataPoint instance.\nModel representing a data point with versioning and metadata support.  Public methods include: - get_embeddable_data - get_embeddable_properties - get_embeddable_property_names - update_version - to_json - from_json - to_pickle - from_pickle - to_dict - from_dict"", 'CLASS graphiti.graphiti_core.prompts.dedupe_nodes.Prompt has field or attribute or property graphiti.graphiti_core.prompts.dedupe_nodes.Prompt.nodes', 'FUNCTION cognee.cognee.cli.echo.warning has parameter or accepts argument cognee.cognee.cli.echo.warning.message\nPrint a warning in yellow', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 2, 'pr': 605, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",26.778688192367554,1126
How does the Graphiti class enforce group ID validation across its single-episode and bulk-episode addition methods?,"The Graphiti class enforces group ID validation across its single-episode and bulk-episode addition methods by calling the shared utility `validate_group_id(group_id)` at the beginning of both `Graphiti.add_episode` and `Graphiti.add_episode_bulk`. If the given group ID fails validation, `validate_group_id(group_id)` raises an exception immediately, aborting the rest of the work. This ensures that all episode-addition paths enforce the same group-ID rules.","Both Graphiti.add_episode (in graphiti_core/graphiti.py) and Graphiti.add_episode_bulk start by calling the shared utility validate_group_id(group_id). If the given group_id fails validation, that function raises immediately and aborts the rest of the work. Thus every episode-add path enforces the same group-ID rules.","['CLASS graphiti.graphiti_core.nodes.EpisodeType has method graphiti.graphiti_core.nodes.EpisodeType.from_str\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'MODULE graphiti.graphiti_core.errors contains CLASS graphiti.graphiti_core.errors.GroupIdValidationError\nRaised when a group_id contains invalid characters.', 'MODULE graphiti.graphiti_core.nodes contains CLASS graphiti.graphiti_core.nodes.EpisodeType\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', 'CLASS graphiti.graphiti_core.errors.GroupIdValidationError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a group_id contains invalid characters.', 'CLASS graphiti.graphiti_core.nodes.EpisodeType has field or attribute or property graphiti.graphiti_core.nodes.episode_type\nEnumeration of different types of episodes that can be processed.  This enum defines the various sources or formats of episodes that the system can handle. It\'s used to categorize and potentially handle different types of input data differently.  Attributes: ----------- message : str     Represents a standard message-type episode. The content for this type     should be formatted as ""actor: content"". For example, ""user: Hello, how are you?""     or ""assistant: I\'m doing well, thank you for asking."" json : str     Represents an episode containing a JSON string object with structured data. text : str     Represents a plain text episode.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 2}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 2, 'n_files_pr': 3, 'pr': 618, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.474316358566284,1127
Which Graphiti class methods now use the instance’s max_coroutines when invoking the concurrent gather helper?,"The following Graphiti class methods now use the instance’s max_coroutines when invoking the concurrent gather helper: add_episode, build_communities, get_nodes_and_edges_by_episode, remove_episode.","In graphiti_core/graphiti.py, the following Graphiti methods now pass `self.max_coroutines` to `semaphore_gather`:

• add_episode  
• build_communities  
• get_nodes_and_edges_by_episode  
• remove_episode","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.graphiti.Graphiti has field or attribute or property graphiti.graphiti_core.graphiti.max_coroutines', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.USE_PARALLEL_RUNTIME', ""CLASS graphiti.graphiti_core.graphiti.Graphiti has method graphiti.graphiti_core.graphiti.Graphiti.__init__\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 7}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 2, 'n_context_nodes': 7, 'n_files_pr': 2, 'pr': 619, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.078609943389893,1128
What condition in the embedder configuration builder triggers selection of the default OpenAI setup over Azure?,"The condition that triggers selection of the default OpenAI setup over Azure is when `azure_openai_endpoint` is None, indicating that the environment variable AZURE_OPENAI_EMBEDDING_ENDPOINT is not set. This causes the code to fall back to the default OpenAI setup.","In `GraphitiEmbedderConfig.from_env` (mcp_server/graphiti_mcp_server.py) the code checks:

• If `AZURE_OPENAI_EMBEDDING_ENDPOINT` is set (i.e. `azure_openai_endpoint is not None`), it builds an Azure config.  
• Otherwise (`azure_openai_endpoint is None`), it falls back to the default OpenAI setup.","['MODULE graphiti.graphiti_core.llm_client.openai_base_client contains CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.server.graph_service.config.Settings has field or attribute or property graphiti.server.graph_service.config.Settings.openai_base_url', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has method graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient.__init__\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.graphiti_core.llm_client.openai_base_client.BaseOpenAIClient has field or attribute or property graphiti.graphiti_core.llm_client.openai_base_client.config\nBase client class for OpenAI-compatible APIs (OpenAI and Azure OpenAI).  This class contains shared logic for both OpenAI and Azure OpenAI clients, reducing code duplication while allowing for implementation-specific differences.', 'CLASS graphiti.server.graph_service.config.Settings has field or attribute or property graphiti.server.graph_service.config.Settings.openai_api_key', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 9}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 9, 'n_files_pr': 5, 'pr': 620, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",29.354734182357788,1129
"Can you outline the end-to-end flow for resolving extracted entities into unique nodes, from search results through the deduplication prompt to final duplicate filtering?","The end-to-end flow for resolving extracted entities into unique nodes in graphiti is as follows:

1. The `resolve_extracted_nodes` function iterates over each extracted EntityNode.
2. For each extracted node, it calls the search function with NODE_HYBRID_SEARCH_RRF to retrieve candidate nodes.
3. It builds existing nodes context and extracted nodes context from the search results.
4. It invokes the LLM client's generate response method with a prompt library dedupe nodes node context and response model NodeResolutions.
5. The LLM returns a list of entity resolutions, each containing id, duplicate_idx, name, and duplicates.
6. The code maps each entity resolution id to its extracted node.
7. It checks if the duplicate index is greater than or equal to zero; if so, it selects the corresponding existing node.
8. If the duplicate index is less than zero, it retains the extracted node.
9. It populates resolved nodes and uuid map.
10. The code collects raw duplicate pairs (resolved_node, existing_node) from entity resolution duplicates.
11. Finally, it calls filter_existing_duplicate_of_edges to remove duplicates implied by existing edges.","1. In graphiti_core/utils/maintenance/node_operations.py → resolve_extracted_nodes():  
   a. For each extracted EntityNode, call search(...) (NODE_HYBRID_SEARCH_RRF) to retrieve candidate_nodes.  
   b. Build existing_nodes_context (from search results) and extracted_nodes_context.  
   c. Invoke llm_client.generate_response( prompt_library.dedupe_nodes.node(context), response_model=NodeResolutions ) using the NodeDuplicate schema (graphiti_core/prompts/dedupe_nodes.py).  

2. LLM returns a list of entity_resolutions (each with id, duplicate_idx, name, duplicates):  
   – Map each resolution.id to its extracted_node;  
   – If duplicate_idx ≥0, pick that existing_node, else keep the extracted_node;  
   – Populate resolved_nodes and uuid_map;  
   – Collect raw duplicate pairs (resolved_node, existing_node) from duplicates[].  

3. Call filter_existing_duplicate_of_edges(driver, node_duplicates) (to remove duplicates implied by existing edges).  

4. Return (resolved_nodes, uuid_map, new_node_duplicates).","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.prompts.extract_nodes contains CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassificationTriple', 'MODULE cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges contains FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'CLASS graphiti.graphiti_core.prompts.extract_nodes.EntityClassification inherits from or is a subclass of pydantic.BaseModel', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.data_chunks\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 5, 'pr': 622, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",49.62658214569092,1130
How is the configured semaphore limit applied when initializing the Graphiti client?,"The configured semaphore limit is applied when initializing the Graphiti client by passing the SEMAPHORE_LIMIT value to the Graphiti constructor as the max_coroutines argument. This value is used by Graphiti to cap the number of concurrent coroutines using its internal asyncio.Semaphore, limiting them to SEMAPHORE_LIMIT.","In mcp_server/graphiti_mcp_server.py (in initialize_graphiti) you pass SEMAPHORE_LIMIT straight into the Graphiti constructor as max_coroutines:

```python
graphiti_client = Graphiti(
    …,
    max_coroutines=SEMAPHORE_LIMIT,
)
```

Graphiti then uses that value to back its internal asyncio.Semaphore, capping the number of concurrent coroutines to SEMAPHORE_LIMIT.","['MODULE graphiti.mcp_server.graphiti_mcp_server contains CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.helpers contains GLOBAL_VARIABLE graphiti.graphiti_core.helpers.SEMAPHORE_LIMIT', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.destroy_graph\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.mcp_server.graphiti_mcp_server contains GLOBAL_VARIABLE graphiti.mcp_server.graphiti_mcp_server.SEMAPHORE_LIMIT', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has field or attribute or property graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.use_custom_entities\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 7, 'pr': 623, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",30.743431329727173,1131
What parts of the episode ingestion pipeline leverage the exclusion list to skip specified entity types in both single and bulk processing?,"The parts of the episode ingestion pipeline that leverage the exclusion list to skip specified entity types in both single and bulk processing are:

1. `extract_nodes` function: This function checks the excluded_entity_types list and skips any extracted entities whose type is in this list.
2. `add_episode` function: This function passes the excluded_entity_types list to the `extract_nodes` function.
3. `add_episode_bulk` function: This function also passes the excluded_entity_types list to the `extract_nodes_and_edges_bulk` function, which calls `extract_nodes` with this list.

These functions are used for both single and bulk processing of episodes in graphiti.","The exclusion list is applied in two places:

• In single‐episode ingestion (graphiti_core/graphiti.py → Graphiti.add_episode) you pass excluded_entity_types into extract_nodes.  
• extract_nodes (graphiti_core/utils/maintenance/node_operations.py) checks excluded_entity_types and skips any extracted entity whose type is in that list.  
• In bulk ingestion (graphiti_core/graphiti.py → Graphiti.add_episode_bulk) you pass excluded_entity_types into extract_nodes_and_edges_bulk.  
• extract_nodes_and_edges_bulk (graphiti_core/utils/bulk_utils.py) in turn calls extract_nodes with excluded_entity_types, so the same filter is applied per episode.","['FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.chunk_graphs\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'MODULE graphiti.graphiti_core.utils.bulk_utils contains GLOBAL_VARIABLE graphiti.graphiti_core.utils.bulk_utils.previous_episodes_list', 'MODULE graphiti.graphiti_core.helpers contains FUNCTION graphiti.graphiti_core.helpers.validate_excluded_entity_types\nValidate that excluded entity types are valid type names.  Args:     excluded_entity_types: List of entity type names to exclude     entity_types: Dictionary of available custom entity types  Returns:     True if valid  Raises:     ValueError: If any excluded type names are invalid', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.existing_edges_map\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'FUNCTION cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges has parameter or accepts argument cognee.cognee.modules.graph.utils.expand_with_nodes_and_edges.expand_with_nodes_and_edges.ontology_resolver\n- LLM generated docstring Expand knowledge graphs with validated nodes and edges, integrating ontology information.  This function processes document chunks and their associated knowledge graphs to create a comprehensive graph structure with entity nodes, entity type nodes, and their relationships. It validates entities against an ontology resolver and adds ontology-derived nodes and edges to enhance the knowledge representation.  Args:     data_chunks (list[DocumentChunk]): List of document chunks that contain the source data.         Each chunk should have metadata about what entities it contains.     chunk_graphs (list[KnowledgeGraph]): List of knowledge graphs corresponding to each         data chunk. Each graph contains nodes (entities) and edges (relationships) extracted         from the chunk content.     ontology_resolver (BaseOntologyResolver, optional): Resolver for validating entities and         types against an ontology. If None, a default RDFLibOntologyResolver is created.         Defaults to None.     existing_edges_map (dict[str, bool], optional): Mapping of existing edge keys to prevent         duplicate edge creation. Keys are formatted as ""{source_id}_{target_id}_{relation}"".         If None, an empty dictionary is created. Defaults to None.  Returns:     tuple[list, list]: A tuple containing:         - graph_nodes (list): Combined list of data chunks and ontology nodes (EntityType and Entity objects)         - graph_edges (list): List of edge tuples in format (source_id, target_id, relationship_name, properties)  Note:     - Entity nodes are created for each entity found in the knowledge graphs     - EntityType nodes are created for each unique entity type     - Ontology validation is performed to map entities to canonical ontology terms     - Duplicate nodes and edges are prevented using internal mapping and the existing_edges_map     - The function modifies data_chunks in-place by adding entities to their \'contains\' attribute', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=""json""`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=""Customer Profile"",\nepisode_body=""{\\""company\\"": {\\""name\\"": \\""Acme Technologies\\""}, \\""products\\"": [{\\""id\\"": \\""P001\\"", \\""name\\"": \\""CloudSync\\""}, {\\""id\\"": \\""P002\\"", \\""name\\"": \\""DataMiner\\""}]}"",\nsource=""json"",\nsource_description=""CRM data""\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use ""default"" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  ""mcpServers"": {\n    ""graphiti-memory"": {\n      ""url"": ""http://localhost:8000/sse""\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor\'s User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti\'s knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you\'ll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      ""mcpServers"": {\n        ""graphiti-memory"": {\n          // You can choose a different name if you prefer\n          ""command"": ""npx"", // Or the full path to mcp-remote if npx is not in your PATH\n          ""args"": [\n            ""mcp-remote"",\n            ""http://localhost:8000/sse"" // Ensure this matches your Graphiti server\'s SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 5}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 3, 'n_context_nodes': 5, 'n_files_pr': 6, 'pr': 624, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",38.60335397720337,1132
How do the edge and node BFS search routines incorporate filter_query fragments into their Cypher templates before executing the query?,"The edge and node BFS search routines in graphiti incorporate filter_query fragments into their Cypher templates before executing the query by concatenating the filter_query with a Cypher snippet. For edge_bfs_search, it constructs its query string by concatenating `...MATCH … WHERE r.uuid = rel.uuid
`, `filter_query`, and `RETURN DISTINCT …`. For node_bfs_search, it constructs its query string by concatenating `...MATCH … WHERE n.group_id = origin.group_id
`, `filter_query`, `ENTITY_NODE_RETURN`, and `LIMIT $limit`. The filter_query is a Cypher snippet such as extra WHERE or AND clauses. The filter_params dictionary contains the corresponding parameters for the filter_query. Both edge_bfs_search and node_bfs_search pass filter_params and BFS parameters into driver.execute_query.","In both edge_bfs_search and node_bfs_search (in graphiti_core/search/search_utils.py) you’ll see they

1. Call the appropriate filter‐builder  
   • edge_search_filter_query_constructor(search_filter)  
   • node_search_filter_query_constructor(search_filter)  

   which each return  
   – filter_query: a Cypher snippet (e.g. extra WHERE/AND clauses)  
   – filter_params: the corresponding parameter dict  

2. Splice that snippet into their multi‐line query strings via string concatenation:

   edge_bfs_search:  
   ```python
   query = (
     ""...MATCH … WHERE r.uuid = rel.uuid\n""
     + filter_query +
     ""RETURN DISTINCT …""
   )
   ```

   node_bfs_search:  
   ```python
   query = (
     ""...MATCH … WHERE n.group_id = origin.group_id\n""
     + filter_query +
     ENTITY_NODE_RETURN +
     ""LIMIT $limit""
   )
   ```

3. Pass filter_params (alongside bfs params) into driver.execute_query.","['MODULE cognee.cognee.modules.retrieval.cypher_search_retriever contains CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'MODULE graphiti.graphiti_core.search.search_filters contains FUNCTION graphiti.graphiti_core.search.search_filters.node_search_filter_query_constructor', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever.system_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', 'MODULE graphiti.graphiti_core.search.search_filters contains FUNCTION graphiti.graphiti_core.search.search_filters.edge_search_filter_query_constructor', 'CLASS cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever has field or attribute or property cognee.cognee.modules.retrieval.cypher_search_retriever.CypherSearchRetriever.user_prompt_path\nRetriever for handling cypher-based search.  Public methods include: - get_context: Retrieves relevant context using a cypher query. - get_completion: Returns the graph connections context.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 2, 'pr': 626, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",38.03045463562012,1133
How can a user prevent the initial telemetry event from being sent when creating a Graphiti instance?,"To prevent the initial telemetry event from being sent when creating a Graphiti instance, you can set the `GRAPHITI_TELEMETRY_ENABLED` environment variable to `false`. This can be done in your shell profile or for a specific Python session by importing the `os` module and setting the environment variable before initializing the Graphiti instance. Alternatively, you can also set the `GRAPHITI_TELEMETRY_DISABLED` environment variable to `true` to achieve the same result.","Before you initialize Graphiti you can disable telemetry in two ways:

1. Set the env-var that the telemetry module checks, e.g. in your shell:  
   export GRAPHITI_TELEMETRY_DISABLED=true  

2. Or in Python, turn it off at runtime:  
   from graphiti_core.telemetry import disable_telemetry  
   disable_telemetry()  
   from graphiti_core.graphiti import Graphiti  
   g = Graphiti(...)  

Either approach causes Graphiti._capture_initialization_telemetry (in graphiti_core/graphiti.py) to no-op.","['MODULE graphiti.graphiti_core.telemetry.telemetry contains FUNCTION graphiti.graphiti_core.telemetry.telemetry.capture_event\nCapture a telemetry event.', 'CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig has method graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig.from_env\nCreate a configuration instance from environment variables.\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'FUNCTION graphiti.graphiti_core.telemetry.telemetry.capture_event has parameter or accepts argument graphiti.graphiti_core.telemetry.telemetry.capture_event.properties\nCapture a telemetry event.', 'MODULE graphiti.graphiti_core.telemetry.__init__ contains GLOBAL_VARIABLE graphiti.graphiti_core.telemetry.__init__.__all__', 'FUNCTION graphiti.graphiti_core.telemetry.telemetry.capture_event has parameter or accepts argument graphiti.graphiti_core.telemetry.telemetry.capture_event.event_name\nCapture a telemetry event.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Telemetry\n\nThe Graphiti MCP server uses the Graphiti core library, which includes anonymous telemetry collection. When you initialize the Graphiti MCP server, anonymous usage statistics are collected to help improve the framework.\n\n### What's Collected\n\n- Anonymous identifier and system information (OS, Python version)\n- Graphiti version and configuration choices (LLM provider, database backend, embedder type)\n- **No personal data, API keys, or actual graph content is ever collected**\n\n### How to Disable\n\nTo disable telemetry in the MCP server, set the environment variable:\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\nOr add it to your `.env` file:\n\n```\nGRAPHITI_TELEMETRY_ENABLED=false\n```\n\nFor complete details about what's collected and why, see the [Telemetry section in the main Graphiti README](../README.md#telemetry).\n\n## License\n\nThis project is licensed under the same license as the parent Graphiti project."", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 10, 'pr': 633, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",39.03456997871399,1134
How does fulltext_query differentiate the first group filter from subsequent ones when building the OR-separated list?,"The fulltext_query function differentiates the first group filter from subsequent ones when building the OR-separated list by using an inline conditional expression. The expression checks if the `group_ids_filter` is empty, and if it is, it appends the current filter without any ""OR"". If the `group_ids_filter` is not empty, it prepends "" OR "" before each filter. This way, the first filter does not have ""OR"" appended to it, while subsequent filters do.","In fulltext_query (graphiti_core/search/search_utils.py) the for-loop uses a simple emptiness check on the accumulating string:

```
for f in group_ids_filter_list:
    group_ids_filter += f if not group_ids_filter else f' OR {f}'
```

Since group_ids_filter starts out empty, the first filter is appended with no “OR”; thereafter the else-branch prepends “ OR ” to each subsequent filter.","['CLASS graphiti.graphiti_core.errors.GroupsEdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no edges are found for a list of group ids.', 'FUNCTION graphiti.graphiti_core.search.search_utils.fulltext_query has parameter or accepts argument graphiti.graphiti_core.search.search_utils.fulltext_query.group_ids', 'CLASS graphiti.graphiti_core.errors.GroupsNodesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when no nodes are found for a list of group ids.', 'FUNCTION graphiti.graphiti_core.graph_queries.get_fulltext_indices has parameter or accepts argument graphiti.graphiti_core.graph_queries.get_fulltext_indices.db_type', 'CLASS graphiti.graphiti_core.errors.EdgesNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a list of edges is not found.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 636, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",31.529738903045654,1135
Which error conditions cause the high-level response method to abort immediately without retrying?,"The high-level response method aborts immediately without retrying for the following error conditions in graphiti:

* RateLimitError: covers rate limit, quota, resource_exhausted, and HTTP 429 errors.
* Exceptions with messages containing ""safety"": originate from _check_safety_blocks or _check_prompt_blocks.
* Exceptions with messages containing ""blocked"": also originate from _check_safety_blocks or _check_prompt_blocks.","In graphiti_core/llm_client/gemini_client.py → GeminiClient.generate_response(), retries are skipped (i.e. the call aborts immediately) when:

• A RateLimitError is raised by _generate_response() (e.g. any “rate limit,” “quota,” “resource_exhausted” or HTTP 429 error)  
• Any Exception whose message contains “safety” or “blocked” (i.e. errors from _check_safety_blocks or _check_prompt_blocks)","['METHOD graphiti.server.graph_service.zep_graphiti.ZepGraphiti.__init__ has parameter or accepts argument graphiti.server.graph_service.zep_graphiti.ZepGraphiti.__init__.llm_client', 'MODULE graphiti.graphiti_core.llm_client.client contains FUNCTION graphiti.graphiti_core.llm_client.client.is_server_or_retry_error', 'MODULE keras.keras.src.ops.numpy contains FUNCTION keras.keras.src.ops.numpy.select\nReturn elements from `choicelist`, based on conditions in `condlist`.  Args:     condlist: List of boolean tensors.         The list of conditions which determine from which array         in choicelist the output elements are taken.         When multiple conditions are satisfied,         the first one encountered in condlist is used.     choicelist: List of tensors.         The list of tensors from which the output elements are taken.         This list has to be of the same length as `condlist`.     defaults: Optional scalar value.         The element inserted in the output         when all conditions evaluate to `False`.  Returns:     Tensor where the output at position `m` is the `m`-th element     of the tensor in `choicelist` where the `m`-th element of the     corresponding tensor in `condlist` is `True`.  Example:  ```python from keras import ops  x = ops.arange(6) condlist = [x<3, x>3] choicelist = [x, x**2] ops.select(condlist, choicelist, 42) # Returns: tensor([0,  1,  2, 42, 16, 25]) ```', 'MODULE graphiti.graphiti_core.llm_client.gemini_client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.gemini_client.DEFAULT_MODEL', 'FUNCTION keras.keras.src.ops.numpy.select has parameter or accepts argument keras.keras.src.ops.numpy.select.condlist\nReturn elements from `choicelist`, based on conditions in `condlist`.  Args:     condlist: List of boolean tensors.         The list of conditions which determine from which array         in choicelist the output elements are taken.         When multiple conditions are satisfied,         the first one encountered in condlist is used.     choicelist: List of tensors.         The list of tensors from which the output elements are taken.         This list has to be of the same length as `condlist`.     defaults: Optional scalar value.         The element inserted in the output         when all conditions evaluate to `False`.  Returns:     Tensor where the output at position `m` is the `m`-th element     of the tensor in `choicelist` where the `m`-th element of the     corresponding tensor in `condlist` is `True`.  Example:  ```python from keras import ops  x = ops.arange(6) condlist = [x<3, x>3] choicelist = [x, x**2] ops.select(condlist, choicelist, 42) # Returns: tensor([0,  1,  2, 42, 16, 25]) ```', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'hard', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 7, 'pr': 645, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.85584044456482,1136
What are the main steps FalkorDriver uses to execute a Cypher query and format its results?,"The main steps FalkorDriver uses to execute a Cypher query and format its results in graphiti are:

1. `self._get_graph(database_ or self._database)` is called to pick the target graph.
2. Any datetime parameters are converted to ISO strings using `convert_datetimes_to_strings`.
3. The `await graph.query(cypher_query_, params)` function is called with the Cypher query and parameters.
4. If an ""already indexed"" error occurs, it is handled specially.
5. A header list is built from `result.header` by extracting `h[1]` for each `h` in `result.header`.
6. The results are iterated over and zipped with the header to produce a list of dictionaries.
7. Missing fields are filled with `None` when constructing the list of dictionaries.
8. A tuple of `(records, header, None)` is returned.","In graphiti_core/driver/falkordb_driver.py, FalkorDriver.execute_query does the following:

1. Picks the target graph via self._get_graph(database_ or self._database).  
2. Converts any datetime parameters to ISO strings (via convert_datetimes_to_strings).  
3. Calls await graph.query(cypher_query_, params) and handles “already indexed” errors specially.  
4. Builds a header list from result.header ([h[1] for h in result.header]).  
5. Iterates result.result_set, zipping each row with the header to produce a list of dicts (filling missing fields with None).  
6. Returns (records, header, None).","['MODULE cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate contains FUNCTION cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate.run_migration_step\nUses the given python_exe to execute a short snippet that connects to the Kùzu database and runs a Cypher command.', 'FUNCTION graphiti.graphiti_core.graph_queries.get_nodes_query has parameter or accepts argument graphiti.graphiti_core.graph_queries.NEO4J_TO_FALKORDB_MAPPING', 'FUNCTION cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate.run_migration_step has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate.run_migration_step.db_path\nUses the given python_exe to execute a short snippet that connects to the Kùzu database and runs a Cypher command.', 'MODULE graphiti.graphiti_core.graph_queries contains GLOBAL_VARIABLE graphiti.graphiti_core.graph_queries.NEO4J_TO_FALKORDB_MAPPING', 'FUNCTION cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate.run_migration_step has parameter or accepts argument cognee.cognee.infrastructure.databases.graph.kuzu.kuzu_migrate.run_migration_step.python_exe\nUses the given python_exe to execute a short snippet that connects to the Kùzu database and runs a Cypher command.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 3}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 3, 'n_files_pr': 1, 'pr': 650, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",42.23166537284851,1137
How does the driver ensure that datetime parameters are converted to ISO-formatted strings before sending queries to FalkorDB?,"The driver ensures that datetime parameters are converted to ISO-formatted strings before sending queries to FalkorDB through the `convert_datetimes_to_strings` function, which recursively walks through input objects (dicts, lists, or tuples) and replaces any datetime objects with their corresponding ISO 8601 strings. This behavior is verified by two test cases: `TestFalkorDriver.test_execute_query_converts_datetime_parameters` and `TestFalkorDriverSession.test_run_converts_datetime_objects_to_iso_strings`. The function is defined in the `graphiti_core/driver/falkordb_driver.py` file.","Before submitting parameters to FalkorDB, both FalkorDriver.execute_query and FalkorDriverSession.run call the utility convert_datetimes_to_strings (in graphiti_core/driver/falkordb_driver.py). That function recursively walks your kwargs (dicts, lists, tuples) and replaces any datetime objects with datetime.isoformat() strings. Tests like TestFalkorDriver.test_execute_query_converts_datetime_parameters and TestFalkorDriverSession.test_run_converts_datetime_objects_to_iso_strings verify this behavior.","['MODULE cognee.cognee.infrastructure.databases.vector.pgvector.serialize_data contains FUNCTION cognee.cognee.infrastructure.databases.vector.pgvector.serialize_data.serialize_data\nRecursively convert various data types to serializable formats.  This function processes dictionaries and lists, converting any datetime objects to ISO 8601 strings, and UUID objects to their string representation. Other data types are returned unchanged. It handles recursive structures if present.  Parameters: -----------      - data: The input data to serialize, which can be a dict, list, datetime, UUID, or       other types.  Returns: --------      The serialized representation of the input data, with datetime objects converted to     ISO format and UUIDs to strings.', 'MODULE graphiti.graphiti_core.driver.falkordb_driver contains FUNCTION graphiti.graphiti_core.driver.falkordb_driver.convert_datetimes_to_strings', 'FUNCTION cognee.cognee.infrastructure.databases.vector.pgvector.serialize_data.serialize_data has parameter or accepts argument cognee.cognee.infrastructure.databases.vector.pgvector.serialize_data.serialize_data.data\nRecursively convert various data types to serializable formats.  This function processes dictionaries and lists, converting any datetime objects to ISO 8601 strings, and UUID objects to their string representation. Other data types are returned unchanged. It handles recursive structures if present.  Parameters: -----------      - data: The input data to serialize, which can be a dict, list, datetime, UUID, or       other types.  Returns: --------      The serialized representation of the input data, with datetime objects converted to     ISO format and UUIDs to strings.', 'FUNCTION graphiti.graphiti_core.driver.falkordb_driver.convert_datetimes_to_strings has parameter or accepts argument graphiti.graphiti_core.driver.falkordb_driver.convert_datetimes_to_strings.obj', 'CLASS graphiti.graphiti_core.driver.falkordb_driver.FalkorDriver has method graphiti.graphiti_core.driver.falkordb_driver.FalkorDriver.__init__\nInitialize the FalkorDB driver.  FalkorDB is a multi-tenant graph database. To connect, provide the host and port. The default parameters assume a local (on-premises) FalkorDB instance.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result\'s source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### ""Graph not found: default_db"" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn\'t exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=""your_db_name"")\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 25}, 'includes_code': False, 'includes_location_hints': False, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 25, 'n_files_pr': 8, 'pr': 655, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",45.41471290588379,1138
Which structured response classes are returned when no relevant facts are found and when no episodes are found?,"When no relevant facts are found, the FactSearchResponse class is returned by the search_memory_facts function in graphiti. When no episodes are found, the EpisodeSearchResponse class is returned by the get_episodes function in graphiti. Both classes have a message indicating that no relevant content was found and an empty list for their respective attributes.","In mcp_server/graphiti_mcp_server.py:

- search_memory_facts returns a FactSearchResponse(message='No relevant facts found', facts=[]) when no facts are found.  
- get_episodes returns an EpisodeSearchResponse(message='No episodes found…', episodes=[]) when no episodes are found.","[""MODULE keras.keras.src.saving.object_registration contains FUNCTION keras.keras.src.saving.object_registration.get_custom_objects\nRetrieves a live reference to the global dictionary of custom objects.  Custom objects set using `custom_object_scope()` are not added to the global dictionary of custom objects, and will not appear in the returned dictionary.  Example:  ```python get_custom_objects().clear() get_custom_objects()['MyObject'] = MyObject ```  Returns:     Global dictionary mapping registered class names to classes."", 'CLASS graphiti.graphiti_core.search.search_config.EpisodeSearchMethod inherits from or is a subclass of enum.Enum', ""CLASS keras.keras.src.metrics.iou_metrics.BinaryIoU has field or attribute or property keras.keras.src.metrics.iou_metrics.BinaryIoU.threshold\nComputes the Intersection-Over-Union metric for class 0 and/or 1.  Formula:  ```python iou = true_positives / (true_positives + false_positives + false_negatives) ``` Intersection-Over-Union is a common evaluation metric for semantic image segmentation.  To compute IoUs, the predictions are accumulated in a confusion matrix, weighted by `sample_weight` and the metric is then calculated from it.  If `sample_weight` is `None`, weights default to 1. Use `sample_weight` of 0 to mask values.  This class can be used to compute IoUs for a binary classification task where the predictions are provided as logits. First a `threshold` is applied to the predicted values such that those that are below the `threshold` are converted to class 0 and those that are above the `threshold` are converted to class 1.  IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes that are specified by `target_class_ids` is returned.  Note: with `threshold=0`, this metric has the same behavior as `IoU`.  Args:     target_class_ids: A tuple or list of target class ids for which the         metric is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With         `[0]` (or `[1]`), the IoU metric for class 0 (or class 1,         respectively) is returned. With `[0, 1]`, the mean of IoUs for the         two classes is returned.     threshold: A threshold that applies to the prediction logits to convert         them to either predicted class 0 if the logit is below `threshold`         or predicted class 1 if the logit is above `threshold`.     name: (Optional) string name of the metric instance.     dtype: (Optional) data type of the metric result.  Example:  >>> m = keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3) >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7]) >>> m.result() 0.33333334  >>> m.reset_state() >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7], ...                sample_weight=[0.2, 0.3, 0.4, 0.1]) >>> # cm = [[0.2, 0.4], >>> #        [0.3, 0.1]] >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], >>> # true_positives = [0.2, 0.1] >>> # iou = [0.222, 0.125] >>> m.result() 0.17361112  Usage with `compile()` API:  ```python model.compile(     optimizer='sgd',     loss='mse',     metrics=[keras.metrics.BinaryIoU(         target_class_ids=[0],         threshold=0.5     )] ) ```"", 'CLASS graphiti.graphiti_core.search.search_config.EpisodeSearchConfig inherits from or is a subclass of pydantic.BaseModel', ""MODULE keras.keras.src.metrics.iou_metrics contains CLASS keras.keras.src.metrics.iou_metrics.BinaryIoU\nComputes the Intersection-Over-Union metric for class 0 and/or 1.  Formula:  ```python iou = true_positives / (true_positives + false_positives + false_negatives) ``` Intersection-Over-Union is a common evaluation metric for semantic image segmentation.  To compute IoUs, the predictions are accumulated in a confusion matrix, weighted by `sample_weight` and the metric is then calculated from it.  If `sample_weight` is `None`, weights default to 1. Use `sample_weight` of 0 to mask values.  This class can be used to compute IoUs for a binary classification task where the predictions are provided as logits. First a `threshold` is applied to the predicted values such that those that are below the `threshold` are converted to class 0 and those that are above the `threshold` are converted to class 1.  IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes that are specified by `target_class_ids` is returned.  Note: with `threshold=0`, this metric has the same behavior as `IoU`.  Args:     target_class_ids: A tuple or list of target class ids for which the         metric is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With         `[0]` (or `[1]`), the IoU metric for class 0 (or class 1,         respectively) is returned. With `[0, 1]`, the mean of IoUs for the         two classes is returned.     threshold: A threshold that applies to the prediction logits to convert         them to either predicted class 0 if the logit is below `threshold`         or predicted class 1 if the logit is above `threshold`.     name: (Optional) string name of the metric instance.     dtype: (Optional) data type of the metric result.  Example:  >>> m = keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3) >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7]) >>> m.result() 0.33333334  >>> m.reset_state() >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7], ...                sample_weight=[0.2, 0.3, 0.4, 0.1]) >>> # cm = [[0.2, 0.4], >>> #        [0.3, 0.1]] >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], >>> # true_positives = [0.2, 0.1] >>> # iou = [0.222, 0.125] >>> m.result() 0.17361112  Usage with `compile()` API:  ```python model.compile(     optimizer='sgd',     loss='mse',     metrics=[keras.metrics.BinaryIoU(         target_class_ids=[0],         threshold=0.5     )] ) ```"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"", ""graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.""]","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 8}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 8, 'n_files_pr': 2, 'pr': 668, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",30.280954360961914,1139
How does the reranker constructor handle an OpenAIClient instance to set its internal client?,"The OpenAIRerankerClient constructor handles an OpenAIClient instance by unwrapping it and using its internal `.client` attribute for the reranker. This is done in the `__init__` method of OpenAIRerankerClient, which checks if the passed client is an instance of OpenAIClient and sets `self.client` to `client.client` if it is.","In OpenAIRerankerClient.__init__ (graphiti_core/cross_encoder/openai_reranker_client.py), if you pass in an OpenAIClient instance it does:

```python
elif isinstance(client, OpenAIClient):
    self.client = client.client
```

i.e. it “unwraps” your OpenAIClient and uses its internal `.client` for the reranker.","[""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.llm_client\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient has method graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient.__init__\nInitialize the OpenAIRerankerClient with the provided configuration and client.  This reranker uses the OpenAI API to run a simple boolean classifier prompt concurrently for each passage. Log-probabilities are used to rank the passages.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (AsyncOpenAI | AsyncAzureOpenAI | OpenAIClient | None): An optional async client instance to use. If not provided, a new AsyncOpenAI client is created.', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.password\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'CLASS graphiti.graphiti_core.cross_encoder.openai_reranker_client.OpenAIRerankerClient inherits from or is a subclass of client.CrossEncoderClient', ""METHOD graphiti.graphiti_core.graphiti.Graphiti.__init__ has parameter or accepts argument graphiti.graphiti_core.graphiti.Graphiti.__init__.max_coroutines\nInitialize a Graphiti instance.  This constructor sets up a connection to the Neo4j database and initializes the LLM client for natural language processing tasks.  Parameters ---------- uri : str     The URI of the Neo4j database. user : str     The username for authenticating with the Neo4j database. password : str     The password for authenticating with the Neo4j database. llm_client : LLMClient | None, optional     An instance of LLMClient for natural language processing tasks.     If not provided, a default OpenAIClient will be initialized. embedder : EmbedderClient | None, optional     An instance of EmbedderClient for embedding tasks.     If not provided, a default OpenAIEmbedder will be initialized. cross_encoder : CrossEncoderClient | None, optional     An instance of CrossEncoderClient for reranking tasks.     If not provided, a default OpenAIRerankerClient will be initialized. store_raw_episode_content : bool, optional     Whether to store the raw content of episodes. Defaults to True. graph_driver : GraphDriver | None, optional     An instance of GraphDriver for database operations.     If not provided, a default Neo4jDriver will be initialized. max_coroutines : int | None, optional     The maximum number of concurrent operations allowed. Overrides SEMAPHORE_LIMIT set in the environment.     If not set, the Graphiti default is used.  Returns ------- None  Notes ----- This method establishes a connection to the Neo4j database using the provided credentials. It also sets up the LLM client, either using the provided client or by creating a default OpenAIClient.  The default database name is set to 'neo4j'. If a different database name is required, it should be specified in the URI or set separately after initialization.  The OpenAI API key is expected to be set in the environment variables. Make sure to set the OPENAI_API_KEY environment variable before initializing Graphiti if you're using the default OpenAIClient."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=""bolt://localhost:7687"",\n    user=""neo4j"",\n    password=""password"",\n    database=""my_custom_database""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=""localhost"",\n    port=6379,\n    username=""falkor_user"",  # Optional\n    password=""falkor_password"",  # Optional\n    database=""my_custom_graph""  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j\'s parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = ""<your-api-key>""\napi_version = ""<your-api-version>""\nllm_endpoint = ""<your-llm-endpoint>""  # e.g., ""https://your-llm-resource.openai.azure.com/""\nembedding_endpoint = ""<your-embedding-endpoint>""  # e.g., ""https://your-embedding-resource.openai.azure.com/""\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=""gpt-4.1-nano"",\n    model=""gpt-4.1-mini"",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=""text-embedding-3-small-deployment""  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 1}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 1, 'n_context_nodes': 1, 'n_files_pr': 1, 'pr': 676, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'deep', 'type': 'open_question'}",42.00703024864197,1140
Which Gemini wrapper constructors now accept a custom genai.Client?,"The Gemini wrapper constructors that now accept a custom genai.Client in graphiti are: * GeminiEmbedder, * GeminiRerankerClient, and * GeminiClient.","The following Gemini wrappers now let you pass in your own `genai.Client`:

• GeminiRerankerClient.__init__ (in graphiti_core/cross_encoder/gemini_reranker_client.py)  
• GeminiEmbedder.__init__ (in graphiti_core/embedder/gemini.py)  
• GeminiClient.__init__ (in graphiti_core/llm_client/gemini_client.py)","['CLASS graphiti.graphiti_core.embedder.gemini.GeminiEmbedder has method graphiti.graphiti_core.embedder.gemini.GeminiEmbedder.__init__\nInitialize the GeminiEmbedder with the provided configuration and client.  Args:     config (GeminiEmbedderConfig | None): The configuration for the GeminiEmbedder, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.     batch_size (int | None): An optional batch size to use. If not provided, the default batch size will be used.\nGoogle Gemini Embedder Client', 'CLASS graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient has method graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.\nGoogle Gemini Reranker Client', 'METHOD graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__.client\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.', 'CLASS graphiti.graphiti_core.embedder.gemini.GeminiEmbedder inherits from or is a subclass of client.EmbedderClient\nGoogle Gemini Embedder Client', 'METHOD graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__ has parameter or accepts argument graphiti.graphiti_core.cross_encoder.gemini_reranker_client.GeminiRerankerClient.__init__.config\nInitialize the GeminiRerankerClient with the provided configuration and client.  The Gemini Developer API does not yet support logprobs. Unlike the OpenAI reranker, this reranker uses the Gemini API to perform direct relevance scoring of passages. Each passage is scored individually on a 0-100 scale.  Args:     config (LLMConfig | None): The configuration for the LLM client, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m ""Your detailed commit message""\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you\'re changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [""your-package>=1.0.0""]\n   dev = [\n       # ... existing dev dependencies\n       ""your-package>=1.0.0"",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               \'your-package is required for YourServiceClient. \'\n               \'Install it with: pip install graphiti-core[your-service]\'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 4}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': False, 'n_context_files': 3, 'n_context_nodes': 4, 'n_files_pr': 3, 'pr': 679, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",28.92381978034973,1141
How does create_batch recover when a batch embedding request fails?,"When a batch embedding request fails in graphiti, `create_batch` recovers by attempting to embed each item individually using client.aio.models.embed_content(contents=[item]). This involves logging a warning about the failed batch, looping over each item in the failed batch, and for each item, calling `client.aio.models.embed_content(contents=[item])`. If any of these per-item embed_content calls error, `create_batch` logs an error and re-raises the exception.","In graphiti_core/embedder/gemini.py (class GeminiEmbedder), create_batch wraps the batch embed_content call in a try/except. If the batch call fails it:

1. Logs a warning about the failed batch.  
2. Falls back to “individual processing”: it loops over each item in the failed batch, calls client.aio.models.embed_content(contents=[item]) for each, and appends each embedding to all_embeddings.  
3. If any per-item call errors (e.g. no embeddings or empty values), it logs an error and re-raises.","['CLASS cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.EmbeddingRateLimiter has method cognee.cognee.infrastructure.databases.vector.embeddings.embedding_rate_limiter.EmbeddingRateLimiter.wait_if_needed\nBlock until a request can be made without exceeding the rate limit.  This method will wait if the current request would exceed the rate limit and returns the time waited in seconds.  Returns: - float: Time waited in seconds before a request is allowed.  Returns: --------      - float: Time waited in seconds before proceeding.\nRate limiter for embedding API calls.  This class implements a singleton pattern to ensure that rate limiting is consistent across all embedding requests. It uses the limits library with a moving window strategy to control request rates.  The rate limiter uses the same configuration as the LLM API rate limiter but uses a separate key to track embedding API calls independently.  Public Methods: - get_instance - reset_instance - hit_limit - wait_if_needed - async_wait_if_needed  Instance Variables: - enabled - requests_limit - interval_seconds - request_times - lock', 'MODULE graphiti.graphiti_core.embedder.gemini contains GLOBAL_VARIABLE graphiti.graphiti_core.embedder.gemini.DEFAULT_BATCH_SIZE', 'CLASS graphiti.graphiti_core.errors.NodeNotFoundError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'METHOD graphiti.graphiti_core.embedder.gemini.GeminiEmbedder.__init__ has parameter or accepts argument graphiti.graphiti_core.embedder.gemini.DEFAULT_BATCH_SIZE\nInitialize the GeminiEmbedder with the provided configuration and client.  Args:     config (GeminiEmbedderConfig | None): The configuration for the GeminiEmbedder, including API key, model, base URL, temperature, and max tokens.     client (genai.Client | None): An optional async client instance to use. If not provided, a new genai.Client is created.     batch_size (int | None): An optional batch size to use. If not provided, the default batch size will be used.', 'CLASS graphiti.graphiti_core.errors.SearchRerankerError inherits from or is a subclass of graphiti.graphiti_core.errors.GraphitiError\nBase exception class for Graphiti Core.\nRaised when a node is not found.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to ""default"".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti\'s ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it\'s optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```', ""graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."", 'graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: \'3.8\'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - ""8000:8000""\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - ""7474:7474""  # HTTP\n            - ""${NEO4J_PORT}:${NEO4J_PORT}""  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 16}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 16, 'n_files_pr': 5, 'pr': 680, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",35.26712965965271,1142
What precedence order does the GeminiClient use when resolving the max_output_tokens value for a request?,"The precedence order for resolving the max_output_tokens value in GeminiClient class is as follows:

1. Explicit `max_tokens` argument passed to `generate_response`
2. Client’s `self.max_tokens` attribute set in the constructor
3. Model’s built-in limit obtained via `_get_max_tokens_for_model`, which originates from `GEMINI_MODEL_MAX_TOKENS[model]` or `DEFAULT_GEMINI_MAX_TOKENS`","In gemini_client.py (see GeminiClient._generate_response → _resolve_max_tokens), the max_output_tokens are picked in this order:

1. The explicit `max_tokens` argument you passed to `generate_response`  
2. The client’s `self.max_tokens` (the value you passed into the constructor)  
3. The model’s built-in limit via `_get_max_tokens_for_model` (i.e. `GEMINI_MODEL_MAX_TOKENS[model]` or `DEFAULT_GEMINI_MAX_TOKENS`)","['CLASS graphiti.mcp_server.graphiti_mcp_server.GraphitiConfig inherits from or is a subclass of pydantic.BaseModel\nConfiguration for Graphiti client.  Centralizes all configuration parameters for the Graphiti client.', 'MODULE graphiti.graphiti_core.llm_client.gemini_client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.gemini_client.DEFAULT_GEMINI_MAX_TOKENS', ""CLASS graphiti.graphiti_core.llm_client.gemini_client.GeminiClient inherits from or is a subclass of client.LLMClient\nGeminiClient is a client class for interacting with Google's Gemini language models.  This class extends the LLMClient and provides methods to initialize the client and generate responses from the Gemini language model.  Attributes:     model (str): The model name to use for generating responses.     temperature (float): The temperature to use for generating responses.     max_tokens (int): The maximum number of tokens to generate in a response.     thinking_config (types.ThinkingConfig | None): Optional thinking configuration for models that support it. Methods:     __init__(config: LLMConfig | None = None, cache: bool = False, thinking_config: types.ThinkingConfig | None = None):         Initializes the GeminiClient with the provided configuration, cache setting, and optional thinking config.      _generate_response(messages: list[Message]) -> dict[str, typing.Any]:         Generates a response from the language model based on the provided messages."", 'MODULE graphiti.graphiti_core.llm_client.gemini_client contains GLOBAL_VARIABLE graphiti.graphiti_core.llm_client.gemini_client.GEMINI_MODEL_MAX_TOKENS', 'CLASS graphiti.graphiti_core.errors.GraphitiError inherits from or is a subclass of graphiti.graphiti_core.errors.Exception\nBase exception class for Graphiti Core.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google\'s Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you\'ll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add ""graphiti-core[google-genai]""\n\n# or\n\npip install ""graphiti-core[google-genai]""\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = ""<your-google-api-key>""\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.0-flash""\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=""embedding-001""\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=""gemini-2.5-flash-lite-preview-06-17""\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini\'s log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama\'s OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=""abc"",  # Ollama doesn\'t require a real API key\n    model=""deepseek-r1:7b"",\n    small_model=""deepseek-r1:7b"",\n    base_url=""http://localhost:11434/v1"", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    ""bolt://localhost:7687"",\n    ""neo4j"",\n    ""password"",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=""abc"",\n            embedding_model=""nomic-embed-text"",\n            embedding_dim=768,\n            base_url=""http://localhost:11434/v1"",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=""center"">\n  <a href=""https://www.getzep.com/"">\n    <img src=""https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73"" width=""150"" alt=""Zep Logo"">\n  </a>\n</p>\n\n<h1 align=""center"">\nGraphiti\n</h1>\n<h2 align=""center""> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=""center"">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=""center"">\n\n<a href=""https://trendshift.io/repositories/12986"" target=""_blank""><img src=""https://trendshift.io/api/badge/repositories/12986"" alt=""getzep%2Fgraphiti | Trendshift"" style=""width: 250px; height: 55px;"" width=""250"" height=""55""/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=""center"">\n    <img src=""images/graphiti-graph-intro.gif"" alt=""Graphiti temporal walkthrough"" width=""700px"">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _""Kendra loves Adidas shoes.""_ Each fact is a ""triplet"" represented by two entities, or\nnodes (""Kendra"", ""Adidas shoes""), and their relationship, or edge (""loves""). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.', 'graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = ""basic""`, server uses `typeCheckingMode = ""standard""`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k ""_int""`\n- Run only unit tests: `pytest tests/ -k ""not _int""`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences']","{'commit': '3200afa363cc71db8533c09040d1d7091c6ad8fe', 'difficulty': 'moderate', 'found_stats': {'path': 6}, 'includes_code': False, 'includes_location_hints': True, 'is_core_question': True, 'n_context_files': 1, 'n_context_nodes': 6, 'n_files_pr': 2, 'pr': 712, 'repo': 'https://github.com/getzep/graphiti.git', 'scope': 'broad', 'type': 'open_question'}",31.901565551757812,1143

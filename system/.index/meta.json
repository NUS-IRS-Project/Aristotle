[{"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nXGBoost Change Log\n==================\n\n**Starting from 2.1.0, release note is recorded in the documentation.**\n\nThis file records the changes in xgboost library in reverse chronological order.\n\n## 2.0.0 (2023 Aug 16)\n\nWe are excited to announce the release of XGBoost 2.0. This note will begin by covering some overall changes and then highlight specific updates to the package.\n\n### Initial work on multi-target trees with vector-leaf outputs\nWe have been working on vector-leaf tree models for multi-target regression, multi-label classification, and multi-class classification in version 2.0. Previously, XGBoost would build a separate model for each target. However, with this new feature that's still being developed, XGBoost can build one tree for all targets. The feature has multiple benefits and trade-offs compared to the existing approach. It can help prevent overfitting, produce smaller models, and build trees that consider the correlation between targets. In addition, users can combine vector leaf and scalar leaf trees during a training session using a callback. Please note that the feature is still a working in progress, and many parts are not yet available. See #9043 for the current status. Related PRs: (#8538, #8697, #8902, #8884, #8895, #8898, #8612, #8652, #8698, #8908, #8928, #8968, #8616, #8922, #8890, #8872, #8889, #9509) Please note that, only the `hist` (default) tree method on CPU can be used for building vector leaf trees at the moment.\n\n### New `device` parameter.\n\nA new `device` parameter is set to replace the existing `gpu_id`, `gpu_hist`, `gpu_predictor`, `cpu_predictor`, `gpu_coord_descent`, and the PySpark specific parameter `use_gpu`. Onward, users need only the `device` parameter to select which device to run along with the ordinal of the device. For more information, please see our document page (https://xgboost.readthedocs.io/en/stable/parameter.html#general-parameters) . For example, with  `device=\"cuda\", tree_method=\"hist\"`, XGBoost will run the `hist` tree method on GPU. (#9363, #8528, #8604, #9354, #9274, #9243, #8896, #9129, #9362, #9402, #9385, #9398, #9390, #9386, #9412, #9507, #9536). The old behavior of ``gpu_hist``  is preserved but deprecated. In addition, the `predictor` parameter is removed.\n\n\n### `hist` is now the default tree method\nStarting from 2.0, the `hist` tree method will be the default. In previous versions, XGBoost chooses `approx` or `exact` depending on the input data and training environment. The new default can help XGBoost train models more efficiently and consistently. (#9320, #9353)\n\n### GPU-based approx tree method\nThere's initial support for using the `approx` tree method on GPU. The performance of the `approx` is not yet well optimized but is feature complete except for the JVM packages. It can be accessed through the use of the parameter combination `device=\"cuda\", tree_method=\"approx\"`. (#9414, #9399, #9478). Please note that the Scala-based Spark interface is not yet supported.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nXGBoost Change Log\n==================\n\n**Starting from 2.1.0, release note is recorded in the documentation.**\n\nThis file records the changes in xgboost library in reverse chronological order.\n\n## 2.0.0 (2023 Aug 16)\n\nWe are excited to announce the release of XGBoost 2.0. This note will begin by covering some overall changes and then highlight specific updates to the package.\n\n### Initial work on multi-target trees with vector-leaf outputs\nWe have been working on vector-leaf tree models for multi-target regression, multi-label classification, and multi-class classification in version 2.0. Previously, XGBoost would build a separate model for each target. However, with this new feature that's still being developed, XGBoost can build one tree for all targets. The feature has multiple benefits and trade-offs compared to the existing approach. It can help prevent overfitting, produce smaller models, and build trees that consider the correlation between targets. In addition, users can combine vector leaf and scalar leaf trees during a training session using a callback. Please note that the feature is still a working in progress, and many parts are not yet available. See #9043 for the current status. Related PRs: (#8538, #8697, #8902, #8884, #8895, #8898, #8612, #8652, #8698, #8908, #8928, #8968, #8616, #8922, #8890, #8872, #8889, #9509) Please note that, only the `hist` (default) tree method on CPU can be used for building vector leaf trees at the moment.\n\n### New `device` parameter.\n\nA new `device` parameter is set to replace the existing `gpu_id`, `gpu_hist`, `gpu_predictor`, `cpu_predictor`, `gpu_coord_descent`, and the PySpark specific parameter `use_gpu`. Onward, users need only the `device` parameter to select which device to run along with the ordinal of the device. For more information, please see our document page (https://xgboost.readthedocs.io/en/stable/parameter.html#general-parameters) . For example, with  `device=\"cuda\", tree_method=\"hist\"`, XGBoost will run the `hist` tree method on GPU. (#9363, #8528, #8604, #9354, #9274, #9243, #8896, #9129, #9362, #9402, #9385, #9398, #9390, #9386, #9412, #9507, #9536). The old behavior of ``gpu_hist``  is preserved but deprecated. In addition, the `predictor` parameter is removed.\n\n\n### `hist` is now the default tree method\nStarting from 2.0, the `hist` tree method will be the default. In previous versions, XGBoost chooses `approx` or `exact` depending on the input data and training environment. The new default can help XGBoost train models more efficiently and consistently. (#9320, #9353)\n\n### GPU-based approx tree method\nThere's initial support for using the `approx` tree method on GPU. The performance of the `approx` is not yet well optimized but is feature complete except for the JVM packages. It can be accessed through the use of the parameter combination `device=\"cuda\", tree_method=\"approx\"`. (#9414, #9399, #9478). Please note that the Scala-based Spark interface is not yet supported."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Optimize and bound the size of the histogram on CPU, to control memory footprint\n\nXGBoost has a new parameter `max_cached_hist_node` for users to limit the CPU cache size for histograms. It can help prevent XGBoost from caching histograms too aggressively. Without the cache, performance is likely to decrease. However, the size of the cache grows exponentially with the depth of the tree. The limit can be crucial when growing deep trees. In most cases, users need not configure this parameter as it does not affect the model's accuracy. (#9455, #9441, #9440, #9427, #9400).\n\nAlong with the cache limit, XGBoost also reduces the memory usage of the `hist` and `approx` tree method on distributed systems by cutting the size of the cache by half. (#9433)\n\n### Improved external memory support\nThere is some exciting development around external memory support in XGBoost. It's still an experimental feature, but the performance has been significantly improved with the default `hist` tree method. We replaced the old file IO logic with memory map. In addition to performance, we have reduced CPU memory usage and added extensive documentation. Beginning from 2.0.0, we encourage users to try it with the `hist` tree method when the memory saving by `QuantileDMatrix` is not sufficient. (#9361, #9317, #9282, #9315, #8457)\n\n### Learning to rank\nWe created a brand-new implementation for the learning-to-rank task. With the latest version, XGBoost gained a set of new features for ranking task including:\n\n- A new parameter `lambdarank_pair_method` for choosing the pair construction strategy.\n- A new parameter `lambdarank_num_pair_per_sample` for controlling the number of samples for each group.\n- An experimental implementation of unbiased learning-to-rank, which can be accessed using the `lambdarank_unbiased` parameter.\n- Support for custom gain function with `NDCG` using the `ndcg_exp_gain` parameter.\n- Deterministic GPU computation for all objectives and metrics.\n- `NDCG` is now the default objective function.\n- Improved performance of metrics using caches.\n- Support scikit-learn utilities for `XGBRanker`.\n- Extensive documentation on how learning-to-rank works with XGBoost.\n\nFor more information, please see the [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/learning_to_rank.html). Related PRs: (#8771, #8692, #8783, #8789, #8790, #8859, #8887, #8893, #8906, #8931, #9075, #9015, #9381, #9336, #8822, #9222, #8984, #8785, #8786, #8768)\n\n### Automatically estimated intercept\n\nIn the previous version, `base_score` was a constant that could be set as a training parameter. In the new version, XGBoost can automatically estimate this parameter based on input labels for optimal accuracy. (#8539, #8498, #8272, #8793, #8607)\n\n### Quantile regression\nThe XGBoost algorithm now supports quantile regression, which involves minimizing the quantile loss (also called \"pinball loss\"). Furthermore, XGBoost allows for training with multiple target quantiles simultaneously with one tree per quantile. (#8775, #8761, #8760, #8758, #8750)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Optimize and bound the size of the histogram on CPU, to control memory footprint\n\nXGBoost has a new parameter `max_cached_hist_node` for users to limit the CPU cache size for histograms. It can help prevent XGBoost from caching histograms too aggressively. Without the cache, performance is likely to decrease. However, the size of the cache grows exponentially with the depth of the tree. The limit can be crucial when growing deep trees. In most cases, users need not configure this parameter as it does not affect the model's accuracy. (#9455, #9441, #9440, #9427, #9400).\n\nAlong with the cache limit, XGBoost also reduces the memory usage of the `hist` and `approx` tree method on distributed systems by cutting the size of the cache by half. (#9433)\n\n### Improved external memory support\nThere is some exciting development around external memory support in XGBoost. It's still an experimental feature, but the performance has been significantly improved with the default `hist` tree method. We replaced the old file IO logic with memory map. In addition to performance, we have reduced CPU memory usage and added extensive documentation. Beginning from 2.0.0, we encourage users to try it with the `hist` tree method when the memory saving by `QuantileDMatrix` is not sufficient. (#9361, #9317, #9282, #9315, #8457)\n\n### Learning to rank\nWe created a brand-new implementation for the learning-to-rank task. With the latest version, XGBoost gained a set of new features for ranking task including:\n\n- A new parameter `lambdarank_pair_method` for choosing the pair construction strategy.\n- A new parameter `lambdarank_num_pair_per_sample` for controlling the number of samples for each group.\n- An experimental implementation of unbiased learning-to-rank, which can be accessed using the `lambdarank_unbiased` parameter.\n- Support for custom gain function with `NDCG` using the `ndcg_exp_gain` parameter.\n- Deterministic GPU computation for all objectives and metrics.\n- `NDCG` is now the default objective function.\n- Improved performance of metrics using caches.\n- Support scikit-learn utilities for `XGBRanker`.\n- Extensive documentation on how learning-to-rank works with XGBoost.\n\nFor more information, please see the [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/learning_to_rank.html). Related PRs: (#8771, #8692, #8783, #8789, #8790, #8859, #8887, #8893, #8906, #8931, #9075, #9015, #9381, #9336, #8822, #9222, #8984, #8785, #8786, #8768)\n\n### Automatically estimated intercept\n\nIn the previous version, `base_score` was a constant that could be set as a training parameter. In the new version, XGBoost can automatically estimate this parameter based on input labels for optimal accuracy. (#8539, #8498, #8272, #8793, #8607)\n\n### Quantile regression\nThe XGBoost algorithm now supports quantile regression, which involves minimizing the quantile loss (also called \"pinball loss\"). Furthermore, XGBoost allows for training with multiple target quantiles simultaneously with one tree per quantile. (#8775, #8761, #8760, #8758, #8750)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### L1 and Quantile regression now supports learning rate\nBoth objectives use adaptive trees due to the lack of proper Hessian values. In the new version, XGBoost can scale the leaf value with the learning rate accordingly. (#8866)\n\n### Export cut value\n\nUsing the Python or the C package, users can export the quantile values (not to be confused with quantile regression) used for the `hist` tree method. (#9356)\n\n### column-based split and federated learning\nWe made progress on column-based split for federated learning. In 2.0, both `approx`, `hist`, and `hist` with vector leaf can work with column-based data split, along with support for vertical federated learning. Work on GPU support is still on-going, stay tuned. (#8576, #8468, #8442, #8847, #8811, #8985, #8623, #8568, #8828, #8932, #9081, #9102, #9103, #9124, #9120, #9367, #9370, #9343, #9171, #9346, #9270, #9244, #8494, #8434, #8742, #8804, #8710, #8676, #9020, #9002, #9058, #9037, #9018, #9295, #9006, #9300, #8765, #9365, #9060)\n\n### PySpark\nAfter the initial introduction of the PySpark interface, it has gained some new features and optimizations in 2.0.\n\n- GPU-based prediction. (#9292, #9542)\n- Optimization for data initialization by avoiding the stack operation. (#9088)\n- Support predict feature contribution. (#8633)\n- Python typing support. (#9156, #9172, #9079, #8375)\n- `use_gpu` is deprecated. The `device` parameter is preferred.\n- Update eval_metric validation to support list of strings (#8826)\n- Improved logs for training (#9449)\n- Maintenance, including refactoring and document updates (#8324, #8465, #8605, #9202, #9460, #9302, #8385, #8630, #8525, #8496)\n- Fix for GPU setup. (#9495)\n\n### Other General New Features\nHere's a list of new features that don't have their own section and yet are general to all language bindings.\n\n- Use array interface for CSC matrix. This helps XGBoost to use a consistent number of threads and align the interface of the CSC matrix with other interfaces. In addition, memory usage is likely to decrease with CSC input thanks to on-the-fly type conversion. (#8672)\n- CUDA compute 90 is now part of the default build.. (#9397)\n\n### Other General Optimization\nThese optimizations are general to all language bindings. For language-specific optimization, please visit the corresponding sections.\n\n- Performance for input with `array_interface` on CPU (like `numpy`) is significantly improved. (#9090)\n- Some optimization with CUDA for data initialization. (#9199, #9209, #9144)\n- Use the latest thrust policy to prevent synchronizing GPU devices. (#9212)\n- XGBoost now uses a per-thread CUDA stream, which prevents synchronization with other streams. (#9416, #9396, #9413)\n\n### Notable breaking change\n\nOther than the aforementioned change with the `device` parameter, here's a list of breaking changes affecting all packages.\n\n- Users must specify the format for text input (#9077). However, we suggest using third-party data structures such as `numpy.ndarray` instead of relying on text inputs. See https://github.com/dmlc/xgboost/issues/9472 for more info.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### L1 and Quantile regression now supports learning rate\nBoth objectives use adaptive trees due to the lack of proper Hessian values. In the new version, XGBoost can scale the leaf value with the learning rate accordingly. (#8866)\n\n### Export cut value\n\nUsing the Python or the C package, users can export the quantile values (not to be confused with quantile regression) used for the `hist` tree method. (#9356)\n\n### column-based split and federated learning\nWe made progress on column-based split for federated learning. In 2.0, both `approx`, `hist`, and `hist` with vector leaf can work with column-based data split, along with support for vertical federated learning. Work on GPU support is still on-going, stay tuned. (#8576, #8468, #8442, #8847, #8811, #8985, #8623, #8568, #8828, #8932, #9081, #9102, #9103, #9124, #9120, #9367, #9370, #9343, #9171, #9346, #9270, #9244, #8494, #8434, #8742, #8804, #8710, #8676, #9020, #9002, #9058, #9037, #9018, #9295, #9006, #9300, #8765, #9365, #9060)\n\n### PySpark\nAfter the initial introduction of the PySpark interface, it has gained some new features and optimizations in 2.0.\n\n- GPU-based prediction. (#9292, #9542)\n- Optimization for data initialization by avoiding the stack operation. (#9088)\n- Support predict feature contribution. (#8633)\n- Python typing support. (#9156, #9172, #9079, #8375)\n- `use_gpu` is deprecated. The `device` parameter is preferred.\n- Update eval_metric validation to support list of strings (#8826)\n- Improved logs for training (#9449)\n- Maintenance, including refactoring and document updates (#8324, #8465, #8605, #9202, #9460, #9302, #8385, #8630, #8525, #8496)\n- Fix for GPU setup. (#9495)\n\n### Other General New Features\nHere's a list of new features that don't have their own section and yet are general to all language bindings.\n\n- Use array interface for CSC matrix. This helps XGBoost to use a consistent number of threads and align the interface of the CSC matrix with other interfaces. In addition, memory usage is likely to decrease with CSC input thanks to on-the-fly type conversion. (#8672)\n- CUDA compute 90 is now part of the default build.. (#9397)\n\n### Other General Optimization\nThese optimizations are general to all language bindings. For language-specific optimization, please visit the corresponding sections.\n\n- Performance for input with `array_interface` on CPU (like `numpy`) is significantly improved. (#9090)\n- Some optimization with CUDA for data initialization. (#9199, #9209, #9144)\n- Use the latest thrust policy to prevent synchronizing GPU devices. (#9212)\n- XGBoost now uses a per-thread CUDA stream, which prevents synchronization with other streams. (#9416, #9396, #9413)\n\n### Notable breaking change\n\nOther than the aforementioned change with the `device` parameter, here's a list of breaking changes affecting all packages.\n\n- Users must specify the format for text input (#9077). However, we suggest using third-party data structures such as `numpy.ndarray` instead of relying on text inputs. See https://github.com/dmlc/xgboost/issues/9472 for more info."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Notable bug fixes\n\nSome noteworthy bug fixes that are not related to specific language bindings are listed in this section.\n\n- Some language environments use a different thread to perform garbage collection, which breaks the thread-local cache used in XGBoost. XGBoost 2.0 implements a new thread-safe cache using a light weight lock to replace the thread-local cache. (#8851)\n- Fix model IO by clearing the prediction cache. (#8904)\n- `inf` is checked during data construction. (#8911)\n- Preserve order of saved updaters configuration. Usually, this is not an issue unless the `updater` parameter is used instead of the `tree_method` parameter (#9355)\n- Fix GPU memory allocation issue with categorical splits. (#9529)\n- Handle escape sequence like `\\t\\n` in feature names for JSON model dump. (#9474)\n- Normalize file path for model IO and text input. This handles short paths on Windows and paths that contain `~` on Unix (#9463). In addition, all path inputs are required to be encoded in UTF-8 (#9448, #9443)\n- Fix integer overflow on H100. (#9380)\n- Fix weighted sketching on GPU with categorical features. (#9341)\n- Fix metric serialization. The bug might cause some of the metrics to be dropped during evaluation. (#9405)\n- Fixes compilation errors on MSVC x86 targets (#8823)\n- Pick up the dmlc-core fix for the CSV parser. (#8897)\n\n\n### Documentation\nAside from documents for new features, we have many smaller updates to improve user experience, from troubleshooting guides to typo fixes.\n\n- Explain CPU/GPU interop. (#8450)\n- Guide to troubleshoot NCCL errors. (#8943, #9206)\n- Add a note for rabit port selection. (#8879)\n- How to build the docs using conda (#9276)\n- Explain how to obtain reproducible results on distributed systems. (#8903)\n\n* Fixes and small updates to document and demonstration scripts. (#8626, #8436, #8995, #8907, #8923, #8926, #9358, #9232, #9201, #9469, #9462, #9458, #8543, #8597, #8401, #8784, #9213, #9098, #9008, #9223, #9333, #9434, #9435, #9415, #8773, #8752, #9291, #9549)\n\n### Python package\n* New Features and Improvements\n- Support primitive types of pyarrow-backed pandas dataframe. (#8653)\n- Warning messages emitted by XGBoost are now emitted using Python warnings. (#9387)\n- User can now format the value printed near the bars on the `plot_importance` plot (#8540)\n- XGBoost has improved half-type support (float16) with pandas, cupy, and cuDF. With GPU input, the handling is through CUDA `__half` type, and no data copy is made. (#8487, #9207, #8481)\n- Support `Series` and Python primitive types in `inplace_predict` and `QuantileDMatrix` (#8547, #8542)\n- Support all pandas' nullable integer types. (#8480)\n- Custom metric with the scikit-learn interface now supports `sample_weight`. (#8706)\n- Enable Installation of Python Package with System lib in a Virtual Environment (#9349)\n- Raise if expected workers are not alive in `xgboost.dask.train` (#9421)\n\n* Optimization\n- Cache transformed data in `QuantileDMatrix` for efficiency. (#8666, #9445)\n- Take datatable as row-major input. (#8472)\n- Remove unnecessary conversions between data structures (#8546)\n\n* Adopt modern Python packaging conventions (PEP 517, PEP 518, PEP 621)\n-  XGBoost adopted the modern Python packaging conventions. The old setup script `setup.py` is now replaced with the new configuration file `pyproject.toml`. Along with this, XGBoost now supports Python 3.11. (#9021, #9112, #9114, #9115) Consult the latest documentation for the updated instructions to build and install XGBoost.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Notable bug fixes\n\nSome noteworthy bug fixes that are not related to specific language bindings are listed in this section.\n\n- Some language environments use a different thread to perform garbage collection, which breaks the thread-local cache used in XGBoost. XGBoost 2.0 implements a new thread-safe cache using a light weight lock to replace the thread-local cache. (#8851)\n- Fix model IO by clearing the prediction cache. (#8904)\n- `inf` is checked during data construction. (#8911)\n- Preserve order of saved updaters configuration. Usually, this is not an issue unless the `updater` parameter is used instead of the `tree_method` parameter (#9355)\n- Fix GPU memory allocation issue with categorical splits. (#9529)\n- Handle escape sequence like `\\t\\n` in feature names for JSON model dump. (#9474)\n- Normalize file path for model IO and text input. This handles short paths on Windows and paths that contain `~` on Unix (#9463). In addition, all path inputs are required to be encoded in UTF-8 (#9448, #9443)\n- Fix integer overflow on H100. (#9380)\n- Fix weighted sketching on GPU with categorical features. (#9341)\n- Fix metric serialization. The bug might cause some of the metrics to be dropped during evaluation. (#9405)\n- Fixes compilation errors on MSVC x86 targets (#8823)\n- Pick up the dmlc-core fix for the CSV parser. (#8897)\n\n\n### Documentation\nAside from documents for new features, we have many smaller updates to improve user experience, from troubleshooting guides to typo fixes.\n\n- Explain CPU/GPU interop. (#8450)\n- Guide to troubleshoot NCCL errors. (#8943, #9206)\n- Add a note for rabit port selection. (#8879)\n- How to build the docs using conda (#9276)\n- Explain how to obtain reproducible results on distributed systems. (#8903)\n\n* Fixes and small updates to document and demonstration scripts. (#8626, #8436, #8995, #8907, #8923, #8926, #9358, #9232, #9201, #9469, #9462, #9458, #8543, #8597, #8401, #8784, #9213, #9098, #9008, #9223, #9333, #9434, #9435, #9415, #8773, #8752, #9291, #9549)\n\n### Python package\n* New Features and Improvements\n- Support primitive types of pyarrow-backed pandas dataframe. (#8653)\n- Warning messages emitted by XGBoost are now emitted using Python warnings. (#9387)\n- User can now format the value printed near the bars on the `plot_importance` plot (#8540)\n- XGBoost has improved half-type support (float16) with pandas, cupy, and cuDF. With GPU input, the handling is through CUDA `__half` type, and no data copy is made. (#8487, #9207, #8481)\n- Support `Series` and Python primitive types in `inplace_predict` and `QuantileDMatrix` (#8547, #8542)\n- Support all pandas' nullable integer types. (#8480)\n- Custom metric with the scikit-learn interface now supports `sample_weight`. (#8706)\n- Enable Installation of Python Package with System lib in a Virtual Environment (#9349)\n- Raise if expected workers are not alive in `xgboost.dask.train` (#9421)\n\n* Optimization\n- Cache transformed data in `QuantileDMatrix` for efficiency. (#8666, #9445)\n- Take datatable as row-major input. (#8472)\n- Remove unnecessary conversions between data structures (#8546)\n\n* Adopt modern Python packaging conventions (PEP 517, PEP 518, PEP 621)\n-  XGBoost adopted the modern Python packaging conventions. The old setup script `setup.py` is now replaced with the new configuration file `pyproject.toml`. Along with this, XGBoost now supports Python 3.11. (#9021, #9112, #9114, #9115) Consult the latest documentation for the updated instructions to build and install XGBoost."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n- `DataIter` now accepts only keyword arguments. (#9431)\n- Fix empty DMatrix with categorical features. (#8739)\n- Convert ``DaskXGBClassifier.classes_`` to an array (#8452)\n- Define `best_iteration` only if early stopping is used to be consistent with documented behavior. (#9403)\n- Make feature validation immutable. (#9388)\n\n* Breaking changes\n- Discussed in the new `device` parameter section,  the `predictor` parameter is now removed. (#9129)\n- Remove support for single-string feature info. Feature type and names should be a sequence of strings (#9401)\n- Remove parameters in the `save_model` call for the scikit-learn interface. (#8963)\n- Remove the `ntree_limit` in the python package. This has been deprecated in previous versions. (#8345)\n\n* Maintenance including formatting and refactoring along with type hints.\n- More consistent use of `black` and `isort` for code formatting (#8420, #8748, #8867)\n- Improved type support. Most of the type changes happen in the PySpark module; here, we list the remaining changes. (#8444, #8617, #9197, #9005)\n- Set `enable_categorical` to True in predict. (#8592)\n- Some refactoring and updates for tests (#8395, #8372, #8557, #8379, #8702, #9459, #9316, #8446, #8695, #8409, #8993, #9480)\n\n* Documentation\n- Add introduction and notes for the sklearn interface. (#8948)\n- Demo for using dask for hyper-parameter optimization. (#8891)\n- Document all supported Python input types. (#8643)\n- Other documentation updates (#8944, #9304)\n\n### R package\n- Use the new data consumption interface for CSR and CSC. This provides better control for the number of threads and improves performance. (#8455, #8673)\n- Accept multiple evaluation metrics during training. (#8657)\n- Fix integer inputs with `NA`. (#9522)\n- Some refactoring for the R package (#8545, #8430, #8614, #8624, #8613, #9457, #8689, #8563, #9461, #8647, #8564, #8565, #8736, #8610, #8609, #8599, #8704, #9456, #9450, #9476, #9477, #9481). Special thanks to @jameslamb.\n- Document updates (#8886, #9323, #9437, #8998)\n\n### JVM packages\nFollowing are changes specific to various JVM-based packages.\n\n- Stop using Rabit in prediction (#9054)\n- Set feature_names and feature_types in jvm-packages. This is to prepare support for categorical features (#9364)\n- Scala 2.13 support. (#9099)\n- Change training stage from `ResultStage` to `ShuffleMapStage` (#9423)\n- Automatically set the max/min direction for the best score during early stopping. (#9404)\n* Revised support for `flink` (#9046)\n\n* Breaking changes\n- Scala-based tracker is removed. (#9078, #9045)\n- Change `DeviceQuantileDmatrix` into `QuantileDMatrix` (#8461)\n\n* Maintenance (#9253, #9166, #9395, #9389, #9224, #9233, #9351, #9479)\n\n* CI bot PRs\nWe employed GitHub dependent bot to help us keep the dependencies up-to-date for JVM packages. With the help from the bot, we have cleared up all the dependencies that are lagging behind (#8501, #8507).\n\nHere's a list of dependency update PRs including those made by dependent bots (#8456, #8560, #8571, #8561, #8562, #8600, #8594, #8524, #8509, #8548, #8549, #8533, #8521, #8534, #8532, #8516, #8503, #8531, #8530, #8518, #8512, #8515, #8517, #8506, #8504, #8502, #8629, #8815, #8813, #8814, #8877, #8876, #8875, #8874, #8873, #9049, #9070, #9073, #9039, #9083, #8917, #8952, #8980, #8973, #8962, #9252, #9208, #9131, #9136, #9219, #9160, #9158, #9163, #9184, #9192, #9265, #9268, #8882, #8837, #8662, #8661, #8390, #9056, #8508, #8925, #8920, #9149, #9230, #9097, #8648, #9203, #8593)."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance\nMaintenance work includes refactoring, fixing small issues that don't affect end users. (#9256, #8627, #8756, #8735, #8966, #8864, #8747, #8892, #9057, #8921, #8949, #8941, #8942, #9108, #9125, #9155, #9153, #9176, #9447, #9444, #9436, #9438, #9430, #9200, #9210, #9055, #9014, #9004, #8999, #9154, #9148, #9283, #9246, #8888, #8900, #8871, #8861, #8858, #8791, #8807, #8751, #8703, #8696, #8693, #8677, #8686, #8665, #8660, #8386, #8371, #8410, #8578, #8574, #8483, #8443, #8454, #8733)\n\n### CI\n- Build pip wheel with RMM support (#9383)\n- Other CI updates including updating dependencies and work on the CI infrastructure. (#9464, #9428, #8767, #9394, #9278, #9214, #9234, #9205, #9034, #9104, #8878, #9294, #8625, #8806, #8741, #8707, #8381, #8382, #8388, #8402, #8397, #8445, #8602, #8628, #8583, #8460, #9544)\n\n## 1.7.6 (2023 Jun 16)\n\nThis is a patch release for bug fixes. The CRAN package for the R binding is kept at 1.7.5.\n\n### Bug Fixes\n* Fix distributed training with mixed dense and sparse partitions. (#9272)\n* Fix monotone constraints on CPU with large trees. (#9122)\n* [spark] Make the spark model have the same UID as its estimator (#9022)\n* Optimize prediction with `QuantileDMatrix`. (#9096)\n\n### Document\n* Improve doxygen (#8959)\n* Update the cuDF pip index URL. (#9106)\n\n### Maintenance\n* Fix tests with pandas 2.0. (#9014)\n\n## 1.7.5 (2023 Mar 30)\nThis is a patch release for bug fixes.\n\n* C++ requirement is updated to C++-17, along with which, CUDA 11.8 is used as the default CTK. (#8860, #8855, #8853)\n* Fix import for pyspark ranker. (#8692)\n* Fix Windows binary wheel to be compatible with Poetry (#8991)\n* Fix GPU hist with column sampling. (#8850)\n* Make sure iterative DMatrix is properly initialized. (#8997)\n* [R] Update link in document. (#8998)\n\n## 1.7.4 (2023 Feb 16)\nThis is a patch release for bug fixes.\n\n* [R] Fix OpenMP detection on macOS. (#8684)\n* [Python] Make sure input numpy array is aligned. (#8690)\n* Fix feature interaction with column sampling in gpu_hist evaluator. (#8754)\n* Fix GPU L1 error. (#8749)\n* [PySpark] Fix feature types param (#8772)\n* Fix ranking with quantile dmatrix and group weight. (#8762)\n\n## 1.7.3 (2023 Jan 6)\nThis is a patch release for bug fixes.\n\n* [Breaking] XGBoost Sklearn estimator method `get_params` no longer returns internally configured values. (#8634)\n* Fix linalg iterator, which may crash the L1 error. (#8603)\n* Fix loading pickled GPU model with a CPU-only XGBoost build. (#8632)\n* Fix inference with unseen categories with categorical features. (#8591, #8602)\n* CI fixes. (#8620, #8631, #8579)\n\n## v1.7.2 (2022 Dec 8)\nThis is a patch release for bug fixes.\n\n* Work with newer thrust and libcudacxx (#8432)\n* Support null value in CUDA array interface namespace. (#8486)\n* Use `getsockname` instead of `SO_DOMAIN` on AIX. (#8437)\n* [pyspark] Make QDM optional based on a cuDF check (#8471)\n* [pyspark] sort qid for SparkRanker. (#8497)\n* [dask] Properly await async method client.wait_for_workers. (#8558)\n\n* [R] Fix CRAN test notes. (#8428)\n\n* [doc] Fix outdated document [skip ci]. (#8527)\n* [CI] Fix github action mismatched glibcxx. (#8551)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance\nMaintenance work includes refactoring, fixing small issues that don't affect end users. (#9256, #8627, #8756, #8735, #8966, #8864, #8747, #8892, #9057, #8921, #8949, #8941, #8942, #9108, #9125, #9155, #9153, #9176, #9447, #9444, #9436, #9438, #9430, #9200, #9210, #9055, #9014, #9004, #8999, #9154, #9148, #9283, #9246, #8888, #8900, #8871, #8861, #8858, #8791, #8807, #8751, #8703, #8696, #8693, #8677, #8686, #8665, #8660, #8386, #8371, #8410, #8578, #8574, #8483, #8443, #8454, #8733)\n\n### CI\n- Build pip wheel with RMM support (#9383)\n- Other CI updates including updating dependencies and work on the CI infrastructure. (#9464, #9428, #8767, #9394, #9278, #9214, #9234, #9205, #9034, #9104, #8878, #9294, #8625, #8806, #8741, #8707, #8381, #8382, #8388, #8402, #8397, #8445, #8602, #8628, #8583, #8460, #9544)\n\n## 1.7.6 (2023 Jun 16)\n\nThis is a patch release for bug fixes. The CRAN package for the R binding is kept at 1.7.5.\n\n### Bug Fixes\n* Fix distributed training with mixed dense and sparse partitions. (#9272)\n* Fix monotone constraints on CPU with large trees. (#9122)\n* [spark] Make the spark model have the same UID as its estimator (#9022)\n* Optimize prediction with `QuantileDMatrix`. (#9096)\n\n### Document\n* Improve doxygen (#8959)\n* Update the cuDF pip index URL. (#9106)\n\n### Maintenance\n* Fix tests with pandas 2.0. (#9014)\n\n## 1.7.5 (2023 Mar 30)\nThis is a patch release for bug fixes.\n\n* C++ requirement is updated to C++-17, along with which, CUDA 11.8 is used as the default CTK. (#8860, #8855, #8853)\n* Fix import for pyspark ranker. (#8692)\n* Fix Windows binary wheel to be compatible with Poetry (#8991)\n* Fix GPU hist with column sampling. (#8850)\n* Make sure iterative DMatrix is properly initialized. (#8997)\n* [R] Update link in document. (#8998)\n\n## 1.7.4 (2023 Feb 16)\nThis is a patch release for bug fixes.\n\n* [R] Fix OpenMP detection on macOS. (#8684)\n* [Python] Make sure input numpy array is aligned. (#8690)\n* Fix feature interaction with column sampling in gpu_hist evaluator. (#8754)\n* Fix GPU L1 error. (#8749)\n* [PySpark] Fix feature types param (#8772)\n* Fix ranking with quantile dmatrix and group weight. (#8762)\n\n## 1.7.3 (2023 Jan 6)\nThis is a patch release for bug fixes.\n\n* [Breaking] XGBoost Sklearn estimator method `get_params` no longer returns internally configured values. (#8634)\n* Fix linalg iterator, which may crash the L1 error. (#8603)\n* Fix loading pickled GPU model with a CPU-only XGBoost build. (#8632)\n* Fix inference with unseen categories with categorical features. (#8591, #8602)\n* CI fixes. (#8620, #8631, #8579)\n\n## v1.7.2 (2022 Dec 8)\nThis is a patch release for bug fixes.\n\n* Work with newer thrust and libcudacxx (#8432)\n* Support null value in CUDA array interface namespace. (#8486)\n* Use `getsockname` instead of `SO_DOMAIN` on AIX. (#8437)\n* [pyspark] Make QDM optional based on a cuDF check (#8471)\n* [pyspark] sort qid for SparkRanker. (#8497)\n* [dask] Properly await async method client.wait_for_workers. (#8558)\n\n* [R] Fix CRAN test notes. (#8428)\n\n* [doc] Fix outdated document [skip ci]. (#8527)\n* [CI] Fix github action mismatched glibcxx. (#8551)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v1.7.1 (2022 Nov 3)\nThis is a patch release to incorporate the following hotfix:\n\n* Add back xgboost.rabit for backwards compatibility (#8411)\n\n\n## v1.7.0 (2022 Oct 20)\n\nWe are excited to announce the feature packed XGBoost 1.7 release. The release note will walk through some of the major new features first, then make a summary for other improvements and language-binding-specific changes.\n\n### PySpark\n\nXGBoost 1.7 features initial support for PySpark integration. The new interface is adapted from the existing PySpark XGBoost interface developed by databricks with additional features like `QuantileDMatrix` and the rapidsai plugin (GPU pipeline) support. The new Spark XGBoost Python estimators not only benefit from PySpark ml facilities for powerful distributed computing but also enjoy the rest of the Python ecosystem. Users can define a custom objective, callbacks, and metrics in Python and use them with this interface on distributed clusters. The support is labeled as experimental with more features to come in future releases. For a brief introduction please visit the tutorial on XGBoost's [document page](https://xgboost.readthedocs.io/en/latest/tutorials/spark_estimator.html). (#8355, #8344, #8335, #8284, #8271, #8283, #8250, #8231, #8219, #8245, #8217, #8200, #8173, #8172, #8145, #8117, #8131, #8088, #8082, #8085, #8066, #8068, #8067, #8020, #8385)\n\nDue to its initial support status, the new interface has some limitations; categorical features and multi-output models are not yet supported.\n\n### Development of categorical data support\nMore progress on the experimental support for categorical features. In 1.7, XGBoost can handle missing values in categorical features and features a new parameter `max_cat_threshold`, which limits the number of categories that can be used in the split evaluation. The parameter is enabled when the partitioning algorithm is used and helps prevent over-fitting. Also, the sklearn interface can now accept the `feature_types` parameter to use data types other than dataframe for categorical features. (#8280, #7821, #8285, #8080, #7948, #7858, #7853, #8212, #7957, #7937, #7934)\n\n\n###  Experimental support for federated learning and new communication collective\n\nAn exciting addition to XGBoost is the experimental federated learning support. The federated learning is implemented with a gRPC federated server that aggregates allreduce calls, and federated clients that train on local data and use existing tree methods (approx, hist, gpu_hist). Currently, this only supports horizontal federated learning (samples are split across participants, and each participant has all the features and labels). Future plans include vertical federated learning (features split across participants), and stronger privacy guarantees with homomorphic encryption and differential privacy. See [Demo with NVFlare integration](demo/nvflare/README.md) for example usage with nvflare.\n\nAs part of the work, XGBoost 1.7 has replaced the old rabit module with the new collective module as the network communication interface with added support for runtime backend selection. In previous versions, the backend is defined at compile time and can not be changed once built. In this new release, users can choose between `rabit` and `federated.` (#8029, #8351, #8350, #8342, #8340, #8325, #8279, #8181, #8027, #7958, #7831, #7879, #8257, #8316, #8242, #8057, #8203, #8038, #7965, #7930, #7911)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v1.7.1 (2022 Nov 3)\nThis is a patch release to incorporate the following hotfix:\n\n* Add back xgboost.rabit for backwards compatibility (#8411)\n\n\n## v1.7.0 (2022 Oct 20)\n\nWe are excited to announce the feature packed XGBoost 1.7 release. The release note will walk through some of the major new features first, then make a summary for other improvements and language-binding-specific changes.\n\n### PySpark\n\nXGBoost 1.7 features initial support for PySpark integration. The new interface is adapted from the existing PySpark XGBoost interface developed by databricks with additional features like `QuantileDMatrix` and the rapidsai plugin (GPU pipeline) support. The new Spark XGBoost Python estimators not only benefit from PySpark ml facilities for powerful distributed computing but also enjoy the rest of the Python ecosystem. Users can define a custom objective, callbacks, and metrics in Python and use them with this interface on distributed clusters. The support is labeled as experimental with more features to come in future releases. For a brief introduction please visit the tutorial on XGBoost's [document page](https://xgboost.readthedocs.io/en/latest/tutorials/spark_estimator.html). (#8355, #8344, #8335, #8284, #8271, #8283, #8250, #8231, #8219, #8245, #8217, #8200, #8173, #8172, #8145, #8117, #8131, #8088, #8082, #8085, #8066, #8068, #8067, #8020, #8385)\n\nDue to its initial support status, the new interface has some limitations; categorical features and multi-output models are not yet supported.\n\n### Development of categorical data support\nMore progress on the experimental support for categorical features. In 1.7, XGBoost can handle missing values in categorical features and features a new parameter `max_cat_threshold`, which limits the number of categories that can be used in the split evaluation. The parameter is enabled when the partitioning algorithm is used and helps prevent over-fitting. Also, the sklearn interface can now accept the `feature_types` parameter to use data types other than dataframe for categorical features. (#8280, #7821, #8285, #8080, #7948, #7858, #7853, #8212, #7957, #7937, #7934)\n\n\n###  Experimental support for federated learning and new communication collective\n\nAn exciting addition to XGBoost is the experimental federated learning support. The federated learning is implemented with a gRPC federated server that aggregates allreduce calls, and federated clients that train on local data and use existing tree methods (approx, hist, gpu_hist). Currently, this only supports horizontal federated learning (samples are split across participants, and each participant has all the features and labels). Future plans include vertical federated learning (features split across participants), and stronger privacy guarantees with homomorphic encryption and differential privacy. See [Demo with NVFlare integration](demo/nvflare/README.md) for example usage with nvflare.\n\nAs part of the work, XGBoost 1.7 has replaced the old rabit module with the new collective module as the network communication interface with added support for runtime backend selection. In previous versions, the backend is defined at compile time and can not be changed once built. In this new release, users can choose between `rabit` and `federated.` (#8029, #8351, #8350, #8342, #8340, #8325, #8279, #8181, #8027, #7958, #7831, #7879, #8257, #8316, #8242, #8057, #8203, #8038, #7965, #7930, #7911)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nThe feature is available in the public PyPI binary package for testing.\n\n### Quantile DMatrix\nBefore 1.7, XGBoost has an internal data structure called `DeviceQuantileDMatrix` (and its distributed version). We now extend its support to CPU and renamed it to `QuantileDMatrix`. This data structure is used for optimizing memory usage for the `hist` and `gpu_hist` tree methods. The new feature helps reduce CPU memory usage significantly, especially for dense data. The new `QuantileDMatrix` can be initialized from both CPU and GPU data, and regardless of where the data comes from, the constructed instance can be used by both the CPU algorithm and GPU algorithm including training and prediction (with some overhead of conversion if the device of data and training algorithm doesn't match). Also, a new parameter `ref` is added to `QuantileDMatrix`, which can be used to construct validation/test datasets. Lastly, it's set as default in the scikit-learn interface when a supported tree method is specified by users. (#7889, #7923, #8136, #8215, #8284, #8268, #8220, #8346, #8327, #8130, #8116, #8103, #8094, #8086, #7898, #8060, #8019, #8045, #7901, #7912, #7922)\n\n### Mean absolute error\nThe mean absolute error is a new member of the collection of objectives in XGBoost. It's noteworthy since MAE has zero hessian value, which is unusual to XGBoost as XGBoost relies on Newton optimization. Without valid Hessian values, the convergence speed can be slow. As part of the support for MAE, we added line searches into the XGBoost training algorithm to overcome the difficulty of training without valid Hessian values. In the future, we will extend the line search to other objectives where it's appropriate for faster convergence speed. (#8343, #8107, #7812, #8380)\n\n### XGBoost on Browser\nWith the help of the [pyodide](https://github.com/pyodide/pyodide) project, you can now run XGBoost on browsers. (#7954, #8369)\n\n### Experimental IPv6 Support for Dask\n\nWith the growing adaption of the new internet protocol, XGBoost joined the club. In the latest release, the Dask interface can be used on IPv6 clusters, see XGBoost's Dask tutorial for details. (#8225, #8234)\n\n### Optimizations\nWe have new optimizations for both the `hist` and `gpu_hist` tree methods to make XGBoost's training even more efficient.\n\n* Hist\nHist now supports optional by-column histogram build, which is automatically configured based on various conditions of input data. This helps the XGBoost CPU hist algorithm to scale better with different shapes of training datasets. (#8233, #8259). Also, the build histogram kernel now can better utilize CPU registers (#8218)\n\n* GPU Hist\nGPU hist performance is significantly improved for wide datasets. GPU hist now supports batched node build, which reduces kernel latency and increases throughput. The improvement is particularly significant when growing deep trees with the default ``depthwise`` policy. (#7919, #8073, #8051, #8118, #7867, #7964, #8026)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Breaking Changes\nBreaking changes made in the 1.7 release are summarized below.\n- The  `grow_local_histmaker`  updater is removed. This updater is rarely used in practice and has no test. We decided to remove it and focus have XGBoot focus on other more efficient algorithms. (#7992, #8091)\n- Single precision histogram is removed due to its lack of accuracy caused by significant floating point error. In some cases the error can be difficult to detect due to log-scale operations, which makes the parameter dangerous to use. (#7892, #7828)\n- Deprecated CUDA architectures are no longer supported in the release binaries. (#7774)\n- As part of the federated learning development, the `rabit` module is replaced with the new `collective` module. It's a drop-in replacement with added runtime backend selection, see the federated learning section for more details (#8257)\n\n### General new features and improvements\nBefore diving into package-specific changes, some general new features other than those listed at the beginning are summarized here.\n* Users of `DMatrix` and `QuantileDMatrix` can get the data from XGBoost. In previous versions, only getters for meta info like labels are available. The new method is available in Python (`DMatrix::get_data`) and C. (#8269, #8323)\n* In previous versions, the GPU histogram tree method may generate phantom gradient for missing values due to floating point error. We fixed such an error in this release and XGBoost is much better equated to handle floating point errors when training on GPU. (#8274, #8246)\n* Parameter validation is no longer experimental. (#8206)\n* C pointer parameters and JSON parameters are vigorously checked. (#8254, #8254)\n* Improved handling of JSON model input. (#7953, #7918)\n* Support IBM i OS (#7920, #8178)\n\n### Fixes\nSome noteworthy bug fixes that are not related to specific language binding are listed in this section.\n* Rename misspelled config parameter for pseudo-Huber (#7904)\n* Fix feature weights with nested column sampling. (#8100)\n* Fix loading DMatrix binary in distributed env. (#8149)\n* Force auc.cc to be statically linked for unusual compiler platforms. (#8039)\n* New logic for detecting libomp on macos (#8384).\n\n### Python Package\n* Python 3.8 is now the minimum required Python version. (#8071)\n* More progress on type hint support. Except for the new PySpark interface, the XGBoost module is fully typed. (#7742, #7945, #8302, #7914, #8052)\n* XGBoost now validates the feature names in `inplace_predict`, which also affects the predict function in scikit-learn estimators as it uses `inplace_predict` internally. (#8359)\n* Users can now get the data from `DMatrix` using `DMatrix::get_data` or `QuantileDMatrix::get_data`.\n* Show `libxgboost.so` path in build info. (#7893)\n* Raise import error when using the sklearn module while scikit-learn is missing. (#8049)\n* Use `config_context` in the sklearn interface. (#8141)\n* Validate features for inplace prediction. (#8359)\n* Pandas dataframe handling is refactored to reduce data fragmentation. (#7843)\n* Support more pandas nullable types (#8262)\n* Remove pyarrow workaround. (#7884)\n\n* Binary wheel size\nWe aim to enable as many features as possible in XGBoost's default binary distribution on PyPI (package installed with pip), but there's a upper limit on the size of the binary wheel. In 1.7, XGBoost reduces the size of the wheel by pruning unused CUDA architectures. (#8179, #8152, #8150)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Breaking Changes\nBreaking changes made in the 1.7 release are summarized below.\n- The  `grow_local_histmaker`  updater is removed. This updater is rarely used in practice and has no test. We decided to remove it and focus have XGBoot focus on other more efficient algorithms. (#7992, #8091)\n- Single precision histogram is removed due to its lack of accuracy caused by significant floating point error. In some cases the error can be difficult to detect due to log-scale operations, which makes the parameter dangerous to use. (#7892, #7828)\n- Deprecated CUDA architectures are no longer supported in the release binaries. (#7774)\n- As part of the federated learning development, the `rabit` module is replaced with the new `collective` module. It's a drop-in replacement with added runtime backend selection, see the federated learning section for more details (#8257)\n\n### General new features and improvements\nBefore diving into package-specific changes, some general new features other than those listed at the beginning are summarized here.\n* Users of `DMatrix` and `QuantileDMatrix` can get the data from XGBoost. In previous versions, only getters for meta info like labels are available. The new method is available in Python (`DMatrix::get_data`) and C. (#8269, #8323)\n* In previous versions, the GPU histogram tree method may generate phantom gradient for missing values due to floating point error. We fixed such an error in this release and XGBoost is much better equated to handle floating point errors when training on GPU. (#8274, #8246)\n* Parameter validation is no longer experimental. (#8206)\n* C pointer parameters and JSON parameters are vigorously checked. (#8254, #8254)\n* Improved handling of JSON model input. (#7953, #7918)\n* Support IBM i OS (#7920, #8178)\n\n### Fixes\nSome noteworthy bug fixes that are not related to specific language binding are listed in this section.\n* Rename misspelled config parameter for pseudo-Huber (#7904)\n* Fix feature weights with nested column sampling. (#8100)\n* Fix loading DMatrix binary in distributed env. (#8149)\n* Force auc.cc to be statically linked for unusual compiler platforms. (#8039)\n* New logic for detecting libomp on macos (#8384).\n\n### Python Package\n* Python 3.8 is now the minimum required Python version. (#8071)\n* More progress on type hint support. Except for the new PySpark interface, the XGBoost module is fully typed. (#7742, #7945, #8302, #7914, #8052)\n* XGBoost now validates the feature names in `inplace_predict`, which also affects the predict function in scikit-learn estimators as it uses `inplace_predict` internally. (#8359)\n* Users can now get the data from `DMatrix` using `DMatrix::get_data` or `QuantileDMatrix::get_data`.\n* Show `libxgboost.so` path in build info. (#7893)\n* Raise import error when using the sklearn module while scikit-learn is missing. (#8049)\n* Use `config_context` in the sklearn interface. (#8141)\n* Validate features for inplace prediction. (#8359)\n* Pandas dataframe handling is refactored to reduce data fragmentation. (#7843)\n* Support more pandas nullable types (#8262)\n* Remove pyarrow workaround. (#7884)\n\n* Binary wheel size\nWe aim to enable as many features as possible in XGBoost's default binary distribution on PyPI (package installed with pip), but there's a upper limit on the size of the binary wheel. In 1.7, XGBoost reduces the size of the wheel by pruning unused CUDA architectures. (#8179, #8152, #8150)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n  Some noteworthy fixes are listed here:\n  - Fix the Dask interface with the latest cupy. (#8210)\n  - Check cuDF lazily to avoid potential errors with cuda-python. (#8084)\n* Fix potential error in DMatrix constructor on 32-bit platform. (#8369)\n\n* Maintenance work\n  - Linter script is moved from dmlc-core to XGBoost with added support for formatting, mypy, and parallel run, along with some fixes (#7967, #8101, #8216)\n  - We now require the use of `isort` and `black` for selected files. (#8137, #8096)\n  - Code cleanups. (#7827)\n  - Deprecate `use_label_encoder` in XGBClassifier. The label encoder has already been deprecated and removed in the previous version. These changes only affect the indicator parameter (#7822)\n  - Remove the use of distutils. (#7770)\n  - Refactor and fixes for tests (#8077, #8064, #8078, #8076, #8013, #8010, #8244, #7833)\n\n* Documents\n  - [dask] Fix potential error in demo. (#8079)\n  - Improved documentation for the ranker. (#8356, #8347)\n  - Indicate lack of py-xgboost-gpu on Windows (#8127)\n  - Clarification for feature importance. (#8151)\n  - Simplify Python getting started example (#8153)\n\n### R Package\nWe summarize improvements for the R package briefly here:\n* Feature info including names and types are now passed to DMatrix in preparation for categorical feature support. (#804)\n* XGBoost 1.7 can now gracefully load old R models from RDS for better compatibility with 3-party tuning libraries (#7864)\n* The R package now can be built with parallel compilation, along with fixes for warnings in CRAN tests. (#8330)\n* Emit error early if DiagrammeR is missing (#8037)\n* Fix R package Windows build. (#8065)\n\n### JVM Packages\nThe consistency between JVM packages and other language bindings is greatly improved in 1.7, improvements range from model serialization format to the default value of hyper-parameters.\n\n* Java package now supports feature names and feature types for DMatrix in preparation for categorical feature support. (#7966)\n* Models trained by the JVM packages can now be safely used with other language bindings. (#7896, #7907)\n* Users can specify the model format when saving models with a stream. (#7940, #7955)\n* The default value for training parameters is now sourced from XGBoost directly, which helps JVM packages be consistent with other packages. (#7938)\n* Set the correct objective if the user doesn't explicitly set it (#7781)\n* Auto-detection of MUSL is replaced by system properties (#7921)\n* Improved error message for launching tracker. (#7952, #7968)\n* Fix a race condition in parameter configuration. (#8025)\n* [Breaking] ` timeoutRequestWorkers` is now removed. With the support for barrier mode, this parameter is no longer needed. (#7839)\n* Dependencies updates. (#7791, #8157, #7801, #8240)\n\n### Documents\n- Document for the C interface is greatly improved and is now displayed at the [sphinx document page](https://xgboost.readthedocs.io/en/latest/c.html). Thanks to the breathe project, you can view the C API just like the Python API. (#8300)\n- We now avoid having XGBoost internal text parser in demos and recommend users use dedicated libraries for loading data whenever it's feasible. (#7753)\n- Python survival training demos are now displayed at [sphinx gallery](https://xgboost.readthedocs.io/en/latest/python/survival-examples/index.html). (#8328)\n- Some typos, links, format, and grammar fixes. (#7800, #7832, #7861, #8099, #8163, #8166, #8229, #8028, #8214, #7777, #7905, #8270, #8309, d70e59fef, #7806)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fixes\n  Some noteworthy fixes are listed here:\n  - Fix the Dask interface with the latest cupy. (#8210)\n  - Check cuDF lazily to avoid potential errors with cuda-python. (#8084)\n* Fix potential error in DMatrix constructor on 32-bit platform. (#8369)\n\n* Maintenance work\n  - Linter script is moved from dmlc-core to XGBoost with added support for formatting, mypy, and parallel run, along with some fixes (#7967, #8101, #8216)\n  - We now require the use of `isort` and `black` for selected files. (#8137, #8096)\n  - Code cleanups. (#7827)\n  - Deprecate `use_label_encoder` in XGBClassifier. The label encoder has already been deprecated and removed in the previous version. These changes only affect the indicator parameter (#7822)\n  - Remove the use of distutils. (#7770)\n  - Refactor and fixes for tests (#8077, #8064, #8078, #8076, #8013, #8010, #8244, #7833)\n\n* Documents\n  - [dask] Fix potential error in demo. (#8079)\n  - Improved documentation for the ranker. (#8356, #8347)\n  - Indicate lack of py-xgboost-gpu on Windows (#8127)\n  - Clarification for feature importance. (#8151)\n  - Simplify Python getting started example (#8153)\n\n### R Package\nWe summarize improvements for the R package briefly here:\n* Feature info including names and types are now passed to DMatrix in preparation for categorical feature support. (#804)\n* XGBoost 1.7 can now gracefully load old R models from RDS for better compatibility with 3-party tuning libraries (#7864)\n* The R package now can be built with parallel compilation, along with fixes for warnings in CRAN tests. (#8330)\n* Emit error early if DiagrammeR is missing (#8037)\n* Fix R package Windows build. (#8065)\n\n### JVM Packages\nThe consistency between JVM packages and other language bindings is greatly improved in 1.7, improvements range from model serialization format to the default value of hyper-parameters.\n\n* Java package now supports feature names and feature types for DMatrix in preparation for categorical feature support. (#7966)\n* Models trained by the JVM packages can now be safely used with other language bindings. (#7896, #7907)\n* Users can specify the model format when saving models with a stream. (#7940, #7955)\n* The default value for training parameters is now sourced from XGBoost directly, which helps JVM packages be consistent with other packages. (#7938)\n* Set the correct objective if the user doesn't explicitly set it (#7781)\n* Auto-detection of MUSL is replaced by system properties (#7921)\n* Improved error message for launching tracker. (#7952, #7968)\n* Fix a race condition in parameter configuration. (#8025)\n* [Breaking] ` timeoutRequestWorkers` is now removed. With the support for barrier mode, this parameter is no longer needed. (#7839)\n* Dependencies updates. (#7791, #8157, #7801, #8240)\n\n### Documents\n- Document for the C interface is greatly improved and is now displayed at the [sphinx document page](https://xgboost.readthedocs.io/en/latest/c.html). Thanks to the breathe project, you can view the C API just like the Python API. (#8300)\n- We now avoid having XGBoost internal text parser in demos and recommend users use dedicated libraries for loading data whenever it's feasible. (#7753)\n- Python survival training demos are now displayed at [sphinx gallery](https://xgboost.readthedocs.io/en/latest/python/survival-examples/index.html). (#8328)\n- Some typos, links, format, and grammar fixes. (#7800, #7832, #7861, #8099, #8163, #8166, #8229, #8028, #8214, #7777, #7905, #8270, #8309, d70e59fef, #7806)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Updated winning solution under readme.md (#7862)\n- New security policy. (#8360)\n- GPU document is overhauled as we consider CUDA support to be feature-complete. (#8378)\n\n### Maintenance\n* Code refactoring and cleanups. (#7850, #7826, #7910, #8332, #8204)\n* Reduce compiler warnings. (#7768, #7916, #8046, #8059, #7974, #8031, #8022)\n* Compiler workarounds. (#8211, #8314, #8226, #8093)\n* Dependencies update. (#8001, #7876, #7973, #8298, #7816)\n* Remove warnings emitted in previous versions. (#7815)\n* Small fixes occurred during development. (#8008)\n\n### CI and Tests\n* We overhauled the CI infrastructure to reduce the CI cost and lift the maintenance burdens. Jenkins is replaced with buildkite for better automation, with which, finer control of test runs is implemented to reduce overall cost. Also, we refactored some of the existing tests to reduce their runtime, drooped the size of docker images, and removed multi-GPU C++ tests. Lastly, `pytest-timeout` is added as an optional dependency for running Python tests to keep the test time in check. (#7772, #8291, #8286, #8276, #8306, #8287, #8243, #8313, #8235, #8288, #8303, #8142, #8092, #8333, #8312, #8348)\n* New documents for how to reproduce the CI environment (#7971, #8297)\n* Improved automation for JVM release. (#7882)\n* GitHub Action security-related updates. (#8263, #8267, #8360)\n* Other fixes and maintenance work. (#8154, #7848, #8069, #7943)\n* Small updates and fixes to GitHub action pipelines. (#8364, #8321, #8241, #7950, #8011)\n\n## v1.6.1 (2022 May 9)\nThis is a patch release for bug fixes and Spark barrier mode support. The R package is unchanged.\n\n### Experimental support for categorical data\n- Fix segfault when the number of samples is smaller than the number of categories. (https://github.com/dmlc/xgboost/pull/7853)\n- Enable partition-based split for all model types. (https://github.com/dmlc/xgboost/pull/7857)\n\n### JVM packages\nWe replaced the old parallelism tracker with spark barrier mode to improve the robustness of the JVM package and fix the GPU training pipeline.\n- Fix GPU training pipeline quantile synchronization. (#7823, #7834)\n- Use barrier model in spark package. (https://github.com/dmlc/xgboost/pull/7836, https://github.com/dmlc/xgboost/pull/7840, https://github.com/dmlc/xgboost/pull/7845, https://github.com/dmlc/xgboost/pull/7846)\n- Fix shared object loading on some platforms. (https://github.com/dmlc/xgboost/pull/7844)\n\n## v1.6.0 (2022 Apr 16)\n\nAfter a long period of development, XGBoost v1.6.0 is packed with many new features and\nimprovements. We summarize them in the following sections starting with an introduction to\nsome major new features, then moving on to language binding specific changes including new\nfeatures and notable bug fixes for that binding.\n\n### Development of categorical data support\nThis version of XGBoost features new improvements and full coverage of experimental\ncategorical data support in Python and C package with tree model.  Both `hist`, `approx`\nand `gpu_hist` now support training with categorical data.  Also, partition-based\ncategorical split is introduced in this release. This split type is first available in\nLightGBM in the context of gradient boosting. The previous XGBoost release supported one-hot split where the splitting criteria is of form `x \\in {c}`, i.e. the categorical feature `x` is tested against a single candidate. The new release allows for more expressive conditions: `x \\in S` where the categorical feature `x` is tested against multiple candidates. Moreover, it is now possible to use any tree algorithms (`hist`, `approx`, `gpu_hist`) when creating categorical splits. For more", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Updated winning solution under readme.md (#7862)\n- New security policy. (#8360)\n- GPU document is overhauled as we consider CUDA support to be feature-complete. (#8378)\n\n### Maintenance\n* Code refactoring and cleanups. (#7850, #7826, #7910, #8332, #8204)\n* Reduce compiler warnings. (#7768, #7916, #8046, #8059, #7974, #8031, #8022)\n* Compiler workarounds. (#8211, #8314, #8226, #8093)\n* Dependencies update. (#8001, #7876, #7973, #8298, #7816)\n* Remove warnings emitted in previous versions. (#7815)\n* Small fixes occurred during development. (#8008)\n\n### CI and Tests\n* We overhauled the CI infrastructure to reduce the CI cost and lift the maintenance burdens. Jenkins is replaced with buildkite for better automation, with which, finer control of test runs is implemented to reduce overall cost. Also, we refactored some of the existing tests to reduce their runtime, drooped the size of docker images, and removed multi-GPU C++ tests. Lastly, `pytest-timeout` is added as an optional dependency for running Python tests to keep the test time in check. (#7772, #8291, #8286, #8276, #8306, #8287, #8243, #8313, #8235, #8288, #8303, #8142, #8092, #8333, #8312, #8348)\n* New documents for how to reproduce the CI environment (#7971, #8297)\n* Improved automation for JVM release. (#7882)\n* GitHub Action security-related updates. (#8263, #8267, #8360)\n* Other fixes and maintenance work. (#8154, #7848, #8069, #7943)\n* Small updates and fixes to GitHub action pipelines. (#8364, #8321, #8241, #7950, #8011)\n\n## v1.6.1 (2022 May 9)\nThis is a patch release for bug fixes and Spark barrier mode support. The R package is unchanged.\n\n### Experimental support for categorical data\n- Fix segfault when the number of samples is smaller than the number of categories. (https://github.com/dmlc/xgboost/pull/7853)\n- Enable partition-based split for all model types. (https://github.com/dmlc/xgboost/pull/7857)\n\n### JVM packages\nWe replaced the old parallelism tracker with spark barrier mode to improve the robustness of the JVM package and fix the GPU training pipeline.\n- Fix GPU training pipeline quantile synchronization. (#7823, #7834)\n- Use barrier model in spark package. (https://github.com/dmlc/xgboost/pull/7836, https://github.com/dmlc/xgboost/pull/7840, https://github.com/dmlc/xgboost/pull/7845, https://github.com/dmlc/xgboost/pull/7846)\n- Fix shared object loading on some platforms. (https://github.com/dmlc/xgboost/pull/7844)\n\n## v1.6.0 (2022 Apr 16)\n\nAfter a long period of development, XGBoost v1.6.0 is packed with many new features and\nimprovements. We summarize them in the following sections starting with an introduction to\nsome major new features, then moving on to language binding specific changes including new\nfeatures and notable bug fixes for that binding.\n\n### Development of categorical data support\nThis version of XGBoost features new improvements and full coverage of experimental\ncategorical data support in Python and C package with tree model.  Both `hist`, `approx`\nand `gpu_hist` now support training with categorical data.  Also, partition-based\ncategorical split is introduced in this release. This split type is first available in\nLightGBM in the context of gradient boosting. The previous XGBoost release supported one-hot split where the splitting criteria is of form `x \\in {c}`, i.e. the categorical feature `x` is tested against a single candidate. The new release allows for more expressive conditions: `x \\in S` where the categorical feature `x` is tested against multiple candidates. Moreover, it is now possible to use any tree algorithms (`hist`, `approx`, `gpu_hist`) when creating categorical splits. For more"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\ninformation, please see our tutorial on [categorical\ndata](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html), along with\nexamples linked on that page. (#7380, #7708, #7695, #7330, #7307, #7322, #7705,\n#7652, #7592, #7666, #7576, #7569, #7529, #7575, #7393, #7465, #7385, #7371, #7745, #7810)\n\nIn the future, we will continue to improve categorical data support with new features and\noptimizations. Also, we are looking forward to bringing the feature beyond Python binding,\ncontributions and feedback are welcomed! Lastly, as a result of experimental status, the\nbehavior might be subject to change, especially the default value of related\nhyper-parameters.\n\n### Experimental support for multi-output model\n\nXGBoost 1.6 features initial support for the multi-output model, which includes\nmulti-output regression and multi-label classification. Along with this, the XGBoost\nclassifier has proper support for base margin without to need for the user to flatten the\ninput. In this initial support, XGBoost builds one model for each target similar to the\nsklearn meta estimator, for more details, please see our [quick\nintroduction](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html).\n\n(#7365, #7736, #7607, #7574, #7521, #7514, #7456, #7453, #7455, #7434, #7429, #7405, #7381)\n\n### External memory support\nExternal memory support for both approx and hist tree method is considered feature\ncomplete in XGBoost 1.6.  Building upon the iterator-based interface introduced in the\nprevious version, now both `hist` and `approx` iterates over each batch of data during\ntraining and prediction.  In previous versions, `hist` concatenates all the batches into\nan internal representation, which is removed in this version.  As a result, users can\nexpect higher scalability in terms of data size but might experience lower performance due\nto disk IO. (#7531, #7320, #7638, #7372)\n\n### Rewritten approx\n\nThe `approx` tree method is rewritten based on the existing `hist` tree method. The\nrewrite closes the feature gap between `approx` and `hist` and improves the performance.\nNow the behavior of `approx` should be more aligned with `hist` and `gpu_hist`. Here is a\nlist of user-visible changes:\n\n- Supports both `max_leaves` and `max_depth`.\n- Supports `grow_policy`.\n- Supports monotonic constraint.\n- Supports feature weights.\n- Use `max_bin` to replace `sketch_eps`.\n- Supports categorical data.\n- Faster performance for many of the datasets.\n- Improved performance and robustness for distributed training.\n- Supports prediction cache.\n- Significantly better performance for external memory when `depthwise` policy is used.\n\n### New serialization format\nBased on the existing JSON serialization format, we introduce UBJSON support as a more\nefficient alternative. Both formats will be available in the future and we plan to\ngradually [phase out](https://github.com/dmlc/xgboost/issues/7547) support for the old\nbinary model format.  Users can opt to use the different formats in the serialization\nfunction by providing the file extension `json` or `ubj`. Also, the `save_raw` function in\nall supported languages bindings gains a new parameter for exporting the model in different\nformats, available options are `json`, `ubj`, and `deprecated`, see document for the\nlanguage binding you are using for details. Lastly, the default internal serialization", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\ninformation, please see our tutorial on [categorical\ndata](https://xgboost.readthedocs.io/en/latest/tutorials/categorical.html), along with\nexamples linked on that page. (#7380, #7708, #7695, #7330, #7307, #7322, #7705,\n#7652, #7592, #7666, #7576, #7569, #7529, #7575, #7393, #7465, #7385, #7371, #7745, #7810)\n\nIn the future, we will continue to improve categorical data support with new features and\noptimizations. Also, we are looking forward to bringing the feature beyond Python binding,\ncontributions and feedback are welcomed! Lastly, as a result of experimental status, the\nbehavior might be subject to change, especially the default value of related\nhyper-parameters.\n\n### Experimental support for multi-output model\n\nXGBoost 1.6 features initial support for the multi-output model, which includes\nmulti-output regression and multi-label classification. Along with this, the XGBoost\nclassifier has proper support for base margin without to need for the user to flatten the\ninput. In this initial support, XGBoost builds one model for each target similar to the\nsklearn meta estimator, for more details, please see our [quick\nintroduction](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html).\n\n(#7365, #7736, #7607, #7574, #7521, #7514, #7456, #7453, #7455, #7434, #7429, #7405, #7381)\n\n### External memory support\nExternal memory support for both approx and hist tree method is considered feature\ncomplete in XGBoost 1.6.  Building upon the iterator-based interface introduced in the\nprevious version, now both `hist` and `approx` iterates over each batch of data during\ntraining and prediction.  In previous versions, `hist` concatenates all the batches into\nan internal representation, which is removed in this version.  As a result, users can\nexpect higher scalability in terms of data size but might experience lower performance due\nto disk IO. (#7531, #7320, #7638, #7372)\n\n### Rewritten approx\n\nThe `approx` tree method is rewritten based on the existing `hist` tree method. The\nrewrite closes the feature gap between `approx` and `hist` and improves the performance.\nNow the behavior of `approx` should be more aligned with `hist` and `gpu_hist`. Here is a\nlist of user-visible changes:\n\n- Supports both `max_leaves` and `max_depth`.\n- Supports `grow_policy`.\n- Supports monotonic constraint.\n- Supports feature weights.\n- Use `max_bin` to replace `sketch_eps`.\n- Supports categorical data.\n- Faster performance for many of the datasets.\n- Improved performance and robustness for distributed training.\n- Supports prediction cache.\n- Significantly better performance for external memory when `depthwise` policy is used.\n\n### New serialization format\nBased on the existing JSON serialization format, we introduce UBJSON support as a more\nefficient alternative. Both formats will be available in the future and we plan to\ngradually [phase out](https://github.com/dmlc/xgboost/issues/7547) support for the old\nbinary model format.  Users can opt to use the different formats in the serialization\nfunction by providing the file extension `json` or `ubj`. Also, the `save_raw` function in\nall supported languages bindings gains a new parameter for exporting the model in different\nformats, available options are `json`, `ubj`, and `deprecated`, see document for the\nlanguage binding you are using for details. Lastly, the default internal serialization"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nformat is set to UBJSON, which affects Python pickle and R RDS. (#7572, #7570, #7358,\n#7571, #7556, #7549, #7416)\n\n### General new features and improvements\nAside from the major new features mentioned above, some others are summarized here:\n\n* Users can now access the build information of XGBoost binary in Python and C\n  interface. (#7399, #7553)\n* Auto-configuration of `seed_per_iteration` is removed, now distributed training should\n  generate closer results to single node training when sampling is used. (#7009)\n* A new parameter `huber_slope` is introduced for the `Pseudo-Huber` objective.\n* During source build, XGBoost can choose cub in the system path automatically. (#7579)\n* XGBoost now honors the CPU counts from CFS, which is usually set in docker\n  environments. (#7654, #7704)\n* The metric `aucpr` is rewritten for better performance and GPU support. (#7297, #7368)\n* Metric calculation is now performed in double precision. (#7364)\n* XGBoost no longer mutates the global OpenMP thread limit. (#7537, #7519, #7608, #7590,\n  #7589, #7588, #7687)\n* The default behavior of `max_leave` and `max_depth` is now unified (#7302, #7551).\n* CUDA fat binary is now compressed. (#7601)\n* Deterministic result for evaluation metric and linear model. In previous versions of\n  XGBoost, evaluation results might differ slightly for each run due to parallel reduction\n  for floating-point values, which is now addressed. (#7362, #7303, #7316, #7349)\n* XGBoost now uses double for GPU Hist node sum, which improves the accuracy of\n  `gpu_hist`. (#7507)\n\n### Performance improvements\nMost of the performance improvements are integrated into other refactors during feature\ndevelopments. The `approx` should see significant performance gain for many datasets as\nmentioned in the previous section, while the `hist` tree method also enjoys improved\nperformance with the removal of the internal `pruner` along with some other\nrefactoring. Lastly, `gpu_hist` no longer synchronizes the device during training. (#7737)\n\n### General bug fixes\nThis section lists bug fixes that are not specific to any language binding.\n* The `num_parallel_tree` is now a model parameter instead of a training hyper-parameter,\n  which fixes model IO with random forest. (#7751)\n* Fixes in CMake script for exporting configuration. (#7730)\n* XGBoost can now handle unsorted sparse input. This includes text file formats like\n  libsvm and scipy sparse matrix where column index might not be sorted. (#7731)\n* Fix tree param feature type, this affects inputs with the number of columns greater than\n  the maximum value of int32. (#7565)\n* Fix external memory with gpu_hist and subsampling. (#7481)\n* Check the number of trees in inplace predict, this avoids a potential segfault when an\n  incorrect value for `iteration_range` is provided. (#7409)\n* Fix non-stable result in cox regression (#7756)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nformat is set to UBJSON, which affects Python pickle and R RDS. (#7572, #7570, #7358,\n#7571, #7556, #7549, #7416)\n\n### General new features and improvements\nAside from the major new features mentioned above, some others are summarized here:\n\n* Users can now access the build information of XGBoost binary in Python and C\n  interface. (#7399, #7553)\n* Auto-configuration of `seed_per_iteration` is removed, now distributed training should\n  generate closer results to single node training when sampling is used. (#7009)\n* A new parameter `huber_slope` is introduced for the `Pseudo-Huber` objective.\n* During source build, XGBoost can choose cub in the system path automatically. (#7579)\n* XGBoost now honors the CPU counts from CFS, which is usually set in docker\n  environments. (#7654, #7704)\n* The metric `aucpr` is rewritten for better performance and GPU support. (#7297, #7368)\n* Metric calculation is now performed in double precision. (#7364)\n* XGBoost no longer mutates the global OpenMP thread limit. (#7537, #7519, #7608, #7590,\n  #7589, #7588, #7687)\n* The default behavior of `max_leave` and `max_depth` is now unified (#7302, #7551).\n* CUDA fat binary is now compressed. (#7601)\n* Deterministic result for evaluation metric and linear model. In previous versions of\n  XGBoost, evaluation results might differ slightly for each run due to parallel reduction\n  for floating-point values, which is now addressed. (#7362, #7303, #7316, #7349)\n* XGBoost now uses double for GPU Hist node sum, which improves the accuracy of\n  `gpu_hist`. (#7507)\n\n### Performance improvements\nMost of the performance improvements are integrated into other refactors during feature\ndevelopments. The `approx` should see significant performance gain for many datasets as\nmentioned in the previous section, while the `hist` tree method also enjoys improved\nperformance with the removal of the internal `pruner` along with some other\nrefactoring. Lastly, `gpu_hist` no longer synchronizes the device during training. (#7737)\n\n### General bug fixes\nThis section lists bug fixes that are not specific to any language binding.\n* The `num_parallel_tree` is now a model parameter instead of a training hyper-parameter,\n  which fixes model IO with random forest. (#7751)\n* Fixes in CMake script for exporting configuration. (#7730)\n* XGBoost can now handle unsorted sparse input. This includes text file formats like\n  libsvm and scipy sparse matrix where column index might not be sorted. (#7731)\n* Fix tree param feature type, this affects inputs with the number of columns greater than\n  the maximum value of int32. (#7565)\n* Fix external memory with gpu_hist and subsampling. (#7481)\n* Check the number of trees in inplace predict, this avoids a potential segfault when an\n  incorrect value for `iteration_range` is provided. (#7409)\n* Fix non-stable result in cox regression (#7756)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Changes in the Python package\nOther than the changes in Dask, the XGBoost Python package gained some new features and\nimprovements along with small bug fixes.\n\n* Python 3.7 is required as the lowest Python version. (#7682)\n* Pre-built binary wheel for Apple Silicon. (#7621, #7612, #7747) Apple Silicon users will\n  now be able to run `pip install xgboost` to install XGBoost.\n* MacOS users no longer need to install `libomp` from Homebrew, as the XGBoost wheel now\n  bundles `libomp.dylib` library.\n* There are new parameters for users to specify the custom metric with new\n  behavior. XGBoost can now output transformed prediction values when a custom objective is\n  not supplied.  See our explanation in the\n  [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#reverse-link-function)\n  for details.\n* For the sklearn interface, following the estimator guideline from scikit-learn, all\n  parameters in `fit` that are not related to input data are moved into the constructor\n  and can be set by `set_params`. (#6751, #7420, #7375, #7369)\n* Apache arrow format is now supported, which can bring better performance to users'\n  pipeline (#7512)\n* Pandas nullable types are now supported (#7760)\n* A new function `get_group` is introduced for `DMatrix` to allow users to get the group\n  information in the custom objective function. (#7564)\n* More training parameters are exposed in the sklearn interface instead of relying on the\n  `**kwargs`. (#7629)\n* A new attribute `feature_names_in_` is defined for all sklearn estimators like\n  `XGBRegressor` to follow the convention of sklearn. (#7526)\n* More work on Python type hint. (#7432, #7348, #7338, #7513, #7707)\n* Support the latest pandas Index type. (#7595)\n* Fix for Feature shape mismatch error on s390x platform (#7715)\n* Fix using feature names for constraints with multiple groups (#7711)\n* We clarified the behavior of the callback function when it contains mutable\n  states. (#7685)\n* Lastly, there are some code cleanups and maintenance work. (#7585, #7426, #7634, #7665,\n  #7667, #7377, #7360, #7498, #7438, #7667, #7752, #7749, #7751)\n\n### Changes in the Dask interface\n* Dask module now supports user-supplied host IP and port address of scheduler node.\n  Please see [introduction](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#troubleshooting) and\n  [API document](https://xgboost.readthedocs.io/en/latest/python/python_api.html#optional-dask-configuration)\n  for reference. (#7645, #7581)\n* Internal `DMatrix` construction in dask now honers thread configuration. (#7337)\n* A fix for `nthread` configuration using the Dask sklearn interface. (#7633)\n* The Dask interface can now handle empty partitions.  An empty partition is different\n  from an empty worker, the latter refers to the case when a worker has no partition of an\n  input dataset, while the former refers to some partitions on a worker that has zero\n  sizes. (#7644, #7510)\n* Scipy sparse matrix is supported as Dask array partition. (#7457)\n* Dask interface is no longer considered experimental. (#7509)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Changes in the Python package\nOther than the changes in Dask, the XGBoost Python package gained some new features and\nimprovements along with small bug fixes.\n\n* Python 3.7 is required as the lowest Python version. (#7682)\n* Pre-built binary wheel for Apple Silicon. (#7621, #7612, #7747) Apple Silicon users will\n  now be able to run `pip install xgboost` to install XGBoost.\n* MacOS users no longer need to install `libomp` from Homebrew, as the XGBoost wheel now\n  bundles `libomp.dylib` library.\n* There are new parameters for users to specify the custom metric with new\n  behavior. XGBoost can now output transformed prediction values when a custom objective is\n  not supplied.  See our explanation in the\n  [tutorial](https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#reverse-link-function)\n  for details.\n* For the sklearn interface, following the estimator guideline from scikit-learn, all\n  parameters in `fit` that are not related to input data are moved into the constructor\n  and can be set by `set_params`. (#6751, #7420, #7375, #7369)\n* Apache arrow format is now supported, which can bring better performance to users'\n  pipeline (#7512)\n* Pandas nullable types are now supported (#7760)\n* A new function `get_group` is introduced for `DMatrix` to allow users to get the group\n  information in the custom objective function. (#7564)\n* More training parameters are exposed in the sklearn interface instead of relying on the\n  `**kwargs`. (#7629)\n* A new attribute `feature_names_in_` is defined for all sklearn estimators like\n  `XGBRegressor` to follow the convention of sklearn. (#7526)\n* More work on Python type hint. (#7432, #7348, #7338, #7513, #7707)\n* Support the latest pandas Index type. (#7595)\n* Fix for Feature shape mismatch error on s390x platform (#7715)\n* Fix using feature names for constraints with multiple groups (#7711)\n* We clarified the behavior of the callback function when it contains mutable\n  states. (#7685)\n* Lastly, there are some code cleanups and maintenance work. (#7585, #7426, #7634, #7665,\n  #7667, #7377, #7360, #7498, #7438, #7667, #7752, #7749, #7751)\n\n### Changes in the Dask interface\n* Dask module now supports user-supplied host IP and port address of scheduler node.\n  Please see [introduction](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#troubleshooting) and\n  [API document](https://xgboost.readthedocs.io/en/latest/python/python_api.html#optional-dask-configuration)\n  for reference. (#7645, #7581)\n* Internal `DMatrix` construction in dask now honers thread configuration. (#7337)\n* A fix for `nthread` configuration using the Dask sklearn interface. (#7633)\n* The Dask interface can now handle empty partitions.  An empty partition is different\n  from an empty worker, the latter refers to the case when a worker has no partition of an\n  input dataset, while the former refers to some partitions on a worker that has zero\n  sizes. (#7644, #7510)\n* Scipy sparse matrix is supported as Dask array partition. (#7457)\n* Dask interface is no longer considered experimental. (#7509)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Changes in the R package\nThis section summarizes the new features, improvements, and bug fixes to the R package.\n\n* `load.raw` can optionally construct a booster as return. (#7686)\n* Fix parsing decision stump, which affects both transforming text representation to data\n  table and plotting. (#7689)\n* Implement feature weights. (#7660)\n* Some improvements for complying the CRAN release policy. (#7672, #7661, #7763)\n* Support CSR data for predictions (#7615)\n* Document update (#7263, #7606)\n* New maintainer for the CRAN package (#7691, #7649)\n* Handle non-standard installation of toolchain on macos (#7759)\n\n### Changes in JVM-packages\nSome new features for JVM-packages are introduced for a more integrated GPU pipeline and\nbetter compatibility with musl-based Linux. Aside from this, we have a few notable bug\nfixes.\n\n* User can specify the tracker IP address for training, which helps running XGBoost on\n  restricted network environments. (#7808)\n* Add support for detecting musl-based Linux (#7624)\n* Add `DeviceQuantileDMatrix` to Scala binding (#7459)\n* Add Rapids plugin support, now more of the JVM pipeline can be accelerated by RAPIDS (#7491, #7779, #7793, #7806)\n* The setters for CPU and GPU are more aligned (#7692, #7798)\n* Control logging for early stopping (#7326)\n* Do not repartition when nWorker = 1 (#7676)\n* Fix the prediction issue for `multi:softmax` (#7694)\n* Fix for serialization of custom objective and eval (#7274)\n* Update documentation about Python tracker (#7396)\n* Remove jackson from dependency, which fixes CVE-2020-36518. (#7791)\n* Some refactoring to the training pipeline for better compatibility between CPU and\n  GPU. (#7440, #7401, #7789, #7784)\n* Maintenance work. (#7550, #7335, #7641, #7523, #6792, #4676)\n\n### Deprecation\nOther than the changes in the Python package and serialization, we removed some deprecated\nfeatures in previous releases. Also, as mentioned in the previous section, we plan to\nphase out the old binary format in future releases.\n\n* Remove old warning in 1.3 (#7279)\n* Remove label encoder deprecated in 1.3. (#7357)\n* Remove old callback deprecated in 1.3. (#7280)\n* Pre-built binary will no longer support deprecated CUDA architectures including sm35 and\n  sm50. Users can continue to use these platforms with source build. (#7767)\n\n### Documentation\nThis section lists some of the general changes to XGBoost's document, for language binding\nspecific change please visit related sections.\n\n* Document is overhauled to use the new RTD theme, along with integration of Python\n  examples using Sphinx gallery. Also, we replaced most of the hard-coded URLs with sphinx\n  references. (#7347, #7346, #7468, #7522, #7530)\n* Small update along with fixes for broken links, typos, etc. (#7684, #7324, #7334, #7655,\n  #7628, #7623, #7487, #7532, #7500, #7341, #7648, #7311)\n* Update document for GPU. [skip ci] (#7403)\n* Document the status of RTD hosting. (#7353)\n* Update document for building from source. (#7664)\n* Add note about CRAN release [skip ci] (#7395)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Changes in the R package\nThis section summarizes the new features, improvements, and bug fixes to the R package.\n\n* `load.raw` can optionally construct a booster as return. (#7686)\n* Fix parsing decision stump, which affects both transforming text representation to data\n  table and plotting. (#7689)\n* Implement feature weights. (#7660)\n* Some improvements for complying the CRAN release policy. (#7672, #7661, #7763)\n* Support CSR data for predictions (#7615)\n* Document update (#7263, #7606)\n* New maintainer for the CRAN package (#7691, #7649)\n* Handle non-standard installation of toolchain on macos (#7759)\n\n### Changes in JVM-packages\nSome new features for JVM-packages are introduced for a more integrated GPU pipeline and\nbetter compatibility with musl-based Linux. Aside from this, we have a few notable bug\nfixes.\n\n* User can specify the tracker IP address for training, which helps running XGBoost on\n  restricted network environments. (#7808)\n* Add support for detecting musl-based Linux (#7624)\n* Add `DeviceQuantileDMatrix` to Scala binding (#7459)\n* Add Rapids plugin support, now more of the JVM pipeline can be accelerated by RAPIDS (#7491, #7779, #7793, #7806)\n* The setters for CPU and GPU are more aligned (#7692, #7798)\n* Control logging for early stopping (#7326)\n* Do not repartition when nWorker = 1 (#7676)\n* Fix the prediction issue for `multi:softmax` (#7694)\n* Fix for serialization of custom objective and eval (#7274)\n* Update documentation about Python tracker (#7396)\n* Remove jackson from dependency, which fixes CVE-2020-36518. (#7791)\n* Some refactoring to the training pipeline for better compatibility between CPU and\n  GPU. (#7440, #7401, #7789, #7784)\n* Maintenance work. (#7550, #7335, #7641, #7523, #6792, #4676)\n\n### Deprecation\nOther than the changes in the Python package and serialization, we removed some deprecated\nfeatures in previous releases. Also, as mentioned in the previous section, we plan to\nphase out the old binary format in future releases.\n\n* Remove old warning in 1.3 (#7279)\n* Remove label encoder deprecated in 1.3. (#7357)\n* Remove old callback deprecated in 1.3. (#7280)\n* Pre-built binary will no longer support deprecated CUDA architectures including sm35 and\n  sm50. Users can continue to use these platforms with source build. (#7767)\n\n### Documentation\nThis section lists some of the general changes to XGBoost's document, for language binding\nspecific change please visit related sections.\n\n* Document is overhauled to use the new RTD theme, along with integration of Python\n  examples using Sphinx gallery. Also, we replaced most of the hard-coded URLs with sphinx\n  references. (#7347, #7346, #7468, #7522, #7530)\n* Small update along with fixes for broken links, typos, etc. (#7684, #7324, #7334, #7655,\n  #7628, #7623, #7487, #7532, #7500, #7341, #7648, #7311)\n* Update document for GPU. [skip ci] (#7403)\n* Document the status of RTD hosting. (#7353)\n* Update document for building from source. (#7664)\n* Add note about CRAN release [skip ci] (#7395)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance\nThis is a summary of maintenance work that is not specific to any language binding.\n\n* Add CMake option to use /MD runtime (#7277)\n* Add clang-format configuration. (#7383)\n* Code cleanups (#7539, #7536, #7466, #7499, #7533, #7735, #7722, #7668, #7304, #7293,\n  #7321, #7356, #7345, #7387, #7577, #7548, #7469, #7680, #7433, #7398)\n* Improved tests with better coverage and latest dependency (#7573, #7446, #7650, #7520,\n  #7373, #7723, #7611, #7771)\n* Improved automation of the release process. (#7278, #7332, #7470)\n* Compiler workarounds (#7673)\n* Change shebang used in CLI demo. (#7389)\n* Update affiliation (#7289)\n\n### CI\nSome fixes and update to XGBoost's CI infrastructure. (#7739, #7701, #7382, #7662, #7646,\n#7582, #7407, #7417, #7475, #7474, #7479, #7472, #7626)\n\n\n## v1.5.0 (2021 Oct 11)\n\nThis release comes with many exciting new features and optimizations, along with some bug\nfixes.  We will describe the experimental categorical data support and the external memory\ninterface independently. Package-specific new features will be listed in respective\nsections.\n\n### Development on categorical data support\nIn version 1.3, XGBoost introduced an experimental feature for handling categorical data\nnatively, without one-hot encoding. XGBoost can fit categorical splits in decision\ntrees. (Currently, the generated splits will be of form `x \\in {v}`, where the input is\ncompared to a single category value. A future version of XGBoost will generate splits that\ncompare the input against a list of multiple category values.)\n\nMost of the other features, including prediction, SHAP value computation, feature\nimportance, and model plotting were revised to natively handle categorical splits.  Also,\nall Python interfaces including native interface with and without quantized `DMatrix`,\nscikit-learn interface, and Dask interface now accept categorical data with a wide range\nof data structures support including numpy/cupy array and cuDF/pandas/modin dataframe.  In\npractice, the following are required for enabling categorical data support during\ntraining:\n\n  - Use Python package.\n  - Use `gpu_hist` to train the model.\n  - Use JSON model file format for saving the model.\n\nOnce the model is trained, it can be used with most of the features that are available on\nthe Python package.  For a quick introduction, see\nhttps://xgboost.readthedocs.io/en/latest/tutorials/categorical.html\n\nRelated PRs: (#7011, #7001, #7042, #7041, #7047, #7043, #7036, #7054, #7053, #7065, #7213, #7228, #7220, #7221, #7231, #7306)\n\n* Next steps\n\n\t- Revise the CPU training algorithm to handle categorical data natively and generate categorical splits\n\t- Extend the CPU and GPU algorithms to generate categorical splits of form `x \\in S`\n\twhere the input is compared with multiple category values.  split. (#7081)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance\nThis is a summary of maintenance work that is not specific to any language binding.\n\n* Add CMake option to use /MD runtime (#7277)\n* Add clang-format configuration. (#7383)\n* Code cleanups (#7539, #7536, #7466, #7499, #7533, #7735, #7722, #7668, #7304, #7293,\n  #7321, #7356, #7345, #7387, #7577, #7548, #7469, #7680, #7433, #7398)\n* Improved tests with better coverage and latest dependency (#7573, #7446, #7650, #7520,\n  #7373, #7723, #7611, #7771)\n* Improved automation of the release process. (#7278, #7332, #7470)\n* Compiler workarounds (#7673)\n* Change shebang used in CLI demo. (#7389)\n* Update affiliation (#7289)\n\n### CI\nSome fixes and update to XGBoost's CI infrastructure. (#7739, #7701, #7382, #7662, #7646,\n#7582, #7407, #7417, #7475, #7474, #7479, #7472, #7626)\n\n\n## v1.5.0 (2021 Oct 11)\n\nThis release comes with many exciting new features and optimizations, along with some bug\nfixes.  We will describe the experimental categorical data support and the external memory\ninterface independently. Package-specific new features will be listed in respective\nsections.\n\n### Development on categorical data support\nIn version 1.3, XGBoost introduced an experimental feature for handling categorical data\nnatively, without one-hot encoding. XGBoost can fit categorical splits in decision\ntrees. (Currently, the generated splits will be of form `x \\in {v}`, where the input is\ncompared to a single category value. A future version of XGBoost will generate splits that\ncompare the input against a list of multiple category values.)\n\nMost of the other features, including prediction, SHAP value computation, feature\nimportance, and model plotting were revised to natively handle categorical splits.  Also,\nall Python interfaces including native interface with and without quantized `DMatrix`,\nscikit-learn interface, and Dask interface now accept categorical data with a wide range\nof data structures support including numpy/cupy array and cuDF/pandas/modin dataframe.  In\npractice, the following are required for enabling categorical data support during\ntraining:\n\n  - Use Python package.\n  - Use `gpu_hist` to train the model.\n  - Use JSON model file format for saving the model.\n\nOnce the model is trained, it can be used with most of the features that are available on\nthe Python package.  For a quick introduction, see\nhttps://xgboost.readthedocs.io/en/latest/tutorials/categorical.html\n\nRelated PRs: (#7011, #7001, #7042, #7041, #7047, #7043, #7036, #7054, #7053, #7065, #7213, #7228, #7220, #7221, #7231, #7306)\n\n* Next steps\n\n\t- Revise the CPU training algorithm to handle categorical data natively and generate categorical splits\n\t- Extend the CPU and GPU algorithms to generate categorical splits of form `x \\in S`\n\twhere the input is compared with multiple category values.  split. (#7081)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### External memory\nThis release features a brand-new interface and implementation for external memory (also\nknown as out-of-core training).  (#6901, #7064, #7088, #7089, #7087, #7092, #7070,\n#7216). The new implementation leverages the data iterator interface, which is currently\nused to create `DeviceQuantileDMatrix`. For a quick introduction, see\nhttps://xgboost.readthedocs.io/en/latest/tutorials/external_memory.html#data-iterator\n. During the development of this new interface, `lz4` compression is removed. (#7076).\nPlease note that external memory support is still experimental and not ready for\nproduction use yet.  All future development will focus on this new interface and users are\nadvised to migrate. (You are using the old interface if you are using a URL suffix to use\nexternal memory.)\n\n### New features in Python package\n* Support numpy array interface and all numeric types from numpy in `DMatrix`\n  construction and `inplace_predict` (#6998, #7003).  Now XGBoost no longer makes data\n  copy when input is numpy array view.\n* The early stopping callback in Python has a new `min_delta` parameter to control the\n  stopping behavior (#7137)\n* Python package now supports calculating feature scores for the linear model, which is\n  also available on R package. (#7048)\n* Python interface now supports configuring constraints using feature names instead of\n  feature indices.\n* Typehint support for more Python code including scikit-learn interface and rabit\n  module. (#6799, #7240)\n* Add tutorial for XGBoost-Ray (#6884)\n\n### New features in R package\n* In 1.4 we have a new prediction function in the C API which is used by the Python\n  package.  This release revises the R package to use the new prediction function as well.\n  A new parameter `iteration_range` for the predict function is available, which can be\n  used for specifying the range of trees for running prediction. (#6819, #7126)\n* R package now supports the `nthread` parameter in `DMatrix` construction. (#7127)\n\n### New features in JVM packages\n* Support GPU dataframe and `DeviceQuantileDMatrix` (#7195).  Constructing `DMatrix`\n  with GPU data structures and the interface for quantized `DMatrix` were first\n  introduced in the Python package and are now available in the xgboost4j package.\n* JVM packages now support saving and getting early stopping attributes. (#7095) Here is a\n  quick [example](https://github.com/dmlc/xgboost/jvm-packages/xgboost4j-example/src/main/java/ml/dmlc/xgboost4j/java/example/EarlyStopping.java \"example\") in JAVA (#7252).\n\n### General new features\n* We now have a pre-built binary package for R on Windows with GPU support. (#7185)\n* CUDA compute capability 86 is now part of the default CMake build configuration with\n  newly added support for CUDA 11.4. (#7131, #7182, #7254)\n* XGBoost can be compiled using system CUB provided by CUDA 11.x installation. (#7232)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### External memory\nThis release features a brand-new interface and implementation for external memory (also\nknown as out-of-core training).  (#6901, #7064, #7088, #7089, #7087, #7092, #7070,\n#7216). The new implementation leverages the data iterator interface, which is currently\nused to create `DeviceQuantileDMatrix`. For a quick introduction, see\nhttps://xgboost.readthedocs.io/en/latest/tutorials/external_memory.html#data-iterator\n. During the development of this new interface, `lz4` compression is removed. (#7076).\nPlease note that external memory support is still experimental and not ready for\nproduction use yet.  All future development will focus on this new interface and users are\nadvised to migrate. (You are using the old interface if you are using a URL suffix to use\nexternal memory.)\n\n### New features in Python package\n* Support numpy array interface and all numeric types from numpy in `DMatrix`\n  construction and `inplace_predict` (#6998, #7003).  Now XGBoost no longer makes data\n  copy when input is numpy array view.\n* The early stopping callback in Python has a new `min_delta` parameter to control the\n  stopping behavior (#7137)\n* Python package now supports calculating feature scores for the linear model, which is\n  also available on R package. (#7048)\n* Python interface now supports configuring constraints using feature names instead of\n  feature indices.\n* Typehint support for more Python code including scikit-learn interface and rabit\n  module. (#6799, #7240)\n* Add tutorial for XGBoost-Ray (#6884)\n\n### New features in R package\n* In 1.4 we have a new prediction function in the C API which is used by the Python\n  package.  This release revises the R package to use the new prediction function as well.\n  A new parameter `iteration_range` for the predict function is available, which can be\n  used for specifying the range of trees for running prediction. (#6819, #7126)\n* R package now supports the `nthread` parameter in `DMatrix` construction. (#7127)\n\n### New features in JVM packages\n* Support GPU dataframe and `DeviceQuantileDMatrix` (#7195).  Constructing `DMatrix`\n  with GPU data structures and the interface for quantized `DMatrix` were first\n  introduced in the Python package and are now available in the xgboost4j package.\n* JVM packages now support saving and getting early stopping attributes. (#7095) Here is a\n  quick [example](https://github.com/dmlc/xgboost/jvm-packages/xgboost4j-example/src/main/java/ml/dmlc/xgboost4j/java/example/EarlyStopping.java \"example\") in JAVA (#7252).\n\n### General new features\n* We now have a pre-built binary package for R on Windows with GPU support. (#7185)\n* CUDA compute capability 86 is now part of the default CMake build configuration with\n  newly added support for CUDA 11.4. (#7131, #7182, #7254)\n* XGBoost can be compiled using system CUB provided by CUDA 11.x installation. (#7232)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Optimizations\nThe performance for both `hist` and `gpu_hist` has been significantly improved in 1.5\nwith the following optimizations:\n* GPU multi-class model training now supports prediction cache. (#6860)\n* GPU histogram building is sped up and the overall training time is 2-3 times faster on\n  large datasets (#7180, #7198).  In addition, we removed the parameter `deterministic_histogram` and now\n  the GPU algorithm is always deterministic.\n* CPU hist has an optimized procedure for data sampling (#6922)\n* More performance optimization in regression and binary classification objectives on\n  CPU (#7206)\n* Tree model dump is now performed in parallel (#7040)\n\n### Breaking changes\n* `n_gpus` was deprecated in 1.0 release and is now removed.\n* Feature grouping in CPU hist tree method is removed, which was disabled long\n  ago. (#7018)\n* C API for Quantile DMatrix is changed to be consistent with the new external memory\n  implementation. (#7082)\n\n### Notable general bug fixes\n* XGBoost no long changes global CUDA device ordinal when `gpu_id` is specified (#6891,\n  #6987)\n* Fix `gamma` negative likelihood evaluation metric. (#7275)\n* Fix integer value of `verbose_eal` for `xgboost.cv` function in Python. (#7291)\n* Remove extra sync in CPU hist for dense data, which can lead to incorrect tree node\n  statistics. (#7120, #7128)\n* Fix a bug in GPU hist when data size is larger than `UINT32_MAX` with missing\n  values. (#7026)\n* Fix a thread safety issue in prediction with the `softmax` objective. (#7104)\n* Fix a thread safety issue in CPU SHAP value computation. (#7050) Please note that all\n  prediction functions in Python are thread-safe.\n* Fix model slicing. (#7149, #7078)\n* Workaround a bug in old GCC which can lead to segfault during construction of\n  DMatrix. (#7161)\n* Fix histogram truncation in GPU hist, which can lead to slightly-off results. (#7181)\n* Fix loading GPU linear model pickle files on CPU-only machine. (#7154)\n* Check input value is duplicated when CPU quantile queue is full (#7091)\n* Fix parameter loading with training continuation. (#7121)\n* Fix CMake interface for exposing C library by specifying dependencies. (#7099)\n* Callback and early stopping are explicitly disabled for the scikit-learn interface\n  random forest estimator. (#7236)\n* Fix compilation error on x86 (32-bit machine) (#6964)\n* Fix CPU memory usage with extremely sparse datasets (#7255)\n* Fix a bug in GPU multi-class AUC implementation with weighted data (#7300)\n\n### Python package\nOther than the items mentioned in the previous sections, there are some Python-specific\nimprovements.\n* Change development release postfix to `dev` (#6988)\n* Fix early stopping behavior with MAPE metric (#7061)\n* Fixed incorrect feature mismatch error message (#6949)\n* Add predictor to skl constructor. (#7000, #7159)\n* Re-enable feature validation in predict proba. (#7177)\n* scikit learn interface regression estimator now can pass the scikit-learn estimator\n  check and is fully compatible with scikit-learn utilities.  `__sklearn_is_fitted__` is\n  implemented as part of the changes (#7130, #7230)\n* Conform the latest pylint. (#7071, #7241)\n* Support latest panda range index in DMatrix construction. (#7074)\n* Fix DMatrix construction from pandas series. (#7243)\n* Fix typo and grammatical mistake in error message (#7134)\n* [dask] disable work stealing explicitly for training tasks (#6794)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Optimizations\nThe performance for both `hist` and `gpu_hist` has been significantly improved in 1.5\nwith the following optimizations:\n* GPU multi-class model training now supports prediction cache. (#6860)\n* GPU histogram building is sped up and the overall training time is 2-3 times faster on\n  large datasets (#7180, #7198).  In addition, we removed the parameter `deterministic_histogram` and now\n  the GPU algorithm is always deterministic.\n* CPU hist has an optimized procedure for data sampling (#6922)\n* More performance optimization in regression and binary classification objectives on\n  CPU (#7206)\n* Tree model dump is now performed in parallel (#7040)\n\n### Breaking changes\n* `n_gpus` was deprecated in 1.0 release and is now removed.\n* Feature grouping in CPU hist tree method is removed, which was disabled long\n  ago. (#7018)\n* C API for Quantile DMatrix is changed to be consistent with the new external memory\n  implementation. (#7082)\n\n### Notable general bug fixes\n* XGBoost no long changes global CUDA device ordinal when `gpu_id` is specified (#6891,\n  #6987)\n* Fix `gamma` negative likelihood evaluation metric. (#7275)\n* Fix integer value of `verbose_eal` for `xgboost.cv` function in Python. (#7291)\n* Remove extra sync in CPU hist for dense data, which can lead to incorrect tree node\n  statistics. (#7120, #7128)\n* Fix a bug in GPU hist when data size is larger than `UINT32_MAX` with missing\n  values. (#7026)\n* Fix a thread safety issue in prediction with the `softmax` objective. (#7104)\n* Fix a thread safety issue in CPU SHAP value computation. (#7050) Please note that all\n  prediction functions in Python are thread-safe.\n* Fix model slicing. (#7149, #7078)\n* Workaround a bug in old GCC which can lead to segfault during construction of\n  DMatrix. (#7161)\n* Fix histogram truncation in GPU hist, which can lead to slightly-off results. (#7181)\n* Fix loading GPU linear model pickle files on CPU-only machine. (#7154)\n* Check input value is duplicated when CPU quantile queue is full (#7091)\n* Fix parameter loading with training continuation. (#7121)\n* Fix CMake interface for exposing C library by specifying dependencies. (#7099)\n* Callback and early stopping are explicitly disabled for the scikit-learn interface\n  random forest estimator. (#7236)\n* Fix compilation error on x86 (32-bit machine) (#6964)\n* Fix CPU memory usage with extremely sparse datasets (#7255)\n* Fix a bug in GPU multi-class AUC implementation with weighted data (#7300)\n\n### Python package\nOther than the items mentioned in the previous sections, there are some Python-specific\nimprovements.\n* Change development release postfix to `dev` (#6988)\n* Fix early stopping behavior with MAPE metric (#7061)\n* Fixed incorrect feature mismatch error message (#6949)\n* Add predictor to skl constructor. (#7000, #7159)\n* Re-enable feature validation in predict proba. (#7177)\n* scikit learn interface regression estimator now can pass the scikit-learn estimator\n  check and is fully compatible with scikit-learn utilities.  `__sklearn_is_fitted__` is\n  implemented as part of the changes (#7130, #7230)\n* Conform the latest pylint. (#7071, #7241)\n* Support latest panda range index in DMatrix construction. (#7074)\n* Fix DMatrix construction from pandas series. (#7243)\n* Fix typo and grammatical mistake in error message (#7134)\n* [dask] disable work stealing explicitly for training tasks (#6794)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [dask] Set dataframe index in predict. (#6944)\n* [dask] Fix prediction on df with latest dask. (#6969)\n* [dask] Fix dask predict on `DaskDMatrix` with `iteration_range`. (#7005)\n* [dask] Disallow importing non-dask estimators from xgboost.dask (#7133)\n\n### R package\nImprovements other than new features on R package:\n* Optimization for updating R handles in-place (#6903)\n* Removed the magrittr dependency. (#6855, #6906, #6928)\n* The R package now hides all C++ symbols to avoid conflicts. (#7245)\n* Other maintenance including code cleanups, document updates. (#6863, #6915, #6930, #6966, #6967)\n\n### JVM packages\nImprovements other than new features on JVM packages:\n* Constructors with implicit missing value are deprecated due to confusing behaviors. (#7225)\n* Reduce scala-compiler, scalatest dependency scopes (#6730)\n* Making the Java library loader emit helpful error messages on missing dependencies. (#6926)\n* JVM packages now use the Python tracker in XGBoost instead of dmlc.  The one in XGBoost\n  is shared between JVM packages and Python Dask and enjoys better maintenance (#7132)\n* Fix \"key not found: train\" error (#6842)\n* Fix model loading from stream (#7067)\n\n### General document improvements\n* Overhaul the installation documents. (#6877)\n* A few demos are added for AFT with dask (#6853), callback with dask (#6995), inference\n  in C (#7151), `process_type`. (#7135)\n* Fix PDF format of document. (#7143)\n* Clarify the behavior of `use_rmm`. (#6808)\n* Clarify prediction function. (#6813)\n* Improve tutorial on feature interactions (#7219)\n* Add small example for dask sklearn interface. (#6970)\n* Update Python intro.  (#7235)\n* Some fixes/updates (#6810, #6856, #6935, #6948, #6976, #7084, #7097, #7170, #7173, #7174, #7226, #6979, #6809, #6796, #6979)\n\n### Maintenance\n* Some refactoring around CPU hist, which lead to better performance but are listed under general maintenance tasks:\n  - Extract evaluate splits from CPU hist. (#7079)\n  - Merge lossgude and depthwise strategies for CPU hist (#7007)\n  - Simplify sparse and dense CPU hist kernels (#7029)\n  - Extract histogram builder from CPU Hist. (#7152)\n\n* Others\n  - Fix `gpu_id` with custom objective. (#7015)\n  - Fix typos in AUC. (#6795)\n  - Use constexpr in `dh::CopyIf`. (#6828)\n  - Update dmlc-core. (#6862)\n  - Bump version to 1.5.0 snapshot in master. (#6875)\n  - Relax shotgun test. (#6900)\n  - Guard against index error in prediction. (#6982)\n  - Hide symbols in CI build + hide symbols for C and CUDA (#6798)\n  - Persist data in dask test. (#7077)\n  - Fix typo in arguments of PartitionBuilder::Init (#7113)\n  - Fix typo in src/common/hist.cc BuildHistKernel (#7116)\n  - Use upstream URI in distributed quantile tests. (#7129)\n  - Include cpack (#7160)\n  - Remove synchronization in monitor. (#7164)\n  - Remove unused code. (#7175)\n  - Fix building on CUDA 11.0. (#7187)\n  - Better error message for `ncclUnhandledCudaError`. (#7190)\n  - Add noexcept to JSON objects. (#7205)\n  - Improve wording for warning (#7248)\n  - Fix typo in release script. [skip ci] (#7238)\n  - Relax shotgun test. (#6918)\n  - Relax test for decision stump in distributed environment. (#6919)\n  -\t[dask] speed up tests (#7020)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [dask] Set dataframe index in predict. (#6944)\n* [dask] Fix prediction on df with latest dask. (#6969)\n* [dask] Fix dask predict on `DaskDMatrix` with `iteration_range`. (#7005)\n* [dask] Disallow importing non-dask estimators from xgboost.dask (#7133)\n\n### R package\nImprovements other than new features on R package:\n* Optimization for updating R handles in-place (#6903)\n* Removed the magrittr dependency. (#6855, #6906, #6928)\n* The R package now hides all C++ symbols to avoid conflicts. (#7245)\n* Other maintenance including code cleanups, document updates. (#6863, #6915, #6930, #6966, #6967)\n\n### JVM packages\nImprovements other than new features on JVM packages:\n* Constructors with implicit missing value are deprecated due to confusing behaviors. (#7225)\n* Reduce scala-compiler, scalatest dependency scopes (#6730)\n* Making the Java library loader emit helpful error messages on missing dependencies. (#6926)\n* JVM packages now use the Python tracker in XGBoost instead of dmlc.  The one in XGBoost\n  is shared between JVM packages and Python Dask and enjoys better maintenance (#7132)\n* Fix \"key not found: train\" error (#6842)\n* Fix model loading from stream (#7067)\n\n### General document improvements\n* Overhaul the installation documents. (#6877)\n* A few demos are added for AFT with dask (#6853), callback with dask (#6995), inference\n  in C (#7151), `process_type`. (#7135)\n* Fix PDF format of document. (#7143)\n* Clarify the behavior of `use_rmm`. (#6808)\n* Clarify prediction function. (#6813)\n* Improve tutorial on feature interactions (#7219)\n* Add small example for dask sklearn interface. (#6970)\n* Update Python intro.  (#7235)\n* Some fixes/updates (#6810, #6856, #6935, #6948, #6976, #7084, #7097, #7170, #7173, #7174, #7226, #6979, #6809, #6796, #6979)\n\n### Maintenance\n* Some refactoring around CPU hist, which lead to better performance but are listed under general maintenance tasks:\n  - Extract evaluate splits from CPU hist. (#7079)\n  - Merge lossgude and depthwise strategies for CPU hist (#7007)\n  - Simplify sparse and dense CPU hist kernels (#7029)\n  - Extract histogram builder from CPU Hist. (#7152)\n\n* Others\n  - Fix `gpu_id` with custom objective. (#7015)\n  - Fix typos in AUC. (#6795)\n  - Use constexpr in `dh::CopyIf`. (#6828)\n  - Update dmlc-core. (#6862)\n  - Bump version to 1.5.0 snapshot in master. (#6875)\n  - Relax shotgun test. (#6900)\n  - Guard against index error in prediction. (#6982)\n  - Hide symbols in CI build + hide symbols for C and CUDA (#6798)\n  - Persist data in dask test. (#7077)\n  - Fix typo in arguments of PartitionBuilder::Init (#7113)\n  - Fix typo in src/common/hist.cc BuildHistKernel (#7116)\n  - Use upstream URI in distributed quantile tests. (#7129)\n  - Include cpack (#7160)\n  - Remove synchronization in monitor. (#7164)\n  - Remove unused code. (#7175)\n  - Fix building on CUDA 11.0. (#7187)\n  - Better error message for `ncclUnhandledCudaError`. (#7190)\n  - Add noexcept to JSON objects. (#7205)\n  - Improve wording for warning (#7248)\n  - Fix typo in release script. [skip ci] (#7238)\n  - Relax shotgun test. (#6918)\n  - Relax test for decision stump in distributed environment. (#6919)\n  -\t[dask] speed up tests (#7020)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### CI\n* [CI] Rotate access keys for uploading MacOS artifacts from Travis CI (#7253)\n* Reduce Travis environment setup time. (#6912)\n* Restore R cache on github action. (#6985)\n* [CI] Remove stray build artifact to avoid error in artifact packaging (#6994)\n* [CI] Move appveyor tests to action (#6986)\n* Remove appveyor badge. [skip ci] (#7035)\n* [CI] Configure RAPIDS, dask, modin (#7033)\n* Test on s390x. (#7038)\n* [CI] Upgrade to CMake 3.14 (#7060)\n* [CI] Update R cache. (#7102)\n* [CI] Pin libomp to 11.1.0  (#7107)\n* [CI] Upgrade build image to CentOS 7 + GCC 8; require CUDA 10.1 and later (#7141)\n* [dask] Work around segfault in prediction. (#7112)\n* [dask] Remove the workaround for segfault. (#7146)\n* [CI] Fix hanging Python setup in Windows CI (#7186)\n* [CI] Clean up in beginning of each task in Win CI (#7189)\n* Fix travis. (#7237)\n\n### Acknowledgement\n* **Contributors**: Adam Pocock (@Craigacp), Jeff H (@JeffHCross), Johan Hansson (@JohanWork), Jose Manuel Llorens (@JoseLlorensRipolles), Benjamin Sz\u0151ke (@Livius90), @ReeceGoding, @ShvetsKS, Robert Zabel (@ZabelTech), Ali (@ali5h), Andrew Ziem (@az0), Andy Adinets (@canonizer), @david-cortes, Daniel Saxton (@dsaxton), Emil Sadek (@esadek), @farfarawayzyt, Gil Forsyth (@gforsyth), @giladmaya, @graue70, Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jos\u00e9 Morales (@jmoralez), Kai Fricke (@krfricke), Christian Lorentzen (@lorentzenchr), Mads R. B. Kristensen (@madsbk), Anton Kostin (@masguit42), Martin Pet\u0159\u00ed\u010dek (@mpetricek-corp), @naveenkb, Taewoo Kim (@oOTWK), Viktor Szathm\u00e1ry (@phraktle), Robert Maynard (@robertmaynard), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), Paul Taylor (@trxcllnt), @vslaykovsky, Bobby Wang (@wbo4958),\n* **Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Jose Manuel Llorens (@JoseLlorensRipolles), Kodi Arfer (@Kodiologist), Benjamin Sz\u0151ke (@Livius90), Mark Guryanov (@MarkGuryanov), Rory Mitchell (@RAMitchell), @ReeceGoding, @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Ziem (@az0), @candalfigomoro, Andy Adinets (@canonizer), Dante Gama Dessavre (@dantegd), @david-cortes, Daniel Saxton (@dsaxton), @farfarawayzyt, Gil Forsyth (@gforsyth), Harutaka Kawamura (@harupy), Philip Hyunsu Cho (@hcho3), @jakirkham, James Lamb (@jameslamb), Jos\u00e9 Morales (@jmoralez), James Bourbeau (@jrbourbeau), Christian Lorentzen (@lorentzenchr), Martin Pet\u0159\u00ed\u010dek (@mpetricek-corp), Nikolay Petrov (@napetrov), @naveenkb, Viktor Szathm\u00e1ry (@phraktle), Robin Teuwens (@rteuwens), Yuan Tang (@terrytangyuan), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), @vkuzmin-uber, Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n\n## v1.4.2 (2021.05.13)\nThis is a patch release for Python package with following fixes:\n\n* Handle the latest version of cupy.ndarray in inplace_predict. (#6933)\n* Ensure output array from predict_leaf is (n_samples, ) when there's only 1 tree. 1.4.0 outputs (n_samples, 1). (#6889)\n* Fix empty dataset handling with multi-class AUC. (#6947)\n* Handle object type from pandas in inplace_predict. (#6927)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### CI\n* [CI] Rotate access keys for uploading MacOS artifacts from Travis CI (#7253)\n* Reduce Travis environment setup time. (#6912)\n* Restore R cache on github action. (#6985)\n* [CI] Remove stray build artifact to avoid error in artifact packaging (#6994)\n* [CI] Move appveyor tests to action (#6986)\n* Remove appveyor badge. [skip ci] (#7035)\n* [CI] Configure RAPIDS, dask, modin (#7033)\n* Test on s390x. (#7038)\n* [CI] Upgrade to CMake 3.14 (#7060)\n* [CI] Update R cache. (#7102)\n* [CI] Pin libomp to 11.1.0  (#7107)\n* [CI] Upgrade build image to CentOS 7 + GCC 8; require CUDA 10.1 and later (#7141)\n* [dask] Work around segfault in prediction. (#7112)\n* [dask] Remove the workaround for segfault. (#7146)\n* [CI] Fix hanging Python setup in Windows CI (#7186)\n* [CI] Clean up in beginning of each task in Win CI (#7189)\n* Fix travis. (#7237)\n\n### Acknowledgement\n* **Contributors**: Adam Pocock (@Craigacp), Jeff H (@JeffHCross), Johan Hansson (@JohanWork), Jose Manuel Llorens (@JoseLlorensRipolles), Benjamin Sz\u0151ke (@Livius90), @ReeceGoding, @ShvetsKS, Robert Zabel (@ZabelTech), Ali (@ali5h), Andrew Ziem (@az0), Andy Adinets (@canonizer), @david-cortes, Daniel Saxton (@dsaxton), Emil Sadek (@esadek), @farfarawayzyt, Gil Forsyth (@gforsyth), @giladmaya, @graue70, Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jos\u00e9 Morales (@jmoralez), Kai Fricke (@krfricke), Christian Lorentzen (@lorentzenchr), Mads R. B. Kristensen (@madsbk), Anton Kostin (@masguit42), Martin Pet\u0159\u00ed\u010dek (@mpetricek-corp), @naveenkb, Taewoo Kim (@oOTWK), Viktor Szathm\u00e1ry (@phraktle), Robert Maynard (@robertmaynard), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), Paul Taylor (@trxcllnt), @vslaykovsky, Bobby Wang (@wbo4958),\n* **Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Jose Manuel Llorens (@JoseLlorensRipolles), Kodi Arfer (@Kodiologist), Benjamin Sz\u0151ke (@Livius90), Mark Guryanov (@MarkGuryanov), Rory Mitchell (@RAMitchell), @ReeceGoding, @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Ziem (@az0), @candalfigomoro, Andy Adinets (@canonizer), Dante Gama Dessavre (@dantegd), @david-cortes, Daniel Saxton (@dsaxton), @farfarawayzyt, Gil Forsyth (@gforsyth), Harutaka Kawamura (@harupy), Philip Hyunsu Cho (@hcho3), @jakirkham, James Lamb (@jameslamb), Jos\u00e9 Morales (@jmoralez), James Bourbeau (@jrbourbeau), Christian Lorentzen (@lorentzenchr), Martin Pet\u0159\u00ed\u010dek (@mpetricek-corp), Nikolay Petrov (@napetrov), @naveenkb, Viktor Szathm\u00e1ry (@phraktle), Robin Teuwens (@rteuwens), Yuan Tang (@terrytangyuan), TP Boudreau (@tpboudreau), Jiaming Yuan (@trivialfis), @vkuzmin-uber, Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n\n## v1.4.2 (2021.05.13)\nThis is a patch release for Python package with following fixes:\n\n* Handle the latest version of cupy.ndarray in inplace_predict. (#6933)\n* Ensure output array from predict_leaf is (n_samples, ) when there's only 1 tree. 1.4.0 outputs (n_samples, 1). (#6889)\n* Fix empty dataset handling with multi-class AUC. (#6947)\n* Handle object type from pandas in inplace_predict. (#6927)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v1.4.1 (2021.04.20)\nThis is a bug fix release.\n\n* Fix GPU implementation of AUC on some large datasets. (#6866)\n\n## v1.4.0 (2021.04.12)\n\n### Introduction of pre-built binary package for R, with GPU support\nStarting with release 1.4.0, users now have the option of installing `{xgboost}` without\nhaving to build it from the source. This is particularly advantageous for users who want\nto take advantage of the GPU algorithm (`gpu_hist`), as previously they'd have to build\n`{xgboost}` from the source using CMake and NVCC. Now installing `{xgboost}` with GPU\nsupport is as easy as: `R CMD INSTALL ./xgboost_r_gpu_linux.tar.gz`. (#6827)\n\nSee the instructions at https://xgboost.readthedocs.io/en/latest/build.html\n\n### Improvements on prediction functions\nXGBoost has many prediction types including shap value computation and inplace prediction.\nIn 1.4 we overhauled the underlying prediction functions for C API and Python API with an\nunified interface. (#6777, #6693, #6653, #6662, #6648, #6668, #6804)\n* Starting with 1.4, sklearn interface prediction will use inplace predict by default when\n  input data is supported.\n* Users can use inplace predict with `dart` booster and enable GPU acceleration just\n  like `gbtree`.\n* Also all prediction functions with tree models are now thread-safe.  Inplace predict is\n  improved with `base_margin` support.\n* A new set of C predict functions are exposed in the public interface.\n* A user-visible change is a newly added parameter called `strict_shape`.  See\n  https://xgboost.readthedocs.io/en/latest/prediction.html for more details.\n\n\n### Improvement on Dask interface\n* Starting with 1.4, the Dask interface is considered to be feature-complete, which means\n  all of the models found in the single node Python interface are now supported in Dask,\n  including but not limited to ranking and random forest.  Also, the prediction function\n  is significantly faster and supports shap value computation.\n  - Most of the parameters found in single node sklearn interface are supported by\n    Dask interface. (#6471, #6591)\n  - Implements learning to rank.  On the Dask interface, we use the newly added support of\n    query ID to enable group structure. (#6576)\n  - The Dask interface has Python type hints support. (#6519)\n  - All models can be safely pickled. (#6651)\n  - Random forest estimators are now supported. (#6602)\n  - Shap value computation is now supported. (#6575, #6645, #6614)\n  - Evaluation result is printed on the scheduler process. (#6609)\n  - `DaskDMatrix` (and device quantile dmatrix) now accepts all meta-information. (#6601)\n\n* Prediction optimization.  We enhanced and speeded up the prediction function for the\n  Dask interface.  See the latest Dask tutorial page in our document for an overview of\n  how you can optimize it even further. (#6650, #6645, #6648, #6668)\n\n* Bug fixes\n  - If you are using the latest Dask and distributed where `distributed.MultiLock` is\n    present, XGBoost supports training multiple models on the same cluster in\n    parallel. (#6743)\n  - A bug fix for when using `dask.client` to launch async task, XGBoost might use a\n    different client object internally. (#6722)\n\n* Other improvements on documents, blogs, tutorials, and demos. (#6389, #6366, #6687,\n  #6699, #6532, #6501)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v1.4.1 (2021.04.20)\nThis is a bug fix release.\n\n* Fix GPU implementation of AUC on some large datasets. (#6866)\n\n## v1.4.0 (2021.04.12)\n\n### Introduction of pre-built binary package for R, with GPU support\nStarting with release 1.4.0, users now have the option of installing `{xgboost}` without\nhaving to build it from the source. This is particularly advantageous for users who want\nto take advantage of the GPU algorithm (`gpu_hist`), as previously they'd have to build\n`{xgboost}` from the source using CMake and NVCC. Now installing `{xgboost}` with GPU\nsupport is as easy as: `R CMD INSTALL ./xgboost_r_gpu_linux.tar.gz`. (#6827)\n\nSee the instructions at https://xgboost.readthedocs.io/en/latest/build.html\n\n### Improvements on prediction functions\nXGBoost has many prediction types including shap value computation and inplace prediction.\nIn 1.4 we overhauled the underlying prediction functions for C API and Python API with an\nunified interface. (#6777, #6693, #6653, #6662, #6648, #6668, #6804)\n* Starting with 1.4, sklearn interface prediction will use inplace predict by default when\n  input data is supported.\n* Users can use inplace predict with `dart` booster and enable GPU acceleration just\n  like `gbtree`.\n* Also all prediction functions with tree models are now thread-safe.  Inplace predict is\n  improved with `base_margin` support.\n* A new set of C predict functions are exposed in the public interface.\n* A user-visible change is a newly added parameter called `strict_shape`.  See\n  https://xgboost.readthedocs.io/en/latest/prediction.html for more details.\n\n\n### Improvement on Dask interface\n* Starting with 1.4, the Dask interface is considered to be feature-complete, which means\n  all of the models found in the single node Python interface are now supported in Dask,\n  including but not limited to ranking and random forest.  Also, the prediction function\n  is significantly faster and supports shap value computation.\n  - Most of the parameters found in single node sklearn interface are supported by\n    Dask interface. (#6471, #6591)\n  - Implements learning to rank.  On the Dask interface, we use the newly added support of\n    query ID to enable group structure. (#6576)\n  - The Dask interface has Python type hints support. (#6519)\n  - All models can be safely pickled. (#6651)\n  - Random forest estimators are now supported. (#6602)\n  - Shap value computation is now supported. (#6575, #6645, #6614)\n  - Evaluation result is printed on the scheduler process. (#6609)\n  - `DaskDMatrix` (and device quantile dmatrix) now accepts all meta-information. (#6601)\n\n* Prediction optimization.  We enhanced and speeded up the prediction function for the\n  Dask interface.  See the latest Dask tutorial page in our document for an overview of\n  how you can optimize it even further. (#6650, #6645, #6648, #6668)\n\n* Bug fixes\n  - If you are using the latest Dask and distributed where `distributed.MultiLock` is\n    present, XGBoost supports training multiple models on the same cluster in\n    parallel. (#6743)\n  - A bug fix for when using `dask.client` to launch async task, XGBoost might use a\n    different client object internally. (#6722)\n\n* Other improvements on documents, blogs, tutorials, and demos. (#6389, #6366, #6687,\n  #6699, #6532, #6501)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Python package\nWith changes from Dask and general improvement on prediction, we have made some\nenhancements on the general Python interface and IO for booster information.  Starting\nfrom 1.4, booster feature names and types can be saved into the JSON model.  Also some\nmodel attributes like `best_iteration`, `best_score` are restored upon model load.  On\nsklearn interface, some attributes are now implemented as Python object property with\nbetter documents.\n\n* Breaking change: All `data` parameters in prediction functions are renamed to `X`\n  for better compliance to sklearn estimator interface guidelines.\n* Breaking change: XGBoost used to generate some pseudo feature names with `DMatrix`\n  when inputs like `np.ndarray` don't have column names.  The procedure is removed to\n  avoid conflict with other inputs. (#6605)\n* Early stopping with training continuation is now supported. (#6506)\n* Optional import for Dask and cuDF are now lazy. (#6522)\n* As mentioned in the prediction improvement summary, the sklearn interface uses inplace\n  prediction whenever possible. (#6718)\n* Booster information like feature names and feature types are now saved into the JSON\n  model file. (#6605)\n* All `DMatrix` interfaces including `DeviceQuantileDMatrix` and counterparts in Dask\n  interface (as mentioned in the Dask changes summary) now accept all the meta-information\n  like `group` and `qid` in their constructor for better consistency. (#6601)\n* Booster attributes are restored upon model load so users don't have to call `attr`\n  manually. (#6593)\n* On sklearn interface, all models accept `base_margin` for evaluation datasets. (#6591)\n* Improvements over the setup script including smaller sdist size and faster installation\n  if the C++ library is already built (#6611, #6694, #6565).\n\n* Bug fixes for Python package:\n  - Don't validate feature when number of rows is 0. (#6472)\n  - Move metric configuration into booster. (#6504)\n  - Calling XGBModel.fit() should clear the Booster by default (#6562)\n  - Support `_estimator_type`. (#6582)\n  - [dask, sklearn] Fix predict proba. (#6566, #6817)\n  - Restore unknown data support. (#6595)\n  - Fix learning rate scheduler with cv. (#6720)\n  - Fixes small typo in sklearn documentation (#6717)\n  - [python-package] Fix class Booster: feature_types = None (#6705)\n  - Fix divide by 0 in feature importance when no split is found. (#6676)\n\n\n### JVM package\n* [jvm-packages] fix early stopping doesn't work even without custom_eval setting (#6738)\n* fix potential TaskFailedListener's callback won't be called (#6612)\n* [jvm] Add ability to load booster direct from byte array (#6655)\n* [jvm-packages] JVM library loader extensions (#6630)\n\n### R package\n* R documentation: Make construction of DMatrix consistent.\n* Fix R documentation for xgb.train. (#6764)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Python package\nWith changes from Dask and general improvement on prediction, we have made some\nenhancements on the general Python interface and IO for booster information.  Starting\nfrom 1.4, booster feature names and types can be saved into the JSON model.  Also some\nmodel attributes like `best_iteration`, `best_score` are restored upon model load.  On\nsklearn interface, some attributes are now implemented as Python object property with\nbetter documents.\n\n* Breaking change: All `data` parameters in prediction functions are renamed to `X`\n  for better compliance to sklearn estimator interface guidelines.\n* Breaking change: XGBoost used to generate some pseudo feature names with `DMatrix`\n  when inputs like `np.ndarray` don't have column names.  The procedure is removed to\n  avoid conflict with other inputs. (#6605)\n* Early stopping with training continuation is now supported. (#6506)\n* Optional import for Dask and cuDF are now lazy. (#6522)\n* As mentioned in the prediction improvement summary, the sklearn interface uses inplace\n  prediction whenever possible. (#6718)\n* Booster information like feature names and feature types are now saved into the JSON\n  model file. (#6605)\n* All `DMatrix` interfaces including `DeviceQuantileDMatrix` and counterparts in Dask\n  interface (as mentioned in the Dask changes summary) now accept all the meta-information\n  like `group` and `qid` in their constructor for better consistency. (#6601)\n* Booster attributes are restored upon model load so users don't have to call `attr`\n  manually. (#6593)\n* On sklearn interface, all models accept `base_margin` for evaluation datasets. (#6591)\n* Improvements over the setup script including smaller sdist size and faster installation\n  if the C++ library is already built (#6611, #6694, #6565).\n\n* Bug fixes for Python package:\n  - Don't validate feature when number of rows is 0. (#6472)\n  - Move metric configuration into booster. (#6504)\n  - Calling XGBModel.fit() should clear the Booster by default (#6562)\n  - Support `_estimator_type`. (#6582)\n  - [dask, sklearn] Fix predict proba. (#6566, #6817)\n  - Restore unknown data support. (#6595)\n  - Fix learning rate scheduler with cv. (#6720)\n  - Fixes small typo in sklearn documentation (#6717)\n  - [python-package] Fix class Booster: feature_types = None (#6705)\n  - Fix divide by 0 in feature importance when no split is found. (#6676)\n\n\n### JVM package\n* [jvm-packages] fix early stopping doesn't work even without custom_eval setting (#6738)\n* fix potential TaskFailedListener's callback won't be called (#6612)\n* [jvm] Add ability to load booster direct from byte array (#6655)\n* [jvm-packages] JVM library loader extensions (#6630)\n\n### R package\n* R documentation: Make construction of DMatrix consistent.\n* Fix R documentation for xgb.train. (#6764)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### ROC-AUC\nWe re-implemented the ROC-AUC metric in XGBoost.  The new implementation supports\nmulti-class classification and has better support for learning to rank tasks that are not\nbinary.  Also, it has a better-defined average on distributed environments with additional\nhandling for invalid datasets. (#6749, #6747, #6797)\n\n### Global configuration.\nStarting from 1.4, XGBoost's Python, R and C interfaces support a new global configuration\nmodel where users can specify some global parameters.  Currently, supported parameters are\n`verbosity` and `use_rmm`.  The latter is experimental, see rmm plugin demo and\nrelated README file for details. (#6414, #6656)\n\n### Other New features.\n* Better handling for input data types that support `__array_interface__`.  For some\n  data types including GPU inputs and `scipy.sparse.csr_matrix`, XGBoost employs\n  `__array_interface__` for processing the underlying data.  Starting from 1.4, XGBoost\n  can accept arbitrary array strides (which means column-major is supported) without\n  making data copies, potentially reducing a significant amount of memory consumption.\n  Also version 3 of `__cuda_array_interface__` is now supported.  (#6776, #6765, #6459,\n  #6675)\n* Improved parameter validation, now feeding XGBoost with parameters that contain\n  whitespace will trigger an error. (#6769)\n* For Python and R packages, file paths containing the home indicator `~` are supported.\n* As mentioned in the Python changes summary, the JSON model can now save feature\n  information of the trained booster.  The JSON schema is updated accordingly. (#6605)\n* Development of categorical data support is continued.  Newly added weighted data support\n  and `dart` booster support. (#6508, #6693)\n* As mentioned in Dask change summary, ranking now supports the `qid` parameter for\n  query groups. (#6576)\n* `DMatrix.slice` can now consume a numpy array. (#6368)\n\n### Other breaking changes\n* Aside from the feature name generation, there are 2 breaking changes:\n  - Drop saving binary format for memory snapshot. (#6513, #6640)\n  - Change default evaluation metric for binary:logitraw objective to logloss (#6647)\n\n### CPU Optimization\n* Aside from the general changes on predict function, some optimizations are applied on\n  CPU implementation. (#6683, #6550, #6696, #6700)\n* Also performance for sampling initialization in `hist` is improved. (#6410)\n\n### Notable fixes in the core library\nThese fixes do not reside in particular language bindings:\n* Fixes for gamma regression.  This includes checking for invalid input values, fixes for\n  gamma deviance metric, and better floating point guard for gamma negative log-likelihood\n  metric. (#6778, #6537, #6761)\n* Random forest with `gpu_hist` might generate low accuracy in previous versions. (#6755)\n* Fix a bug in GPU sketching when data size exceeds limit of 32-bit integer. (#6826)\n* Memory consumption fix for row-major adapters (#6779)\n* Don't estimate sketch batch size when rmm is used. (#6807) (#6830)\n* Fix in-place predict with missing value. (#6787)\n* Re-introduce double buffer in UpdatePosition, to fix perf regression in gpu_hist (#6757)\n* Pass correct split_type to GPU predictor (#6491)\n* Fix DMatrix feature names/types IO. (#6507)\n* Use view for `SparsePage` exclusively to avoid some data access races. (#6590)\n* Check for invalid data. (#6742)\n* Fix relocatable include in CMakeList (#6734) (#6737)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### ROC-AUC\nWe re-implemented the ROC-AUC metric in XGBoost.  The new implementation supports\nmulti-class classification and has better support for learning to rank tasks that are not\nbinary.  Also, it has a better-defined average on distributed environments with additional\nhandling for invalid datasets. (#6749, #6747, #6797)\n\n### Global configuration.\nStarting from 1.4, XGBoost's Python, R and C interfaces support a new global configuration\nmodel where users can specify some global parameters.  Currently, supported parameters are\n`verbosity` and `use_rmm`.  The latter is experimental, see rmm plugin demo and\nrelated README file for details. (#6414, #6656)\n\n### Other New features.\n* Better handling for input data types that support `__array_interface__`.  For some\n  data types including GPU inputs and `scipy.sparse.csr_matrix`, XGBoost employs\n  `__array_interface__` for processing the underlying data.  Starting from 1.4, XGBoost\n  can accept arbitrary array strides (which means column-major is supported) without\n  making data copies, potentially reducing a significant amount of memory consumption.\n  Also version 3 of `__cuda_array_interface__` is now supported.  (#6776, #6765, #6459,\n  #6675)\n* Improved parameter validation, now feeding XGBoost with parameters that contain\n  whitespace will trigger an error. (#6769)\n* For Python and R packages, file paths containing the home indicator `~` are supported.\n* As mentioned in the Python changes summary, the JSON model can now save feature\n  information of the trained booster.  The JSON schema is updated accordingly. (#6605)\n* Development of categorical data support is continued.  Newly added weighted data support\n  and `dart` booster support. (#6508, #6693)\n* As mentioned in Dask change summary, ranking now supports the `qid` parameter for\n  query groups. (#6576)\n* `DMatrix.slice` can now consume a numpy array. (#6368)\n\n### Other breaking changes\n* Aside from the feature name generation, there are 2 breaking changes:\n  - Drop saving binary format for memory snapshot. (#6513, #6640)\n  - Change default evaluation metric for binary:logitraw objective to logloss (#6647)\n\n### CPU Optimization\n* Aside from the general changes on predict function, some optimizations are applied on\n  CPU implementation. (#6683, #6550, #6696, #6700)\n* Also performance for sampling initialization in `hist` is improved. (#6410)\n\n### Notable fixes in the core library\nThese fixes do not reside in particular language bindings:\n* Fixes for gamma regression.  This includes checking for invalid input values, fixes for\n  gamma deviance metric, and better floating point guard for gamma negative log-likelihood\n  metric. (#6778, #6537, #6761)\n* Random forest with `gpu_hist` might generate low accuracy in previous versions. (#6755)\n* Fix a bug in GPU sketching when data size exceeds limit of 32-bit integer. (#6826)\n* Memory consumption fix for row-major adapters (#6779)\n* Don't estimate sketch batch size when rmm is used. (#6807) (#6830)\n* Fix in-place predict with missing value. (#6787)\n* Re-introduce double buffer in UpdatePosition, to fix perf regression in gpu_hist (#6757)\n* Pass correct split_type to GPU predictor (#6491)\n* Fix DMatrix feature names/types IO. (#6507)\n* Use view for `SparsePage` exclusively to avoid some data access races. (#6590)\n* Check for invalid data. (#6742)\n* Fix relocatable include in CMakeList (#6734) (#6737)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fix DMatrix slice with feature types. (#6689)\n\n### Other deprecation notices:\n\n* This release will be the last release to support CUDA 10.0. (#6642)\n\n* Starting in the next release, the Python package will require Pip 19.3+ due to the use\n  of manylinux2014 tag. Also, CentOS 6, RHEL 6 and other old distributions will not be\n  supported.\n\n### Known issue:\n\nMacOS build of the JVM packages doesn't support multi-threading out of the box. To enable\nmulti-threading with JVM packages, MacOS users will need to build the JVM packages from\nthe source. See https://xgboost.readthedocs.io/en/latest/jvm/index.html#installation-from-source\n\n\n### Doc\n* Dedicated page for `tree_method` parameter is added. (#6564, #6633)\n* [doc] Add FLAML as a fast tuning tool for XGBoost  (#6770)\n* Add document for tests directory. [skip ci] (#6760)\n* Fix doc string of config.py to use correct `versionadded` (#6458)\n* Update demo for prediction. (#6789)\n* [Doc] Document that AUCPR is for binary classification/ranking (#5899)\n* Update the C API comments (#6457)\n* Fix document. [skip ci] (#6669)\n\n### Maintenance: Testing, continuous integration\n* Use CPU input for test_boost_from_prediction. (#6818)\n* [CI] Upload xgboost4j.dll to S3 (#6781)\n* Update dmlc-core submodule (#6745)\n* [CI] Use manylinux2010_x86_64 container to vendor libgomp (#6485)\n* Add conda-forge badge (#6502)\n* Fix merge conflict. (#6512)\n* [CI] Split up main.yml, add mypy. (#6515)\n* [Breaking] Upgrade cuDF and RMM to 0.18 nightlies; require RMM 0.18+ for RMM plugin (#6510)\n* \"featue_map\" typo changed to  \"feature_map\" (#6540)\n* Add script for generating release tarball. (#6544)\n* Add credentials to .gitignore (#6559)\n* Remove warnings in tests. (#6554)\n* Update dmlc-core submodule and conform to new API (#6431)\n* Suppress hypothesis health check for dask client. (#6589)\n* Fix pylint. (#6714)\n* [CI] Clear R package cache (#6746)\n* Exclude dmlc test on github action. (#6625)\n* Tests for regression metrics with weights. (#6729)\n* Add helper script and doc for releasing pip package. (#6613)\n* Support pylint 2.7.0 (#6726)\n* Remove R cache in github action. (#6695)\n* [CI] Do not mix up stashed executable built for ARM and x86_64 platforms (#6646)\n* [CI] Add ARM64 test to Jenkins pipeline (#6643)\n* Disable s390x and arm64 tests on travis for now. (#6641)\n* Move sdist test to action. (#6635)\n* [dask] Rework base margin test. (#6627)\n\n\n### Maintenance: Refactor code for legibility and maintainability\n* Improve OpenMP exception handling (#6680)\n* Improve string view to reduce string allocation. (#6644)\n* Simplify Span checks. (#6685)\n* Use generic dispatching routine for array interface. (#6672)\n\n\n## v1.3.0 (2020.12.08)\n\n### XGBoost4J-Spark: Exceptions should cancel jobs gracefully instead of killing SparkContext (#6019).\n* By default, exceptions in XGBoost4J-Spark causes the whole SparkContext to shut down, necessitating the restart of the Spark cluster. This behavior is often a major inconvenience.\n* Starting from 1.3.0 release, XGBoost adds a new parameter `killSparkContextOnWorkerFailure` to optionally prevent killing SparkContext. If this parameter is set, exceptions will gracefully cancel training jobs instead of killing SparkContext.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fix DMatrix slice with feature types. (#6689)\n\n### Other deprecation notices:\n\n* This release will be the last release to support CUDA 10.0. (#6642)\n\n* Starting in the next release, the Python package will require Pip 19.3+ due to the use\n  of manylinux2014 tag. Also, CentOS 6, RHEL 6 and other old distributions will not be\n  supported.\n\n### Known issue:\n\nMacOS build of the JVM packages doesn't support multi-threading out of the box. To enable\nmulti-threading with JVM packages, MacOS users will need to build the JVM packages from\nthe source. See https://xgboost.readthedocs.io/en/latest/jvm/index.html#installation-from-source\n\n\n### Doc\n* Dedicated page for `tree_method` parameter is added. (#6564, #6633)\n* [doc] Add FLAML as a fast tuning tool for XGBoost  (#6770)\n* Add document for tests directory. [skip ci] (#6760)\n* Fix doc string of config.py to use correct `versionadded` (#6458)\n* Update demo for prediction. (#6789)\n* [Doc] Document that AUCPR is for binary classification/ranking (#5899)\n* Update the C API comments (#6457)\n* Fix document. [skip ci] (#6669)\n\n### Maintenance: Testing, continuous integration\n* Use CPU input for test_boost_from_prediction. (#6818)\n* [CI] Upload xgboost4j.dll to S3 (#6781)\n* Update dmlc-core submodule (#6745)\n* [CI] Use manylinux2010_x86_64 container to vendor libgomp (#6485)\n* Add conda-forge badge (#6502)\n* Fix merge conflict. (#6512)\n* [CI] Split up main.yml, add mypy. (#6515)\n* [Breaking] Upgrade cuDF and RMM to 0.18 nightlies; require RMM 0.18+ for RMM plugin (#6510)\n* \"featue_map\" typo changed to  \"feature_map\" (#6540)\n* Add script for generating release tarball. (#6544)\n* Add credentials to .gitignore (#6559)\n* Remove warnings in tests. (#6554)\n* Update dmlc-core submodule and conform to new API (#6431)\n* Suppress hypothesis health check for dask client. (#6589)\n* Fix pylint. (#6714)\n* [CI] Clear R package cache (#6746)\n* Exclude dmlc test on github action. (#6625)\n* Tests for regression metrics with weights. (#6729)\n* Add helper script and doc for releasing pip package. (#6613)\n* Support pylint 2.7.0 (#6726)\n* Remove R cache in github action. (#6695)\n* [CI] Do not mix up stashed executable built for ARM and x86_64 platforms (#6646)\n* [CI] Add ARM64 test to Jenkins pipeline (#6643)\n* Disable s390x and arm64 tests on travis for now. (#6641)\n* Move sdist test to action. (#6635)\n* [dask] Rework base margin test. (#6627)\n\n\n### Maintenance: Refactor code for legibility and maintainability\n* Improve OpenMP exception handling (#6680)\n* Improve string view to reduce string allocation. (#6644)\n* Simplify Span checks. (#6685)\n* Use generic dispatching routine for array interface. (#6672)\n\n\n## v1.3.0 (2020.12.08)\n\n### XGBoost4J-Spark: Exceptions should cancel jobs gracefully instead of killing SparkContext (#6019).\n* By default, exceptions in XGBoost4J-Spark causes the whole SparkContext to shut down, necessitating the restart of the Spark cluster. This behavior is often a major inconvenience.\n* Starting from 1.3.0 release, XGBoost adds a new parameter `killSparkContextOnWorkerFailure` to optionally prevent killing SparkContext. If this parameter is set, exceptions will gracefully cancel training jobs instead of killing SparkContext."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### GPUTreeSHAP: GPU acceleration of the TreeSHAP algorithm (#6038, #6064, #6087, #6099, #6163, #6281, #6332)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) is a game theoretic approach to explain predictions of machine learning models. It computes feature importance scores for individual examples, establishing how each feature influences a particular prediction. TreeSHAP is an optimized SHAP algorithm specifically designed for decision tree ensembles.\n* Starting with 1.3.0 release, it is now possible to leverage CUDA-capable GPUs to accelerate the TreeSHAP algorithm. Check out [the demo notebook](https://github.com/dmlc/xgboost/blob/master/demo/gpu_acceleration/shap.ipynb).\n* The CUDA implementation of the TreeSHAP algorithm is hosted at [rapidsai/GPUTreeSHAP](https://github.com/rapidsai/gputreeshap). XGBoost imports it as a Git submodule.\n\n### New style Python callback API (#6199, #6270, #6320, #6348, #6376, #6399, #6441)\n* The XGBoost Python package now offers a re-designed callback API. The new callback API lets you design various extensions of training in idomatic Python. In addition, the new callback API allows you to use early stopping with the native Dask API (`xgboost.dask`). Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.3.0/python/callbacks.html) and [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/callbacks.py).\n\n### Enable the use of `DeviceQuantileDMatrix` / `DaskDeviceQuantileDMatrix` with large data (#6201, #6229, #6234).\n* `DeviceQuantileDMatrix` can achieve memory saving by avoiding extra copies of the training data, and the saving is bigger for large data. Unfortunately, large data with more than 2^31 elements was triggering integer overflow bugs in CUB and Thrust. Tracking issue: #6228.\n* This release contains a series of work-arounds to allow the use of `DeviceQuantileDMatrix` with large data:\n  - Loop over `copy_if` (#6201)\n  - Loop over `thrust::reduce` (#6229)\n  - Implement the inclusive scan algorithm in-house, to handle large offsets (#6234)\n\n### Support slicing of tree models (#6302)\n* Accessing the best iteration of a model after the application of early stopping used to be error-prone, need to manually pass the `ntree_limit` argument to the `predict()` function.\n* Now we provide a simple interface to slice tree models by specifying a range of boosting rounds. The tree ensemble can be split into multiple sub-ensembles via the slicing interface. Check out [an example](https://xgboost.readthedocs.io/en/release_1.3.0/python/model.html).\n* In addition, the early stopping callback now supports `save_best` option. When enabled, XGBoost will save (persist) the model at the best boosting round and discard the trees that were fit subsequent to the best round.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### GPUTreeSHAP: GPU acceleration of the TreeSHAP algorithm (#6038, #6064, #6087, #6099, #6163, #6281, #6332)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) is a game theoretic approach to explain predictions of machine learning models. It computes feature importance scores for individual examples, establishing how each feature influences a particular prediction. TreeSHAP is an optimized SHAP algorithm specifically designed for decision tree ensembles.\n* Starting with 1.3.0 release, it is now possible to leverage CUDA-capable GPUs to accelerate the TreeSHAP algorithm. Check out [the demo notebook](https://github.com/dmlc/xgboost/blob/master/demo/gpu_acceleration/shap.ipynb).\n* The CUDA implementation of the TreeSHAP algorithm is hosted at [rapidsai/GPUTreeSHAP](https://github.com/rapidsai/gputreeshap). XGBoost imports it as a Git submodule.\n\n### New style Python callback API (#6199, #6270, #6320, #6348, #6376, #6399, #6441)\n* The XGBoost Python package now offers a re-designed callback API. The new callback API lets you design various extensions of training in idomatic Python. In addition, the new callback API allows you to use early stopping with the native Dask API (`xgboost.dask`). Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.3.0/python/callbacks.html) and [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/callbacks.py).\n\n### Enable the use of `DeviceQuantileDMatrix` / `DaskDeviceQuantileDMatrix` with large data (#6201, #6229, #6234).\n* `DeviceQuantileDMatrix` can achieve memory saving by avoiding extra copies of the training data, and the saving is bigger for large data. Unfortunately, large data with more than 2^31 elements was triggering integer overflow bugs in CUB and Thrust. Tracking issue: #6228.\n* This release contains a series of work-arounds to allow the use of `DeviceQuantileDMatrix` with large data:\n  - Loop over `copy_if` (#6201)\n  - Loop over `thrust::reduce` (#6229)\n  - Implement the inclusive scan algorithm in-house, to handle large offsets (#6234)\n\n### Support slicing of tree models (#6302)\n* Accessing the best iteration of a model after the application of early stopping used to be error-prone, need to manually pass the `ntree_limit` argument to the `predict()` function.\n* Now we provide a simple interface to slice tree models by specifying a range of boosting rounds. The tree ensemble can be split into multiple sub-ensembles via the slicing interface. Check out [an example](https://xgboost.readthedocs.io/en/release_1.3.0/python/model.html).\n* In addition, the early stopping callback now supports `save_best` option. When enabled, XGBoost will save (persist) the model at the best boosting round and discard the trees that were fit subsequent to the best round."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (\"one-vs-rest split\"). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Weighted subsampling of features (columns) (#5962)\n* It is now possible to sample features (columns) via weighted subsampling, in which features with higher weights are more likely to be selected in the sample. Weighted subsampling allows you to encode domain knowledge by emphasizing a particular set of features in the choice of tree splits. In addition, you can prevent particular features from being used in any splits, by assigning them zero weights.\n* Check out [the demo](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/feature_weights.py).\n\n### Improved integration with Dask\n* Support reverse-proxy environment such as Google Kubernetes Engine (#6343, #6475)\n* An XGBoost training job will no longer use all available workers. Instead, it will only use the workers that contain input data (#6343).\n* The new callback API works well with the Dask training API.\n* The `predict()` and `fit()` function of `DaskXGBClassifier` and `DaskXGBRegressor` now accept a base margin (#6155).\n* Support more meta data in the Dask API (#6130, #6132, #6333).\n* Allow passing extra keyword arguments as `kwargs` in `predict()` (#6117)\n* Fix typo in dask interface: `sample_weights` -> `sample_weight` (#6240)\n* Allow empty data matrix in AFT survival, as Dask may produce empty partitions (#6379)\n* Speed up prediction by overlapping prediction jobs in all workers (#6412)\n\n### Experimental support for direct splits with categorical features (#6028, #6128, #6137, #6140, #6164, #6165, #6166, #6179, #6194, #6219)\n* Currently, XGBoost requires users to one-hot-encode categorical variables. This has adverse performance implications, as the creation of many dummy variables results into higher memory consumption and may require fitting deeper trees to achieve equivalent model accuracy.\n* The 1.3.0 release of XGBoost contains an experimental support for direct handling of categorical variables in test nodes. Each test node will have the condition of form `feature_value \\in match_set`, where the `match_set` on the right hand side contains one or more matching categories. The matching categories in `match_set` represent the condition for traversing to the right child node. Currently, XGBoost will only generate categorical splits with only a single matching category (\"one-vs-rest split\"). In a future release, we plan to remove this restriction and produce splits with multiple matching categories in `match_set`.\n* The categorical split requires the use of JSON model serialization. The legacy binary serialization method cannot be used to save (persist) models with categorical splits.\n* Note. This feature is currently highly experimental. Use it at your own risk. See the detailed list of limitations at [#5949](https://github.com/dmlc/xgboost/pull/5949).\n\n### Experimental plugin for RAPIDS Memory Manager (#5873, #6131, #6146, #6150, #6182)\n* RAPIDS Memory Manager library ([rapidsai/rmm](https://github.com/rapidsai/rmm)) provides a collection of efficient memory allocators for NVIDIA GPUs. It is now possible to use XGBoost with memory allocators provided by RMM, by enabling the RMM integration plugin. With this plugin, XGBoost is now able to share a common GPU memory pool with other applications using RMM, such as the RAPIDS data science packages.\n* See [the demo](https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.md) for a working example, as well as directions for building XGBoost with the RMM plugin."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* The plugin will be soon considered non-experimental, once #6297 is resolved.\n\n### Experimental plugin for oneAPI programming model (#5825)\n* oneAPI is a programming interface developed by Intel aimed at providing one programming model for many types of hardware such as CPU, GPU, FGPA and other hardware accelerators.\n* XGBoost now includes an experimental plugin for using oneAPI for the predictor and objective functions. The plugin is hosted in the directory `plugin/updater_oneapi`.\n* Roadmap: #5442\n\n### Pickling the XGBoost model will now trigger JSON serialization (#6027)\n* The pickle will now contain the JSON string representation of the XGBoost model, as well as related configuration.\n\n### Performance improvements\n* Various performance improvement on multi-core CPUs\n  - Optimize DMatrix build time by up to 3.7x. (#5877)\n  - CPU predict performance improvement, by up to 3.6x. (#6127)\n  - Optimize CPU sketch allreduce for sparse data (#6009)\n  - Thread local memory allocation for BuildHist, leading to speedup up to 1.7x. (#6358)\n  - Disable hyperthreading for DMatrix creation (#6386). This speeds up DMatrix creation by up to 2x.\n  - Simple fix for static shedule in predict (#6357)\n* Unify thread configuration, to make it easy to utilize all CPU cores (#6186)\n* [jvm-packages] Clean the way deterministic paritioning is computed (#6033)\n* Speed up JSON serialization by implementing an intrusive pointer class (#6129). It leads to 1.5x-2x performance boost.\n\n### API additions\n* [R] Add SHAP summary plot using ggplot2 (#5882)\n* Modin DataFrame can now be used as input (#6055)\n* [jvm-packages] Add `getNumFeature` method (#6075)\n* Add MAPE metric (#6119)\n* Implement GPU predict leaf. (#6187)\n* Enable cuDF/cuPy inputs in `XGBClassifier` (#6269)\n* Document tree method for feature weights. (#6312)\n* Add `fail_on_invalid_gpu_id` parameter, which will cause XGBoost to terminate upon seeing an invalid value of `gpu_id` (#6342)\n\n### Breaking: the default evaluation metric for classification is changed to `logloss` / `mlogloss` (#6183)\n* The default metric used to be accuracy, and it is not statistically consistent to perform early stopping with the accuracy metric when we are really optimizing the log loss for the `binary:logistic` objective.\n* For statistical consistency, the default metric for classification has been changed to `logloss`. Users may choose to preserve the old behavior by explicitly specifying `eval_metric`.\n\n### Breaking: `skmaker` is now removed (#5971)\n* The `skmaker` updater has not been documented nor tested.\n\n### Breaking: the JSON model format no longer stores the leaf child count (#6094).\n* The leaf child count field has been deprecated and is not used anywhere in the XGBoost codebase.\n\n### Breaking: XGBoost now requires MacOS 10.14 (Mojave) and later.\n* Homebrew has dropped support for MacOS 10.13 (High Sierra), so we are not able to install the OpenMP runtime (`libomp`) from Homebrew on MacOS 10.13. Please use MacOS 10.14 (Mojave) or later.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* The plugin will be soon considered non-experimental, once #6297 is resolved.\n\n### Experimental plugin for oneAPI programming model (#5825)\n* oneAPI is a programming interface developed by Intel aimed at providing one programming model for many types of hardware such as CPU, GPU, FGPA and other hardware accelerators.\n* XGBoost now includes an experimental plugin for using oneAPI for the predictor and objective functions. The plugin is hosted in the directory `plugin/updater_oneapi`.\n* Roadmap: #5442\n\n### Pickling the XGBoost model will now trigger JSON serialization (#6027)\n* The pickle will now contain the JSON string representation of the XGBoost model, as well as related configuration.\n\n### Performance improvements\n* Various performance improvement on multi-core CPUs\n  - Optimize DMatrix build time by up to 3.7x. (#5877)\n  - CPU predict performance improvement, by up to 3.6x. (#6127)\n  - Optimize CPU sketch allreduce for sparse data (#6009)\n  - Thread local memory allocation for BuildHist, leading to speedup up to 1.7x. (#6358)\n  - Disable hyperthreading for DMatrix creation (#6386). This speeds up DMatrix creation by up to 2x.\n  - Simple fix for static shedule in predict (#6357)\n* Unify thread configuration, to make it easy to utilize all CPU cores (#6186)\n* [jvm-packages] Clean the way deterministic paritioning is computed (#6033)\n* Speed up JSON serialization by implementing an intrusive pointer class (#6129). It leads to 1.5x-2x performance boost.\n\n### API additions\n* [R] Add SHAP summary plot using ggplot2 (#5882)\n* Modin DataFrame can now be used as input (#6055)\n* [jvm-packages] Add `getNumFeature` method (#6075)\n* Add MAPE metric (#6119)\n* Implement GPU predict leaf. (#6187)\n* Enable cuDF/cuPy inputs in `XGBClassifier` (#6269)\n* Document tree method for feature weights. (#6312)\n* Add `fail_on_invalid_gpu_id` parameter, which will cause XGBoost to terminate upon seeing an invalid value of `gpu_id` (#6342)\n\n### Breaking: the default evaluation metric for classification is changed to `logloss` / `mlogloss` (#6183)\n* The default metric used to be accuracy, and it is not statistically consistent to perform early stopping with the accuracy metric when we are really optimizing the log loss for the `binary:logistic` objective.\n* For statistical consistency, the default metric for classification has been changed to `logloss`. Users may choose to preserve the old behavior by explicitly specifying `eval_metric`.\n\n### Breaking: `skmaker` is now removed (#5971)\n* The `skmaker` updater has not been documented nor tested.\n\n### Breaking: the JSON model format no longer stores the leaf child count (#6094).\n* The leaf child count field has been deprecated and is not used anywhere in the XGBoost codebase.\n\n### Breaking: XGBoost now requires MacOS 10.14 (Mojave) and later.\n* Homebrew has dropped support for MacOS 10.13 (High Sierra), so we are not able to install the OpenMP runtime (`libomp`) from Homebrew on MacOS 10.13. Please use MacOS 10.14 (Mojave) or later."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Deprecation notices\n* The use of `LabelEncoder` in `XGBClassifier` is now deprecated and will be removed in the next minor release (#6269). The deprecation is necessary to support multiple types of inputs, such as cuDF data frames or cuPy arrays.\n* The use of certain positional arguments in the Python interface is deprecated (#6365). Users will use deprecation warnings for the use of position arguments for certain function parameters. New code should use keyword arguments as much as possible. We have not yet decided when we will fully require the use of keyword arguments.\n\n### Bug-fixes\n* On big-endian arch, swap the byte order in the binary serializer to enable loading models that were produced by a little-endian machine (#5813).\n* [jvm-packages] Fix deterministic partitioning with dataset containing Double.NaN (#5996)\n* Limit tree depth for GPU hist to 31 to prevent integer overflow (#6045)\n* [jvm-packages] Set `maxBins` to 256 to align with the default value in the C++ code (#6066)\n* [R] Fix CRAN check (#6077)\n* Add back support for `scipy.sparse.coo_matrix` (#6162)\n* Handle duplicated values in sketching. (#6178)\n* Catch all standard exceptions in C API. (#6220)\n* Fix linear GPU input (#6255)\n* Fix inplace prediction interval. (#6259)\n* [R] allow `xgb.plot.importance()` calls to fill a grid (#6294)\n* Lazy import dask libraries. (#6309)\n* Deterministic data partitioning for external memory (#6317)\n* Avoid resetting seed for every configuration. (#6349)\n* Fix label errors in graph visualization (#6369)\n* [jvm-packages] fix potential unit test suites aborted issue due to race condition (#6373)\n* [R] Fix warnings from `R check --as-cran` (#6374)\n* [R] Fix a crash that occurs with noLD R (#6378)\n* [R] Do not convert continuous labels to factors (#6380)\n* [R] remove uses of `exists()` (#6387)\n* Propagate parameters to the underlying `Booster` handle from `XGBClassifier.set_param` / `XGBRegressor.set_param`. (#6416)\n* [R] Fix R package installation via CMake (#6423)\n* Enforce row-major order in cuPy array (#6459)\n* Fix filtering callable objects in the parameters passed to the scikit-learn API. (#6466)\n\n### Maintenance: Testing, continuous integration, build system\n* [CI] Improve JVM test in GitHub Actions (#5930)\n* Refactor plotting test so that it can run independently (#6040)\n* [CI] Cancel builds on subsequent pushes (#6011)\n* Fix Dask Pytest fixture (#6024)\n* [CI] Migrate linters to GitHub Actions (#6035)\n* [CI] Remove win2016 JVM test from GitHub Actions (#6042)\n* Fix CMake build with `BUILD_STATIC_LIB` option (#6090)\n* Don't link imported target in CMake (#6093)\n* Work around a compiler bug in MacOS AppleClang 11 (#6103)\n* [CI] Fix CTest by running it in a correct directory (#6104)\n* [R] Check warnings explicitly for model compatibility tests (#6114)\n* [jvm-packages] add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)\n* [CI] Time GPU tests. (#6141)\n* [R] remove warning in configure.ac (#6152)\n* [CI] Upgrade cuDF and RMM to 0.16 nightlies; upgrade to Ubuntu 18.04 (#6157)\n* [CI] Test C API demo (#6159)\n* Option for generating device debug info. (#6168)\n* Update `.gitignore` (#6175, #6193, #6346)\n* Hide C++ symbols from dmlc-core (#6188)\n* [CI] Added arm64 job in Travis-CI (#6200)\n* [CI] Fix Docker build for CUDA 11 (#6202)\n* [CI] Move non-OpenMP gtest to GitHub Actions (#6210)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Deprecation notices\n* The use of `LabelEncoder` in `XGBClassifier` is now deprecated and will be removed in the next minor release (#6269). The deprecation is necessary to support multiple types of inputs, such as cuDF data frames or cuPy arrays.\n* The use of certain positional arguments in the Python interface is deprecated (#6365). Users will use deprecation warnings for the use of position arguments for certain function parameters. New code should use keyword arguments as much as possible. We have not yet decided when we will fully require the use of keyword arguments.\n\n### Bug-fixes\n* On big-endian arch, swap the byte order in the binary serializer to enable loading models that were produced by a little-endian machine (#5813).\n* [jvm-packages] Fix deterministic partitioning with dataset containing Double.NaN (#5996)\n* Limit tree depth for GPU hist to 31 to prevent integer overflow (#6045)\n* [jvm-packages] Set `maxBins` to 256 to align with the default value in the C++ code (#6066)\n* [R] Fix CRAN check (#6077)\n* Add back support for `scipy.sparse.coo_matrix` (#6162)\n* Handle duplicated values in sketching. (#6178)\n* Catch all standard exceptions in C API. (#6220)\n* Fix linear GPU input (#6255)\n* Fix inplace prediction interval. (#6259)\n* [R] allow `xgb.plot.importance()` calls to fill a grid (#6294)\n* Lazy import dask libraries. (#6309)\n* Deterministic data partitioning for external memory (#6317)\n* Avoid resetting seed for every configuration. (#6349)\n* Fix label errors in graph visualization (#6369)\n* [jvm-packages] fix potential unit test suites aborted issue due to race condition (#6373)\n* [R] Fix warnings from `R check --as-cran` (#6374)\n* [R] Fix a crash that occurs with noLD R (#6378)\n* [R] Do not convert continuous labels to factors (#6380)\n* [R] remove uses of `exists()` (#6387)\n* Propagate parameters to the underlying `Booster` handle from `XGBClassifier.set_param` / `XGBRegressor.set_param`. (#6416)\n* [R] Fix R package installation via CMake (#6423)\n* Enforce row-major order in cuPy array (#6459)\n* Fix filtering callable objects in the parameters passed to the scikit-learn API. (#6466)\n\n### Maintenance: Testing, continuous integration, build system\n* [CI] Improve JVM test in GitHub Actions (#5930)\n* Refactor plotting test so that it can run independently (#6040)\n* [CI] Cancel builds on subsequent pushes (#6011)\n* Fix Dask Pytest fixture (#6024)\n* [CI] Migrate linters to GitHub Actions (#6035)\n* [CI] Remove win2016 JVM test from GitHub Actions (#6042)\n* Fix CMake build with `BUILD_STATIC_LIB` option (#6090)\n* Don't link imported target in CMake (#6093)\n* Work around a compiler bug in MacOS AppleClang 11 (#6103)\n* [CI] Fix CTest by running it in a correct directory (#6104)\n* [R] Check warnings explicitly for model compatibility tests (#6114)\n* [jvm-packages] add xgboost4j-gpu/xgboost4j-spark-gpu module to facilitate release (#6136)\n* [CI] Time GPU tests. (#6141)\n* [R] remove warning in configure.ac (#6152)\n* [CI] Upgrade cuDF and RMM to 0.16 nightlies; upgrade to Ubuntu 18.04 (#6157)\n* [CI] Test C API demo (#6159)\n* Option for generating device debug info. (#6168)\n* Update `.gitignore` (#6175, #6193, #6346)\n* Hide C++ symbols from dmlc-core (#6188)\n* [CI] Added arm64 job in Travis-CI (#6200)\n* [CI] Fix Docker build for CUDA 11 (#6202)\n* [CI] Move non-OpenMP gtest to GitHub Actions (#6210)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)\n* Add more tests for categorical data support (#6219)\n* [dask] Test for data initializaton. (#6226)\n* Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j (#6230)\n* Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j-gpu (#6233)\n* [CI] Reduce testing load with RMM (#6249)\n* [CI] Build a Python wheel for aarch64 platform (#6253)\n* [CI] Time the CPU tests on Jenkins. (#6257)\n* [CI] Skip Dask tests on ARM. (#6267)\n* Fix a typo in `is_arm()` in testing.py (#6271)\n* [CI] replace `egrep` with `grep -E` (#6287)\n* Support unity build. (#6295)\n* [CI] Mark flaky tests as XFAIL (#6299)\n* [CI] Use separate Docker cache for each CUDA version (#6305)\n* Added `USE_NCCL_LIB_PATH` option to enable user to set `NCCL_LIBRARY` during build  (#6310)\n* Fix flaky data initialization test. (#6318)\n* Add a badge for GitHub Actions (#6321)\n* Optional `find_package` for sanitizers. (#6329)\n* Use pytest conventions consistently in Python tests (#6337)\n* Fix missing space in warning message (#6340)\n* Update `custom_metric_obj.rst` (#6367)\n* [CI] Run R check with `--as-cran` flag on GitHub Actions (#6371)\n* [CI] Remove R check from Jenkins (#6372)\n* Mark GPU external memory test as XFAIL. (#6381)\n* [CI] Add noLD R test (#6382)\n* Fix MPI build. (#6403)\n* [CI] Upgrade to MacOS Mojave image (#6406)\n* Fix flaky sparse page dmatrix test. (#6417)\n* [CI] Upgrade cuDF and RMM to 0.17 nightlies (#6434)\n* [CI] Fix CentOS 6 Docker images (#6467)\n* [CI] Vendor libgomp in the manylinux Python wheel (#6461)\n* [CI] Hot fix for libgomp vendoring (#6482)\n\n### Maintenance: Clean up and merge the Rabit submodule (#6023, #6095, #6096, #6105, #6110, #6262, #6275, #6290)\n* The Rabit submodule is now maintained as part of the XGBoost codebase.\n* Tests for Rabit are now part of the test suites of XGBoost.\n* Rabit can now be built on the Windows platform.\n* We made various code re-formatting for the C++ code with clang-tidy.\n* Public headers of XGBoost no longer depend on Rabit headers.\n* Unused CMake targets for Rabit were removed.\n* Single-point model recovery has been dropped and removed from Rabit, simplifying the Rabit code greatly. The single-point model recovery feature has not been adequately maintained over the years.\n* We removed the parts of Rabit that were not useful for XGBoost.\n\n### Maintenance: Refactor code for legibility and maintainability\n* Unify CPU hist sketching (#5880)\n* [R] fix uses of 1:length(x) and other small things (#5992)\n* Unify evaluation functions. (#6037)\n* Make binary bin search reusable. (#6058)\n* Unify set index data. (#6062)\n* [R] Remove `stringi` dependency (#6109)\n* Merge extract cuts into QuantileContainer. (#6125)\n* Reduce C++ compiler warnings (#6197, #6198, #6213, #6286, #6325)\n* Cleanup Python code. (#6223)\n* Small cleanup to evaluator. (#6400)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [jvm-packages] Fix up build for xgboost4j-gpu, xgboost4j-spark-gpu (#6216)\n* Add more tests for categorical data support (#6219)\n* [dask] Test for data initializaton. (#6226)\n* Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j (#6230)\n* Bump junit from 4.11 to 4.13.1 in /jvm-packages/xgboost4j-gpu (#6233)\n* [CI] Reduce testing load with RMM (#6249)\n* [CI] Build a Python wheel for aarch64 platform (#6253)\n* [CI] Time the CPU tests on Jenkins. (#6257)\n* [CI] Skip Dask tests on ARM. (#6267)\n* Fix a typo in `is_arm()` in testing.py (#6271)\n* [CI] replace `egrep` with `grep -E` (#6287)\n* Support unity build. (#6295)\n* [CI] Mark flaky tests as XFAIL (#6299)\n* [CI] Use separate Docker cache for each CUDA version (#6305)\n* Added `USE_NCCL_LIB_PATH` option to enable user to set `NCCL_LIBRARY` during build  (#6310)\n* Fix flaky data initialization test. (#6318)\n* Add a badge for GitHub Actions (#6321)\n* Optional `find_package` for sanitizers. (#6329)\n* Use pytest conventions consistently in Python tests (#6337)\n* Fix missing space in warning message (#6340)\n* Update `custom_metric_obj.rst` (#6367)\n* [CI] Run R check with `--as-cran` flag on GitHub Actions (#6371)\n* [CI] Remove R check from Jenkins (#6372)\n* Mark GPU external memory test as XFAIL. (#6381)\n* [CI] Add noLD R test (#6382)\n* Fix MPI build. (#6403)\n* [CI] Upgrade to MacOS Mojave image (#6406)\n* Fix flaky sparse page dmatrix test. (#6417)\n* [CI] Upgrade cuDF and RMM to 0.17 nightlies (#6434)\n* [CI] Fix CentOS 6 Docker images (#6467)\n* [CI] Vendor libgomp in the manylinux Python wheel (#6461)\n* [CI] Hot fix for libgomp vendoring (#6482)\n\n### Maintenance: Clean up and merge the Rabit submodule (#6023, #6095, #6096, #6105, #6110, #6262, #6275, #6290)\n* The Rabit submodule is now maintained as part of the XGBoost codebase.\n* Tests for Rabit are now part of the test suites of XGBoost.\n* Rabit can now be built on the Windows platform.\n* We made various code re-formatting for the C++ code with clang-tidy.\n* Public headers of XGBoost no longer depend on Rabit headers.\n* Unused CMake targets for Rabit were removed.\n* Single-point model recovery has been dropped and removed from Rabit, simplifying the Rabit code greatly. The single-point model recovery feature has not been adequately maintained over the years.\n* We removed the parts of Rabit that were not useful for XGBoost.\n\n### Maintenance: Refactor code for legibility and maintainability\n* Unify CPU hist sketching (#5880)\n* [R] fix uses of 1:length(x) and other small things (#5992)\n* Unify evaluation functions. (#6037)\n* Make binary bin search reusable. (#6058)\n* Unify set index data. (#6062)\n* [R] Remove `stringi` dependency (#6109)\n* Merge extract cuts into QuantileContainer. (#6125)\n* Reduce C++ compiler warnings (#6197, #6198, #6213, #6286, #6325)\n* Cleanup Python code. (#6223)\n* Small cleanup to evaluator. (#6400)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Usability Improvements, Documentation\n* [jvm-packages] add example to handle missing value other than 0 (#5677)\n* Add DMatrix usage examples to the C API demo (#5854)\n* List `DaskDeviceQuantileDMatrix` in the doc. (#5975)\n* Update Python custom objective demo. (#5981)\n* Update the JSON model schema to document more objective functions. (#5982)\n* [Python] Fix warning when `missing` field is not used. (#5969)\n* Fix typo in tracker logging (#5994)\n* Move a warning about empty dataset, so that it's shown for all objectives and metrics (#5998)\n* Fix the instructions for installing the nightly build. (#6004)\n* [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)\n* [jvm-packages] [doc] Update install doc for JVM packages (#6051)\n* Fix typo in `xgboost.callback.early_stop` docstring (#6071)\n* Add cache suffix to the files used in the external memory demo. (#6088)\n* [Doc] Document the parameter `kill_spark_context_on_worker_failure` (#6097)\n* Fix link to the demo for custom objectives (#6100)\n* Update Dask doc. (#6108)\n* Validate weights are positive values. (#6115)\n* Document the updated CMake version requirement. (#6123)\n* Add demo for `DaskDeviceQuantileDMatrix`. (#6156)\n* Cosmetic fixes in `faq.rst` (#6161)\n* Fix error message. (#6176)\n* [Doc] Add list of winning solutions in data science competitions using XGBoost (#6177)\n* Fix a comment in demo to use correct reference (#6190)\n* Update the list of winning solutions using XGBoost (#6192)\n* Consistent style for build status badge (#6203)\n* [Doc] Add info on GPU compiler (#6204)\n* Update the list of winning solutions (#6222, #6254)\n* Add link to XGBoost's Twitter handle (#6244)\n* Fix minor typos in XGBClassifier methods' docstrings (#6247)\n* Add sponsors link to FUNDING.yml (#6252)\n* Group CLI demo into subdirectory. (#6258)\n* Reduce warning messages from `gbtree`. (#6273)\n* Create a tutorial for using the C API in a C/C++ application (#6285)\n* Update plugin instructions for CMake build (#6289)\n* [doc] make Dask distributed example copy-pastable (#6345)\n* [Python] Add option to use `libxgboost.so` from the system path (#6362)\n* Fixed few grammatical mistakes in doc (#6393)\n* Fix broken link in CLI doc (#6396)\n* Improve documentation for the Dask API (#6413)\n* Revise misleading exception information: no such param of `allow_non_zero_missing` (#6418)\n* Fix CLI ranking demo. (#6439)\n* Fix broken links. (#6455)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @FelixYBW, Jack Dunn (@JackDunnNZ), Jean Lescut-Muller (@JeanLescut),  Boris Feld (@Lothiraldan), Nikhil Choudhary (@Nikhil1O1), Rory Mitchell (@RAMitchell), @ShvetsKS, Anthony D'Amato (@Totoketchup), @Wittty-Panda, neko (@akiyamaneko), Alexander Gugel (@alexanderGugel), @dependabot[bot], DIVYA CHAUHAN (@divya661), Daniel Steinberg (@dstein64), Akira Funahashi (@funasoul), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Hristo Iliev (@hiliev), Honza Sterba (@honzasterba), @hzy001, Igor Moura (@igormp), @jameskrach, James Lamb (@jameslamb), Naveed Ahmed Saleem Janvekar (@janvekarnaveed), Kyle Nicholson (@kylejn27), lacrosse91 (@lacrosse91), Christian Lorentzen (@lorentzenchr), Manikya Bardhan (@manikyabard), @nabokovas, John Quitto-Graham (@nvidia-johnq), @odidev, Qi Zhang (@qzhang90), Sergio Gavil\u00e1n (@sgavil), Tanuja Kirthi Doddapaneni (@tanuja3), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), Zeno Gantner (@zenogantner), zhang_jf (@zuston)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Usability Improvements, Documentation\n* [jvm-packages] add example to handle missing value other than 0 (#5677)\n* Add DMatrix usage examples to the C API demo (#5854)\n* List `DaskDeviceQuantileDMatrix` in the doc. (#5975)\n* Update Python custom objective demo. (#5981)\n* Update the JSON model schema to document more objective functions. (#5982)\n* [Python] Fix warning when `missing` field is not used. (#5969)\n* Fix typo in tracker logging (#5994)\n* Move a warning about empty dataset, so that it's shown for all objectives and metrics (#5998)\n* Fix the instructions for installing the nightly build. (#6004)\n* [Doc] Add dtreeviz as a showcase example of integration with 3rd-party software (#6013)\n* [jvm-packages] [doc] Update install doc for JVM packages (#6051)\n* Fix typo in `xgboost.callback.early_stop` docstring (#6071)\n* Add cache suffix to the files used in the external memory demo. (#6088)\n* [Doc] Document the parameter `kill_spark_context_on_worker_failure` (#6097)\n* Fix link to the demo for custom objectives (#6100)\n* Update Dask doc. (#6108)\n* Validate weights are positive values. (#6115)\n* Document the updated CMake version requirement. (#6123)\n* Add demo for `DaskDeviceQuantileDMatrix`. (#6156)\n* Cosmetic fixes in `faq.rst` (#6161)\n* Fix error message. (#6176)\n* [Doc] Add list of winning solutions in data science competitions using XGBoost (#6177)\n* Fix a comment in demo to use correct reference (#6190)\n* Update the list of winning solutions using XGBoost (#6192)\n* Consistent style for build status badge (#6203)\n* [Doc] Add info on GPU compiler (#6204)\n* Update the list of winning solutions (#6222, #6254)\n* Add link to XGBoost's Twitter handle (#6244)\n* Fix minor typos in XGBClassifier methods' docstrings (#6247)\n* Add sponsors link to FUNDING.yml (#6252)\n* Group CLI demo into subdirectory. (#6258)\n* Reduce warning messages from `gbtree`. (#6273)\n* Create a tutorial for using the C API in a C/C++ application (#6285)\n* Update plugin instructions for CMake build (#6289)\n* [doc] make Dask distributed example copy-pastable (#6345)\n* [Python] Add option to use `libxgboost.so` from the system path (#6362)\n* Fixed few grammatical mistakes in doc (#6393)\n* Fix broken link in CLI doc (#6396)\n* Improve documentation for the Dask API (#6413)\n* Revise misleading exception information: no such param of `allow_non_zero_missing` (#6418)\n* Fix CLI ranking demo. (#6439)\n* Fix broken links. (#6455)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @FelixYBW, Jack Dunn (@JackDunnNZ), Jean Lescut-Muller (@JeanLescut),  Boris Feld (@Lothiraldan), Nikhil Choudhary (@Nikhil1O1), Rory Mitchell (@RAMitchell), @ShvetsKS, Anthony D'Amato (@Totoketchup), @Wittty-Panda, neko (@akiyamaneko), Alexander Gugel (@alexanderGugel), @dependabot[bot], DIVYA CHAUHAN (@divya661), Daniel Steinberg (@dstein64), Akira Funahashi (@funasoul), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Hristo Iliev (@hiliev), Honza Sterba (@honzasterba), @hzy001, Igor Moura (@igormp), @jameskrach, James Lamb (@jameslamb), Naveed Ahmed Saleem Janvekar (@janvekarnaveed), Kyle Nicholson (@kylejn27), lacrosse91 (@lacrosse91), Christian Lorentzen (@lorentzenchr), Manikya Bardhan (@manikyabard), @nabokovas, John Quitto-Graham (@nvidia-johnq), @odidev, Qi Zhang (@qzhang90), Sergio Gavil\u00e1n (@sgavil), Tanuja Kirthi Doddapaneni (@tanuja3), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), Zeno Gantner (@zenogantner), zhang_jf (@zuston)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), John Zedlewski (@JohnZed), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Anthony D'Amato (@Totoketchup), @Wittty-Panda, Alexander Gugel (@alexanderGugel), Codecov Comments Bot (@codecov-commenter), Codecov (@codecov-io), DIVYA CHAUHAN (@divya661), Devin Robison (@drobison00), Geoffrey Blake (@geoffreyblake), Mark Harris (@harrism), Philip Hyunsu Cho (@hcho3), Honza Sterba (@honzasterba), Igor Moura (@igormp), @jakirkham, @jameskrach, James Lamb (@jameslamb), Janakarajan Natarajan (@janaknat), Jake Hemstad (@jrhemstad), Keith Kraus (@kkraus14), Kyle Nicholson (@kylejn27), Christian Lorentzen (@lorentzenchr), Michael Mayer (@mayer79), Nikolay Petrov (@napetrov), @odidev, PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Qi Zhang (@qzhang90), Sergio Gavil\u00e1n (@sgavil), Scott Lundberg (@slundberg), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vincent Nijs (@vnijs), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n## v1.2.0 (2020.08.22)\n\n### XGBoost4J-Spark now supports the GPU algorithm (#5171)\n* Now XGBoost4J-Spark is able to leverage NVIDIA GPU hardware to speed up training.\n* There is on-going work for accelerating the rest of the data pipeline with NVIDIA GPUs (#5950, #5972).\n\n### XGBoost now supports CUDA 11 (#5808)\n* It is now possible to build XGBoost with CUDA 11. Note that we do not yet distribute pre-built binaries built with CUDA 11; all current distributions use CUDA 10.0.\n\n### Better guidance for persisting XGBoost models in an R environment (#5940, #5964)\n* Users are strongly encouraged to use `xgb.save()` and `xgb.save.raw()` instead of `saveRDS()`. This is so that the persisted models can be accessed with future releases of XGBoost.\n* The previous release (1.1.0) had problems loading models that were saved with `saveRDS()`. This release adds a compatibility layer to restore access to the old RDS files. Note that this is meant to be a temporary measure; users are advised to stop using `saveRDS()` and migrate to `xgb.save()` and `xgb.save.raw()`.\n\n### New objectives and metrics\n* The pseudo-Huber loss `reg:pseudohubererror` is added (#5647). The corresponding metric is `mphe`. Right now, the slope is hard-coded to 1.\n* The Accelerated Failure Time objective for survival analysis (`survival:aft`) is now accelerated on GPUs (#5714, #5716). The survival metrics `aft-nloglik` and `interval-regression-accuracy` are also accelerated on GPUs.\n\n### Improved integration with scikit-learn\n* Added `n_features_in_` attribute to the scikit-learn interface to store the number of features used (#5780). This is useful for integrating with some scikit-learn features such as `StackingClassifier`.  See [this link](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for more details.\n* `XGBoostError` now inherits `ValueError`, which conforms scikit-learn's exception requirement (#5696).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), John Zedlewski (@JohnZed), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Anthony D'Amato (@Totoketchup), @Wittty-Panda, Alexander Gugel (@alexanderGugel), Codecov Comments Bot (@codecov-commenter), Codecov (@codecov-io), DIVYA CHAUHAN (@divya661), Devin Robison (@drobison00), Geoffrey Blake (@geoffreyblake), Mark Harris (@harrism), Philip Hyunsu Cho (@hcho3), Honza Sterba (@honzasterba), Igor Moura (@igormp), @jakirkham, @jameskrach, James Lamb (@jameslamb), Janakarajan Natarajan (@janaknat), Jake Hemstad (@jrhemstad), Keith Kraus (@kkraus14), Kyle Nicholson (@kylejn27), Christian Lorentzen (@lorentzenchr), Michael Mayer (@mayer79), Nikolay Petrov (@napetrov), @odidev, PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Qi Zhang (@qzhang90), Sergio Gavil\u00e1n (@sgavil), Scott Lundberg (@slundberg), Cuong Duong (@tcuongd), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), vcarpani (@vcarpani), Vladislav Epifanov (@vepifanov), Vincent Nijs (@vnijs), Vitalie Spinu (@vspinu), Bobby Wang (@wbo4958), William Hicks (@wphicks)\n\n## v1.2.0 (2020.08.22)\n\n### XGBoost4J-Spark now supports the GPU algorithm (#5171)\n* Now XGBoost4J-Spark is able to leverage NVIDIA GPU hardware to speed up training.\n* There is on-going work for accelerating the rest of the data pipeline with NVIDIA GPUs (#5950, #5972).\n\n### XGBoost now supports CUDA 11 (#5808)\n* It is now possible to build XGBoost with CUDA 11. Note that we do not yet distribute pre-built binaries built with CUDA 11; all current distributions use CUDA 10.0.\n\n### Better guidance for persisting XGBoost models in an R environment (#5940, #5964)\n* Users are strongly encouraged to use `xgb.save()` and `xgb.save.raw()` instead of `saveRDS()`. This is so that the persisted models can be accessed with future releases of XGBoost.\n* The previous release (1.1.0) had problems loading models that were saved with `saveRDS()`. This release adds a compatibility layer to restore access to the old RDS files. Note that this is meant to be a temporary measure; users are advised to stop using `saveRDS()` and migrate to `xgb.save()` and `xgb.save.raw()`.\n\n### New objectives and metrics\n* The pseudo-Huber loss `reg:pseudohubererror` is added (#5647). The corresponding metric is `mphe`. Right now, the slope is hard-coded to 1.\n* The Accelerated Failure Time objective for survival analysis (`survival:aft`) is now accelerated on GPUs (#5714, #5716). The survival metrics `aft-nloglik` and `interval-regression-accuracy` are also accelerated on GPUs.\n\n### Improved integration with scikit-learn\n* Added `n_features_in_` attribute to the scikit-learn interface to store the number of features used (#5780). This is useful for integrating with some scikit-learn features such as `StackingClassifier`.  See [this link](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep010/proposal.html) for more details.\n* `XGBoostError` now inherits `ValueError`, which conforms scikit-learn's exception requirement (#5696)."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ry\u016b algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Improved integration with Dask\n* The XGBoost Dask API now exposes an asynchronous interface (#5862). See [the document](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html#working-with-asyncio) for details.\n* Zero-copy ingestion of GPU arrays via `DaskDeviceQuantileDMatrix` (#5623, #5799, #5800, #5803, #5837, #5874, #5901): Previously, the Dask interface had to make 2 data copies: one for concatenating the Dask partition/block into a single block and another for internal representation. To save memory, we introduce `DaskDeviceQuantileDMatrix`. As long as Dask partitions are resident in the GPU memory, `DaskDeviceQuantileDMatrix` is able to ingest them directly without making copies. This matrix type wraps `DeviceQuantileDMatrix`.\n* The prediction function now returns GPU Series type if the input is from Dask-cuDF (#5710). This is to preserve the input data type.\n\n### Robust handling of external data types (#5689, #5893)\n- As we support more and more external data types, the handling logic has proliferated all over the code base and became hard to keep track. It also became unclear how missing values and threads are handled. We refactored the Python package code to collect all data handling logic to a central location, and now we have an explicit list of of all supported data types.\n\n### Improvements in GPU-side data matrix (`DeviceQuantileDMatrix`)\n* The GPU-side data matrix now implements its own quantile sketching logic, so that data don't have to be transported back to the main memory (#5700, #5747, #5760, #5846, #5870, #5898). The GK sketching algorithm is also now better documented.\n  - Now we can load extremely sparse dataset like URL, although performance is still sub-optimal.\n* The GPU-side data matrix now exposes an iterative interface (#5783), so that users are able to construct a matrix from a data iterator. See the [Python demo](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/data_iterator.py).\n\n### New language binding: Swift (#5728)\n* Visit https://github.com/kongzii/SwiftXGBoost for more details.\n\n### Robust model serialization with JSON (#5772, #5804, #5831, #5857, #5934)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly.\n* JSON model IO is significantly faster and produces smaller model files.\n* Round-trip reproducibility is guaranteed, via the introduction of an efficient float-to-string conversion algorithm known as [the Ry\u016b algorithm](https://dl.acm.org/doi/10.1145/3192366.3192369). The conversion is locale-independent, producing consistent numeric representation regardless of the locale setting of the user's machine.\n* We fixed an issue in loading large JSON files to memory.\n* It is now possible to load a JSON file from a remote source such as S3."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* CPU hist tree method optimization\n  - Skip missing lookup in hist row partitioning if data is dense. (#5644)\n  - Specialize training procedures for CPU hist tree method on distributed environment. (#5557)\n  - Add single point histogram for CPU hist.  Previously gradient histogram for CPU hist is hard coded to be 64 bit, now users can specify the parameter `single_precision_histogram` to use 32 bit histogram instead for faster training performance. (#5624, #5811)\n* GPU hist tree method optimization\n  - Removed some unnecessary synchronizations and better memory allocation pattern. (#5707)\n  - Optimize GPU Hist for wide dataset.  Previously for wide dataset the atomic operation is performed on global memory, now it can run on shared memory for faster histogram building. But there's a known small regression on GeForce cards with dense data. (#5795, #5926, #5948, #5631)\n\n### API additions\n* Support passing fmap to importance plot (#5719). Now importance plot can show actual names of features instead of default ones.\n* Support 64bit seed. (#5643)\n* A new C API `XGBoosterGetNumFeature` is added for getting number of features in booster (#5856).\n* Feature names and feature types are now stored in C++ core and saved in binary DMatrix (#5858).\n\n### Breaking: The `predict()` method of `DaskXGBClassifier` now produces class predictions (#5986). Use `predict_proba()` to obtain probability predictions.\n* Previously, `DaskXGBClassifier.predict()` produced probability predictions. This is inconsistent with the behavior of other scikit-learn classifiers, where `predict()` returns class predictions. We make a breaking change in 1.2.0 release so that `DaskXGBClassifier.predict()` now correctly produces class predictions and thus behave like other scikit-learn classifiers. Furthermore, we introduce the `predict_proba()` method for obtaining probability predictions, again to be in line with other scikit-learn classifiers.\n\n### Breaking: Custom evaluation metric now receives raw prediction (#5954)\n* Previously, the custom evaluation metric received a transformed prediction result when used with a classifier. Now the custom metric will receive a raw (untransformed) prediction and will need to transform the prediction itself.  See [demo/guide-python/custom\\_softmax.py](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/custom_softmax.py) for an example.\n* This change is to make the custom metric behave consistently with the custom objective, which already receives raw prediction (#5564).\n\n### Breaking: XGBoost4J-Spark now requires Spark 3.0 and Scala 2.12 (#5836, #5890)\n* Starting with version 3.0, Spark can manage GPU resources and allocate them among executors.\n* Spark 3.0 dropped support for Scala 2.11 and now only supports Scala 2.12. Thus, XGBoost4J-Spark also only supports Scala 2.12.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* CPU hist tree method optimization\n  - Skip missing lookup in hist row partitioning if data is dense. (#5644)\n  - Specialize training procedures for CPU hist tree method on distributed environment. (#5557)\n  - Add single point histogram for CPU hist.  Previously gradient histogram for CPU hist is hard coded to be 64 bit, now users can specify the parameter `single_precision_histogram` to use 32 bit histogram instead for faster training performance. (#5624, #5811)\n* GPU hist tree method optimization\n  - Removed some unnecessary synchronizations and better memory allocation pattern. (#5707)\n  - Optimize GPU Hist for wide dataset.  Previously for wide dataset the atomic operation is performed on global memory, now it can run on shared memory for faster histogram building. But there's a known small regression on GeForce cards with dense data. (#5795, #5926, #5948, #5631)\n\n### API additions\n* Support passing fmap to importance plot (#5719). Now importance plot can show actual names of features instead of default ones.\n* Support 64bit seed. (#5643)\n* A new C API `XGBoosterGetNumFeature` is added for getting number of features in booster (#5856).\n* Feature names and feature types are now stored in C++ core and saved in binary DMatrix (#5858).\n\n### Breaking: The `predict()` method of `DaskXGBClassifier` now produces class predictions (#5986). Use `predict_proba()` to obtain probability predictions.\n* Previously, `DaskXGBClassifier.predict()` produced probability predictions. This is inconsistent with the behavior of other scikit-learn classifiers, where `predict()` returns class predictions. We make a breaking change in 1.2.0 release so that `DaskXGBClassifier.predict()` now correctly produces class predictions and thus behave like other scikit-learn classifiers. Furthermore, we introduce the `predict_proba()` method for obtaining probability predictions, again to be in line with other scikit-learn classifiers.\n\n### Breaking: Custom evaluation metric now receives raw prediction (#5954)\n* Previously, the custom evaluation metric received a transformed prediction result when used with a classifier. Now the custom metric will receive a raw (untransformed) prediction and will need to transform the prediction itself.  See [demo/guide-python/custom\\_softmax.py](https://github.com/dmlc/xgboost/blob/release_1.2.0/demo/guide-python/custom_softmax.py) for an example.\n* This change is to make the custom metric behave consistently with the custom objective, which already receives raw prediction (#5564).\n\n### Breaking: XGBoost4J-Spark now requires Spark 3.0 and Scala 2.12 (#5836, #5890)\n* Starting with version 3.0, Spark can manage GPU resources and allocate them among executors.\n* Spark 3.0 dropped support for Scala 2.11 and now only supports Scala 2.12. Thus, XGBoost4J-Spark also only supports Scala 2.12."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Breaking: XGBoost Python package now requires Python 3.6 and later (#5715)\n* Python 3.6 has many useful features such as f-strings.\n\n### Breaking: XGBoost now adopts the C++14 standard (#5664)\n* Make sure to use a sufficiently modern C++ compiler that supports C++14, such as Visual Studio 2017, GCC 5.0+, and Clang 3.4+.\n\n### Bug-fixes\n* Fix a data race in the prediction function (#5853). As a byproduct, the prediction function now uses a thread-local data store and became thread-safe.\n* Restore capability to run prediction when the test input has fewer features than the training data (#5955). This capability is necessary to support predicting with LIBSVM inputs. The previous release (1.1) had broken this capability, so we restore it in this version with better tests.\n* Fix OpenMP build with CMake for R package, to support CMake 3.13 (#5895).\n* Fix Windows 2016 build (#5902, #5918).\n* Fix edge cases in scikit-learn interface with Pandas input by disabling feature validation. (#5953)\n* [R] Enable weighted learning to rank (#5945)\n* [R] Fix early stopping with custom objective (#5923)\n* Fix NDK Build (#5886)\n* Add missing explicit template specializations for greater portability (#5921)\n* Handle empty rows in data iterators correctly (#5929). This bug affects file loader and JVM data frames.\n* Fix `IsDense` (#5702)\n* [jvm-packages] Fix wrong method name `setAllowZeroForMissingValue` (#5740)\n* Fix shape inference for Dask predict (#5989)\n\n### Usability Improvements, Documentation\n* [Doc] Document that CUDA 10.0 is required (#5872)\n* Refactored command line interface (CLI). Now CLI is able to handle user errors and output basic document. (#5574)\n* Better error handling in Python: use `raise from` syntax to preserve full stacktrace (#5787).\n* The JSON model dump now has a formal schema (#5660, #5818). The benefit is to prevent `dump_model()` function from breaking. See [this document](https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html#difference-between-saving-model-and-dumping-model) to understand the difference between saving and dumping models.\n* Add a reference to the GPU external memory paper (#5684)\n* Document more objective parameters in the R package (#5682)\n* Document the existence of pre-built binary wheels for MacOS (#5711)\n* Remove `max.depth` in the R gblinear example. (#5753)\n* Added conda environment file for building docs (#5773)\n* Mention dask blog post in the doc, which introduces using Dask with GPU and some internal workings. (#5789)\n* Fix rendering of Markdown docs (#5821)\n* Document new objectives and metrics available on GPUs (#5909)\n* Better message when no GPU is found. (#5594)\n* Remove the use of `silent` parameter from R demos. (#5675)\n* Don't use masked array in array interface. (#5730)\n* Update affiliation of @terrytangyuan: Ant Financial -> Ant Group (#5827)\n* Move dask tutorial closer other distributed tutorials (#5613)\n* Update XGBoost + Dask overview documentation (#5961)\n* Show `n_estimators` in the docstring of the scikit-learn interface (#6041)\n* Fix a type in a doctring of the scikit-learn interface (#5980)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Breaking: XGBoost Python package now requires Python 3.6 and later (#5715)\n* Python 3.6 has many useful features such as f-strings.\n\n### Breaking: XGBoost now adopts the C++14 standard (#5664)\n* Make sure to use a sufficiently modern C++ compiler that supports C++14, such as Visual Studio 2017, GCC 5.0+, and Clang 3.4+.\n\n### Bug-fixes\n* Fix a data race in the prediction function (#5853). As a byproduct, the prediction function now uses a thread-local data store and became thread-safe.\n* Restore capability to run prediction when the test input has fewer features than the training data (#5955). This capability is necessary to support predicting with LIBSVM inputs. The previous release (1.1) had broken this capability, so we restore it in this version with better tests.\n* Fix OpenMP build with CMake for R package, to support CMake 3.13 (#5895).\n* Fix Windows 2016 build (#5902, #5918).\n* Fix edge cases in scikit-learn interface with Pandas input by disabling feature validation. (#5953)\n* [R] Enable weighted learning to rank (#5945)\n* [R] Fix early stopping with custom objective (#5923)\n* Fix NDK Build (#5886)\n* Add missing explicit template specializations for greater portability (#5921)\n* Handle empty rows in data iterators correctly (#5929). This bug affects file loader and JVM data frames.\n* Fix `IsDense` (#5702)\n* [jvm-packages] Fix wrong method name `setAllowZeroForMissingValue` (#5740)\n* Fix shape inference for Dask predict (#5989)\n\n### Usability Improvements, Documentation\n* [Doc] Document that CUDA 10.0 is required (#5872)\n* Refactored command line interface (CLI). Now CLI is able to handle user errors and output basic document. (#5574)\n* Better error handling in Python: use `raise from` syntax to preserve full stacktrace (#5787).\n* The JSON model dump now has a formal schema (#5660, #5818). The benefit is to prevent `dump_model()` function from breaking. See [this document](https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html#difference-between-saving-model-and-dumping-model) to understand the difference between saving and dumping models.\n* Add a reference to the GPU external memory paper (#5684)\n* Document more objective parameters in the R package (#5682)\n* Document the existence of pre-built binary wheels for MacOS (#5711)\n* Remove `max.depth` in the R gblinear example. (#5753)\n* Added conda environment file for building docs (#5773)\n* Mention dask blog post in the doc, which introduces using Dask with GPU and some internal workings. (#5789)\n* Fix rendering of Markdown docs (#5821)\n* Document new objectives and metrics available on GPUs (#5909)\n* Better message when no GPU is found. (#5594)\n* Remove the use of `silent` parameter from R demos. (#5675)\n* Don't use masked array in array interface. (#5730)\n* Update affiliation of @terrytangyuan: Ant Financial -> Ant Group (#5827)\n* Move dask tutorial closer other distributed tutorials (#5613)\n* Update XGBoost + Dask overview documentation (#5961)\n* Show `n_estimators` in the docstring of the scikit-learn interface (#6041)\n* Fix a type in a doctring of the scikit-learn interface (#5980)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance: testing, continuous integration, build system\n* [CI] Remove CUDA 9.0 from CI (#5674, #5745)\n* Require CUDA 10.0+ in CMake build (#5718)\n* [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764). This enables building XGBoost with GPU support with R 4.x.\n* [R-package] Reduce duplication in configure.ac (#5693)\n* Bump com.esotericsoftware to 4.0.2 (#5690)\n* Migrate some tests from AppVeyor to GitHub Actions to speed up the tests. (#5911, #5917, #5919, #5922, #5928)\n* Reduce cost of the Jenkins CI server (#5884, #5904, #5892). We now enforce a daily budget via an automated monitor. We also dramatically reduced the workload for the Windows platform, since the cloud VM cost is vastly greater for Windows.\n* [R] Set up automated R linter (#5944)\n* [R] replace uses of T and F with TRUE and FALSE (#5778)\n* Update Docker container 'CPU' (#5956)\n* Simplify CMake build with modern CMake techniques (#5871)\n* Use `hypothesis` package for testing (#5759, #5835, #5849).\n* Define `_CRT_SECURE_NO_WARNINGS` to remove unneeded warnings in MSVC (#5434)\n* Run all Python demos in CI, to ensure that they don't break (#5651)\n* Enhance nvtx support (#5636). Now we can use unified timer between CPU and GPU. Also CMake is able to find nvtx automatically.\n* Speed up python test. (#5752)\n* Add helper for generating batches of data. (#5756)\n* Add c-api-demo to .gitignore (#5855)\n* Add option to enable all compiler warnings in GCC/Clang (#5897)\n* Make Python model compatibility test runnable locally (#5941)\n* Add cupy to Windows CI (#5797)\n* [CI] Fix cuDF install; merge 'gpu' and 'cudf' test suite (#5814)\n* Update rabit submodule (#5680, #5876)\n* Force colored output for Ninja build. (#5959)\n* [CI] Assign larger /dev/shm to NCCL (#5966)\n* Add missing Pytest marks to AsyncIO unit test (#5968)\n* [CI] Use latest cuDF and dask-cudf (#6048)\n* Add CMake flag to log C API invocations, to aid debugging (#5925)\n* Fix a unit test on CLI, to handle RC versions (#6050)\n* [CI] Use mgpu machine to run gpu hist unit tests (#6050)\n* [CI] Build GPU-enabled JAR artifact and deploy to xgboost-maven-repo (#6050)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Remove dead code in DMatrix initialization. (#5635)\n* Catch dmlc error by ref. (#5678)\n* Refactor the `gpu_hist` split evaluation in preparation for batched nodes enumeration. (#5610)\n* Remove column major specialization. (#5755)\n* Remove unused imports in Python (#5776)\n* Avoid including `c_api.h` in header files. (#5782)\n* Remove unweighted GK quantile, which is unused. (#5816)\n* Add Python binding for rabit ops. (#5743)\n* Implement `Empty` method for host device vector. (#5781)\n* Remove print (#5867)\n* Enforce tree order in JSON (#5974)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @LionOrCatThatIsTheQuestion, Dmitry Mottl (@Mottl), Rory Mitchell (@RAMitchell), @ShvetsKS, Alex Wozniakowski (@a-wozniakowski), Alexander Gugel (@alexanderGugel), @anttisaukko, @boxdot, Andy Adinets (@canonizer), Ram Rachum (@cool-RR), Elliot Hershberg (@elliothershberg), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), @jameskrach, James Lamb (@jameslamb), James Bourbeau (@jrbourbeau), Peter Jung (@kongzii), Lorenz Walthert (@lorenzwalthert), Oleksandr Kuvshynov (@okuvshynov), Rong Ou (@rongou), Shaochen Shi (@shishaochen), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance: testing, continuous integration, build system\n* [CI] Remove CUDA 9.0 from CI (#5674, #5745)\n* Require CUDA 10.0+ in CMake build (#5718)\n* [R] Remove dependency on gendef for Visual Studio builds (fixes #5608) (#5764). This enables building XGBoost with GPU support with R 4.x.\n* [R-package] Reduce duplication in configure.ac (#5693)\n* Bump com.esotericsoftware to 4.0.2 (#5690)\n* Migrate some tests from AppVeyor to GitHub Actions to speed up the tests. (#5911, #5917, #5919, #5922, #5928)\n* Reduce cost of the Jenkins CI server (#5884, #5904, #5892). We now enforce a daily budget via an automated monitor. We also dramatically reduced the workload for the Windows platform, since the cloud VM cost is vastly greater for Windows.\n* [R] Set up automated R linter (#5944)\n* [R] replace uses of T and F with TRUE and FALSE (#5778)\n* Update Docker container 'CPU' (#5956)\n* Simplify CMake build with modern CMake techniques (#5871)\n* Use `hypothesis` package for testing (#5759, #5835, #5849).\n* Define `_CRT_SECURE_NO_WARNINGS` to remove unneeded warnings in MSVC (#5434)\n* Run all Python demos in CI, to ensure that they don't break (#5651)\n* Enhance nvtx support (#5636). Now we can use unified timer between CPU and GPU. Also CMake is able to find nvtx automatically.\n* Speed up python test. (#5752)\n* Add helper for generating batches of data. (#5756)\n* Add c-api-demo to .gitignore (#5855)\n* Add option to enable all compiler warnings in GCC/Clang (#5897)\n* Make Python model compatibility test runnable locally (#5941)\n* Add cupy to Windows CI (#5797)\n* [CI] Fix cuDF install; merge 'gpu' and 'cudf' test suite (#5814)\n* Update rabit submodule (#5680, #5876)\n* Force colored output for Ninja build. (#5959)\n* [CI] Assign larger /dev/shm to NCCL (#5966)\n* Add missing Pytest marks to AsyncIO unit test (#5968)\n* [CI] Use latest cuDF and dask-cudf (#6048)\n* Add CMake flag to log C API invocations, to aid debugging (#5925)\n* Fix a unit test on CLI, to handle RC versions (#6050)\n* [CI] Use mgpu machine to run gpu hist unit tests (#6050)\n* [CI] Build GPU-enabled JAR artifact and deploy to xgboost-maven-repo (#6050)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Remove dead code in DMatrix initialization. (#5635)\n* Catch dmlc error by ref. (#5678)\n* Refactor the `gpu_hist` split evaluation in preparation for batched nodes enumeration. (#5610)\n* Remove column major specialization. (#5755)\n* Remove unused imports in Python (#5776)\n* Avoid including `c_api.h` in header files. (#5782)\n* Remove unweighted GK quantile, which is unused. (#5816)\n* Add Python binding for rabit ops. (#5743)\n* Implement `Empty` method for host device vector. (#5781)\n* Remove print (#5867)\n* Enforce tree order in JSON (#5974)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), @LionOrCatThatIsTheQuestion, Dmitry Mottl (@Mottl), Rory Mitchell (@RAMitchell), @ShvetsKS, Alex Wozniakowski (@a-wozniakowski), Alexander Gugel (@alexanderGugel), @anttisaukko, @boxdot, Andy Adinets (@canonizer), Ram Rachum (@cool-RR), Elliot Hershberg (@elliothershberg), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), @jameskrach, James Lamb (@jameslamb), James Bourbeau (@jrbourbeau), Peter Jung (@kongzii), Lorenz Walthert (@lorenzwalthert), Oleksandr Kuvshynov (@okuvshynov), Rong Ou (@rongou), Shaochen Shi (@shishaochen), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LionOrCatThatIsTheQuestion, Hao Yang (@QuantHao), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Alex Wozniakowski (@a-wozniakowski), Amit Kumar (@aktech), Avinash Barnwal (@avinashbarnwal), @boxdot, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Ram Rachum (@cool-RR), Cristiano Goncalves (@cristianogoncalves), Elliot Hershberg (@elliothershberg), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), James Lamb (@jameslamb), James Bourbeau (@jrbourbeau), Lee Drake (@leedrake5), DougM (@mengdong), Oleksandr Kuvshynov (@okuvshynov), RongOu (@rongou), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), Yuan Tang (@terrytangyuan), Theodore Vasiloudis (@thvasilo), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10)\n\n## v1.1.1 (2020.06.06)\nThis patch release applies the following patches to 1.1.0 release:\n\n* CPU performance improvement in the PyPI wheels (#5720)\n* Fix loading old model (#5724)\n* Install pkg-config file (#5744)\n\n## v1.1.0 (2020.05.17)\n\n### Better performance on multi-core CPUs (#5244, #5334, #5522)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #5244 concludes the ongoing effort to improve performance scaling on multi-CPUs, in particular Intel CPUs. Roadmap: #5104\n* #5334 makes steps toward reducing memory consumption for the `hist` tree method on CPU.\n* #5522 optimizes random number generation for data sampling.\n\n### Deterministic GPU algorithm for regression and classification (#5361)\n* GPU algorithm for regression and classification tasks is now deterministic.\n* Roadmap: #5023. Currently only single-GPU training is deterministic. Distributed training with multiple GPUs is not yet deterministic.\n\n### Improve external memory support on GPUs (#5093, #5365)\n* Starting from 1.0.0 release, we added support for external memory on GPUs to enable training with larger datasets. Gradient-based sampling (#5093) speeds up the external memory algorithm by intelligently sampling a subset of the training data to copy into the GPU memory. [Learn more about out-of-core GPU gradient boosting.](https://arxiv.org/abs/2005.09148)\n* GPU-side data sketching now works with data from external memory (#5365).\n\n### Parameter validation: detection of unused or incorrect parameters (#5477, #5569, #5508)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. The 1.1.0 release makes parameter validation available to the scikit-learn interface (#5477) and the R binding (#5569).\n\n### Thread-safe, in-place prediction method (#5389, #5512)\n* Previously, the prediction method was not thread-safe (#5339). This release adds a new API function `inplace_predict()` that is thread-safe. It is now possible to serve concurrent requests for prediction using a shared model object.\n* It is now possible to compute prediction in-place for selected data formats (`numpy.ndarray` / `scipy.sparse.csr_matrix` / `cupy.ndarray` / `cudf.DataFrame` / `pd.DataFrame`) without creating a `DMatrix` object.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LionOrCatThatIsTheQuestion, Hao Yang (@QuantHao), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Alex Wozniakowski (@a-wozniakowski), Amit Kumar (@aktech), Avinash Barnwal (@avinashbarnwal), @boxdot, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Ram Rachum (@cool-RR), Cristiano Goncalves (@cristianogoncalves), Elliot Hershberg (@elliothershberg), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), James Lamb (@jameslamb), James Bourbeau (@jrbourbeau), Lee Drake (@leedrake5), DougM (@mengdong), Oleksandr Kuvshynov (@okuvshynov), RongOu (@rongou), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), Yuan Tang (@terrytangyuan), Theodore Vasiloudis (@thvasilo), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10)\n\n## v1.1.1 (2020.06.06)\nThis patch release applies the following patches to 1.1.0 release:\n\n* CPU performance improvement in the PyPI wheels (#5720)\n* Fix loading old model (#5724)\n* Install pkg-config file (#5744)\n\n## v1.1.0 (2020.05.17)\n\n### Better performance on multi-core CPUs (#5244, #5334, #5522)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #5244 concludes the ongoing effort to improve performance scaling on multi-CPUs, in particular Intel CPUs. Roadmap: #5104\n* #5334 makes steps toward reducing memory consumption for the `hist` tree method on CPU.\n* #5522 optimizes random number generation for data sampling.\n\n### Deterministic GPU algorithm for regression and classification (#5361)\n* GPU algorithm for regression and classification tasks is now deterministic.\n* Roadmap: #5023. Currently only single-GPU training is deterministic. Distributed training with multiple GPUs is not yet deterministic.\n\n### Improve external memory support on GPUs (#5093, #5365)\n* Starting from 1.0.0 release, we added support for external memory on GPUs to enable training with larger datasets. Gradient-based sampling (#5093) speeds up the external memory algorithm by intelligently sampling a subset of the training data to copy into the GPU memory. [Learn more about out-of-core GPU gradient boosting.](https://arxiv.org/abs/2005.09148)\n* GPU-side data sketching now works with data from external memory (#5365).\n\n### Parameter validation: detection of unused or incorrect parameters (#5477, #5569, #5508)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. The 1.1.0 release makes parameter validation available to the scikit-learn interface (#5477) and the R binding (#5569).\n\n### Thread-safe, in-place prediction method (#5389, #5512)\n* Previously, the prediction method was not thread-safe (#5339). This release adds a new API function `inplace_predict()` that is thread-safe. It is now possible to serve concurrent requests for prediction using a shared model object.\n* It is now possible to compute prediction in-place for selected data formats (`numpy.ndarray` / `scipy.sparse.csr_matrix` / `cupy.ndarray` / `cudf.DataFrame` / `pd.DataFrame`) without creating a `DMatrix` object."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Addition of Accelerated Failure Time objective for survival analysis (#4763, #5473, #5486, #5552, #5553)\n* Survival analysis (regression) models the time it takes for an event of interest to occur. The target label is potentially censored, i.e. the label is a range rather than a single number. We added a new objective `survival:aft` to support survival analysis. Also added is the new API to specify the ranged labels. Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.1.0/tutorials/aft_survival_analysis.html) and the [demos](https://github.com/dmlc/xgboost/tree/release_1.1.0/demo/aft_survival).\n* GPU support is work in progress (#5714).\n\n### Improved installation experience on Mac OSX (#5597, #5602, #5606, #5701)\n* It only takes two commands to install the XGBoost Python package: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores. Even better, starting with this release, we distribute pre-compiled binary wheels targeting Mac OSX. Now the install command `pip install xgboost` finishes instantly, as it no longer compiles the C++ source of XGBoost. The last three Mac versions (High Sierra, Mojave, Catalina) are supported.\n* R package: the 1.1.0 release fixes the error `Initializing libomp.dylib, but found libomp.dylib already initialized` (#5701)\n\n### Ranking metrics are now accelerated on GPUs (#5380, #5387, #5398)\n\n### GPU-side data matrix to ingest data directly from other GPU libraries (#5420, #5465)\n* Previously, data on GPU memory had to be copied back to the main memory before it could be used by XGBoost. Starting with 1.1.0 release, XGBoost provides a dedicated interface (`DeviceQuantileDMatrix`) so that it can ingest data from GPU memory directly. The result is that XGBoost interoperates better with GPU-accelerated data science libraries, such as cuDF, cuPy, and PyTorch.\n* Set device in device dmatrix. (#5596)\n\n### Robust model serialization with JSON (#5123, #5217)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly. Refer to the release note for 1.0.0 to learn more.\n* It is now possible to store internal configuration of the trained model (`Booster`) object in R as a JSON string (#5123, #5217).\n\n### Improved integration with Dask\n* Pass through `verbose` parameter for dask fit (#5413)\n* Use `DMLC_TASK_ID`. (#5415)\n* Order the prediction result. (#5416)\n* Honor `nthreads` from dask worker. (#5414)\n* Enable grid searching with scikit-learn. (#5417)\n* Check non-equal when setting threads. (#5421)\n* Accept other inputs for prediction. (#5428)\n* Fix missing value for scikit-learn interface. (#5435)\n\n### XGBoost4J-Spark: Check number of columns in the data iterator (#5202, #5303)\n* Before, the native layer in XGBoost did not know the number of columns (features) ahead of time and had to guess the number of columns by counting the feature index when ingesting data. This method has a failure more in distributed setting: if the training data is highly sparse, some features may be completely missing in one or more worker partitions. Thus, one or more workers may deduce an incorrect data shape, leading to crashes or silently wrong models.\n* Enforce correct data shape by passing the number of columns explicitly from the JVM layer into the native layer.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Addition of Accelerated Failure Time objective for survival analysis (#4763, #5473, #5486, #5552, #5553)\n* Survival analysis (regression) models the time it takes for an event of interest to occur. The target label is potentially censored, i.e. the label is a range rather than a single number. We added a new objective `survival:aft` to support survival analysis. Also added is the new API to specify the ranged labels. Check out [the tutorial](https://xgboost.readthedocs.io/en/release_1.1.0/tutorials/aft_survival_analysis.html) and the [demos](https://github.com/dmlc/xgboost/tree/release_1.1.0/demo/aft_survival).\n* GPU support is work in progress (#5714).\n\n### Improved installation experience on Mac OSX (#5597, #5602, #5606, #5701)\n* It only takes two commands to install the XGBoost Python package: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores. Even better, starting with this release, we distribute pre-compiled binary wheels targeting Mac OSX. Now the install command `pip install xgboost` finishes instantly, as it no longer compiles the C++ source of XGBoost. The last three Mac versions (High Sierra, Mojave, Catalina) are supported.\n* R package: the 1.1.0 release fixes the error `Initializing libomp.dylib, but found libomp.dylib already initialized` (#5701)\n\n### Ranking metrics are now accelerated on GPUs (#5380, #5387, #5398)\n\n### GPU-side data matrix to ingest data directly from other GPU libraries (#5420, #5465)\n* Previously, data on GPU memory had to be copied back to the main memory before it could be used by XGBoost. Starting with 1.1.0 release, XGBoost provides a dedicated interface (`DeviceQuantileDMatrix`) so that it can ingest data from GPU memory directly. The result is that XGBoost interoperates better with GPU-accelerated data science libraries, such as cuDF, cuPy, and PyTorch.\n* Set device in device dmatrix. (#5596)\n\n### Robust model serialization with JSON (#5123, #5217)\n* We continue efforts from the 1.0.0 release to adopt JSON as the format to save and load models robustly. Refer to the release note for 1.0.0 to learn more.\n* It is now possible to store internal configuration of the trained model (`Booster`) object in R as a JSON string (#5123, #5217).\n\n### Improved integration with Dask\n* Pass through `verbose` parameter for dask fit (#5413)\n* Use `DMLC_TASK_ID`. (#5415)\n* Order the prediction result. (#5416)\n* Honor `nthreads` from dask worker. (#5414)\n* Enable grid searching with scikit-learn. (#5417)\n* Check non-equal when setting threads. (#5421)\n* Accept other inputs for prediction. (#5428)\n* Fix missing value for scikit-learn interface. (#5435)\n\n### XGBoost4J-Spark: Check number of columns in the data iterator (#5202, #5303)\n* Before, the native layer in XGBoost did not know the number of columns (features) ahead of time and had to guess the number of columns by counting the feature index when ingesting data. This method has a failure more in distributed setting: if the training data is highly sparse, some features may be completely missing in one or more worker partitions. Thus, one or more workers may deduce an incorrect data shape, leading to crashes or silently wrong models.\n* Enforce correct data shape by passing the number of columns explicitly from the JVM layer into the native layer."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major refactoring of the `DMatrix` class\n* Continued from 1.0.0 release.\n* Remove update prediction cache from predictors. (#5312)\n* Predict on Ellpack. (#5327)\n* Partial rewrite EllpackPage (#5352)\n* Use ellpack for prediction only when sparsepage doesn't exist. (#5504)\n* RFC: #4354, Roadmap: #5143\n\n### Breaking: XGBoost Python package now requires Pip 19.0 and higher (#5589)\n* Your Linux machine may have an old version of Pip and may attempt to install a source package, leading to long installation time. This is because we are now using `manylinux2010` tag in the binary wheel release. Ensure you have Pip 19.0 or newer by running `python3 -m pip -V` to check the version. Upgrade Pip with command\n```\npython3 -m pip install --upgrade pip\n```\nUpgrading to latest pip allows us to depend on newer versions of system libraries. [TensorFlow](https://www.tensorflow.org/install/pip) also requires Pip 19.0+.\n\n### Breaking: GPU algorithm now requires CUDA 10.0 and higher (#5649)\n* CUDA 10.0 is necessary to make the GPU algorithm deterministic (#5361).\n\n### Breaking: `silent` parameter is now removed (#5476)\n* Please use `verbosity` instead.\n\n### Breaking: Set `output_margin` to True for custom objectives (#5564)\n* Now both R and Python interface custom objectives get un-transformed (raw) prediction outputs.\n\n### Breaking: `Makefile` is now removed. We use CMake exclusively to build XGBoost (#5513)\n* Exception: the R package uses Autotools, as the CRAN ecosystem did not yet adopt CMake widely.\n\n### Breaking: `distcol` updater is now removed (#5507)\n* The `distcol` updater has been long broken, and currently we lack resources to implement a working implementation from scratch.\n\n### Deprecation notices\n* **Python 3.5**. This release is the last release to support Python 3.5. The following release (1.2.0) will require Python 3.6.\n* **Scala 2.11**. Currently XGBoost4J supports Scala 2.11. However, if a future release of XGBoost adopts Spark 3, it will not support Scala 2.11, as Spark 3 requires Scala 2.12+. We do not yet know which XGBoost release will adopt Spark 3.\n\n### Known limitations\n* (Python package) When early stopping is activated with `early_stopping_rounds` at training time, the prediction method (`xgb.predict()`) behaves in a surprising way. If XGBoost runs for M rounds and chooses iteration N (N < M) as the best iteration, then the prediction method will use M trees by default. To use the best iteration (N trees), users will need to manually take the best iteration field `bst.best_iteration` and pass it as the `ntree_limit` argument to `xgb.predict()`. See #5209 and #4052 for additional context.\n* GPU ranking objective is currently not deterministic (#5561).\n* When training parameter `reg_lambda` is set to zero, some leaf nodes may be assigned a NaN value. (See [discussion](https://discuss.xgboost.ai/t/still-getting-unexplained-nans-new-replication-code/1383/9).) For now, please set `reg_lambda` to a nonzero value.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major refactoring of the `DMatrix` class\n* Continued from 1.0.0 release.\n* Remove update prediction cache from predictors. (#5312)\n* Predict on Ellpack. (#5327)\n* Partial rewrite EllpackPage (#5352)\n* Use ellpack for prediction only when sparsepage doesn't exist. (#5504)\n* RFC: #4354, Roadmap: #5143\n\n### Breaking: XGBoost Python package now requires Pip 19.0 and higher (#5589)\n* Your Linux machine may have an old version of Pip and may attempt to install a source package, leading to long installation time. This is because we are now using `manylinux2010` tag in the binary wheel release. Ensure you have Pip 19.0 or newer by running `python3 -m pip -V` to check the version. Upgrade Pip with command\n```\npython3 -m pip install --upgrade pip\n```\nUpgrading to latest pip allows us to depend on newer versions of system libraries. [TensorFlow](https://www.tensorflow.org/install/pip) also requires Pip 19.0+.\n\n### Breaking: GPU algorithm now requires CUDA 10.0 and higher (#5649)\n* CUDA 10.0 is necessary to make the GPU algorithm deterministic (#5361).\n\n### Breaking: `silent` parameter is now removed (#5476)\n* Please use `verbosity` instead.\n\n### Breaking: Set `output_margin` to True for custom objectives (#5564)\n* Now both R and Python interface custom objectives get un-transformed (raw) prediction outputs.\n\n### Breaking: `Makefile` is now removed. We use CMake exclusively to build XGBoost (#5513)\n* Exception: the R package uses Autotools, as the CRAN ecosystem did not yet adopt CMake widely.\n\n### Breaking: `distcol` updater is now removed (#5507)\n* The `distcol` updater has been long broken, and currently we lack resources to implement a working implementation from scratch.\n\n### Deprecation notices\n* **Python 3.5**. This release is the last release to support Python 3.5. The following release (1.2.0) will require Python 3.6.\n* **Scala 2.11**. Currently XGBoost4J supports Scala 2.11. However, if a future release of XGBoost adopts Spark 3, it will not support Scala 2.11, as Spark 3 requires Scala 2.12+. We do not yet know which XGBoost release will adopt Spark 3.\n\n### Known limitations\n* (Python package) When early stopping is activated with `early_stopping_rounds` at training time, the prediction method (`xgb.predict()`) behaves in a surprising way. If XGBoost runs for M rounds and chooses iteration N (N < M) as the best iteration, then the prediction method will use M trees by default. To use the best iteration (N trees), users will need to manually take the best iteration field `bst.best_iteration` and pass it as the `ntree_limit` argument to `xgb.predict()`. See #5209 and #4052 for additional context.\n* GPU ranking objective is currently not deterministic (#5561).\n* When training parameter `reg_lambda` is set to zero, some leaf nodes may be assigned a NaN value. (See [discussion](https://discuss.xgboost.ai/t/still-getting-unexplained-nans-new-replication-code/1383/9).) For now, please set `reg_lambda` to a nonzero value."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Community and Governance\n* The XGBoost Project Management Committee (PMC) is pleased to announce a new committer: Egor Smirnov (@SmirnovEgorRu). He has led a major initiative to improve the performance of XGBoost on multi-core CPUs.\n\n### Bug-fixes\n* Improved compatibility with scikit-learn (#5255, #5505, #5538)\n* Remove f-string, since it's not supported by Python 3.5 (#5330). Note that Python 3.5 support is deprecated and schedule to be dropped in the upcoming release (1.2.0).\n* Fix the pruner so that it doesn't prune the same branch twice (#5335)\n* Enforce only major version in JSON model schema (#5336). Any major revision of the model schema would bump up the major version.\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from a memory buffer (#5360)\n* Define lazy isinstance for Python compat (#5364)\n* [R] fixed uses of `class()` (#5426)\n* Force compressed buffer to be 4 bytes aligned, to keep cuda-memcheck happy (#5441)\n* Remove warning for calling host function (`std::max`) on a GPU device (#5453)\n* Fix uninitialized value bug in xgboost callback (#5463)\n* Fix model dump in CLI (#5485)\n* Fix out-of-bound array access in `WQSummary::SetPrune()` (#5493)\n* Ensure that configured `dmlc/build_config.h` is picked up by Rabit and XGBoost, to fix build on Alpine (#5514)\n* Fix a misspelled method, made in a git merge (#5509)\n* Fix a bug in binary model serialization (#5532)\n* Fix CLI model IO (#5535)\n* Don't use `uint` for threads (#5542)\n* Fix R interaction constraints to handle more than 100000 features (#5543)\n* [jvm-packages] XGBoost Spark should deal with NaN when parsing evaluation output (#5546)\n* GPU-side data sketching is now aware of query groups in learning-to-rank data (#5551)\n* Fix DMatrix slicing for newly added fields (#5552)\n* Fix configuration status with loading binary model (#5562)\n* Fix build when OpenMP is disabled (#5566)\n* R compatibility patches (#5577, #5600)\n* gpu\\_hist performance fixes (#5558)\n* Don't set seed on CLI interface (#5563)\n* [R] When serializing model, preserve model attributes related to early stopping (#5573)\n* Avoid rabit calls in learner configuration (#5581)\n* Hide C++ symbols in libxgboost.so when building Python wheel (#5590). This fixes apache/incubator-tvm#4953.\n* Fix compilation on Mac OSX High Sierra (10.13) (#5597)\n* Fix build on big endian CPUs (#5617)\n* Resolve crash due to use of `vector<bool>::iterator` (#5642)\n* Validation JSON model dump using JSON schema (#5660)\n\n### Performance improvements\n* Wide dataset quantile performance improvement (#5306)\n* Reduce memory usage of GPU-side data sketching (#5407)\n* Reduce span check overhead (#5464)\n* Serialise booster after training to free up GPU memory (#5484)\n* Use the maximum amount of GPU shared memory available to speed up the histogram kernel (#5491)\n* Use non-synchronising scan in Thrust (#5560)\n* Use `cudaDeviceGetAttribute()` instead of `cudaGetDeviceProperties()` for speed (#5570)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Community and Governance\n* The XGBoost Project Management Committee (PMC) is pleased to announce a new committer: Egor Smirnov (@SmirnovEgorRu). He has led a major initiative to improve the performance of XGBoost on multi-core CPUs.\n\n### Bug-fixes\n* Improved compatibility with scikit-learn (#5255, #5505, #5538)\n* Remove f-string, since it's not supported by Python 3.5 (#5330). Note that Python 3.5 support is deprecated and schedule to be dropped in the upcoming release (1.2.0).\n* Fix the pruner so that it doesn't prune the same branch twice (#5335)\n* Enforce only major version in JSON model schema (#5336). Any major revision of the model schema would bump up the major version.\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from a memory buffer (#5360)\n* Define lazy isinstance for Python compat (#5364)\n* [R] fixed uses of `class()` (#5426)\n* Force compressed buffer to be 4 bytes aligned, to keep cuda-memcheck happy (#5441)\n* Remove warning for calling host function (`std::max`) on a GPU device (#5453)\n* Fix uninitialized value bug in xgboost callback (#5463)\n* Fix model dump in CLI (#5485)\n* Fix out-of-bound array access in `WQSummary::SetPrune()` (#5493)\n* Ensure that configured `dmlc/build_config.h` is picked up by Rabit and XGBoost, to fix build on Alpine (#5514)\n* Fix a misspelled method, made in a git merge (#5509)\n* Fix a bug in binary model serialization (#5532)\n* Fix CLI model IO (#5535)\n* Don't use `uint` for threads (#5542)\n* Fix R interaction constraints to handle more than 100000 features (#5543)\n* [jvm-packages] XGBoost Spark should deal with NaN when parsing evaluation output (#5546)\n* GPU-side data sketching is now aware of query groups in learning-to-rank data (#5551)\n* Fix DMatrix slicing for newly added fields (#5552)\n* Fix configuration status with loading binary model (#5562)\n* Fix build when OpenMP is disabled (#5566)\n* R compatibility patches (#5577, #5600)\n* gpu\\_hist performance fixes (#5558)\n* Don't set seed on CLI interface (#5563)\n* [R] When serializing model, preserve model attributes related to early stopping (#5573)\n* Avoid rabit calls in learner configuration (#5581)\n* Hide C++ symbols in libxgboost.so when building Python wheel (#5590). This fixes apache/incubator-tvm#4953.\n* Fix compilation on Mac OSX High Sierra (10.13) (#5597)\n* Fix build on big endian CPUs (#5617)\n* Resolve crash due to use of `vector<bool>::iterator` (#5642)\n* Validation JSON model dump using JSON schema (#5660)\n\n### Performance improvements\n* Wide dataset quantile performance improvement (#5306)\n* Reduce memory usage of GPU-side data sketching (#5407)\n* Reduce span check overhead (#5464)\n* Serialise booster after training to free up GPU memory (#5484)\n* Use the maximum amount of GPU shared memory available to speed up the histogram kernel (#5491)\n* Use non-synchronising scan in Thrust (#5560)\n* Use `cudaDeviceGetAttribute()` instead of `cudaGetDeviceProperties()` for speed (#5570)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David D\u00edaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Support importing data from a Pandas SparseArray (#5431)\n* `HostDeviceVector` (vector shared between CPU and GPU memory) now exposes `HostSpan` interface, to enable access on the CPU side with bound check (#5459)\n* Accept other gradient types for `SplitEntry` (#5467)\n\n### Usability Improvements, Documentation\n* Add `JVM_CHECK_CALL` to prevent C++ exceptions from leaking into the JVM layer (#5199)\n* Updated Windows build docs (#5283)\n* Update affiliation of @hcho3 (#5292)\n* Display Sponsor button, link to OpenCollective (#5325)\n* Update docs for GPU external memory (#5332)\n* Add link to GPU documentation (#5437)\n* Small updates to GPU documentation (#5483)\n* Edits on tutorial for XGBoost job on Kubernetes (#5487)\n* Add reference to GPU external memory (#5490)\n* Fix typos (#5346, #5371, #5384, #5399, #5482, #5515)\n* Update Python doc (#5517)\n* Add Neptune and Optuna to list of examples (#5528)\n* Raise error if the number of data weights doesn't match the number of data sets (#5540)\n* Add a note about GPU ranking (#5572)\n* Clarify meaning of `training` parameter in the C API function `XGBoosterPredict()` (#5604)\n* Better error handling for situations where existing trees cannot be modified (#5406, #5418). This feature is enabled when `process_type` is set to `update`.\n\n### Maintenance: testing, continuous integration, build system\n* Add C++ test coverage for data sketching (#5251)\n* Ignore gdb\\_history (#5257)\n* Rewrite setup.py. (#5271, #5280)\n* Use `scikit-learn` in extra dependencies (#5310)\n* Add CMake option to build static library (#5397)\n* [R] changed FindLibR to take advantage of CMake cache (#5427)\n* [R] fixed inconsistency in R -e calls in FindLibR.cmake (#5438)\n* Refactor tests with data generator (#5439)\n* Resolve failing Travis CI (#5445)\n* Update dmlc-core. (#5466)\n* [CI] Use clang-tidy 10 (#5469)\n* De-duplicate code for checking maximum number of nodes (#5497)\n* [CI] Use Ubuntu 18.04 LTS in JVM CI, because 19.04 is EOL (#5537)\n* [jvm-packages] [CI] Create a Maven repository to host SNAPSHOT JARs (#5533)\n* [jvm-packages] [CI] Publish XGBoost4J JARs with Scala 2.11 and 2.12 (#5539)\n* [CI] Use Vault repository to re-gain access to devtoolset-4 (#5589)\n\n### Maintenance: Refactor code for legibility and maintainability\n* Move prediction cache to Learner (#5220, #5302)\n* Remove SimpleCSRSource (#5315)\n* Refactor SparsePageSource, delete cache files after use (#5321)\n* Remove unnecessary DMatrix methods (#5324)\n* Split up `LearnerImpl` (#5350)\n* Move segment sorter to common (#5378)\n* Move thread local entry into Learner (#5396)\n* Split up test helpers header (#5455)\n* Requires setting leaf stat when expanding tree (#5501)\n* Purge device\\_helpers.cuh (#5534)\n* Use thrust functions instead of custom functions (#5544)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), Bart Broere (@bartbroere), Andy Adinets (@canonizer), Chen Qin (@chenqin), Daiki Katsuragawa (@daikikatsuragawa), David D\u00edaz Vico (@daviddiazvico), Darius Kharazi (@dkharazi), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), Jan Borchmann (@jborchma), Kamil A. Kaczmarek (@kamil-kaczmarek), Melissa Kohl (@mjkohl32), Nicolas Scozzaro (@nscozzaro), Paul Kaefer (@paulkaefer), Rong Ou (@rongou), Samrat Pandiri (@samratp), Sriram Chandramouli (@sriramch), Yuan Tang (@terrytangyuan), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958), Zhang Zhang (@zhangzhang10),"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David D\u00edaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n**Reviewers**: Nan Zhu (@CodingCat), @LeZhengThu, Rory Mitchell (@RAMitchell), @ShvetsKS, Egor Smirnov (@SmirnovEgorRu), Steve Bronder (@SteveBronder), Nikita Titov (@StrikerRUS), Andrew Kane (@ankane), Avinash Barnwal (@avinashbarnwal), @brydag, Andy Adinets (@canonizer), Chandra Shekhar Reddy (@chandrureddy), Chen Qin (@chenqin), Codecov (@codecov-io), David D\u00edaz Vico (@daviddiazvico), Darby Payne (@dpayne), Jason E. Aten, Ph.D. (@glycerine), Philip Hyunsu Cho (@hcho3), James Lamb (@jameslamb), @johnny-cat, Mu Li (@mli), Mate Soos (@msoos), @rnyak, Rong Ou (@rongou), Sriram Chandramouli (@sriramch), Toby Dylan Hocking (@tdhock), Yuan Tang (@terrytangyuan), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Liang-Chi Hsieh (@viirya), Bobby Wang (@wbo4958),\n\n## v1.0.2 (2020.03.03)\nThis patch release applies the following patches to 1.0.0 release:\n\n* Fix a small typo in sklearn.py that broke multiple eval metrics (#5341)\n* Restore loading model from buffer (#5360)\n* Use type name for data type check (#5364)\n\n## v1.0.1 (2020.02.21)\nThis release is identical to the 1.0.0 release, except that it fixes a small bug that rendered 1.0.0 incompatible with Python 3.5. See #5328.\n\n## v1.0.0 (2020.02.19)\nThis release marks a major milestone for the XGBoost project.\n\n### Apache-style governance, contribution policy, and semantic versioning (#4646, #4659)\n* Starting with 1.0.0 release, the XGBoost Project is adopting Apache-style governance. The full community guideline is [available in the doc website](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/community.html). Note that we now have Project Management Committee (PMC) who would steward the project on the long-term basis. The PMC is also entrusted to run and fund the project's continuous integration (CI) infrastructure (https://xgboost-ci.net).\n* We also adopt the [semantic versioning](https://semver.org/). See [our release versioning policy](https://xgboost.readthedocs.io/en/release_1.0.0/contrib/release.html).\n\n### Better performance scaling for multi-core CPUs (#4502, #4529, #4716, #4851, #5008, #5107, #5138, #5156)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). Previous effort #4529 was replaced with a series of pull requests (#5107, #5138, #5156) aimed at achieving the same performance benefits while keeping the C++ codebase legible. The latest performance benchmark results show [up to 5x speedup on Intel CPUs with many cores](https://github.com/dmlc/xgboost/pull/5156#issuecomment-580024413). Note: #5244, which concludes the effort, will become part of the upcoming release 1.1.0.\n\n### Improved installation experience on Mac OSX (#4672, #5074, #5080, #5146, #5240)\n* It used to be quite complicated to install XGBoost on Mac OSX. XGBoost uses OpenMP to distribute work among multiple CPU cores, and Mac's default C++ compiler (Apple Clang) does not come with OpenMP. Existing work-around (using another C++ compiler) was complex and prone to fail with cryptic diagnosis (#4933, #4949, #4969).\n* Now it only takes two commands to install XGBoost: `brew install libomp` followed by `pip install xgboost`. The installed XGBoost will use all CPU cores.\n* Even better, XGBoost is now available from Homebrew: `brew install xgboost`. See Homebrew/homebrew-core#50467.\n* Previously, if you installed the XGBoost R package using the command `install.packages('xgboost')`, it could only use a single CPU core and you would experience slow training performance. With 1.0.0 release, the R package will use all CPU cores out of box."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Distributed XGBoost now available on Kubernetes (#4621, #4939)\n* Check out the [tutorial for setting up distributed XGBoost on a Kubernetes cluster](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/kubernetes.html).\n\n### Ruby binding for XGBoost (#4856)\n\n### New Native Dask interface for multi-GPU and multi-node scaling (#4473, #4507, #4617, #4819, #4907, #4914, #4941, #4942, #4951, #4973, #5048, #5077, #5144, #5270)\n* XGBoost now integrates seamlessly with [Dask](https://dask.org/), a lightweight distributed framework for data processing. Together with the first-class support for cuDF data frames (see below), it is now easier than ever to create end-to-end data pipeline running on one or more NVIDIA GPUs.\n* Multi-GPU training with Dask is now up to 20% faster than the previous release (#4914, #4951).\n\n### First-class support for cuDF data frames and cuPy arrays (#4737, #4745, #4794, #4850, #4891, #4902, #4918, #4927, #4928, #5053, #5189, #5194, #5206, #5219, #5225)\n* [cuDF](https://github.com/rapidsai/cudf) is a data frame library for loading and processing tabular data on NVIDIA GPUs. It provides a Pandas-like API.\n* [cuPy](https://github.com/cupy/cupy) implements a NumPy-compatible multi-dimensional array on NVIDIA GPUs.\n* Now users can keep the data on the GPU memory throughout the end-to-end data pipeline, obviating the need for copying data between the main memory and GPU memory.\n* XGBoost can accept any data structure that exposes `__array_interface__` signature, opening way to support other columar formats that are compatible with Apache Arrow.\n\n### [Feature interaction constraint](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/feature_interaction_constraint.html) is now available with `approx` and `gpu_hist` algorithms (#4534, #4587, #4596, #5034).\n\n### Learning to rank is now GPU accelerated (#4873, #5004, #5129)\n* Supported ranking objectives: NDGC, Map, Pairwise.\n* [Up to 2x improved training performance on GPUs](https://devblogs.nvidia.com/learning-to-rank-with-xgboost-and-gpu/).\n\n### Enable `gamma` parameter for GPU training (#4874, #4953)\n* The `gamma` parameter specifies the minimum loss reduction required to add a new split in a tree. A larger value for `gamma` has the effect of pre-pruning the tree, by making harder to add splits.\n\n### External memory for GPU training (#4486, #4526, #4747, #4833, #4879, #5014)\n* It is now possible to use NVIDIA GPUs even when the size of training data exceeds the available GPU memory. Note that the external memory support for GPU is still experimental. #5093 will further improve performance and will become part of the upcoming release 1.1.0.\n* RFC for enabling external memory with GPU algorithms: #4357\n\n### Improve Scikit-Learn interface (#4558, #4842, #4929, #5049, #5151, #5130, #5227)\n* Many users of XGBoost enjoy the convenience and breadth of Scikit-Learn ecosystem. In this release, we revise the Scikit-Learn API of XGBoost (`XGBRegressor`, `XGBClassifier`, and `XGBRanker`) to achieve feature parity with the traditional XGBoost interface (`xgboost.train()`).\n* Insert check to validate data shapes.\n* Produce an error message if `eval_set` is not a tuple. An error message is better than silently crashing.\n* Allow using `numpy.RandomState` object.\n* Add `n_jobs` as an alias of `nthread`.\n* Roadmap: #5152", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Distributed XGBoost now available on Kubernetes (#4621, #4939)\n* Check out the [tutorial for setting up distributed XGBoost on a Kubernetes cluster](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/kubernetes.html).\n\n### Ruby binding for XGBoost (#4856)\n\n### New Native Dask interface for multi-GPU and multi-node scaling (#4473, #4507, #4617, #4819, #4907, #4914, #4941, #4942, #4951, #4973, #5048, #5077, #5144, #5270)\n* XGBoost now integrates seamlessly with [Dask](https://dask.org/), a lightweight distributed framework for data processing. Together with the first-class support for cuDF data frames (see below), it is now easier than ever to create end-to-end data pipeline running on one or more NVIDIA GPUs.\n* Multi-GPU training with Dask is now up to 20% faster than the previous release (#4914, #4951).\n\n### First-class support for cuDF data frames and cuPy arrays (#4737, #4745, #4794, #4850, #4891, #4902, #4918, #4927, #4928, #5053, #5189, #5194, #5206, #5219, #5225)\n* [cuDF](https://github.com/rapidsai/cudf) is a data frame library for loading and processing tabular data on NVIDIA GPUs. It provides a Pandas-like API.\n* [cuPy](https://github.com/cupy/cupy) implements a NumPy-compatible multi-dimensional array on NVIDIA GPUs.\n* Now users can keep the data on the GPU memory throughout the end-to-end data pipeline, obviating the need for copying data between the main memory and GPU memory.\n* XGBoost can accept any data structure that exposes `__array_interface__` signature, opening way to support other columar formats that are compatible with Apache Arrow.\n\n### [Feature interaction constraint](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/feature_interaction_constraint.html) is now available with `approx` and `gpu_hist` algorithms (#4534, #4587, #4596, #5034).\n\n### Learning to rank is now GPU accelerated (#4873, #5004, #5129)\n* Supported ranking objectives: NDGC, Map, Pairwise.\n* [Up to 2x improved training performance on GPUs](https://devblogs.nvidia.com/learning-to-rank-with-xgboost-and-gpu/).\n\n### Enable `gamma` parameter for GPU training (#4874, #4953)\n* The `gamma` parameter specifies the minimum loss reduction required to add a new split in a tree. A larger value for `gamma` has the effect of pre-pruning the tree, by making harder to add splits.\n\n### External memory for GPU training (#4486, #4526, #4747, #4833, #4879, #5014)\n* It is now possible to use NVIDIA GPUs even when the size of training data exceeds the available GPU memory. Note that the external memory support for GPU is still experimental. #5093 will further improve performance and will become part of the upcoming release 1.1.0.\n* RFC for enabling external memory with GPU algorithms: #4357\n\n### Improve Scikit-Learn interface (#4558, #4842, #4929, #5049, #5151, #5130, #5227)\n* Many users of XGBoost enjoy the convenience and breadth of Scikit-Learn ecosystem. In this release, we revise the Scikit-Learn API of XGBoost (`XGBRegressor`, `XGBClassifier`, and `XGBRanker`) to achieve feature parity with the traditional XGBoost interface (`xgboost.train()`).\n* Insert check to validate data shapes.\n* Produce an error message if `eval_set` is not a tuple. An error message is better than silently crashing.\n* Allow using `numpy.RandomState` object.\n* Add `n_jobs` as an alias of `nthread`.\n* Roadmap: #5152"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### XGBoost4J-Spark: Redesigning checkpointing mechanism\n* RFC is available at #4786\n* Clean up checkpoint file after a successful training job (#4754): The current implementation in XGBoost4J-Spark does not clean up the checkpoint file after a successful training job. If the user runs another job with the same checkpointing directory, she will get a wrong model because the second job will re-use the checkpoint file left over from the first job. To prevent this scenario, we propose to always clean up the checkpoint file after every successful training job.\n* Avoid Multiple Jobs for Checkpointing (#5082): The current method for checkpoint is to collect the booster produced at the last iteration of each checkpoint internal to Driver and persist it in HDFS. The major issue with this approach is that it needs to re-perform the data preparation for training if the user did not choose to cache the training dataset. To avoid re-performing data prep, we build external-memory checkpointing in the XGBoost4J layer as well.\n* Enable deterministic repartitioning when checkpoint is enabled (#4807): Distributed algorithm for gradient boosting assumes a fixed partition of the training data between multiple iterations. In previous versions, there was no guarantee that data partition would stay the same, especially when a worker goes down and some data had to recovered from previous checkpoint. In this release, we make data partition deterministic by using the data hash value of each data row in computing the partition.\n\n### XGBoost4J-Spark: handle errors thrown by the native code (#4560)\n* All core logic of XGBoost is written in C++, so XGBoost4J-Spark internally uses the C++ code via Java Native Interface (JNI). #4560 adds a proper error handling for any errors or exceptions arising from the C++ code, so that the XGBoost Spark application can be torn down in an orderly fashion.\n\n### XGBoost4J-Spark: Refine method to count the number of alive cores  (#4858)\n* The `SparkParallelismTracker` class ensures that sufficient number of executor cores are alive. To that end, it is important to query the number of alive cores reliably.\n\n### XGBoost4J: Add `BigDenseMatrix` to store more than `Integer.MAX_VALUE` elements (#4383)\n\n### Robust model serialization with JSON (#4632, #4708, #4739, #4868, #4936, #4945, #4974, #5086, #5087, #5089, #5091, #5094, #5110, #5111, #5112, #5120, #5137, #5218, #5222, #5236, #5245, #5248, #5281)\n* In this release, we introduce an experimental support of using [JSON](https://www.json.org/json-en.html) for serializing (saving/loading) XGBoost models and related hyperparameters for training. We would like to eventually replace the old binary format with JSON, since it is an open format and parsers are available in many programming languages and platforms. See [the documentation for model I/O using JSON](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/saving_model.html). #3980 explains why JSON was chosen over other alternatives.\n* To maximize interoperability and compatibility of the serialized models, we now split serialization into two parts (#4855):\n  1. Model, e.g. decision trees and strictly related metadata like `num_features`.\n  2. Internal configuration, consisting of training parameters and other configurable parameters. For example, `max_delta_step`, `tree_method`, `objective`, `predictor`, `gpu_id`.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### XGBoost4J-Spark: Redesigning checkpointing mechanism\n* RFC is available at #4786\n* Clean up checkpoint file after a successful training job (#4754): The current implementation in XGBoost4J-Spark does not clean up the checkpoint file after a successful training job. If the user runs another job with the same checkpointing directory, she will get a wrong model because the second job will re-use the checkpoint file left over from the first job. To prevent this scenario, we propose to always clean up the checkpoint file after every successful training job.\n* Avoid Multiple Jobs for Checkpointing (#5082): The current method for checkpoint is to collect the booster produced at the last iteration of each checkpoint internal to Driver and persist it in HDFS. The major issue with this approach is that it needs to re-perform the data preparation for training if the user did not choose to cache the training dataset. To avoid re-performing data prep, we build external-memory checkpointing in the XGBoost4J layer as well.\n* Enable deterministic repartitioning when checkpoint is enabled (#4807): Distributed algorithm for gradient boosting assumes a fixed partition of the training data between multiple iterations. In previous versions, there was no guarantee that data partition would stay the same, especially when a worker goes down and some data had to recovered from previous checkpoint. In this release, we make data partition deterministic by using the data hash value of each data row in computing the partition.\n\n### XGBoost4J-Spark: handle errors thrown by the native code (#4560)\n* All core logic of XGBoost is written in C++, so XGBoost4J-Spark internally uses the C++ code via Java Native Interface (JNI). #4560 adds a proper error handling for any errors or exceptions arising from the C++ code, so that the XGBoost Spark application can be torn down in an orderly fashion.\n\n### XGBoost4J-Spark: Refine method to count the number of alive cores  (#4858)\n* The `SparkParallelismTracker` class ensures that sufficient number of executor cores are alive. To that end, it is important to query the number of alive cores reliably.\n\n### XGBoost4J: Add `BigDenseMatrix` to store more than `Integer.MAX_VALUE` elements (#4383)\n\n### Robust model serialization with JSON (#4632, #4708, #4739, #4868, #4936, #4945, #4974, #5086, #5087, #5089, #5091, #5094, #5110, #5111, #5112, #5120, #5137, #5218, #5222, #5236, #5245, #5248, #5281)\n* In this release, we introduce an experimental support of using [JSON](https://www.json.org/json-en.html) for serializing (saving/loading) XGBoost models and related hyperparameters for training. We would like to eventually replace the old binary format with JSON, since it is an open format and parsers are available in many programming languages and platforms. See [the documentation for model I/O using JSON](https://xgboost.readthedocs.io/en/release_1.0.0/tutorials/saving_model.html). #3980 explains why JSON was chosen over other alternatives.\n* To maximize interoperability and compatibility of the serialized models, we now split serialization into two parts (#4855):\n  1. Model, e.g. decision trees and strictly related metadata like `num_features`.\n  2. Internal configuration, consisting of training parameters and other configurable parameters. For example, `max_delta_step`, `tree_method`, `objective`, `predictor`, `gpu_id`."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nPreviously, users often ran into issues where the model file produced by one machine could not load or run on another machine. For example, models trained using a machine with an NVIDIA GPU could not run on another machine without a GPU (#5291, #5234). The reason is that the old binary format saved some internal configuration that were not universally applicable to all machines, e.g. `predictor='gpu_predictor'`.\n\n  Now, model saving function (`Booster.save_model()` in Python) will save only the model, without internal configuration. This will guarantee that your model file would be used anywhere. Internal configuration will be serialized in limited circumstances such as:\n  * Multiple nodes in a distributed system exchange model details over the network.\n  * Model checkpointing, to recover from possible crashes.\n\n  This work proved to be useful for parameter validation as well (see below).\n* Starting with 1.0.0 release, we will use semantic versioning to indicate whether the model produced by one version of XGBoost would be compatible with another version of XGBoost. Any change in the major version indicates a breaking change in the serialization format.\n* We now provide a robust method to save and load scikit-learn related attributes (#5245). Previously, we used Python pickle to save Python attributes related to `XGBClassifier`, `XGBRegressor`, and `XGBRanker` objects. The attributes are necessary to properly interact with scikit-learn. See #4639 for more details. The use of pickling hampered interoperability, as a pickle from one machine may not necessarily work on another machine. Starting with this release, we use an alternative method to serialize the scikit-learn related attributes. The use of Python pickle is now discouraged (#5236, #5281).\n\n### Parameter validation: detection of unused or incorrect parameters (#4553, #4577, #4738, #4801, #4961, #5101, #5157, #5167, #5256)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. Currently, parameter validation is available to R users and Python XGBoost API users. We are working to extend its support to scikit-learn users.\n* Configuration steps now have well-defined semantics (#4542, #4738), so we know exactly where and how the internal configurable parameters are changed.\n* The user can now use `save_config()` function to inspect all (used) training parameters. This is helpful for debugging model performance.\n\n### Allow individual workers to recover from faults (#4808, #4966)\n* Status quo: if a worker fails, all workers are shut down and restarted, and learning resumes from the last checkpoint. This involves requesting resources from the scheduler (e.g. Spark) and shuffling all the data again from scratch. Both of these operations can be quite costly and block training for extended periods of time, especially if the training data is big and the number of worker nodes is in the hundreds.\n* The proposed solution is to recover the single node that failed, instead of shutting down all workers. The rest of the clusters wait until the single failed worker is bootstrapped and catches up with the rest.\n* See roadmap at #4753. Note that this is work in progress. In particular, the feature is not yet available from XGBoost4J-Spark.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\nPreviously, users often ran into issues where the model file produced by one machine could not load or run on another machine. For example, models trained using a machine with an NVIDIA GPU could not run on another machine without a GPU (#5291, #5234). The reason is that the old binary format saved some internal configuration that were not universally applicable to all machines, e.g. `predictor='gpu_predictor'`.\n\n  Now, model saving function (`Booster.save_model()` in Python) will save only the model, without internal configuration. This will guarantee that your model file would be used anywhere. Internal configuration will be serialized in limited circumstances such as:\n  * Multiple nodes in a distributed system exchange model details over the network.\n  * Model checkpointing, to recover from possible crashes.\n\n  This work proved to be useful for parameter validation as well (see below).\n* Starting with 1.0.0 release, we will use semantic versioning to indicate whether the model produced by one version of XGBoost would be compatible with another version of XGBoost. Any change in the major version indicates a breaking change in the serialization format.\n* We now provide a robust method to save and load scikit-learn related attributes (#5245). Previously, we used Python pickle to save Python attributes related to `XGBClassifier`, `XGBRegressor`, and `XGBRanker` objects. The attributes are necessary to properly interact with scikit-learn. See #4639 for more details. The use of pickling hampered interoperability, as a pickle from one machine may not necessarily work on another machine. Starting with this release, we use an alternative method to serialize the scikit-learn related attributes. The use of Python pickle is now discouraged (#5236, #5281).\n\n### Parameter validation: detection of unused or incorrect parameters (#4553, #4577, #4738, #4801, #4961, #5101, #5157, #5167, #5256)\n* Mis-spelled training parameter is a common user mistake. In previous versions of XGBoost, mis-spelled parameters were silently ignored. Starting with 1.0.0 release, XGBoost will produce a warning message if there is any unused training parameters. Currently, parameter validation is available to R users and Python XGBoost API users. We are working to extend its support to scikit-learn users.\n* Configuration steps now have well-defined semantics (#4542, #4738), so we know exactly where and how the internal configurable parameters are changed.\n* The user can now use `save_config()` function to inspect all (used) training parameters. This is helpful for debugging model performance.\n\n### Allow individual workers to recover from faults (#4808, #4966)\n* Status quo: if a worker fails, all workers are shut down and restarted, and learning resumes from the last checkpoint. This involves requesting resources from the scheduler (e.g. Spark) and shuffling all the data again from scratch. Both of these operations can be quite costly and block training for extended periods of time, especially if the training data is big and the number of worker nodes is in the hundreds.\n* The proposed solution is to recover the single node that failed, instead of shutting down all workers. The rest of the clusters wait until the single failed worker is bootstrapped and catches up with the rest.\n* See roadmap at #4753. Note that this is work in progress. In particular, the feature is not yet available from XGBoost4J-Spark."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Accurate prediction for DART models\n* Use DART tree weights when computing SHAPs (#5050)\n* Don't drop trees during DART prediction by default (#5115)\n* Fix DART prediction in R (#5204)\n\n### Make external memory more robust\n* Fix issues with training with external memory on cpu (#4487)\n* Fix crash with approx tree method on cpu (#4510)\n* Fix external memory race in `exact` (#4980). Note: `dmlc::ThreadedIter` is not actually thread-safe. We would like to re-design it in the long term.\n\n### Major refactoring of the `DMatrix` class (#4686, #4744, #4748, #5044, #5092, #5108, #5188, #5198)\n* Goal 1: improve performance and reduce memory consumption. Right now, if the user trains a model with a NumPy array as training data, the array gets copies 2-3 times before training begins. We'd like to reduce duplication of the data matrix.\n* Goal 2: Expose a common interface to external data, unify the way DMatrix objects are constructed and simplify the process of adding new external data sources. This work is essential for ingesting cuPy arrays.\n* Goal 3: Handle missing values consistently.\n* RFC: #4354, Roadmap: #5143\n* This work is also relevant to external memory support on GPUs.\n\n### Breaking: XGBoost Python package now requires Python 3.5 or newer (#5021, #5274)\n* Python 3.4 has reached its end-of-life on March 16, 2019, so we now require Python 3.5 or newer.\n\n### Breaking: GPU algorithm now requires CUDA 9.0 and higher (#4527, #4580)\n\n### Breaking: `n_gpus` parameter removed; multi-GPU training now requires a distributed framework (#4579, #4749, #4773, #4810, #4867, #4908)\n* #4531 proposed removing support for single-process multi-GPU training. Contributors would focus on multi-GPU support through distributed frameworks such as Dask and Spark, where the framework would be expected to assign a worker process for each GPU independently. By delegating GPU management and data movement to the distributed framework, we can greatly simplify the core XGBoost codebase, make multi-GPU training more robust, and reduce burden for future development.\n\n### Breaking: Some deprecated features have been removed\n* ``gpu_exact`` training method (#4527, #4742, #4777). Use ``gpu_hist`` instead.\n* ``learning_rates`` parameter in Python (#5155). Use the callback API instead.\n* ``num_roots`` (#5059, #5165), since the current training code always uses a single root node.\n* GPU-specific objectives (#4690), such as `gpu:reg:linear`. Use objectives without `gpu:` prefix; GPU will be used automatically if your machine has one.\n\n### Breaking: the C API function `XGBoosterPredict()` now asks for an extra parameter `training`.\n\n### Breaking: We now use CMake exclusively to build XGBoost. `Makefile` is being sunset.\n* Exception: the R package uses Autotools, as the CRAN ecosystem did not yet adopt CMake widely.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Accurate prediction for DART models\n* Use DART tree weights when computing SHAPs (#5050)\n* Don't drop trees during DART prediction by default (#5115)\n* Fix DART prediction in R (#5204)\n\n### Make external memory more robust\n* Fix issues with training with external memory on cpu (#4487)\n* Fix crash with approx tree method on cpu (#4510)\n* Fix external memory race in `exact` (#4980). Note: `dmlc::ThreadedIter` is not actually thread-safe. We would like to re-design it in the long term.\n\n### Major refactoring of the `DMatrix` class (#4686, #4744, #4748, #5044, #5092, #5108, #5188, #5198)\n* Goal 1: improve performance and reduce memory consumption. Right now, if the user trains a model with a NumPy array as training data, the array gets copies 2-3 times before training begins. We'd like to reduce duplication of the data matrix.\n* Goal 2: Expose a common interface to external data, unify the way DMatrix objects are constructed and simplify the process of adding new external data sources. This work is essential for ingesting cuPy arrays.\n* Goal 3: Handle missing values consistently.\n* RFC: #4354, Roadmap: #5143\n* This work is also relevant to external memory support on GPUs.\n\n### Breaking: XGBoost Python package now requires Python 3.5 or newer (#5021, #5274)\n* Python 3.4 has reached its end-of-life on March 16, 2019, so we now require Python 3.5 or newer.\n\n### Breaking: GPU algorithm now requires CUDA 9.0 and higher (#4527, #4580)\n\n### Breaking: `n_gpus` parameter removed; multi-GPU training now requires a distributed framework (#4579, #4749, #4773, #4810, #4867, #4908)\n* #4531 proposed removing support for single-process multi-GPU training. Contributors would focus on multi-GPU support through distributed frameworks such as Dask and Spark, where the framework would be expected to assign a worker process for each GPU independently. By delegating GPU management and data movement to the distributed framework, we can greatly simplify the core XGBoost codebase, make multi-GPU training more robust, and reduce burden for future development.\n\n### Breaking: Some deprecated features have been removed\n* ``gpu_exact`` training method (#4527, #4742, #4777). Use ``gpu_hist`` instead.\n* ``learning_rates`` parameter in Python (#5155). Use the callback API instead.\n* ``num_roots`` (#5059, #5165), since the current training code always uses a single root node.\n* GPU-specific objectives (#4690), such as `gpu:reg:linear`. Use objectives without `gpu:` prefix; GPU will be used automatically if your machine has one.\n\n### Breaking: the C API function `XGBoosterPredict()` now asks for an extra parameter `training`.\n\n### Breaking: We now use CMake exclusively to build XGBoost. `Makefile` is being sunset.\n* Exception: the R package uses Autotools, as the CRAN ecosystem did not yet adopt CMake widely."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Smarter choice of histogram construction for distributed `gpu_hist` (#4519)\n* Optimizations for quantization on device (#4572)\n* Introduce caching memory allocator to avoid latency associated with GPU memory allocation (#4554, #4615)\n* Optimize the initialization stage of the CPU `hist` algorithm for sparse datasets (#4625)\n* Prevent unnecessary data copies from GPU memory to the host (#4795)\n* Improve operation efficiency for single prediction (#5016)\n* Group builder modified for incremental building, to speed up building large `DMatrix` (#5098)\n\n### Bug-fixes\n* Eliminate `FutureWarning: Series.base is deprecated` (#4337)\n* Ensure pandas DataFrame column names are treated as strings in type error message (#4481)\n* [jvm-packages] Add back `reg:linear` for scala, as it is only deprecated and not meant to be removed yet (#4490)\n* Fix library loading for Cygwin users (#4499)\n* Fix prediction from loaded pickle (#4516)\n* Enforce exclusion between `pred_interactions=True` and `pred_interactions=True` (#4522)\n* Do not return dangling reference to local `std::string` (#4543)\n* Set the appropriate device before freeing device memory (#4566)\n* Mark `SparsePageDmatrix` destructor default. (#4568)\n* Choose the appropriate tree method only when the tree method is 'auto' (#4571)\n* Fix `benchmark_tree.py` (#4593)\n* [jvm-packages] Fix silly bug in feature scoring (#4604)\n* Fix GPU predictor when the test data matrix has different number of features than the training data matrix used to train the model (#4613)\n* Fix external memory for get column batches. (#4622)\n* [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)\n* Fix early stopping in the Python package (#4638)\n* Fix AUC error in distributed mode caused by imbalanced dataset (#4645, #4798)\n* [jvm-packages] Expose `setMissing` method in `XGBoostClassificationModel` / `XGBoostRegressionModel` (#4643)\n* Remove initializing stringstream reference. (#4788)\n* [R] `xgb.get.handle` now checks all class listed of `object` (#4800)\n* Do not use `gpu_predictor` unless data comes from GPU (#4836)\n* Fix data loading (#4862)\n* Workaround `isnan` across different environments. (#4883)\n* [jvm-packages] Handle Long-type parameter (#4885)\n* Don't `set_params` at the end of `set_state` (#4947). Ensure that the model does not change after pickling and unpickling multiple times.\n* C++ exceptions should not crash OpenMP loops (#4960)\n* Fix `usegpu` flag in DART. (#4984)\n* Run training with empty `DMatrix` (#4990, #5159)\n* Ensure that no two processes can use the same GPU (#4990)\n* Fix repeated split and 0 cover nodes (#5010)\n* Reset histogram hit counter between multiple data batches (#5035)\n* Fix `feature_name` crated from int64index dataframe. (#5081)\n* Don't use 0 for \"fresh leaf\" (#5084)\n* Throw error when user attempts to use multi-GPU training and XGBoost has not been compiled with NCCL (#5170)\n* Fix metric name loading (#5122)\n* Quick fix for memory leak in CPU `hist` algorithm (#5153)\n* Fix wrapping GPU ID and prevent data copying (#5160)\n* Fix signature of Span constructor (#5166)\n* Lazy initialization of device vector, so that XGBoost compiled with CUDA can run on a machine without any GPU (#5173)\n* Model loading should not change system locale (#5314)\n* Distributed training jobs would sometimes hang; revert Rabit to fix this regression (dmlc/rabit#132, #5237)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Smarter choice of histogram construction for distributed `gpu_hist` (#4519)\n* Optimizations for quantization on device (#4572)\n* Introduce caching memory allocator to avoid latency associated with GPU memory allocation (#4554, #4615)\n* Optimize the initialization stage of the CPU `hist` algorithm for sparse datasets (#4625)\n* Prevent unnecessary data copies from GPU memory to the host (#4795)\n* Improve operation efficiency for single prediction (#5016)\n* Group builder modified for incremental building, to speed up building large `DMatrix` (#5098)\n\n### Bug-fixes\n* Eliminate `FutureWarning: Series.base is deprecated` (#4337)\n* Ensure pandas DataFrame column names are treated as strings in type error message (#4481)\n* [jvm-packages] Add back `reg:linear` for scala, as it is only deprecated and not meant to be removed yet (#4490)\n* Fix library loading for Cygwin users (#4499)\n* Fix prediction from loaded pickle (#4516)\n* Enforce exclusion between `pred_interactions=True` and `pred_interactions=True` (#4522)\n* Do not return dangling reference to local `std::string` (#4543)\n* Set the appropriate device before freeing device memory (#4566)\n* Mark `SparsePageDmatrix` destructor default. (#4568)\n* Choose the appropriate tree method only when the tree method is 'auto' (#4571)\n* Fix `benchmark_tree.py` (#4593)\n* [jvm-packages] Fix silly bug in feature scoring (#4604)\n* Fix GPU predictor when the test data matrix has different number of features than the training data matrix used to train the model (#4613)\n* Fix external memory for get column batches. (#4622)\n* [R] Use built-in label when xgb.DMatrix is given to xgb.cv() (#4631)\n* Fix early stopping in the Python package (#4638)\n* Fix AUC error in distributed mode caused by imbalanced dataset (#4645, #4798)\n* [jvm-packages] Expose `setMissing` method in `XGBoostClassificationModel` / `XGBoostRegressionModel` (#4643)\n* Remove initializing stringstream reference. (#4788)\n* [R] `xgb.get.handle` now checks all class listed of `object` (#4800)\n* Do not use `gpu_predictor` unless data comes from GPU (#4836)\n* Fix data loading (#4862)\n* Workaround `isnan` across different environments. (#4883)\n* [jvm-packages] Handle Long-type parameter (#4885)\n* Don't `set_params` at the end of `set_state` (#4947). Ensure that the model does not change after pickling and unpickling multiple times.\n* C++ exceptions should not crash OpenMP loops (#4960)\n* Fix `usegpu` flag in DART. (#4984)\n* Run training with empty `DMatrix` (#4990, #5159)\n* Ensure that no two processes can use the same GPU (#4990)\n* Fix repeated split and 0 cover nodes (#5010)\n* Reset histogram hit counter between multiple data batches (#5035)\n* Fix `feature_name` crated from int64index dataframe. (#5081)\n* Don't use 0 for \"fresh leaf\" (#5084)\n* Throw error when user attempts to use multi-GPU training and XGBoost has not been compiled with NCCL (#5170)\n* Fix metric name loading (#5122)\n* Quick fix for memory leak in CPU `hist` algorithm (#5153)\n* Fix wrapping GPU ID and prevent data copying (#5160)\n* Fix signature of Span constructor (#5166)\n* Lazy initialization of device vector, so that XGBoost compiled with CUDA can run on a machine without any GPU (#5173)\n* Model loading should not change system locale (#5314)\n* Distributed training jobs would sometimes hang; revert Rabit to fix this regression (dmlc/rabit#132, #5237)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Add support for cross-validation using query ID (#4474)\n* Enable feature importance property for DART model (#4525)\n* Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)\n* All objective and evaluation metrics are now exposed to JVM packages (#4560)\n* `dump_model()` and `get_dump()` now support exporting in GraphViz language (#4602)\n* Support metrics `ndcg-` and `map-` (#4635)\n* [jvm-packages] Allow chaining prediction (transform) in XGBoost4J-Spark (#4667)\n* [jvm-packages] Add option to bypass missing value check in the Spark layer (#4805). Only use this option if you know what you are doing.\n* [jvm-packages] Add public group getter (#4838)\n* `XGDMatrixSetGroup` C API is now deprecated (#4864). Use `XGDMatrixSetUIntInfo` instead.\n* [R] Added new `train_folds` parameter to `xgb.cv()` (#5114)\n* Ingest meta information from Pandas DataFrame, such as data weights (#5216)\n\n### Maintenance: Refactor code for legibility and maintainability\n* De-duplicate GPU parameters (#4454)\n* Simplify INI-style config reader using C++11 STL (#4478, #4521)\n* Refactor histogram building code for `gpu_hist` (#4528)\n* Overload device memory allocator, to enable instrumentation for compiling memory usage statistics (#4532)\n* Refactor out row partitioning logic from `gpu_hist` (#4554)\n* Remove an unused variable (#4588)\n* Implement tree model dump with code generator, to de-duplicate code for generating dumps in 3 different formats (#4602)\n* Remove `RowSet` class which is no longer being used (#4697)\n* Remove some unused functions as reported by cppcheck (#4743)\n* Mimic CUDA assert output in Span check (#4762)\n* [jvm-packages] Refactor `XGBoost.scala` to put all params processing in one place (#4815)\n* Add some comments for GPU row partitioner (#4832)\n* Span: use `size_t' for index_type,  add `front' and `back'. (#4935)\n* Remove dead code in `exact` algorithm (#5034, #5105)\n* Unify integer types used for row and column indices (#5034)\n* Extract feature interaction constraint from `SplitEvaluator` class. (#5034)\n* [Breaking] De-duplicate paramters and docstrings in the constructors of Scikit-Learn models (#5130)\n* Remove benchmark code from GPU tests (#5141)\n* Clean up Python 2 compatibility code. (#5161)\n* Extensible binary serialization format for `DMatrix::MetaInfo` (#5187). This will be useful for implementing censored labels for survival analysis applications.\n* Cleanup clang-tidy warnings. (#5247)\n\n### Maintenance: testing, continuous integration, build system\n* Use `yaml.safe_load` instead of `yaml.load`. (#4537)\n* Ensure GCC is at least 5.x (#4538)\n* Remove all mention of `reg:linear` from tests (#4544)\n* [jvm-packages] Upgrade to Scala 2.12 (#4574)\n* [jvm-packages] Update kryo dependency to 2.22 (#4575)\n* [CI] Specify account ID when logging into ECR Docker registry (#4584)\n* Use Sphinx 2.1+ to compile documentation (#4609)\n* Make Pandas optional for running Python unit tests (#4620)\n* Fix spark tests on machines with many cores (#4634)\n* [jvm-packages] Update local dev build process (#4640)\n* Add optional dependencies to setup.py (#4655)\n* [jvm-packages] Fix maven warnings (#4664)\n* Remove extraneous files from the R package, to comply with CRAN policy (#4699)\n* Remove VC-2013 support, since it is not C++11 compliant (#4701)\n* [CI] Fix broken installation of Pandas (#4704, #4722)\n* [jvm-packages] Clean up temporary files afer running tests (#4706)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### API changes\n* Add support for cross-validation using query ID (#4474)\n* Enable feature importance property for DART model (#4525)\n* Add `rmsle` metric and `reg:squaredlogerror` objective (#4541)\n* All objective and evaluation metrics are now exposed to JVM packages (#4560)\n* `dump_model()` and `get_dump()` now support exporting in GraphViz language (#4602)\n* Support metrics `ndcg-` and `map-` (#4635)\n* [jvm-packages] Allow chaining prediction (transform) in XGBoost4J-Spark (#4667)\n* [jvm-packages] Add option to bypass missing value check in the Spark layer (#4805). Only use this option if you know what you are doing.\n* [jvm-packages] Add public group getter (#4838)\n* `XGDMatrixSetGroup` C API is now deprecated (#4864). Use `XGDMatrixSetUIntInfo` instead.\n* [R] Added new `train_folds` parameter to `xgb.cv()` (#5114)\n* Ingest meta information from Pandas DataFrame, such as data weights (#5216)\n\n### Maintenance: Refactor code for legibility and maintainability\n* De-duplicate GPU parameters (#4454)\n* Simplify INI-style config reader using C++11 STL (#4478, #4521)\n* Refactor histogram building code for `gpu_hist` (#4528)\n* Overload device memory allocator, to enable instrumentation for compiling memory usage statistics (#4532)\n* Refactor out row partitioning logic from `gpu_hist` (#4554)\n* Remove an unused variable (#4588)\n* Implement tree model dump with code generator, to de-duplicate code for generating dumps in 3 different formats (#4602)\n* Remove `RowSet` class which is no longer being used (#4697)\n* Remove some unused functions as reported by cppcheck (#4743)\n* Mimic CUDA assert output in Span check (#4762)\n* [jvm-packages] Refactor `XGBoost.scala` to put all params processing in one place (#4815)\n* Add some comments for GPU row partitioner (#4832)\n* Span: use `size_t' for index_type,  add `front' and `back'. (#4935)\n* Remove dead code in `exact` algorithm (#5034, #5105)\n* Unify integer types used for row and column indices (#5034)\n* Extract feature interaction constraint from `SplitEvaluator` class. (#5034)\n* [Breaking] De-duplicate paramters and docstrings in the constructors of Scikit-Learn models (#5130)\n* Remove benchmark code from GPU tests (#5141)\n* Clean up Python 2 compatibility code. (#5161)\n* Extensible binary serialization format for `DMatrix::MetaInfo` (#5187). This will be useful for implementing censored labels for survival analysis applications.\n* Cleanup clang-tidy warnings. (#5247)\n\n### Maintenance: testing, continuous integration, build system\n* Use `yaml.safe_load` instead of `yaml.load`. (#4537)\n* Ensure GCC is at least 5.x (#4538)\n* Remove all mention of `reg:linear` from tests (#4544)\n* [jvm-packages] Upgrade to Scala 2.12 (#4574)\n* [jvm-packages] Update kryo dependency to 2.22 (#4575)\n* [CI] Specify account ID when logging into ECR Docker registry (#4584)\n* Use Sphinx 2.1+ to compile documentation (#4609)\n* Make Pandas optional for running Python unit tests (#4620)\n* Fix spark tests on machines with many cores (#4634)\n* [jvm-packages] Update local dev build process (#4640)\n* Add optional dependencies to setup.py (#4655)\n* [jvm-packages] Fix maven warnings (#4664)\n* Remove extraneous files from the R package, to comply with CRAN policy (#4699)\n* Remove VC-2013 support, since it is not C++11 compliant (#4701)\n* [CI] Fix broken installation of Pandas (#4704, #4722)\n* [jvm-packages] Clean up temporary files afer running tests (#4706)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Specify version macro in CMake. (#4730)\n* Include dmlc-tracker into XGBoost Python package (#4731)\n* [CI] Use long key ID for Ubuntu repository fingerprints. (#4783)\n* Remove plugin, CUDA related code in automake & autoconf files (#4789)\n* Skip related tests when scikit-learn is not installed. (#4791)\n* Ignore vscode and clion files (#4866)\n* Use bundled Google Test by default (#4900)\n* [CI] Raise timeout threshold in Jenkins (#4938)\n* Copy CMake parameter from dmlc-core. (#4948)\n* Set correct file permission. (#4964)\n* [CI] Update lint configuration to support latest pylint convention (#4971)\n* [CI] Upload nightly builds to S3 (#4976, #4979)\n* Add asan.so.5 to cmake script. (#4999)\n* [CI] Fix Travis tests. (#5062)\n* [CI] Locate vcomp140.dll from System32 directory (#5078)\n* Implement training observer to dump internal states of objects (#5088). This will be useful for debugging.\n* Fix visual studio output library directories (#5119)\n* [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)\n* [CI] Repair download URL for Maven 3.6.1 (#5139)\n* Don't use modernize-use-trailing-return-type in clang-tidy. (#5169)\n* Explicitly use UTF-8 codepage when using MSVC (#5197)\n* Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)\n* Make some GPU tests deterministic (#5229)\n* [R] Robust endian detection in CRAN xgboost build (#5232)\n* Support FreeBSD (#5233)\n* Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)\n* Fix compilation error due to 64-bit integer narrowing to `size_t` (#5250)\n* Remove use of `std::cout` from R package, to comply with CRAN policy (#5261)\n* Update DMLC-Core submodule (#4674, #4688, #4726, #4924)\n* Update Rabit submodule (#4560, #4667, #4718, #4808, #4966, #5237)\n\n### Usability Improvements, Documentation\n* Add Random Forest API to Python API doc (#4500)\n* Fix Python demo and doc. (#4545)\n* Remove doc about not supporting CUDA 10.1 (#4578)\n* Address some sphinx warnings and errors, add doc for building doc. (#4589)\n* Add instruction to run formatting checks locally (#4591)\n* Fix docstring for `XGBModel.predict()` (#4592)\n* Doc and demo for customized metric and objective (#4598, #4608)\n* Add to documentation how to run tests locally (#4610)\n* Empty evaluation list in early stopping should produce meaningful error message (#4633)\n* Fixed year to 2019 in conf.py, helpers.h and LICENSE (#4661)\n* Minor updates to links and grammar (#4673)\n* Remove `silent` in doc (#4689)\n* Remove old Python trouble shooting doc (#4729)\n* Add `os.PathLike` support for file paths to DMatrix and Booster Python classes (#4757)\n* Update XGBoost4J-Spark doc (#4804)\n* Regular formatting for evaluation metrics (#4803)\n* [jvm-packages] Refine documentation for handling missing values in XGBoost4J-Spark (#4805)\n* Monitor for distributed environment (#4829). This is useful for identifying performance bottleneck.\n* Add check for length of weights and produce a good error message (#4872)\n* Fix DMatrix doc (#4884)\n* Export C++ headers in CMake installation (#4897)\n* Update license year in README.md to 2019 (#4940)\n* Fix incorrectly displayed Note in the doc (#4943)\n* Follow PEP 257 Docstring Conventions (#4959)\n* Document minimum version required for Google Test (#5001)\n* Add better error message for invalid feature names (#5024)\n* Some guidelines on device memory usage (#5038)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Specify version macro in CMake. (#4730)\n* Include dmlc-tracker into XGBoost Python package (#4731)\n* [CI] Use long key ID for Ubuntu repository fingerprints. (#4783)\n* Remove plugin, CUDA related code in automake & autoconf files (#4789)\n* Skip related tests when scikit-learn is not installed. (#4791)\n* Ignore vscode and clion files (#4866)\n* Use bundled Google Test by default (#4900)\n* [CI] Raise timeout threshold in Jenkins (#4938)\n* Copy CMake parameter from dmlc-core. (#4948)\n* Set correct file permission. (#4964)\n* [CI] Update lint configuration to support latest pylint convention (#4971)\n* [CI] Upload nightly builds to S3 (#4976, #4979)\n* Add asan.so.5 to cmake script. (#4999)\n* [CI] Fix Travis tests. (#5062)\n* [CI] Locate vcomp140.dll from System32 directory (#5078)\n* Implement training observer to dump internal states of objects (#5088). This will be useful for debugging.\n* Fix visual studio output library directories (#5119)\n* [jvm-packages] Comply with scala style convention + fix broken unit test (#5134)\n* [CI] Repair download URL for Maven 3.6.1 (#5139)\n* Don't use modernize-use-trailing-return-type in clang-tidy. (#5169)\n* Explicitly use UTF-8 codepage when using MSVC (#5197)\n* Add CMake option to run Undefined Behavior Sanitizer (UBSan) (#5211)\n* Make some GPU tests deterministic (#5229)\n* [R] Robust endian detection in CRAN xgboost build (#5232)\n* Support FreeBSD (#5233)\n* Make `pip install xgboost*.tar.gz` work by fixing build-python.sh (#5241)\n* Fix compilation error due to 64-bit integer narrowing to `size_t` (#5250)\n* Remove use of `std::cout` from R package, to comply with CRAN policy (#5261)\n* Update DMLC-Core submodule (#4674, #4688, #4726, #4924)\n* Update Rabit submodule (#4560, #4667, #4718, #4808, #4966, #5237)\n\n### Usability Improvements, Documentation\n* Add Random Forest API to Python API doc (#4500)\n* Fix Python demo and doc. (#4545)\n* Remove doc about not supporting CUDA 10.1 (#4578)\n* Address some sphinx warnings and errors, add doc for building doc. (#4589)\n* Add instruction to run formatting checks locally (#4591)\n* Fix docstring for `XGBModel.predict()` (#4592)\n* Doc and demo for customized metric and objective (#4598, #4608)\n* Add to documentation how to run tests locally (#4610)\n* Empty evaluation list in early stopping should produce meaningful error message (#4633)\n* Fixed year to 2019 in conf.py, helpers.h and LICENSE (#4661)\n* Minor updates to links and grammar (#4673)\n* Remove `silent` in doc (#4689)\n* Remove old Python trouble shooting doc (#4729)\n* Add `os.PathLike` support for file paths to DMatrix and Booster Python classes (#4757)\n* Update XGBoost4J-Spark doc (#4804)\n* Regular formatting for evaluation metrics (#4803)\n* [jvm-packages] Refine documentation for handling missing values in XGBoost4J-Spark (#4805)\n* Monitor for distributed environment (#4829). This is useful for identifying performance bottleneck.\n* Add check for length of weights and produce a good error message (#4872)\n* Fix DMatrix doc (#4884)\n* Export C++ headers in CMake installation (#4897)\n* Update license year in README.md to 2019 (#4940)\n* Fix incorrectly displayed Note in the doc (#4943)\n* Follow PEP 257 Docstring Conventions (#4959)\n* Document minimum version required for Google Test (#5001)\n* Add better error message for invalid feature names (#5024)\n* Some guidelines on device memory usage (#5038)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [doc] Some notes for external memory. (#5065)\n* Update document for `tree_method` (#5106)\n* Update demo for ranking. (#5154)\n* Add new lines for Spark XGBoost missing values section (#5180)\n* Fix simple typo: utilty -> utility (#5182)\n* Update R doc by roxygen2 (#5201)\n* [R] Direct user to use `set.seed()` instead of setting `seed` parameter (#5125)\n* Add Optuna badge to `README.md` (#5208)\n* Fix compilation error in `c-api-demo.c` (#5215)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Crissman Loomis (@Crissman), Cyprien Ricque (@Cyprien-Ricque), Evan Kepner (@EvanKepner), K.O. (@Hi-king), KaiJin Ji (@KerryJi), Peter Badida (@KeyWeeUsr), Kodi Arfer (@Kodiologist), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Jacob Kim (@TheJacobKim), Vibhu Jawa (@VibhuJawa), Marcos (@astrowonk), Andy Adinets (@canonizer), Chen Qin (@chenqin), Christopher Cowden (@cowden), @cpfarrell, @david-cortes, Liangcai Li (@firestarman), @fuhaoda, Philip Hyunsu Cho (@hcho3), @here-nagini, Tong He (@hetong007), Michal Kurka (@michalkurka), Honza Sterba (@honzasterba), @iblumin, @koertkuipers, mattn (@mattn), Mingjie Tang (@merlintang), OrdoAbChao (@mglowacki100), Matthew Jones (@mt-jones), mitama (@nigimitama), Nathan Moore (@nmoorenz), Daniel Stahl (@phillyfan1138), Micha\u00ebl Benesty (@pommedeterresautee), Rong Ou (@rongou), Sebastian (@sfahnens), Xu Xiao (@sperlingxx), @sriramch, Sean Owen (@srowen), Stephanie Yang (@stpyang), Yuan Tang (@terrytangyuan), Mathew Wicks (@thesuperzapper), Tim Gates (@timgates42), TinkleG (@tinkle1129), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Matvey Turkov (@turk0v), Bobby Wang (@wbo4958), yage (@yage99), @yellowdolphin\n\n**Reviewers**: Nan Zhu (@CodingCat), Crissman Loomis (@Crissman), Cyprien Ricque (@Cyprien-Ricque), Evan Kepner (@EvanKepner), John Zedlewski (@JohnZed), KOLANICH (@KOLANICH), KaiJin Ji (@KerryJi), Kodi Arfer (@Kodiologist), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Nikita Titov (@StrikerRUS), Jacob Kim (@TheJacobKim), Vibhu Jawa (@VibhuJawa), Andrew Kane (@ankane), Arno Candel (@arnocandel), Marcos (@astrowonk), Bryan Woods (@bryan-woods), Andy Adinets (@canonizer), Chen Qin (@chenqin), Thomas Franke (@coding-komek), Peter  (@codingforfun), @cpfarrell, Joshua Patterson (@datametrician), @fuhaoda, Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Honza Sterba (@honzasterba), @iblumin, @jakirkham, Vadim Khotilovich (@khotilov), Keith Kraus (@kkraus14), @koertkuipers, @melonki, Mingjie Tang (@merlintang), OrdoAbChao (@mglowacki100), Daniel Mahler (@mhlr), Matthew Rocklin (@mrocklin), Matthew Jones (@mt-jones), Micha\u00ebl Benesty (@pommedeterresautee), PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Rong Ou (@rongou), Vladimir (@sh1ng), Scott Lundberg (@slundberg), Xu Xiao (@sperlingxx), @sriramch, Pasha Stetsenko (@st-pasha), Stephanie Yang (@stpyang), Yuan Tang (@terrytangyuan), Mathew Wicks (@thesuperzapper), Theodore Vasiloudis (@thvasilo), TinkleG (@tinkle1129), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), yage (@yage99), @yellowdolphin, Yin Lou (@yinlou)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* [doc] Some notes for external memory. (#5065)\n* Update document for `tree_method` (#5106)\n* Update demo for ranking. (#5154)\n* Add new lines for Spark XGBoost missing values section (#5180)\n* Fix simple typo: utilty -> utility (#5182)\n* Update R doc by roxygen2 (#5201)\n* [R] Direct user to use `set.seed()` instead of setting `seed` parameter (#5125)\n* Add Optuna badge to `README.md` (#5208)\n* Fix compilation error in `c-api-demo.c` (#5215)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Crissman Loomis (@Crissman), Cyprien Ricque (@Cyprien-Ricque), Evan Kepner (@EvanKepner), K.O. (@Hi-king), KaiJin Ji (@KerryJi), Peter Badida (@KeyWeeUsr), Kodi Arfer (@Kodiologist), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Jacob Kim (@TheJacobKim), Vibhu Jawa (@VibhuJawa), Marcos (@astrowonk), Andy Adinets (@canonizer), Chen Qin (@chenqin), Christopher Cowden (@cowden), @cpfarrell, @david-cortes, Liangcai Li (@firestarman), @fuhaoda, Philip Hyunsu Cho (@hcho3), @here-nagini, Tong He (@hetong007), Michal Kurka (@michalkurka), Honza Sterba (@honzasterba), @iblumin, @koertkuipers, mattn (@mattn), Mingjie Tang (@merlintang), OrdoAbChao (@mglowacki100), Matthew Jones (@mt-jones), mitama (@nigimitama), Nathan Moore (@nmoorenz), Daniel Stahl (@phillyfan1138), Micha\u00ebl Benesty (@pommedeterresautee), Rong Ou (@rongou), Sebastian (@sfahnens), Xu Xiao (@sperlingxx), @sriramch, Sean Owen (@srowen), Stephanie Yang (@stpyang), Yuan Tang (@terrytangyuan), Mathew Wicks (@thesuperzapper), Tim Gates (@timgates42), TinkleG (@tinkle1129), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Matvey Turkov (@turk0v), Bobby Wang (@wbo4958), yage (@yage99), @yellowdolphin\n\n**Reviewers**: Nan Zhu (@CodingCat), Crissman Loomis (@Crissman), Cyprien Ricque (@Cyprien-Ricque), Evan Kepner (@EvanKepner), John Zedlewski (@JohnZed), KOLANICH (@KOLANICH), KaiJin Ji (@KerryJi), Kodi Arfer (@Kodiologist), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Nikita Titov (@StrikerRUS), Jacob Kim (@TheJacobKim), Vibhu Jawa (@VibhuJawa), Andrew Kane (@ankane), Arno Candel (@arnocandel), Marcos (@astrowonk), Bryan Woods (@bryan-woods), Andy Adinets (@canonizer), Chen Qin (@chenqin), Thomas Franke (@coding-komek), Peter  (@codingforfun), @cpfarrell, Joshua Patterson (@datametrician), @fuhaoda, Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), Honza Sterba (@honzasterba), @iblumin, @jakirkham, Vadim Khotilovich (@khotilov), Keith Kraus (@kkraus14), @koertkuipers, @melonki, Mingjie Tang (@merlintang), OrdoAbChao (@mglowacki100), Daniel Mahler (@mhlr), Matthew Rocklin (@mrocklin), Matthew Jones (@mt-jones), Micha\u00ebl Benesty (@pommedeterresautee), PSEUDOTENSOR / Jonathan McKinney (@pseudotensor), Rong Ou (@rongou), Vladimir (@sh1ng), Scott Lundberg (@slundberg), Xu Xiao (@sperlingxx), @sriramch, Pasha Stetsenko (@st-pasha), Stephanie Yang (@stpyang), Yuan Tang (@terrytangyuan), Mathew Wicks (@thesuperzapper), Theodore Vasiloudis (@thvasilo), TinkleG (@tinkle1129), Oleksandr Pryimak (@trams), Jiaming Yuan (@trivialfis), Bobby Wang (@wbo4958), yage (@yage99), @yellowdolphin, Yin Lou (@yinlou)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.90 (2019.05.18)\n\n### XGBoost Python package drops Python 2.x (#4379, #4381)\nPython 2.x is reaching its end-of-life at the end of this year. [Many scientific Python packages are now moving to drop Python 2.x](https://python3statement.github.io/).\n\n### XGBoost4J-Spark now requires Spark 2.4.x (#4377)\n* Spark 2.3 is reaching its end-of-life soon. See discussion at #4389.\n* **Consistent handling of missing values** (#4309, #4349, #4411): Many users had reported issue with inconsistent predictions between XGBoost4J-Spark and the Python XGBoost package. The issue was caused by Spark mis-handling non-zero missing values (NaN, -1, 999 etc). We now alert the user whenever Spark doesn't handle missing values correctly (#4309, #4349). See [the tutorial for dealing with missing values in XGBoost4J-Spark](https://xgboost.readthedocs.io/en/release_0.90/jvm/xgboost4j_spark_tutorial.html#dealing-with-missing-values). This fix also depends on the availability of Spark 2.4.x.\n\n### Roadmap: better performance scaling for multi-core CPUs (#4310)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #4310 optimizes quantile sketches and other pre-processing tasks. Special thanks to @SmirnovEgorRu.\n\n### Roadmap: Harden distributed training (#4250)\n* Make distributed training in XGBoost more robust by hardening [Rabit](https://github.com/dmlc/rabit), which implements [the AllReduce primitive](https://en.wikipedia.org/wiki/Reduce_%28parallel_pattern%29). In particular, improve test coverage on mechanisms for fault tolerance and recovery. Special thanks to @chenqin.\n\n### New feature: Multi-class metric functions for GPUs (#4368)\n* Metrics for multi-class classification have been ported to GPU: `merror`, `mlogloss`. Special thanks to @trivialfis.\n* With supported metrics, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### New feature: Scikit-learn-like random forest API (#4148, #4255, #4258)\n* XGBoost Python package now offers `XGBRFClassifier` and `XGBRFRegressor` API to train random forests. See [the tutorial](https://xgboost.readthedocs.io/en/release_0.90/tutorials/rf.html). Special thanks to @canonizer\n\n### New feature: use external memory in GPU predictor (#4284, #4396, #4438, #4457)\n* It is now possible to make predictions on GPU when the input is read from external memory. This is useful when you want to make predictions with big dataset that does not fit into the GPU memory. Special thanks to @rongou, @canonizer, @sriramch.\n\n  ```python\n  dtest = xgboost.DMatrix('test_data.libsvm#dtest.cache')\n  bst.set_param('predictor', 'gpu_predictor')\n  bst.predict(dtest)\n  ```\n\n* Coming soon: GPU training (`gpu_hist`) with external memory\n\n### New feature: XGBoost can now handle comments in LIBSVM files (#4430)\n* Special thanks to @trivialfis and @hcho3", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.90 (2019.05.18)\n\n### XGBoost Python package drops Python 2.x (#4379, #4381)\nPython 2.x is reaching its end-of-life at the end of this year. [Many scientific Python packages are now moving to drop Python 2.x](https://python3statement.github.io/).\n\n### XGBoost4J-Spark now requires Spark 2.4.x (#4377)\n* Spark 2.3 is reaching its end-of-life soon. See discussion at #4389.\n* **Consistent handling of missing values** (#4309, #4349, #4411): Many users had reported issue with inconsistent predictions between XGBoost4J-Spark and the Python XGBoost package. The issue was caused by Spark mis-handling non-zero missing values (NaN, -1, 999 etc). We now alert the user whenever Spark doesn't handle missing values correctly (#4309, #4349). See [the tutorial for dealing with missing values in XGBoost4J-Spark](https://xgboost.readthedocs.io/en/release_0.90/jvm/xgboost4j_spark_tutorial.html#dealing-with-missing-values). This fix also depends on the availability of Spark 2.4.x.\n\n### Roadmap: better performance scaling for multi-core CPUs (#4310)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #4310 optimizes quantile sketches and other pre-processing tasks. Special thanks to @SmirnovEgorRu.\n\n### Roadmap: Harden distributed training (#4250)\n* Make distributed training in XGBoost more robust by hardening [Rabit](https://github.com/dmlc/rabit), which implements [the AllReduce primitive](https://en.wikipedia.org/wiki/Reduce_%28parallel_pattern%29). In particular, improve test coverage on mechanisms for fault tolerance and recovery. Special thanks to @chenqin.\n\n### New feature: Multi-class metric functions for GPUs (#4368)\n* Metrics for multi-class classification have been ported to GPU: `merror`, `mlogloss`. Special thanks to @trivialfis.\n* With supported metrics, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### New feature: Scikit-learn-like random forest API (#4148, #4255, #4258)\n* XGBoost Python package now offers `XGBRFClassifier` and `XGBRFRegressor` API to train random forests. See [the tutorial](https://xgboost.readthedocs.io/en/release_0.90/tutorials/rf.html). Special thanks to @canonizer\n\n### New feature: use external memory in GPU predictor (#4284, #4396, #4438, #4457)\n* It is now possible to make predictions on GPU when the input is read from external memory. This is useful when you want to make predictions with big dataset that does not fit into the GPU memory. Special thanks to @rongou, @canonizer, @sriramch.\n\n  ```python\n  dtest = xgboost.DMatrix('test_data.libsvm#dtest.cache')\n  bst.set_param('predictor', 'gpu_predictor')\n  bst.predict(dtest)\n  ```\n\n* Coming soon: GPU training (`gpu_hist`) with external memory\n\n### New feature: XGBoost can now handle comments in LIBSVM files (#4430)\n* Special thanks to @trivialfis and @hcho3"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: Embed XGBoost in your C/C++ applications using CMake (#4323, #4333, #4453)\n* It is now easier than ever to embed XGBoost in your C/C++ applications. In your CMakeLists.txt, add `xgboost::xgboost` as a linked library:\n\n  ```cmake\n  find_package(xgboost REQUIRED)\n  add_executable(api-demo c-api-demo.c)\n  target_link_libraries(api-demo xgboost::xgboost)\n  ```\n\n  [XGBoost C API documentation is available.](https://xgboost.readthedocs.io/en/release_0.90/dev) Special thanks to @trivialfis\n\n### Performance improvements\n* Use feature interaction constraints to narrow split search space (#4341, #4428)\n* Additional optimizations for `gpu_hist` (#4248, #4283)\n* Reduce OpenMP thread launches in `gpu_hist` (#4343)\n* Additional optimizations for multi-node multi-GPU random forests. (#4238)\n* Allocate unique prediction buffer for each input matrix, to avoid re-sizing GPU array (#4275)\n* Remove various synchronisations from CUDA API calls (#4205)\n* XGBoost4J-Spark\n  - Allow the user to control whether to cache partitioned training data, to potentially reduce execution time (#4268)\n\n### Bug-fixes\n* Fix node reuse in `hist` (#4404)\n* Fix GPU histogram allocation (#4347)\n* Fix matrix attributes not sliced (#4311)\n* Revise AUC and AUCPR metrics now work with weighted ranking task (#4216, #4436)\n* Fix timer invocation for InitDataOnce() in `gpu_hist` (#4206)\n* Fix R-devel errors (#4251)\n* Make gradient update in GPU linear updater thread-safe (#4259)\n* Prevent out-of-range access in column matrix (#4231)\n* Don't store DMatrix handle in Python object until it's initialized, to improve exception safety (#4317)\n* XGBoost4J-Spark\n  - Fix non-deterministic order within a zipped partition on prediction (#4388)\n  - Remove race condition on tracker shutdown (#4224)\n  - Allow set the parameter `maxLeaves`. (#4226)\n  - Allow partial evaluation of dataframe before prediction (#4407)\n  - Automatically set `maximize_evaluation_metrics` if not explicitly given (#4446)\n\n### API changes\n* Deprecate `reg:linear` in favor of `reg:squarederror`. (#4267, #4427)\n* Add attribute getter and setter to the Booster object in XGBoost4J (#4336)\n\n### Maintenance: Refactor C++ code for legibility and maintainability\n* Fix clang-tidy warnings. (#4149)\n* Remove deprecated C APIs. (#4266)\n* Use Monitor class to time functions in `hist`. (#4273)\n* Retire DVec class in favour of c++20 style span for device memory. (#4293)\n* Improve HostDeviceVector exception safety (#4301)\n\n### Maintenance: testing, continuous integration, build system\n* **Major refactor of CMakeLists.txt** (#4323, #4333, #4453): adopt modern CMake and export XGBoost as a target\n* **Major improvement in Jenkins CI pipeline** (#4234)\n  - Migrate all Linux tests to Jenkins (#4401)\n  - Builds and tests are now de-coupled, to test an artifact against multiple versions of CUDA, JDK, and other dependencies (#4401)\n  - Add Windows GPU to Jenkins CI pipeline (#4463, #4469)\n* Support CUDA 10.1 (#4223, #4232, #4265, #4468)\n* Python wheels are now built with CUDA 9.0, so that JIT is not required on Volta architecture (#4459)\n* Integrate with NVTX CUDA profiler (#4205)\n* Add a test for cpu predictor using external memory (#4308)\n* Refactor tests to get rid of duplication (#4358)\n* Remove test dependency on `craigcitro/r-travis`, since it's deprecated (#4353)\n* Add files from local R build to `.gitignore` (#4346)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: Embed XGBoost in your C/C++ applications using CMake (#4323, #4333, #4453)\n* It is now easier than ever to embed XGBoost in your C/C++ applications. In your CMakeLists.txt, add `xgboost::xgboost` as a linked library:\n\n  ```cmake\n  find_package(xgboost REQUIRED)\n  add_executable(api-demo c-api-demo.c)\n  target_link_libraries(api-demo xgboost::xgboost)\n  ```\n\n  [XGBoost C API documentation is available.](https://xgboost.readthedocs.io/en/release_0.90/dev) Special thanks to @trivialfis\n\n### Performance improvements\n* Use feature interaction constraints to narrow split search space (#4341, #4428)\n* Additional optimizations for `gpu_hist` (#4248, #4283)\n* Reduce OpenMP thread launches in `gpu_hist` (#4343)\n* Additional optimizations for multi-node multi-GPU random forests. (#4238)\n* Allocate unique prediction buffer for each input matrix, to avoid re-sizing GPU array (#4275)\n* Remove various synchronisations from CUDA API calls (#4205)\n* XGBoost4J-Spark\n  - Allow the user to control whether to cache partitioned training data, to potentially reduce execution time (#4268)\n\n### Bug-fixes\n* Fix node reuse in `hist` (#4404)\n* Fix GPU histogram allocation (#4347)\n* Fix matrix attributes not sliced (#4311)\n* Revise AUC and AUCPR metrics now work with weighted ranking task (#4216, #4436)\n* Fix timer invocation for InitDataOnce() in `gpu_hist` (#4206)\n* Fix R-devel errors (#4251)\n* Make gradient update in GPU linear updater thread-safe (#4259)\n* Prevent out-of-range access in column matrix (#4231)\n* Don't store DMatrix handle in Python object until it's initialized, to improve exception safety (#4317)\n* XGBoost4J-Spark\n  - Fix non-deterministic order within a zipped partition on prediction (#4388)\n  - Remove race condition on tracker shutdown (#4224)\n  - Allow set the parameter `maxLeaves`. (#4226)\n  - Allow partial evaluation of dataframe before prediction (#4407)\n  - Automatically set `maximize_evaluation_metrics` if not explicitly given (#4446)\n\n### API changes\n* Deprecate `reg:linear` in favor of `reg:squarederror`. (#4267, #4427)\n* Add attribute getter and setter to the Booster object in XGBoost4J (#4336)\n\n### Maintenance: Refactor C++ code for legibility and maintainability\n* Fix clang-tidy warnings. (#4149)\n* Remove deprecated C APIs. (#4266)\n* Use Monitor class to time functions in `hist`. (#4273)\n* Retire DVec class in favour of c++20 style span for device memory. (#4293)\n* Improve HostDeviceVector exception safety (#4301)\n\n### Maintenance: testing, continuous integration, build system\n* **Major refactor of CMakeLists.txt** (#4323, #4333, #4453): adopt modern CMake and export XGBoost as a target\n* **Major improvement in Jenkins CI pipeline** (#4234)\n  - Migrate all Linux tests to Jenkins (#4401)\n  - Builds and tests are now de-coupled, to test an artifact against multiple versions of CUDA, JDK, and other dependencies (#4401)\n  - Add Windows GPU to Jenkins CI pipeline (#4463, #4469)\n* Support CUDA 10.1 (#4223, #4232, #4265, #4468)\n* Python wheels are now built with CUDA 9.0, so that JIT is not required on Volta architecture (#4459)\n* Integrate with NVTX CUDA profiler (#4205)\n* Add a test for cpu predictor using external memory (#4308)\n* Refactor tests to get rid of duplication (#4358)\n* Remove test dependency on `craigcitro/r-travis`, since it's deprecated (#4353)\n* Add files from local R build to `.gitignore` (#4346)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Make XGBoost4J compatible with Java 9+ by revising NativeLibLoader (#4351)\n* Jenkins build for CUDA 10.0 (#4281)\n* Remove remaining `silent` and `debug_verbose` in Python tests (#4299)\n* Use all cores to build XGBoost4J lib on linux (#4304)\n* Upgrade Jenkins Linux build environment to GCC 5.3.1, CMake 3.6.0 (#4306)\n* Make CMakeLists.txt compatible with CMake 3.3 (#4420)\n* Add OpenMP option in CMakeLists.txt (#4339)\n* Get rid of a few trivial compiler warnings (#4312)\n* Add external Docker build cache, to speed up builds on Jenkins CI (#4331, #4334, #4458)\n* Fix Windows tests (#4403)\n* Fix a broken python test (#4395)\n* Use a fixed seed to split data in XGBoost4J-Spark tests, for reproducibility (#4417)\n* Add additional Python tests to test training under constraints (#4426)\n* Enable building with shared NCCL. (#4447)\n\n### Usability Improvements, Documentation\n* Document limitation of one-split-at-a-time Greedy tree learning heuristic (#4233)\n* Update build doc: PyPI wheel now support multi-GPU (#4219)\n* Fix docs for `num_parallel_tree` (#4221)\n* Fix document about `colsample_by*` parameter (#4340)\n* Make the train and test input with same colnames. (#4329)\n* Update R contribute link. (#4236)\n* Fix travis R tests (#4277)\n* Log version number in crash log in XGBoost4J-Spark (#4271, #4303)\n* Allow supression of Rabit output in Booster::train in XGBoost4J (#4262)\n* Add tutorial on handling missing values in XGBoost4J-Spark (#4425)\n* Fix typos (#4345, #4393, #4432, #4435)\n* Added language classifier in setup.py (#4327)\n* Added Travis CI badge (#4344)\n* Add BentoML to use case section (#4400)\n* Remove subtly sexist remark (#4418)\n* Add R vignette about parsing JSON dumps (#4439)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Daniel Hen (@Daniel8hen), Jiaxiang Li (@JiaxiangBU), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Andy Adinets (@canonizer), Jonas (@elcombato), Harry Braviner (@harrybraviner), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), James Lamb (@jameslamb), Jean-Francois Zinque (@jeffzi), Yang Yang (@jokerkeny), Mayank Suman (@mayanksuman), jess (@monkeywithacupcake), Hajime Morrita (@omo), Ravi Kalia (@project-delphi), @ras44, Rong Ou (@rongou), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), @sriramch, Jiaming Yuan (@trivialfis), Christopher Suchanek (@wsuchy), Bozhao (@yubozhao)\n\n**Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Daniel Hen (@Daniel8hen), Jiaxiang Li (@JiaxiangBU), Laurae (@Laurae2), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), @alois-bissuel, Andy Adinets (@canonizer), Chen Qin (@chenqin), Harry Braviner (@harrybraviner), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), @jakirkham, James Lamb (@jameslamb), Julien Schueller (@jschueller), Mayank Suman (@mayanksuman), Hajime Morrita (@omo), Rong Ou (@rongou), Sara Robinson (@sararob), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), @sriramch, Sean Owen (@srowen), Sergei Lebedev (@superbobry), Yuan (Terry) Tang (@terrytangyuan), Theodore Vasiloudis (@thvasilo), Matthew Tovbin (@tovbinm), Jiaming Yuan (@trivialfis), Xin Yin (@xydrolase)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Make XGBoost4J compatible with Java 9+ by revising NativeLibLoader (#4351)\n* Jenkins build for CUDA 10.0 (#4281)\n* Remove remaining `silent` and `debug_verbose` in Python tests (#4299)\n* Use all cores to build XGBoost4J lib on linux (#4304)\n* Upgrade Jenkins Linux build environment to GCC 5.3.1, CMake 3.6.0 (#4306)\n* Make CMakeLists.txt compatible with CMake 3.3 (#4420)\n* Add OpenMP option in CMakeLists.txt (#4339)\n* Get rid of a few trivial compiler warnings (#4312)\n* Add external Docker build cache, to speed up builds on Jenkins CI (#4331, #4334, #4458)\n* Fix Windows tests (#4403)\n* Fix a broken python test (#4395)\n* Use a fixed seed to split data in XGBoost4J-Spark tests, for reproducibility (#4417)\n* Add additional Python tests to test training under constraints (#4426)\n* Enable building with shared NCCL. (#4447)\n\n### Usability Improvements, Documentation\n* Document limitation of one-split-at-a-time Greedy tree learning heuristic (#4233)\n* Update build doc: PyPI wheel now support multi-GPU (#4219)\n* Fix docs for `num_parallel_tree` (#4221)\n* Fix document about `colsample_by*` parameter (#4340)\n* Make the train and test input with same colnames. (#4329)\n* Update R contribute link. (#4236)\n* Fix travis R tests (#4277)\n* Log version number in crash log in XGBoost4J-Spark (#4271, #4303)\n* Allow supression of Rabit output in Booster::train in XGBoost4J (#4262)\n* Add tutorial on handling missing values in XGBoost4J-Spark (#4425)\n* Fix typos (#4345, #4393, #4432, #4435)\n* Added language classifier in setup.py (#4327)\n* Added Travis CI badge (#4344)\n* Add BentoML to use case section (#4400)\n* Remove subtly sexist remark (#4418)\n* Add R vignette about parsing JSON dumps (#4439)\n\n### Acknowledgement\n**Contributors**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Daniel Hen (@Daniel8hen), Jiaxiang Li (@JiaxiangBU), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), Andy Adinets (@canonizer), Jonas (@elcombato), Harry Braviner (@harrybraviner), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), James Lamb (@jameslamb), Jean-Francois Zinque (@jeffzi), Yang Yang (@jokerkeny), Mayank Suman (@mayanksuman), jess (@monkeywithacupcake), Hajime Morrita (@omo), Ravi Kalia (@project-delphi), @ras44, Rong Ou (@rongou), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), @sriramch, Jiaming Yuan (@trivialfis), Christopher Suchanek (@wsuchy), Bozhao (@yubozhao)\n\n**Reviewers**: Nan Zhu (@CodingCat), Adam Pocock (@Craigacp), Daniel Hen (@Daniel8hen), Jiaxiang Li (@JiaxiangBU), Laurae (@Laurae2), Rory Mitchell (@RAMitchell), Egor Smirnov (@SmirnovEgorRu), @alois-bissuel, Andy Adinets (@canonizer), Chen Qin (@chenqin), Harry Braviner (@harrybraviner), Philip Hyunsu Cho (@hcho3), Tong He (@hetong007), @jakirkham, James Lamb (@jameslamb), Julien Schueller (@jschueller), Mayank Suman (@mayanksuman), Hajime Morrita (@omo), Rong Ou (@rongou), Sara Robinson (@sararob), Shaochen Shi (@shishaochen), Xu Xiao (@sperlingxx), @sriramch, Sean Owen (@srowen), Sergei Lebedev (@superbobry), Yuan (Terry) Tang (@terrytangyuan), Theodore Vasiloudis (@thvasilo), Matthew Tovbin (@tovbinm), Jiaming Yuan (@trivialfis), Xin Yin (@xydrolase)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.82 (2019.03.03)\nThis release is packed with many new features and bug fixes.\n\n### Roadmap: better performance scaling for multi-core CPUs (#3957)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #3957 marks an important step toward better performance scaling, by using software pre-fetching and replacing STL vectors with C-style arrays. Special thanks to @Laurae2 and @SmirnovEgorRu.\n* See #3810 for latest progress on this roadmap.\n\n### New feature: Distributed Fast Histogram Algorithm (`hist`) (#4011, #4102, #4140, #4128)\n* It is now possible to run the `hist` algorithm in distributed setting. Special thanks to @CodingCat. The benefits include:\n  1. Faster local computation via feature binning\n  2. Support for monotonic constraints and feature interaction constraints\n  3. Simpler codebase than `approx`, allowing for future improvement\n* Depth-wise tree growing is now performed in a separate code path, so that cross-node syncronization is performed only once per level.\n\n### New feature: Multi-Node, Multi-GPU training (#4095)\n* Distributed training is now able to utilize clusters equipped with NVIDIA GPUs. In particular, the rabit AllReduce layer will communicate GPU device information. Special thanks to @mt-jones, @RAMitchell, @rongou, @trivialfis, @canonizer, and @jeffdk.\n* Resource management systems will be able to assign a rank for each GPU in the cluster.\n* In Dask, users will be able to construct a collection of XGBoost processes over an inhomogeneous device cluster (i.e. workers with different number and/or kinds of GPUs).\n\n### New feature: Multiple validation datasets in XGBoost4J-Spark (#3904, #3910)\n* You can now track the performance of the model during training with multiple evaluation datasets. By specifying `eval_sets` or call `setEvalSets` over a `XGBoostClassifier` or `XGBoostRegressor`, you can pass in multiple evaluation datasets typed as a `Map` from `String` to `DataFrame`. Special thanks to @CodingCat.\n* See the usage of multiple validation datasets [here](https://github.com/dmlc/xgboost/blob/0c1d5f1120c0a159f2567b267f0ec4ffadee00d0/jvm-packages/xgboost4j-example/src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/SparkTraining.scala#L66-L78)\n\n### New feature: Additional metric functions for GPUs (#3952)\n* Element-wise metrics have been ported to GPU: `rmse`, `mae`, `logloss`, `poisson-nloglik`, `gamma-deviance`, `gamma-nloglik`, `error`, `tweedie-nloglik`. Special thanks to @trivialfis and @RAMitchell.\n* With supported metrics, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### New feature: Column sampling at individual nodes (splits) (#3971)\n* Columns (features) can now be sampled at individual tree nodes, in addition to per-tree and per-level sampling. To enable per-node sampling, set `colsample_bynode` parameter, which represents the fraction of columns sampled at each node. This parameter is set to 1.0 by default (i.e. no sampling per node). Special thanks to @canonizer.\n* The `colsample_bynode` parameter works cumulatively with other `colsample_by*` parameters: for example, `{'colsample_bynode':0.5, 'colsample_bytree':0.5}` with 100 columns will give 25 features to choose from at each split.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.82 (2019.03.03)\nThis release is packed with many new features and bug fixes.\n\n### Roadmap: better performance scaling for multi-core CPUs (#3957)\n* Poor performance scaling of the `hist` algorithm for multi-core CPUs has been under investigation (#3810). #3957 marks an important step toward better performance scaling, by using software pre-fetching and replacing STL vectors with C-style arrays. Special thanks to @Laurae2 and @SmirnovEgorRu.\n* See #3810 for latest progress on this roadmap.\n\n### New feature: Distributed Fast Histogram Algorithm (`hist`) (#4011, #4102, #4140, #4128)\n* It is now possible to run the `hist` algorithm in distributed setting. Special thanks to @CodingCat. The benefits include:\n  1. Faster local computation via feature binning\n  2. Support for monotonic constraints and feature interaction constraints\n  3. Simpler codebase than `approx`, allowing for future improvement\n* Depth-wise tree growing is now performed in a separate code path, so that cross-node syncronization is performed only once per level.\n\n### New feature: Multi-Node, Multi-GPU training (#4095)\n* Distributed training is now able to utilize clusters equipped with NVIDIA GPUs. In particular, the rabit AllReduce layer will communicate GPU device information. Special thanks to @mt-jones, @RAMitchell, @rongou, @trivialfis, @canonizer, and @jeffdk.\n* Resource management systems will be able to assign a rank for each GPU in the cluster.\n* In Dask, users will be able to construct a collection of XGBoost processes over an inhomogeneous device cluster (i.e. workers with different number and/or kinds of GPUs).\n\n### New feature: Multiple validation datasets in XGBoost4J-Spark (#3904, #3910)\n* You can now track the performance of the model during training with multiple evaluation datasets. By specifying `eval_sets` or call `setEvalSets` over a `XGBoostClassifier` or `XGBoostRegressor`, you can pass in multiple evaluation datasets typed as a `Map` from `String` to `DataFrame`. Special thanks to @CodingCat.\n* See the usage of multiple validation datasets [here](https://github.com/dmlc/xgboost/blob/0c1d5f1120c0a159f2567b267f0ec4ffadee00d0/jvm-packages/xgboost4j-example/src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/SparkTraining.scala#L66-L78)\n\n### New feature: Additional metric functions for GPUs (#3952)\n* Element-wise metrics have been ported to GPU: `rmse`, `mae`, `logloss`, `poisson-nloglik`, `gamma-deviance`, `gamma-nloglik`, `error`, `tweedie-nloglik`. Special thanks to @trivialfis and @RAMitchell.\n* With supported metrics, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### New feature: Column sampling at individual nodes (splits) (#3971)\n* Columns (features) can now be sampled at individual tree nodes, in addition to per-tree and per-level sampling. To enable per-node sampling, set `colsample_bynode` parameter, which represents the fraction of columns sampled at each node. This parameter is set to 1.0 by default (i.e. no sampling per node). Special thanks to @canonizer.\n* The `colsample_bynode` parameter works cumulatively with other `colsample_by*` parameters: for example, `{'colsample_bynode':0.5, 'colsample_bytree':0.5}` with 100 columns will give 25 features to choose from at each split."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Major API change: consistent logging level via `verbosity` (#3982, #4002, #4138)\n* XGBoost now allows fine-grained control over logging. You can set `verbosity` to 0 (silent), 1 (warning), 2 (info), and 3 (debug). This is useful for controlling the amount of logging outputs. Special thanks to @trivialfis.\n* Parameters `silent` and `debug_verbose` are now deprecated.\n* Note: Sometimes XGBoost tries to change configurations based on heuristics, which is displayed as warning message.  If there's unexpected behaviour, please try to increase value of verbosity.\n\n### Major bug fix: external memory (#4040, #4193)\n* Clarify object ownership in multi-threaded prefetcher, to avoid memory error.\n* Correctly merge two column batches (which uses [CSC layout](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_column_(CSC_or_CCS))).\n* Add unit tests for external memory.\n* Special thanks to @trivialfis and @hcho3.\n\n### Major bug fix: early stopping fixed in XGBoost4J and XGBoost4J-Spark (#3928, #4176)\n* Early stopping in XGBoost4J and XGBoost4J-Spark is now consistent with its counterpart in the Python package. Training stops if the current iteration is `earlyStoppingSteps` away from the best iteration. If there are multiple evaluation sets, only the last one is used to determinate early stop.\n* See the updated documentation [here](https://xgboost.readthedocs.io/en/release_0.82/jvm/xgboost4j_spark_tutorial.html#early-stopping)\n* Special thanks to @CodingCat, @yanboliang, and @mingyang.\n\n### Major bug fix: infrequent features should not crash distributed training (#4045)\n* For infrequently occuring features, some partitions may not get any instance. This scenario used to crash distributed training due to mal-formed ranges. The problem has now been fixed.\n* In practice, one-hot-encoded categorical variables tend to produce rare features, particularly when the cardinality is high.\n* Special thanks to @CodingCat.\n\n### Performance improvements\n* Faster, more space-efficient radix sorting in `gpu_hist` (#3895)\n* Subtraction trick in histogram calculation in `gpu_hist` (#3945)\n* More performant re-partition in XGBoost4J-Spark (#4049)\n\n### Bug-fixes\n* Fix semantics of `gpu_id` when running multiple XGBoost processes on a multi-GPU machine (#3851)\n* Fix page storage path for external memory on Windows (#3869)\n* Fix configuration setup so that DART utilizes GPU (#4024)\n* Eliminate NAN values from SHAP prediction (#3943)\n* Prevent empty quantile sketches in `hist` (#4155)\n* Enable running objectives with 0 GPU (#3878)\n* Parameters are no longer dependent on system locale (#3891, #3907)\n* Use consistent data type in the GPU coordinate descent code (#3917)\n* Remove undefined behavior in the CLI config parser on the ARM platform (#3976)\n* Initialize counters in GPU AllReduce (#3987)\n* Prevent deadlocks in GPU AllReduce (#4113)\n* Load correct values from sliced NumPy arrays (#4147, #4165)\n* Fix incorrect GPU device selection (#4161)\n* Make feature binning logic in `hist` aware of query groups when running a ranking task (#4115). For ranking task, query groups are weighted, not individual instances.\n* Generate correct C++ exception type for `LOG(FATAL)` macro (#4159)\n* Python package\n  - Python package should run on system without `PATH` environment variable (#3845)\n  - Fix `coef_` and `intercept_` signature to be compatible with `sklearn.RFECV` (#3873)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Use UTF-8 encoding in Python package README, to support non-English locale (#3867)\n  - Add AUC-PR to list of metrics to maximize for early stopping (#3936)\n  - Allow loading pickles without `self.booster` attribute, for backward compatibility (#3938, #3944)\n  - White-list DART for feature importances (#4073)\n  - Update usage of [h2oai/datatable](https://github.com/h2oai/datatable) (#4123)\n* XGBoost4J-Spark\n  - Address scalability issue in prediction (#4033)\n  - Enforce the use of per-group weights for ranking task (#4118)\n  - Fix vector size of `rawPredictionCol` in `XGBoostClassificationModel` (#3932)\n  - More robust error handling in Spark tracker (#4046, #4108)\n  - Fix return type of `setEvalSets` (#4105)\n  - Return correct value of `getMaxLeaves` (#4114)\n\n### API changes\n* Add experimental parameter `single_precision_histogram` to use single-precision histograms for the `gpu_hist` algorithm (#3965)\n* Python package\n  - Add option to select type of feature importances in the scikit-learn inferface (#3876)\n  - Add `trees_to_df()` method to dump decision trees as Pandas data frame (#4153)\n  - Add options to control node shapes in the GraphViz plotting function (#3859)\n  - Add `xgb_model` option to `XGBClassifier`, to load previously saved model (#4092)\n  - Passing lists into `DMatrix` is now deprecated (#3970)\n* XGBoost4J\n  - Support multiple feature importance features (#3801)\n\n### Maintenance: Refactor C++ code for legibility and maintainability\n* Refactor `hist` algorithm code and add unit tests (#3836)\n* Minor refactoring of split evaluator in `gpu_hist` (#3889)\n* Removed unused leaf vector field in the tree model (#3989)\n* Simplify the tree representation by combining `TreeModel` and `RegTree` classes (#3995)\n* Simplify and harden tree expansion code (#4008, #4015)\n* De-duplicate parameter classes in the linear model algorithms (#4013)\n* Robust handling of ranges with C++20 span in `gpu_exact` and `gpu_coord_descent` (#4020, #4029)\n* Simplify tree training code (#3825). Also use Span class for robust handling of ranges.\n\n### Maintenance: testing, continuous integration, build system\n* Disallow `std::regex` since it's not supported by GCC 4.8.x (#3870)\n* Add multi-GPU tests for coordinate descent algorithm for linear models (#3893, #3974)\n* Enforce naming style in Python lint (#3896)\n* Refactor Python tests (#3897, #3901): Use pytest exclusively, display full trace upon failure\n* Address `DeprecationWarning` when using Python collections (#3909)\n* Use correct group for maven site plugin (#3937)\n* Jenkins CI is now using on-demand EC2 instances exclusively, due to unreliability of Spot instances (#3948)\n* Better GPU performance logging (#3945)\n* Fix GPU tests on machines with only 1 GPU (#4053)\n* Eliminate CRAN check warnings and notes (#3988)\n* Add unit tests for tree serialization (#3989)\n* Add unit tests for tree fitting functions in `hist` (#4155)\n* Add a unit test for `gpu_exact` algorithm (#4020)\n* Correct JVM CMake GPU flag (#4071)\n* Fix failing Travis CI on Mac (#4086)\n* Speed up Jenkins by not compiling CMake (#4099)\n* Analyze C++ and CUDA code using clang-tidy, as part of Jenkins CI pipeline (#4034)\n* Fix broken R test: Install Homebrew GCC (#4142)\n* Check for empty datasets in GPU unit tests (#4151)\n* Fix Windows compilation (#4139)\n* Comply with latest convention of cpplint (#4157)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Use UTF-8 encoding in Python package README, to support non-English locale (#3867)\n  - Add AUC-PR to list of metrics to maximize for early stopping (#3936)\n  - Allow loading pickles without `self.booster` attribute, for backward compatibility (#3938, #3944)\n  - White-list DART for feature importances (#4073)\n  - Update usage of [h2oai/datatable](https://github.com/h2oai/datatable) (#4123)\n* XGBoost4J-Spark\n  - Address scalability issue in prediction (#4033)\n  - Enforce the use of per-group weights for ranking task (#4118)\n  - Fix vector size of `rawPredictionCol` in `XGBoostClassificationModel` (#3932)\n  - More robust error handling in Spark tracker (#4046, #4108)\n  - Fix return type of `setEvalSets` (#4105)\n  - Return correct value of `getMaxLeaves` (#4114)\n\n### API changes\n* Add experimental parameter `single_precision_histogram` to use single-precision histograms for the `gpu_hist` algorithm (#3965)\n* Python package\n  - Add option to select type of feature importances in the scikit-learn inferface (#3876)\n  - Add `trees_to_df()` method to dump decision trees as Pandas data frame (#4153)\n  - Add options to control node shapes in the GraphViz plotting function (#3859)\n  - Add `xgb_model` option to `XGBClassifier`, to load previously saved model (#4092)\n  - Passing lists into `DMatrix` is now deprecated (#3970)\n* XGBoost4J\n  - Support multiple feature importance features (#3801)\n\n### Maintenance: Refactor C++ code for legibility and maintainability\n* Refactor `hist` algorithm code and add unit tests (#3836)\n* Minor refactoring of split evaluator in `gpu_hist` (#3889)\n* Removed unused leaf vector field in the tree model (#3989)\n* Simplify the tree representation by combining `TreeModel` and `RegTree` classes (#3995)\n* Simplify and harden tree expansion code (#4008, #4015)\n* De-duplicate parameter classes in the linear model algorithms (#4013)\n* Robust handling of ranges with C++20 span in `gpu_exact` and `gpu_coord_descent` (#4020, #4029)\n* Simplify tree training code (#3825). Also use Span class for robust handling of ranges.\n\n### Maintenance: testing, continuous integration, build system\n* Disallow `std::regex` since it's not supported by GCC 4.8.x (#3870)\n* Add multi-GPU tests for coordinate descent algorithm for linear models (#3893, #3974)\n* Enforce naming style in Python lint (#3896)\n* Refactor Python tests (#3897, #3901): Use pytest exclusively, display full trace upon failure\n* Address `DeprecationWarning` when using Python collections (#3909)\n* Use correct group for maven site plugin (#3937)\n* Jenkins CI is now using on-demand EC2 instances exclusively, due to unreliability of Spot instances (#3948)\n* Better GPU performance logging (#3945)\n* Fix GPU tests on machines with only 1 GPU (#4053)\n* Eliminate CRAN check warnings and notes (#3988)\n* Add unit tests for tree serialization (#3989)\n* Add unit tests for tree fitting functions in `hist` (#4155)\n* Add a unit test for `gpu_exact` algorithm (#4020)\n* Correct JVM CMake GPU flag (#4071)\n* Fix failing Travis CI on Mac (#4086)\n* Speed up Jenkins by not compiling CMake (#4099)\n* Analyze C++ and CUDA code using clang-tidy, as part of Jenkins CI pipeline (#4034)\n* Fix broken R test: Install Homebrew GCC (#4142)\n* Check for empty datasets in GPU unit tests (#4151)\n* Fix Windows compilation (#4139)\n* Comply with latest convention of cpplint (#4157)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fix a unit test in `gpu_hist` (#4158)\n* Speed up data generation in Python tests (#4164)\n\n### Usability Improvements\n* Add link to [InfoWorld 2019 Technology of the Year Award](https://www.infoworld.com/article/3336072/application-development/infoworlds-2019-technology-of-the-year-award-winners.html) (#4116)\n* Remove outdated AWS YARN tutorial (#3885)\n* Document current limitation in number of features (#3886)\n* Remove unnecessary warning when `gblinear` is selected (#3888)\n* Document limitation of CSV parser: header not supported (#3934)\n* Log training parameters in XGBoost4J-Spark (#4091)\n* Clarify early stopping behavior in the scikit-learn interface (#3967)\n* Clarify behavior of `max_depth` parameter (#4078)\n* Revise Python docstrings for ranking task (#4121). In particular, weights must be per-group in learning-to-rank setting.\n* Document parameter `num_parallel_tree` (#4022)\n* Add Jenkins status badge (#4090)\n* Warn users against using internal functions of `Booster` object (#4066)\n* Reformat `benchmark_tree.py` to comply with Python style convention (#4126)\n* Clarify a comment in `objectiveTrait` (#4174)\n* Fix typos and broken links in documentation (#3890, #3872, #3902, #3919, #3975, #4027, #4156, #4167)\n\n### Acknowledgement\n**Contributors** (in no particular order): Jiaming Yuan (@trivialfis), Hyunsu Cho (@hcho3), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Yanbo Liang (@yanboliang), Andy Adinets (@canonizer), Tong He (@hetong007), Yuan Tang (@terrytangyuan)\n\n**First-time Contributors** (in no particular order): Jelle Zijlstra (@JelleZijlstra), Jiacheng Xu (@jiachengxu), @ajing, Kashif Rasul (@kashif), @theycallhimavi, Joey Gao (@pjgao), Prabakaran Kumaresshan (@nixphix), Huafeng Wang (@huafengw), @lyxthe, Sam Wilkinson (@scwilkinson), Tatsuhito Kato (@stabacov), Shayak Banerjee (@shayakbanerjee), Kodi Arfer (@Kodiologist), @KyleLi1985, Egor Smirnov (@SmirnovEgorRu), @tmitanitky, Pasha Stetsenko (@st-pasha), Kenichi Nagahara (@keni-chi), Abhai Kollara Dilip (@abhaikollara), Patrick Ford (@pford221), @hshujuan, Matthew Jones (@mt-jones), Thejaswi Rao (@teju85), Adam November (@anovember)\n\n**First-time Reviewers** (in no particular order): Mingyang Hu (@mingyang), Theodore Vasiloudis (@thvasilo), Jakub Troszok (@troszok), Rong Ou (@rongou), @Denisevi4, Matthew Jones (@mt-jones), Jeff Kaplan (@jeffdk)\n\n## v0.81 (2018.11.04)\n### New feature: feature interaction constraints\n* Users are now able to control which features (independent variables) are allowed to interact by specifying feature interaction constraints (#3466).\n* [Tutorial](https://xgboost.readthedocs.io/en/release_0.81/tutorials/feature_interaction_constraint.html) is available, as well as [R](https://github.com/dmlc/xgboost/blob/9254c58e4dfff6a59dc0829a2ceb02e45ed17cd0/R-package/demo/interaction_constraints.R) and [Python](https://github.com/dmlc/xgboost/blob/9254c58e4dfff6a59dc0829a2ceb02e45ed17cd0/tests/python/test_interaction_constraints.py) examples.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* Fix a unit test in `gpu_hist` (#4158)\n* Speed up data generation in Python tests (#4164)\n\n### Usability Improvements\n* Add link to [InfoWorld 2019 Technology of the Year Award](https://www.infoworld.com/article/3336072/application-development/infoworlds-2019-technology-of-the-year-award-winners.html) (#4116)\n* Remove outdated AWS YARN tutorial (#3885)\n* Document current limitation in number of features (#3886)\n* Remove unnecessary warning when `gblinear` is selected (#3888)\n* Document limitation of CSV parser: header not supported (#3934)\n* Log training parameters in XGBoost4J-Spark (#4091)\n* Clarify early stopping behavior in the scikit-learn interface (#3967)\n* Clarify behavior of `max_depth` parameter (#4078)\n* Revise Python docstrings for ranking task (#4121). In particular, weights must be per-group in learning-to-rank setting.\n* Document parameter `num_parallel_tree` (#4022)\n* Add Jenkins status badge (#4090)\n* Warn users against using internal functions of `Booster` object (#4066)\n* Reformat `benchmark_tree.py` to comply with Python style convention (#4126)\n* Clarify a comment in `objectiveTrait` (#4174)\n* Fix typos and broken links in documentation (#3890, #3872, #3902, #3919, #3975, #4027, #4156, #4167)\n\n### Acknowledgement\n**Contributors** (in no particular order): Jiaming Yuan (@trivialfis), Hyunsu Cho (@hcho3), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Yanbo Liang (@yanboliang), Andy Adinets (@canonizer), Tong He (@hetong007), Yuan Tang (@terrytangyuan)\n\n**First-time Contributors** (in no particular order): Jelle Zijlstra (@JelleZijlstra), Jiacheng Xu (@jiachengxu), @ajing, Kashif Rasul (@kashif), @theycallhimavi, Joey Gao (@pjgao), Prabakaran Kumaresshan (@nixphix), Huafeng Wang (@huafengw), @lyxthe, Sam Wilkinson (@scwilkinson), Tatsuhito Kato (@stabacov), Shayak Banerjee (@shayakbanerjee), Kodi Arfer (@Kodiologist), @KyleLi1985, Egor Smirnov (@SmirnovEgorRu), @tmitanitky, Pasha Stetsenko (@st-pasha), Kenichi Nagahara (@keni-chi), Abhai Kollara Dilip (@abhaikollara), Patrick Ford (@pford221), @hshujuan, Matthew Jones (@mt-jones), Thejaswi Rao (@teju85), Adam November (@anovember)\n\n**First-time Reviewers** (in no particular order): Mingyang Hu (@mingyang), Theodore Vasiloudis (@thvasilo), Jakub Troszok (@troszok), Rong Ou (@rongou), @Denisevi4, Matthew Jones (@mt-jones), Jeff Kaplan (@jeffdk)\n\n## v0.81 (2018.11.04)\n### New feature: feature interaction constraints\n* Users are now able to control which features (independent variables) are allowed to interact by specifying feature interaction constraints (#3466).\n* [Tutorial](https://xgboost.readthedocs.io/en/release_0.81/tutorials/feature_interaction_constraint.html) is available, as well as [R](https://github.com/dmlc/xgboost/blob/9254c58e4dfff6a59dc0829a2ceb02e45ed17cd0/R-package/demo/interaction_constraints.R) and [Python](https://github.com/dmlc/xgboost/blob/9254c58e4dfff6a59dc0829a2ceb02e45ed17cd0/tests/python/test_interaction_constraints.py) examples."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker \"thundering herd\" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `\"reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn't let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### New feature: learning to rank using scikit-learn interface\n* Learning to rank task is now available for the scikit-learn interface of the Python package (#3560, #3848). It is now possible to integrate the XGBoost ranking model into the scikit-learn learning pipeline.\n* Examples of using `XGBRanker` class is found at [demo/rank/rank_sklearn.py](https://github.com/dmlc/xgboost/blob/24a268a2e3cb17302db3d72da8f04016b7d352d9/demo/rank/rank_sklearn.py).\n\n### New feature: R interface for SHAP interactions\n* SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. Previously, this feature was only available from the Python package; now it is available from the R package as well (#3636).\n\n### New feature: GPU predictor now use multiple GPUs to predict\n* GPU predictor is now able to utilize multiple GPUs at once to accelerate prediction (#3738)\n\n### New feature: Scale distributed XGBoost to large-scale clusters\n* Fix OS file descriptor limit assertion error on large cluster (#3835, dmlc/rabit#73) by replacing `select()` based AllReduce/Broadcast with `poll()` based implementation.\n* Mitigate tracker \"thundering herd\" issue on large cluster. Add exponential backoff retry when workers connect to tracker.\n* With this change, we were able to scale to 1.5k executors on a 12 billion row dataset after some tweaks here and there.\n\n### New feature: Additional objective functions for GPUs\n* New objective functions ported to GPU: `hinge`, `multi:softmax`, `multi:softprob`, `count:poisson`, `reg:gamma`, `\"reg:tweedie`.\n* With supported objectives, XGBoost will select the correct devices based on your system and `n_gpus` parameter.\n\n### Major bug fix: learning to rank with XGBoost4J-Spark\n* Previously, `repartitionForData` would shuffle data and lose ordering necessary for ranking task.\n* To fix this issue, data points within each RDD partition is explicitly group by their group (query session) IDs (#3654). Also handle empty RDD partition carefully (#3750).\n\n### Major bug fix: early stopping fixed in XGBoost4J-Spark\n* Earlier implementation of early stopping had incorrect semantics and didn't let users to specify direction for optimizing (maximize / minimize)\n* A parameter `maximize_evaluation_metrics` is defined so as to tell whether a metric should be maximized or minimized as part of early stopping criteria (#3808). Also early stopping now has correct semantics.\n\n### API changes\n* Column sampling by level (`colsample_bylevel`) is now functional for `hist` algorithm (#3635, #3862)\n* GPU tag `gpu:` for regression objectives are now deprecated. XGBoost will select the correct devices automatically (#3643)\n* Add `disable_default_eval_metric` parameter to disable default metric (#3606)\n* Experimental AVX support for gradient computation is removed (#3752)\n* XGBoost4J-Spark\n  - Add `rank:ndcg` and `rank:map` to supported objectives (#3697)\n* Python package\n  - Add `callbacks` argument to `fit()` function of sciki-learn API (#3682)\n  - Add `XGBRanker` to scikit-learn interface (#3560, #3848)\n  - Add `validate_features` argument to `predict()` function of scikit-learn API (#3653)\n  - Allow scikit-learn grid search over parameters specified as keyword arguments (#3791)\n  - Add `coef_` and `intercept_` as properties of scikit-learn wrapper (#3855). Some scikit-learn functions expect these properties."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Address very high GPU memory usage for large data (#3635)\n* Fix performance regression within `EvaluateSplits()` of `gpu_hist` algorithm. (#3680)\n\n### Bug-fixes\n* Fix a problem in GPU quantile sketch with tiny instance weights. (#3628)\n* Fix copy constructor for `HostDeviceVectorImpl` to prevent dangling pointers (#3657)\n* Fix a bug in partitioned file loading (#3673)\n* Fixed an uninitialized pointer in `gpu_hist` (#3703)\n* Reshared data among GPUs when number of GPUs is changed (#3721)\n* Add back `max_delta_step` to split evaluation (#3668)\n* Do not round up integer thresholds for integer features in JSON dump (#3717)\n* Use `dmlc::TemporaryDirectory` to handle temporaries in cross-platform way (#3783)\n* Fix accuracy problem with `gpu_hist` when `min_child_weight` and `lambda` are set to 0 (#3793)\n* Make sure that `tree_method` parameter is recognized and not silently ignored (#3849)\n* XGBoost4J-Spark\n  - Make sure `thresholds` are considered when executing `predict()` method (#3577)\n  - Avoid losing precision when computing probabilities by converting to `Double` early (#3576)\n  - `getTreeLimit()` should return `Int` (#3602)\n  - Fix checkpoint serialization on HDFS (#3614)\n  - Throw `ControlThrowable` instead of `InterruptedException` so that it is properly re-thrown (#3632)\n  - Remove extraneous output to stdout (#3665)\n  - Allow specification of task type for custom objectives and evaluations (#3646)\n  - Fix distributed updater check (#3739)\n  - Fix issue when spark job execution thread cannot return before we execute `first()` (#3758)\n* Python package\n  - Fix accessing `DMatrix.handle` before it is set (#3599)\n  - `XGBClassifier.predict()` should return margin scores when `output_margin` is set to true (#3651)\n  - Early stopping callback should maximize metric of form `NDCG@n-` (#3685)\n  - Preserve feature names when slicing `DMatrix` (#3766)\n* R package\n  - Replace `nround` with `nrounds` to match actual parameter (#3592)\n  - Amend `xgb.createFolds` to handle classes of a single element (#3630)\n  - Fix buggy random generator and make `colsample_bytree` functional (#3781)\n\n### Maintenance: testing, continuous integration, build system\n* Add sanitizers tests to Travis CI (#3557)\n* Add NumPy, Matplotlib, Graphviz as requirements for doc build (#3669)\n* Comply with CRAN submission policy (#3660, #3728)\n* Remove copy-paste error in JVM test suite (#3692)\n* Disable flaky tests in `R-package/tests/testthat/test_update.R` (#3723)\n* Make Python tests compatible with scikit-learn 0.20 release (#3731)\n* Separate out restricted and unrestricted tasks, so that pull requests don't build downloadable artifacts (#3736)\n* Add multi-GPU unit test environment (#3741)\n* Allow plug-ins to be built by CMake (#3752)\n* Test wheel compatibility on CPU containers for pull requests (#3762)\n* Fix broken doc build due to Matplotlib 3.0 release (#3764)\n* Produce `xgboost.so` for XGBoost-R on Mac OSX, so that `make install` works (#3767)\n* Retry Jenkins CI tests up to 3 times to improve reliability (#3769, #3769, #3775, #3776, #3777)\n* Add basic unit tests for `gpu_hist` algorithm (#3785)\n* Fix Python environment for distributed unit tests (#3806)\n* Test wheels on CUDA 10.0 container for compatibility (#3838)\n* Fix JVM doc build (#3853)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Performance improvements\n* Address very high GPU memory usage for large data (#3635)\n* Fix performance regression within `EvaluateSplits()` of `gpu_hist` algorithm. (#3680)\n\n### Bug-fixes\n* Fix a problem in GPU quantile sketch with tiny instance weights. (#3628)\n* Fix copy constructor for `HostDeviceVectorImpl` to prevent dangling pointers (#3657)\n* Fix a bug in partitioned file loading (#3673)\n* Fixed an uninitialized pointer in `gpu_hist` (#3703)\n* Reshared data among GPUs when number of GPUs is changed (#3721)\n* Add back `max_delta_step` to split evaluation (#3668)\n* Do not round up integer thresholds for integer features in JSON dump (#3717)\n* Use `dmlc::TemporaryDirectory` to handle temporaries in cross-platform way (#3783)\n* Fix accuracy problem with `gpu_hist` when `min_child_weight` and `lambda` are set to 0 (#3793)\n* Make sure that `tree_method` parameter is recognized and not silently ignored (#3849)\n* XGBoost4J-Spark\n  - Make sure `thresholds` are considered when executing `predict()` method (#3577)\n  - Avoid losing precision when computing probabilities by converting to `Double` early (#3576)\n  - `getTreeLimit()` should return `Int` (#3602)\n  - Fix checkpoint serialization on HDFS (#3614)\n  - Throw `ControlThrowable` instead of `InterruptedException` so that it is properly re-thrown (#3632)\n  - Remove extraneous output to stdout (#3665)\n  - Allow specification of task type for custom objectives and evaluations (#3646)\n  - Fix distributed updater check (#3739)\n  - Fix issue when spark job execution thread cannot return before we execute `first()` (#3758)\n* Python package\n  - Fix accessing `DMatrix.handle` before it is set (#3599)\n  - `XGBClassifier.predict()` should return margin scores when `output_margin` is set to true (#3651)\n  - Early stopping callback should maximize metric of form `NDCG@n-` (#3685)\n  - Preserve feature names when slicing `DMatrix` (#3766)\n* R package\n  - Replace `nround` with `nrounds` to match actual parameter (#3592)\n  - Amend `xgb.createFolds` to handle classes of a single element (#3630)\n  - Fix buggy random generator and make `colsample_bytree` functional (#3781)\n\n### Maintenance: testing, continuous integration, build system\n* Add sanitizers tests to Travis CI (#3557)\n* Add NumPy, Matplotlib, Graphviz as requirements for doc build (#3669)\n* Comply with CRAN submission policy (#3660, #3728)\n* Remove copy-paste error in JVM test suite (#3692)\n* Disable flaky tests in `R-package/tests/testthat/test_update.R` (#3723)\n* Make Python tests compatible with scikit-learn 0.20 release (#3731)\n* Separate out restricted and unrestricted tasks, so that pull requests don't build downloadable artifacts (#3736)\n* Add multi-GPU unit test environment (#3741)\n* Allow plug-ins to be built by CMake (#3752)\n* Test wheel compatibility on CPU containers for pull requests (#3762)\n* Fix broken doc build due to Matplotlib 3.0 release (#3764)\n* Produce `xgboost.so` for XGBoost-R on Mac OSX, so that `make install` works (#3767)\n* Retry Jenkins CI tests up to 3 times to improve reliability (#3769, #3769, #3775, #3776, #3777)\n* Add basic unit tests for `gpu_hist` algorithm (#3785)\n* Fix Python environment for distributed unit tests (#3806)\n* Test wheels on CUDA 10.0 container for compatibility (#3838)\n* Fix JVM doc build (#3853)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance: Refactor C++ code for legibility and maintainability\n* Merge generic device helper functions into `GPUSet` class (#3626)\n* Re-factor column sampling logic into `ColumnSampler` class (#3635, #3637)\n* Replace `std::vector` with `HostDeviceVector` in `MetaInfo` and `SparsePage` (#3446)\n* Simplify `DMatrix` class (#3395)\n* De-duplicate CPU/GPU code using `Transform` class (#3643, #3751)\n* Remove obsoleted `QuantileHistMaker` class (#3761)\n* Remove obsoleted `NoConstraint` class (#3792)\n\n### Other Features\n* C++20-compliant Span class for safe pointer indexing (#3548, #3588)\n* Add helper functions to manipulate multiple GPU devices (#3693)\n* XGBoost4J-Spark\n  - Allow specifying host ip from the `xgboost-tracker.properties file` (#3833). This comes in handy when `hosts` files doesn't correctly define localhost.\n\n### Usability Improvements\n* Add reference to GitHub repository in `pom.xml` of JVM packages (#3589)\n* Add R demo of multi-class classification (#3695)\n* Document JSON dump functionality (#3600, #3603)\n* Document CUDA requirement and lack of external memory for GPU algorithms (#3624)\n* Document LambdaMART objectives, both pairwise and listwise (#3672)\n* Document `aucpr` evaluation metric (#3687)\n* Document gblinear parameters: `feature_selector` and `top_k` (#3780)\n* Add instructions for using MinGW-built XGBoost with Python. (#3774)\n* Removed nonexistent parameter `use_buffer` from documentation (#3610)\n* Update Python API doc to include all classes and members (#3619, #3682)\n* Fix typos and broken links in documentation (#3618, #3640, #3676, #3713, #3759, #3784, #3843, #3852)\n* Binary classification demo should produce LIBSVM with 0-based indexing (#3652)\n* Process data once for Python and CLI examples of learning to rank (#3666)\n* Include full text of Apache 2.0 license in the repository (#3698)\n* Save predictor parameters in model file (#3856)\n* JVM packages\n  - Let users specify feature names when calling `getModelDump` and `getFeatureScore` (#3733)\n  - Warn the user about the lack of over-the-wire encryption (#3667)\n  - Fix errors in examples (#3719)\n  - Document choice of trackers (#3831)\n  - Document that vanilla Apache Spark is required (#3854)\n* Python package\n  - Document that custom objective can't contain colon (:) (#3601)\n  - Show a better error message for failed library loading (#3690)\n  - Document that feature importance is unavailable for non-tree learners (#3765)\n  - Document behavior of `get_fscore()` for zero-importance features (#3763)\n  - Recommend pickling as the way to save `XGBClassifier` / `XGBRegressor` / `XGBRanker` (#3829)\n* R package\n  - Enlarge variable importance plot to make it more visible (#3820)\n\n### BREAKING CHANGES\n* External memory page files have changed, breaking backwards compatibility for temporary storage used during external memory training. This only affects external memory users upgrading their xgboost version - we recommend clearing all `*.page` files before resuming training. Model serialization is unaffected.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Maintenance: Refactor C++ code for legibility and maintainability\n* Merge generic device helper functions into `GPUSet` class (#3626)\n* Re-factor column sampling logic into `ColumnSampler` class (#3635, #3637)\n* Replace `std::vector` with `HostDeviceVector` in `MetaInfo` and `SparsePage` (#3446)\n* Simplify `DMatrix` class (#3395)\n* De-duplicate CPU/GPU code using `Transform` class (#3643, #3751)\n* Remove obsoleted `QuantileHistMaker` class (#3761)\n* Remove obsoleted `NoConstraint` class (#3792)\n\n### Other Features\n* C++20-compliant Span class for safe pointer indexing (#3548, #3588)\n* Add helper functions to manipulate multiple GPU devices (#3693)\n* XGBoost4J-Spark\n  - Allow specifying host ip from the `xgboost-tracker.properties file` (#3833). This comes in handy when `hosts` files doesn't correctly define localhost.\n\n### Usability Improvements\n* Add reference to GitHub repository in `pom.xml` of JVM packages (#3589)\n* Add R demo of multi-class classification (#3695)\n* Document JSON dump functionality (#3600, #3603)\n* Document CUDA requirement and lack of external memory for GPU algorithms (#3624)\n* Document LambdaMART objectives, both pairwise and listwise (#3672)\n* Document `aucpr` evaluation metric (#3687)\n* Document gblinear parameters: `feature_selector` and `top_k` (#3780)\n* Add instructions for using MinGW-built XGBoost with Python. (#3774)\n* Removed nonexistent parameter `use_buffer` from documentation (#3610)\n* Update Python API doc to include all classes and members (#3619, #3682)\n* Fix typos and broken links in documentation (#3618, #3640, #3676, #3713, #3759, #3784, #3843, #3852)\n* Binary classification demo should produce LIBSVM with 0-based indexing (#3652)\n* Process data once for Python and CLI examples of learning to rank (#3666)\n* Include full text of Apache 2.0 license in the repository (#3698)\n* Save predictor parameters in model file (#3856)\n* JVM packages\n  - Let users specify feature names when calling `getModelDump` and `getFeatureScore` (#3733)\n  - Warn the user about the lack of over-the-wire encryption (#3667)\n  - Fix errors in examples (#3719)\n  - Document choice of trackers (#3831)\n  - Document that vanilla Apache Spark is required (#3854)\n* Python package\n  - Document that custom objective can't contain colon (:) (#3601)\n  - Show a better error message for failed library loading (#3690)\n  - Document that feature importance is unavailable for non-tree learners (#3765)\n  - Document behavior of `get_fscore()` for zero-importance features (#3763)\n  - Recommend pickling as the way to save `XGBClassifier` / `XGBRegressor` / `XGBRanker` (#3829)\n* R package\n  - Enlarge variable importance plot to make it more visible (#3820)\n\n### BREAKING CHANGES\n* External memory page files have changed, breaking backwards compatibility for temporary storage used during external memory training. This only affects external memory users upgrading their xgboost version - we recommend clearing all `*.page` files before resuming training. Model serialization is unaffected."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n### Known issues\n* Quantile sketcher fails to produce any quantile for some edge cases (#2943)\n* The `hist` algorithm leaks memory when used with learning rate decay callback (#3579)\n* Using custom evaluation function together with early stopping causes assertion failure in XGBoost4J-Spark (#3595)\n* Early stopping doesn't work with `gblinear` learner (#3789)\n* Label and weight vectors are not reshared upon the change in number of GPUs (#3794). To get around this issue, delete the `DMatrix` object and re-load.\n* The `DMatrix` Python objects are initialized with incorrect values when given array slices (#3841)\n* The `gpu_id` parameter is broken and not yet properly supported (#3850)\n\n### Acknowledgement\n**Contributors** (in no particular order): Hyunsu Cho (@hcho3), Jiaming Yuan (@trivialfis), Nan Zhu (@CodingCat), Rory Mitchell (@RAMitchell), Andy Adinets (@canonizer), Vadim Khotilovich (@khotilov), Sergei Lebedev (@superbobry)\n\n**First-time Contributors** (in no particular order): Matthew Tovbin (@tovbinm), Jakob Richter (@jakob-r), Grace Lam (@grace-lam), Grant W Schneider (@grantschneider), Andrew Thia (@BlueTea88), Sergei Chipiga (@schipiga), Joseph Bradley (@jkbradley), Chen Qin (@chenqin), Jerry Lin (@linjer), Dmitriy Rybalko (@rdtft), Michael Mui (@mmui), Takahiro Kojima (@515hikaru), Bruce Zhao (@BruceZhaoR), Wei Tian (@weitian), Saumya Bhatnagar (@Sam1301), Juzer Shakir (@JuzerShakir), Zhao Hang (@cleghom), Jonathan Friedman (@jontonsoup), Bruno Tremblay (@meztez), Boris Filippov (@frenzykryger), @Shiki-H, @mrgutkun, @gorogm, @htgeis, @jakehoare, @zengxy, @KOLANICH\n\n**First-time Reviewers** (in no particular order): Nikita Titov (@StrikerRUS), Xiangrui Meng (@mengxr), Nirmal Borah (@Nirmal-Neel)\n\n\n## v0.80 (2018.08.13)\n* **JVM packages received a major upgrade**: To consolidate the APIs and improve the user experience, we refactored the design of XGBoost4J-Spark in a significant manner. (#3387)\n  - Consolidated APIs: It is now much easier to integrate XGBoost models into a Spark ML pipeline. Users can control behaviors like output leaf prediction results by setting corresponding column names. Training is now more consistent with other Estimators in Spark MLLIB: there is now one single method `fit()` to train decision trees.\n  - Better user experience: we refactored the parameters relevant modules in XGBoost4J-Spark to provide both camel-case (Spark ML style) and underscore (XGBoost style) parameters\n  - A brand-new tutorial is [available](https://xgboost.readthedocs.io/en/release_0.80/jvm/xgboost4j_spark_tutorial.html) for XGBoost4J-Spark.\n  - Latest API documentation is now hosted at https://xgboost.readthedocs.io/.\n* XGBoost documentation now keeps track of multiple versions:\n  - Latest master: https://xgboost.readthedocs.io/en/latest\n  - 0.80 stable: https://xgboost.readthedocs.io/en/release_0.80\n  - 0.72 stable: https://xgboost.readthedocs.io/en/release_0.72\n* Support for per-group weights in ranking objective (#3379)\n* Fix inaccurate decimal parsing (#3546)\n* New functionality\n  - Query ID column support in LIBSVM data files (#2749). This is convenient for performing ranking task in distributed setting.\n  - Hinge loss for binary classification (`binary:hinge`) (#3477)\n  - Ability to specify delimiter and instance weight column for CSV files (#3546)\n  - Ability to use 1-based indexing instead of 0-based (#3546)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* GPU support\n  - Quantile sketch, binning, and index compression are now performed on GPU, eliminating PCIe transfer for 'gpu_hist' algorithm (#3319, #3393)\n  - Upgrade to NCCL2 for multi-GPU training (#3404).\n  - Use shared memory atomics for faster training (#3384).\n  - Dynamically allocate GPU memory, to prevent large allocations for deep trees (#3519)\n  - Fix memory copy bug for large files (#3472)\n* Python package\n  - Importing data from Python datatable (#3272)\n  - Pre-built binary wheels available for 64-bit Linux and Windows (#3424, #3443)\n  - Add new importance measures 'total_gain', 'total_cover' (#3498)\n  - Sklearn API now supports saving and loading models (#3192)\n  - Arbitrary cross validation fold indices (#3353)\n  - `predict()` function in Sklearn API uses `best_ntree_limit` if available, to make early stopping easier to use (#3445)\n  - Informational messages are now directed to Python's `print()` rather than standard output (#3438). This way, messages appear inside Jupyter notebooks.\n* R package\n  - Oracle Solaris support, per CRAN policy (#3372)\n* JVM packages\n  - Single-instance prediction (#3464)\n  - Pre-built JARs are now available from Maven Central (#3401)\n  - Add NULL pointer check (#3021)\n  - Consider `spark.task.cpus` when controlling parallelism (#3530)\n  - Handle missing values in prediction (#3529)\n  - Eliminate outputs of `System.out` (#3572)\n* Refactored C++ DMatrix class for simplicity and de-duplication (#3301)\n* Refactored C++ histogram facilities (#3564)\n* Refactored constraints / regularization mechanism for split finding (#3335, #3429). Users may specify an elastic net (L2 + L1 regularization) on leaf weights as well as monotonic constraints on test nodes. The refactor will be useful for a future addition of feature interaction constraints.\n* Statically link `libstdc++` for MinGW32 (#3430)\n* Enable loading from `group`, `base_margin` and `weight` (see [here](http://xgboost.readthedocs.io/en/latest/tutorials/input_format.html#auxiliary-files-for-additional-information)) for Python, R, and JVM packages (#3431)\n* Fix model saving for `count:possion` so that `max_delta_step` doesn't get truncated (#3515)\n* Fix loading of sparse CSC matrix (#3553)\n* Fix incorrect handling of `base_score` parameter for Tweedie regression (#3295)\n\n## v0.72.1 (2018.07.08)\nThis version is only applicable for the Python package. The content is identical to that of v0.72.\n\n## v0.72 (2018.06.01)\n* Starting with this release, we plan to make a new release every two months. See #3252 for more details.\n* Fix a pathological behavior (near-zero second-order gradients) in multiclass objective (#3304)\n* Tree dumps now use high precision in storing floating-point values (#3298)\n* Submodules `rabit` and `dmlc-core` have been brought up to date, bringing bug fixes (#3330, #3221).\n* GPU support\n  - Continuous integration tests for GPU code (#3294, #3309)\n  - GPU accelerated coordinate descent algorithm (#3178)\n  - Abstract 1D vector class now works with multiple GPUs (#3287)\n  - Generate PTX code for most recent architecture (#3316)\n  - Fix a memory bug on NVIDIA K80 cards (#3293)\n  - Address performance instability for single-GPU, multi-core machines (#3324)\n* Python package\n  - FreeBSD support (#3247)\n  - Validation of feature names in `Booster.predict()` is now optional (#3323)\n* Updated Sklearn API", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n* GPU support\n  - Quantile sketch, binning, and index compression are now performed on GPU, eliminating PCIe transfer for 'gpu_hist' algorithm (#3319, #3393)\n  - Upgrade to NCCL2 for multi-GPU training (#3404).\n  - Use shared memory atomics for faster training (#3384).\n  - Dynamically allocate GPU memory, to prevent large allocations for deep trees (#3519)\n  - Fix memory copy bug for large files (#3472)\n* Python package\n  - Importing data from Python datatable (#3272)\n  - Pre-built binary wheels available for 64-bit Linux and Windows (#3424, #3443)\n  - Add new importance measures 'total_gain', 'total_cover' (#3498)\n  - Sklearn API now supports saving and loading models (#3192)\n  - Arbitrary cross validation fold indices (#3353)\n  - `predict()` function in Sklearn API uses `best_ntree_limit` if available, to make early stopping easier to use (#3445)\n  - Informational messages are now directed to Python's `print()` rather than standard output (#3438). This way, messages appear inside Jupyter notebooks.\n* R package\n  - Oracle Solaris support, per CRAN policy (#3372)\n* JVM packages\n  - Single-instance prediction (#3464)\n  - Pre-built JARs are now available from Maven Central (#3401)\n  - Add NULL pointer check (#3021)\n  - Consider `spark.task.cpus` when controlling parallelism (#3530)\n  - Handle missing values in prediction (#3529)\n  - Eliminate outputs of `System.out` (#3572)\n* Refactored C++ DMatrix class for simplicity and de-duplication (#3301)\n* Refactored C++ histogram facilities (#3564)\n* Refactored constraints / regularization mechanism for split finding (#3335, #3429). Users may specify an elastic net (L2 + L1 regularization) on leaf weights as well as monotonic constraints on test nodes. The refactor will be useful for a future addition of feature interaction constraints.\n* Statically link `libstdc++` for MinGW32 (#3430)\n* Enable loading from `group`, `base_margin` and `weight` (see [here](http://xgboost.readthedocs.io/en/latest/tutorials/input_format.html#auxiliary-files-for-additional-information)) for Python, R, and JVM packages (#3431)\n* Fix model saving for `count:possion` so that `max_delta_step` doesn't get truncated (#3515)\n* Fix loading of sparse CSC matrix (#3553)\n* Fix incorrect handling of `base_score` parameter for Tweedie regression (#3295)\n\n## v0.72.1 (2018.07.08)\nThis version is only applicable for the Python package. The content is identical to that of v0.72.\n\n## v0.72 (2018.06.01)\n* Starting with this release, we plan to make a new release every two months. See #3252 for more details.\n* Fix a pathological behavior (near-zero second-order gradients) in multiclass objective (#3304)\n* Tree dumps now use high precision in storing floating-point values (#3298)\n* Submodules `rabit` and `dmlc-core` have been brought up to date, bringing bug fixes (#3330, #3221).\n* GPU support\n  - Continuous integration tests for GPU code (#3294, #3309)\n  - GPU accelerated coordinate descent algorithm (#3178)\n  - Abstract 1D vector class now works with multiple GPUs (#3287)\n  - Generate PTX code for most recent architecture (#3316)\n  - Fix a memory bug on NVIDIA K80 cards (#3293)\n  - Address performance instability for single-GPU, multi-core machines (#3324)\n* Python package\n  - FreeBSD support (#3247)\n  - Validation of feature names in `Booster.predict()` is now optional (#3323)\n* Updated Sklearn API"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Validation sets now support instance weights (#2354)\n  - `XGBClassifier.predict_proba()` should not support `output_margin` option. (#3343) See BREAKING CHANGES below.\n* R package:\n  - Better handling of NULL in `print.xgb.Booster()` (#3338)\n  - Comply with CRAN policy by removing compiler warning suppression (#3329)\n  - Updated CRAN submission\n* JVM packages\n  - JVM packages will now use the same versioning scheme as other packages (#3253)\n  - Update Spark to 2.3 (#3254)\n  - Add scripts to cross-build and deploy artifacts (#3276, #3307)\n  - Fix a compilation error for Scala 2.10 (#3332)\n* BREAKING CHANGES\n  - `XGBClassifier.predict_proba()` no longer accepts parameter `output_margin`. The parameter makes no sense for `predict_proba()` because the method is to predict class probabilities, not raw margin scores.\n\n## v0.71 (2018.04.11)\n* This is a minor release, mainly motivated by issues concerning `pip install`, e.g. #2426, #3189, #3118, and #3194.\n  With this release, users of Linux and MacOS will be able to run `pip install` for the most part.\n* Refactored linear booster class (`gblinear`), so as to support multiple coordinate descent updaters (#3103, #3134). See BREAKING CHANGES below.\n* Fix slow training for multiclass classification with high number of classes (#3109)\n* Fix a corner case in approximate quantile sketch (#3167). Applicable for 'hist' and 'gpu_hist' algorithms\n* Fix memory leak in DMatrix (#3182)\n* New functionality\n  - Better linear booster class (#3103, #3134)\n  - Pairwise SHAP interaction effects (#3043)\n  - Cox loss (#3043)\n  - AUC-PR metric for ranking task (#3172)\n  - Monotonic constraints for 'hist' algorithm (#3085)\n* GPU support\n    - Create an abstract 1D vector class that moves data seamlessly between the main and GPU memory (#2935, #3116, #3068). This eliminates unnecessary PCIe data transfer during training time.\n  - Fix minor bugs (#3051, #3217)\n  - Fix compatibility error for CUDA 9.1 (#3218)\n* Python package:\n  - Correctly handle parameter `verbose_eval=0` (#3115)\n* R package:\n  - Eliminate segmentation fault on 32-bit Windows platform (#2994)\n* JVM packages\n  - Fix a memory bug involving double-freeing Booster objects (#3005, #3011)\n  - Handle empty partition in predict (#3014)\n  - Update docs and unify terminology (#3024)\n  - Delete cache files after job finishes (#3022)\n  - Compatibility fixes for latest Spark versions (#3062, #3093)\n* BREAKING CHANGES: Updated linear modelling algorithms. In particular L1/L2 regularisation penalties are now normalised to number of training examples. This makes the implementation consistent with sklearn/glmnet. L2 regularisation has also been removed from the intercept. To produce linear models with the old regularisation behaviour, the alpha/lambda regularisation parameters can be manually scaled by dividing them by the number of training examples.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Validation sets now support instance weights (#2354)\n  - `XGBClassifier.predict_proba()` should not support `output_margin` option. (#3343) See BREAKING CHANGES below.\n* R package:\n  - Better handling of NULL in `print.xgb.Booster()` (#3338)\n  - Comply with CRAN policy by removing compiler warning suppression (#3329)\n  - Updated CRAN submission\n* JVM packages\n  - JVM packages will now use the same versioning scheme as other packages (#3253)\n  - Update Spark to 2.3 (#3254)\n  - Add scripts to cross-build and deploy artifacts (#3276, #3307)\n  - Fix a compilation error for Scala 2.10 (#3332)\n* BREAKING CHANGES\n  - `XGBClassifier.predict_proba()` no longer accepts parameter `output_margin`. The parameter makes no sense for `predict_proba()` because the method is to predict class probabilities, not raw margin scores.\n\n## v0.71 (2018.04.11)\n* This is a minor release, mainly motivated by issues concerning `pip install`, e.g. #2426, #3189, #3118, and #3194.\n  With this release, users of Linux and MacOS will be able to run `pip install` for the most part.\n* Refactored linear booster class (`gblinear`), so as to support multiple coordinate descent updaters (#3103, #3134). See BREAKING CHANGES below.\n* Fix slow training for multiclass classification with high number of classes (#3109)\n* Fix a corner case in approximate quantile sketch (#3167). Applicable for 'hist' and 'gpu_hist' algorithms\n* Fix memory leak in DMatrix (#3182)\n* New functionality\n  - Better linear booster class (#3103, #3134)\n  - Pairwise SHAP interaction effects (#3043)\n  - Cox loss (#3043)\n  - AUC-PR metric for ranking task (#3172)\n  - Monotonic constraints for 'hist' algorithm (#3085)\n* GPU support\n    - Create an abstract 1D vector class that moves data seamlessly between the main and GPU memory (#2935, #3116, #3068). This eliminates unnecessary PCIe data transfer during training time.\n  - Fix minor bugs (#3051, #3217)\n  - Fix compatibility error for CUDA 9.1 (#3218)\n* Python package:\n  - Correctly handle parameter `verbose_eval=0` (#3115)\n* R package:\n  - Eliminate segmentation fault on 32-bit Windows platform (#2994)\n* JVM packages\n  - Fix a memory bug involving double-freeing Booster objects (#3005, #3011)\n  - Handle empty partition in predict (#3014)\n  - Update docs and unify terminology (#3024)\n  - Delete cache files after job finishes (#3022)\n  - Compatibility fixes for latest Spark versions (#3062, #3093)\n* BREAKING CHANGES: Updated linear modelling algorithms. In particular L1/L2 regularisation penalties are now normalised to number of training examples. This makes the implementation consistent with sklearn/glmnet. L2 regularisation has also been removed from the intercept. To produce linear models with the old regularisation behaviour, the alpha/lambda regularisation parameters can be manually scaled by dividing them by the number of training examples."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.7 (2017.12.30)\n* **This version represents a major change from the last release (v0.6), which was released one year and half ago.**\n* Updated Sklearn API\n  - Add compatibility layer for scikit-learn v0.18: `sklearn.cross_validation` now deprecated\n  - Updated to allow use of all XGBoost parameters via `**kwargs`.\n  - Updated `nthread` to `n_jobs` and `seed` to `random_state` (as per Sklearn convention); `nthread` and `seed` are now marked as deprecated\n  - Updated to allow choice of Booster (`gbtree`, `gblinear`, or `dart`)\n  - `XGBRegressor` now supports instance weights (specify `sample_weight` parameter)\n  - Pass `n_jobs` parameter to the `DMatrix` constructor\n  - Add `xgb_model` parameter to `fit` method, to allow continuation of training\n* Refactored gbm to allow more friendly cache strategy\n  - Specialized some prediction routine\n* Robust `DMatrix` construction from a sparse matrix\n* Faster construction of `DMatrix` from 2D NumPy matrices: elide copies, use of multiple threads\n* Automatically remove nan from input data when it is sparse.\n  - This can solve some of user reported problem of istart != hist.size\n* Fix the single-instance prediction function to obtain correct predictions\n* Minor fixes\n  - Thread local variable is upgraded so it is automatically freed at thread exit.\n  - Fix saving and loading `count::poisson` models\n  - Fix CalcDCG to use base-2 logarithm\n  - Messages are now written to stderr instead of stdout\n  - Keep built-in evaluations while using customized evaluation functions\n  - Use `bst_float` consistently to minimize type conversion\n  - Copy the base margin when slicing `DMatrix`\n  - Evaluation metrics are now saved to the model file\n  - Use `int32_t` explicitly when serializing version\n  - In distributed training, synchronize the number of features after loading a data matrix.\n* Migrate to C++11\n  - The current master version now requires C++11 enabled compiled(g++4.8 or higher)\n* Predictor interface was factored out (in a manner similar to the updater interface).\n* Makefile support for Solaris and ARM\n* Test code coverage using Codecov\n* Add CPP tests\n* Add `Dockerfile` and `Jenkinsfile` to support continuous integration for GPU code\n* New functionality\n  - Ability to adjust tree model's statistics to a new dataset without changing tree structures.\n  - Ability to extract feature contributions from individual predictions, as described in [here](http://blog.datadive.net/interpreting-random-forests/) and [here](https://arxiv.org/abs/1706.06060).\n  - Faster, histogram-based tree algorithm (`tree_method='hist'`) .\n  - GPU/CUDA accelerated tree algorithms (`tree_method='gpu_hist'` or `'gpu_exact'`), including the GPU-based predictor.\n  - Monotonic constraints: when other features are fixed, force the prediction to be monotonic increasing with respect to a certain specified feature.\n  - Faster gradient calculation using AVX SIMD\n  - Ability to export models in JSON format\n  - Support for Tweedie regression\n  - Additional dropout options for DART: binomial+1, epsilon\n  - Ability to update an existing model in-place: this is useful for many applications, such as determining feature importance\n* Python package:\n  - New parameters:\n    - `learning_rates` in `cv()`\n    - `shuffle` in `mknfold()`\n    - `max_features` and `show_values` in `plot_importance()`\n    - `sample_weight` in `XGBRegressor.fit()`", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.7 (2017.12.30)\n* **This version represents a major change from the last release (v0.6), which was released one year and half ago.**\n* Updated Sklearn API\n  - Add compatibility layer for scikit-learn v0.18: `sklearn.cross_validation` now deprecated\n  - Updated to allow use of all XGBoost parameters via `**kwargs`.\n  - Updated `nthread` to `n_jobs` and `seed` to `random_state` (as per Sklearn convention); `nthread` and `seed` are now marked as deprecated\n  - Updated to allow choice of Booster (`gbtree`, `gblinear`, or `dart`)\n  - `XGBRegressor` now supports instance weights (specify `sample_weight` parameter)\n  - Pass `n_jobs` parameter to the `DMatrix` constructor\n  - Add `xgb_model` parameter to `fit` method, to allow continuation of training\n* Refactored gbm to allow more friendly cache strategy\n  - Specialized some prediction routine\n* Robust `DMatrix` construction from a sparse matrix\n* Faster construction of `DMatrix` from 2D NumPy matrices: elide copies, use of multiple threads\n* Automatically remove nan from input data when it is sparse.\n  - This can solve some of user reported problem of istart != hist.size\n* Fix the single-instance prediction function to obtain correct predictions\n* Minor fixes\n  - Thread local variable is upgraded so it is automatically freed at thread exit.\n  - Fix saving and loading `count::poisson` models\n  - Fix CalcDCG to use base-2 logarithm\n  - Messages are now written to stderr instead of stdout\n  - Keep built-in evaluations while using customized evaluation functions\n  - Use `bst_float` consistently to minimize type conversion\n  - Copy the base margin when slicing `DMatrix`\n  - Evaluation metrics are now saved to the model file\n  - Use `int32_t` explicitly when serializing version\n  - In distributed training, synchronize the number of features after loading a data matrix.\n* Migrate to C++11\n  - The current master version now requires C++11 enabled compiled(g++4.8 or higher)\n* Predictor interface was factored out (in a manner similar to the updater interface).\n* Makefile support for Solaris and ARM\n* Test code coverage using Codecov\n* Add CPP tests\n* Add `Dockerfile` and `Jenkinsfile` to support continuous integration for GPU code\n* New functionality\n  - Ability to adjust tree model's statistics to a new dataset without changing tree structures.\n  - Ability to extract feature contributions from individual predictions, as described in [here](http://blog.datadive.net/interpreting-random-forests/) and [here](https://arxiv.org/abs/1706.06060).\n  - Faster, histogram-based tree algorithm (`tree_method='hist'`) .\n  - GPU/CUDA accelerated tree algorithms (`tree_method='gpu_hist'` or `'gpu_exact'`), including the GPU-based predictor.\n  - Monotonic constraints: when other features are fixed, force the prediction to be monotonic increasing with respect to a certain specified feature.\n  - Faster gradient calculation using AVX SIMD\n  - Ability to export models in JSON format\n  - Support for Tweedie regression\n  - Additional dropout options for DART: binomial+1, epsilon\n  - Ability to update an existing model in-place: this is useful for many applications, such as determining feature importance\n* Python package:\n  - New parameters:\n    - `learning_rates` in `cv()`\n    - `shuffle` in `mknfold()`\n    - `max_features` and `show_values` in `plot_importance()`\n    - `sample_weight` in `XGBRegressor.fit()`"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Support binary wheel builds\n  - Fix `MultiIndex` detection to support Pandas 0.21.0 and higher\n  - Support metrics and evaluation sets whose names contain `-`\n  - Support feature maps when plotting trees\n  - Compatibility fix for Python 2.6\n  - Call `print_evaluation` callback at last iteration\n  - Use appropriate integer types when calling native code, to prevent truncation and memory error\n  - Fix shared library loading on Mac OS X\n* R package:\n  - New parameters:\n    - `silent` in `xgb.DMatrix()`\n    - `use_int_id` in `xgb.model.dt.tree()`\n    - `predcontrib` in `predict()`\n    - `monotone_constraints` in `xgb.train()`\n  - Default value of the `save_period` parameter in `xgboost()` changed to NULL (consistent with `xgb.train()`).\n  - It's possible to custom-build the R package with GPU acceleration support.\n  - Enable JVM build for Mac OS X and Windows\n  - Integration with AppVeyor CI\n  - Improved safety for garbage collection\n  - Store numeric attributes with higher precision\n  - Easier installation for devel version\n  - Improved `xgb.plot.tree()`\n  - Various minor fixes to improve user experience and robustness\n  - Register native code to pass CRAN check\n  - Updated CRAN submission\n* JVM packages\n  - Add Spark pipeline persistence API\n  - Fix data persistence: loss evaluation on test data had wrongly used caches for training data.\n  - Clean external cache after training\n  - Implement early stopping\n  - Enable training of multiple models by distinguishing stage IDs\n  - Better Spark integration: support RDD / dataframe / dataset, integrate with Spark ML package\n  - XGBoost4j now supports ranking task\n  - Support training with missing data\n  - Refactor JVM package to separate regression and classification models to be consistent with other machine learning libraries\n  - Support XGBoost4j compilation on Windows\n  - Parameter tuning tool\n  - Publish source code for XGBoost4j to maven local repo\n  - Scala implementation of the Rabit tracker (drop-in replacement for the Java implementation)\n  - Better exception handling for the Rabit tracker\n  - Persist `num_class`, number of classes (for classification task)\n  - `XGBoostModel` now holds `BoosterParams`\n  - libxgboost4j is now part of CMake build\n  - Release `DMatrix` when no longer needed, to conserve memory\n  - Expose `baseMargin`, to allow initialization of boosting with predictions from an external model\n  - Support instance weights\n  - Use `SparkParallelismTracker` to prevent jobs from hanging forever\n  - Expose train-time evaluation metrics via `XGBoostModel.summary`\n  - Option to specify `host-ip` explicitly in the Rabit tracker\n* Documentation\n  - Better math notation for gradient boosting\n  - Updated build instructions for Mac OS X\n  - Template for GitHub issues\n  - Add `CITATION` file for citing XGBoost in scientific writing\n  - Fix dropdown menu in xgboost.readthedocs.io\n  - Document `updater_seq` parameter\n  - Style fixes for Python documentation\n  - Links to additional examples and tutorials\n  - Clarify installation requirements\n* Changes that break backward compatibility\n  - [#1519](https://github.com/dmlc/xgboost/pull/1519) XGBoost-spark no longer contains APIs for DMatrix; use the public booster interface instead.\n  - [#2476](https://github.com/dmlc/xgboost/pull/2476) `XGBoostModel.predict()` now has a different signature", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n- Support binary wheel builds\n  - Fix `MultiIndex` detection to support Pandas 0.21.0 and higher\n  - Support metrics and evaluation sets whose names contain `-`\n  - Support feature maps when plotting trees\n  - Compatibility fix for Python 2.6\n  - Call `print_evaluation` callback at last iteration\n  - Use appropriate integer types when calling native code, to prevent truncation and memory error\n  - Fix shared library loading on Mac OS X\n* R package:\n  - New parameters:\n    - `silent` in `xgb.DMatrix()`\n    - `use_int_id` in `xgb.model.dt.tree()`\n    - `predcontrib` in `predict()`\n    - `monotone_constraints` in `xgb.train()`\n  - Default value of the `save_period` parameter in `xgboost()` changed to NULL (consistent with `xgb.train()`).\n  - It's possible to custom-build the R package with GPU acceleration support.\n  - Enable JVM build for Mac OS X and Windows\n  - Integration with AppVeyor CI\n  - Improved safety for garbage collection\n  - Store numeric attributes with higher precision\n  - Easier installation for devel version\n  - Improved `xgb.plot.tree()`\n  - Various minor fixes to improve user experience and robustness\n  - Register native code to pass CRAN check\n  - Updated CRAN submission\n* JVM packages\n  - Add Spark pipeline persistence API\n  - Fix data persistence: loss evaluation on test data had wrongly used caches for training data.\n  - Clean external cache after training\n  - Implement early stopping\n  - Enable training of multiple models by distinguishing stage IDs\n  - Better Spark integration: support RDD / dataframe / dataset, integrate with Spark ML package\n  - XGBoost4j now supports ranking task\n  - Support training with missing data\n  - Refactor JVM package to separate regression and classification models to be consistent with other machine learning libraries\n  - Support XGBoost4j compilation on Windows\n  - Parameter tuning tool\n  - Publish source code for XGBoost4j to maven local repo\n  - Scala implementation of the Rabit tracker (drop-in replacement for the Java implementation)\n  - Better exception handling for the Rabit tracker\n  - Persist `num_class`, number of classes (for classification task)\n  - `XGBoostModel` now holds `BoosterParams`\n  - libxgboost4j is now part of CMake build\n  - Release `DMatrix` when no longer needed, to conserve memory\n  - Expose `baseMargin`, to allow initialization of boosting with predictions from an external model\n  - Support instance weights\n  - Use `SparkParallelismTracker` to prevent jobs from hanging forever\n  - Expose train-time evaluation metrics via `XGBoostModel.summary`\n  - Option to specify `host-ip` explicitly in the Rabit tracker\n* Documentation\n  - Better math notation for gradient boosting\n  - Updated build instructions for Mac OS X\n  - Template for GitHub issues\n  - Add `CITATION` file for citing XGBoost in scientific writing\n  - Fix dropdown menu in xgboost.readthedocs.io\n  - Document `updater_seq` parameter\n  - Style fixes for Python documentation\n  - Links to additional examples and tutorials\n  - Clarify installation requirements\n* Changes that break backward compatibility\n  - [#1519](https://github.com/dmlc/xgboost/pull/1519) XGBoost-spark no longer contains APIs for DMatrix; use the public booster interface instead.\n  - [#2476](https://github.com/dmlc/xgboost/pull/2476) `XGBoostModel.predict()` now has a different signature"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.6 (2016.07.29)\n* Version 0.5 is skipped due to major improvements in the core\n* Major refactor of core library.\n  - Goal: more flexible and modular code as a portable library.\n  - Switch to use of c++11 standard code.\n  - Random number generator defaults to ```std::mt19937```.\n  - Share the data loading pipeline and logging module from dmlc-core.\n  - Enable registry pattern to allow optionally plugin of objective, metric, tree constructor, data loader.\n    - Future plugin modules can be put into xgboost/plugin and register back to the library.\n  - Remove most of the raw pointers to smart ptrs, for RAII safety.\n* Add official option to approximate algorithm `tree_method` to parameter.\n  - Change default behavior to switch to prefer faster algorithm.\n  - User will get a message when approximate algorithm is chosen.\n* Change library name to libxgboost.so\n* Backward compatiblity\n  - The binary buffer file is not backward compatible with previous version.\n  - The model file is backward compatible on 64 bit platforms.\n* The model file is compatible between 64/32 bit platforms(not yet tested).\n* External memory version and other advanced features will be exposed to R library as well on linux.\n  - Previously some of the features are blocked due to C++11 and threading limits.\n  - The windows version is still blocked due to Rtools do not support ```std::thread```.\n* rabit and dmlc-core are maintained through git submodule\n  - Anyone can open PR to update these dependencies now.\n* Improvements\n  - Rabit and xgboost libs are not thread-safe and use thread local PRNGs\n  - This could fix some of the previous problem which runs xgboost on multiple threads.\n* JVM Package\n  - Enable xgboost4j for java and scala\n  - XGBoost distributed now runs on Flink and Spark.\n* Support model attributes listing for meta data.\n  - https://github.com/dmlc/xgboost/pull/1198\n  - https://github.com/dmlc/xgboost/pull/1166\n* Support callback API\n  - https://github.com/dmlc/xgboost/issues/892\n  - https://github.com/dmlc/xgboost/pull/1211\n  - https://github.com/dmlc/xgboost/pull/1264\n* Support new booster DART(dropout in tree boosting)\n  - https://github.com/dmlc/xgboost/pull/1220\n* Add CMake build system\n  - https://github.com/dmlc/xgboost/pull/1314\n\n## v0.47 (2016.01.14)\n\n* Changes in R library\n  - fixed possible problem of poisson regression.\n  - switched from 0 to NA for missing values.\n  - exposed access to additional model parameters.\n* Changes in Python library\n  - throws exception instead of crash terminal when a parameter error happens.\n  - has importance plot and tree plot functions.\n  - accepts different learning rates for each boosting round.\n  - allows model training continuation from previously saved model.\n  - allows early stopping in CV.\n  - allows feval to return a list of tuples.\n  - allows eval_metric to handle additional format.\n  - improved compatibility in sklearn module.\n  - additional parameters added for sklearn wrapper.\n  - added pip installation functionality.\n  - supports more Pandas DataFrame dtypes.\n  - added best_ntree_limit attribute, in addition to best_score and best_iteration.\n* Java api is ready for use\n* Added more test cases and continuous integration to make each build more robust.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.6 (2016.07.29)\n* Version 0.5 is skipped due to major improvements in the core\n* Major refactor of core library.\n  - Goal: more flexible and modular code as a portable library.\n  - Switch to use of c++11 standard code.\n  - Random number generator defaults to ```std::mt19937```.\n  - Share the data loading pipeline and logging module from dmlc-core.\n  - Enable registry pattern to allow optionally plugin of objective, metric, tree constructor, data loader.\n    - Future plugin modules can be put into xgboost/plugin and register back to the library.\n  - Remove most of the raw pointers to smart ptrs, for RAII safety.\n* Add official option to approximate algorithm `tree_method` to parameter.\n  - Change default behavior to switch to prefer faster algorithm.\n  - User will get a message when approximate algorithm is chosen.\n* Change library name to libxgboost.so\n* Backward compatiblity\n  - The binary buffer file is not backward compatible with previous version.\n  - The model file is backward compatible on 64 bit platforms.\n* The model file is compatible between 64/32 bit platforms(not yet tested).\n* External memory version and other advanced features will be exposed to R library as well on linux.\n  - Previously some of the features are blocked due to C++11 and threading limits.\n  - The windows version is still blocked due to Rtools do not support ```std::thread```.\n* rabit and dmlc-core are maintained through git submodule\n  - Anyone can open PR to update these dependencies now.\n* Improvements\n  - Rabit and xgboost libs are not thread-safe and use thread local PRNGs\n  - This could fix some of the previous problem which runs xgboost on multiple threads.\n* JVM Package\n  - Enable xgboost4j for java and scala\n  - XGBoost distributed now runs on Flink and Spark.\n* Support model attributes listing for meta data.\n  - https://github.com/dmlc/xgboost/pull/1198\n  - https://github.com/dmlc/xgboost/pull/1166\n* Support callback API\n  - https://github.com/dmlc/xgboost/issues/892\n  - https://github.com/dmlc/xgboost/pull/1211\n  - https://github.com/dmlc/xgboost/pull/1264\n* Support new booster DART(dropout in tree boosting)\n  - https://github.com/dmlc/xgboost/pull/1220\n* Add CMake build system\n  - https://github.com/dmlc/xgboost/pull/1314\n\n## v0.47 (2016.01.14)\n\n* Changes in R library\n  - fixed possible problem of poisson regression.\n  - switched from 0 to NA for missing values.\n  - exposed access to additional model parameters.\n* Changes in Python library\n  - throws exception instead of crash terminal when a parameter error happens.\n  - has importance plot and tree plot functions.\n  - accepts different learning rates for each boosting round.\n  - allows model training continuation from previously saved model.\n  - allows early stopping in CV.\n  - allows feval to return a list of tuples.\n  - allows eval_metric to handle additional format.\n  - improved compatibility in sklearn module.\n  - additional parameters added for sklearn wrapper.\n  - added pip installation functionality.\n  - supports more Pandas DataFrame dtypes.\n  - added best_ntree_limit attribute, in addition to best_score and best_iteration.\n* Java api is ready for use\n* Added more test cases and continuous integration to make each build more robust."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//NEWS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.4 (2015.05.11)\n\n* Distributed version of xgboost that runs on YARN, scales to billions of examples\n* Direct save/load data and model from/to S3 and HDFS\n* Feature importance visualization in R module, by Michael Benesty\n* Predict leaf index\n* Poisson regression for counts data\n* Early stopping option in training\n* Native save load support in R and python\n  - xgboost models now can be saved using save/load in R\n  - xgboost python model is now pickable\n* sklearn wrapper is supported in python module\n* Experimental External memory version\n\n\n## v0.3 (2014.09.07)\n\n* Faster tree construction module\n  - Allows subsample columns during tree construction via ```bst:col_samplebytree=ratio```\n* Support for boosting from initial predictions\n* Experimental version of LambdaRank\n* Linear booster is now parallelized, using parallel coordinated descent.\n* Add [Code Guide](src/README.md) for customizing objective function and evaluation\n* Add R module\n\n\n## v0.2x (2014.05.20)\n\n* Python module\n* Weighted samples instances\n* Initial version of pairwise rank\n\n\n## v0.1 (2014.03.26)\n\n* Initial release", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//NEWS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//NEWS.md\n## v0.4 (2015.05.11)\n\n* Distributed version of xgboost that runs on YARN, scales to billions of examples\n* Direct save/load data and model from/to S3 and HDFS\n* Feature importance visualization in R module, by Michael Benesty\n* Predict leaf index\n* Poisson regression for counts data\n* Early stopping option in training\n* Native save load support in R and python\n  - xgboost models now can be saved using save/load in R\n  - xgboost python model is now pickable\n* sklearn wrapper is supported in python module\n* Experimental External memory version\n\n\n## v0.3 (2014.09.07)\n\n* Faster tree construction module\n  - Allows subsample columns during tree construction via ```bst:col_samplebytree=ratio```\n* Support for boosting from initial predictions\n* Experimental version of LambdaRank\n* Linear booster is now parallelized, using parallel coordinated descent.\n* Add [Code Guide](src/README.md) for customizing objective function and evaluation\n* Add R module\n\n\n## v0.2x (2014.05.20)\n\n* Python module\n* Weighted samples instances\n* Initial version of pairwise rank\n\n\n## v0.1 (2014.03.26)\n\n* Initial release"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md\nContributors of DMLC/XGBoost\n============================\nXGBoost has been developed and used by a group of active community. Everyone is more than welcomed to is a great way to make the project better and more accessible to more users.\n\nProject Management Committee(PMC) \n----------\nThe Project Management Committee(PMC) consists group of active committers that moderate the discussion, manage the project release, and proposes new committer/PMC members. \n\n* [Tianqi Chen](https://github.com/tqchen), University of Washington\n  - Tianqi is a Ph.D. student working on large-scale machine learning. He is the creator of the project.\n* [Michael Benesty](https://github.com/pommedeterresautee)\n  - Michael is a lawyer and data scientist in France. He is the creator of XGBoost interactive analysis module in R.\n* [Yuan Tang](https://github.com/terrytangyuan), Red Hat\n  - Yuan is a Senior Principal Software Engineer at Red Hat AI. He contributed mostly in R and Python packages.\n* [Nan Zhu](https://github.com/CodingCat), Uber\n  - Nan is a software engineer in Uber. He contributed mostly in JVM packages.\n* [Jiaming Yuan](https://github.com/trivialfis)\n  - Jiaming contributed to the GPU algorithms. He has also introduced new abstractions to improve the quality of the C++ codebase.\n* [Hyunsu Cho](http://hyunsu-cho.io/), NVIDIA\n  - Hyunsu is the maintainer of the XGBoost Python package. He also manages the Jenkins continuous integration system (https://xgboost-ci.net/). He is the initial author of the CPU 'hist' updater.\n* [Rory Mitchell](https://github.com/RAMitchell), University of Waikato\n  - Rory is a Ph.D. student at University of Waikato. He is the original creator of the GPU training algorithms. He improved the CMake build system and continuous integration. \n* [Hongliang Liu](https://github.com/phunterlau)\n\n\nCommitters\n----------\nCommitters are people who have made substantial contribution to the project and granted write access to the project.\n\n* [Tong He](https://github.com/hetong007), Amazon AI\n  - Tong is an applied scientist in Amazon AI. He is the maintainer of XGBoost R package.\n* [Vadim Khotilovich](https://github.com/khotilov)\n  - Vadim contributes many improvements in R and core packages.\n* [Bing Xu](https://github.com/antinucleon)\n  - Bing is the original creator of XGBoost Python package and currently the maintainer of [XGBoost.jl](https://github.com/antinucleon/XGBoost.jl).\n* [Sergei Lebedev](https://github.com/superbobry), Criteo\n  - Sergei is a software engineer in Criteo. He contributed mostly in JVM packages.\n* [Scott Lundberg](http://scottlundberg.com/), University of Washington\n  - Scott is a Ph.D. student at University of Washington. He is the creator of SHAP, a unified approach to explain the output of machine learning models such as decision tree ensembles. He also helps maintain the XGBoost Julia package.\n* [Egor Smirnov](https://github.com/SmirnovEgorRu), Intel\n  - Egor has led a major effort to improve the performance of XGBoost on multi-core CPUs.\n\n\nBecome a Committer\n------------------\nXGBoost is an open source project and we are actively looking for new committers who are willing to help maintaining and lead the project.\nCommitters come from contributors who:\n* Made substantial contribution to the project.\n* Willing to spent time on maintaining and lead the project.\n\nNew committers will be proposed by current committer members, with support from more than two of current committers.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md\nContributors of DMLC/XGBoost\n============================\nXGBoost has been developed and used by a group of active community. Everyone is more than welcomed to is a great way to make the project better and more accessible to more users.\n\nProject Management Committee(PMC) \n----------\nThe Project Management Committee(PMC) consists group of active committers that moderate the discussion, manage the project release, and proposes new committer/PMC members. \n\n* [Tianqi Chen](https://github.com/tqchen), University of Washington\n  - Tianqi is a Ph.D. student working on large-scale machine learning. He is the creator of the project.\n* [Michael Benesty](https://github.com/pommedeterresautee)\n  - Michael is a lawyer and data scientist in France. He is the creator of XGBoost interactive analysis module in R.\n* [Yuan Tang](https://github.com/terrytangyuan), Red Hat\n  - Yuan is a Senior Principal Software Engineer at Red Hat AI. He contributed mostly in R and Python packages.\n* [Nan Zhu](https://github.com/CodingCat), Uber\n  - Nan is a software engineer in Uber. He contributed mostly in JVM packages.\n* [Jiaming Yuan](https://github.com/trivialfis)\n  - Jiaming contributed to the GPU algorithms. He has also introduced new abstractions to improve the quality of the C++ codebase.\n* [Hyunsu Cho](http://hyunsu-cho.io/), NVIDIA\n  - Hyunsu is the maintainer of the XGBoost Python package. He also manages the Jenkins continuous integration system (https://xgboost-ci.net/). He is the initial author of the CPU 'hist' updater.\n* [Rory Mitchell](https://github.com/RAMitchell), University of Waikato\n  - Rory is a Ph.D. student at University of Waikato. He is the original creator of the GPU training algorithms. He improved the CMake build system and continuous integration. \n* [Hongliang Liu](https://github.com/phunterlau)\n\n\nCommitters\n----------\nCommitters are people who have made substantial contribution to the project and granted write access to the project.\n\n* [Tong He](https://github.com/hetong007), Amazon AI\n  - Tong is an applied scientist in Amazon AI. He is the maintainer of XGBoost R package.\n* [Vadim Khotilovich](https://github.com/khotilov)\n  - Vadim contributes many improvements in R and core packages.\n* [Bing Xu](https://github.com/antinucleon)\n  - Bing is the original creator of XGBoost Python package and currently the maintainer of [XGBoost.jl](https://github.com/antinucleon/XGBoost.jl).\n* [Sergei Lebedev](https://github.com/superbobry), Criteo\n  - Sergei is a software engineer in Criteo. He contributed mostly in JVM packages.\n* [Scott Lundberg](http://scottlundberg.com/), University of Washington\n  - Scott is a Ph.D. student at University of Washington. He is the creator of SHAP, a unified approach to explain the output of machine learning models such as decision tree ensembles. He also helps maintain the XGBoost Julia package.\n* [Egor Smirnov](https://github.com/SmirnovEgorRu), Intel\n  - Egor has led a major effort to improve the performance of XGBoost on multi-core CPUs.\n\n\nBecome a Committer\n------------------\nXGBoost is an open source project and we are actively looking for new committers who are willing to help maintaining and lead the project.\nCommitters come from contributors who:\n* Made substantial contribution to the project.\n* Willing to spent time on maintaining and lead the project.\n\nNew committers will be proposed by current committer members, with support from more than two of current committers."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md\nList of Contributors\n--------------------\n* [Full List of Contributors](https://github.com/dmlc/xgboost/graphs/contributors)\n  - To contributors: please add your name to the list when you submit a patch to the project:)\n* [Kailong Chen](https://github.com/kalenhaha)\n  - Kailong is an early contributor of XGBoost, he is creator of ranking objectives in XGBoost.\n* [Skipper Seabold](https://github.com/jseabold)\n  - Skipper is the major contributor to the scikit-learn module of XGBoost.\n* [Zygmunt Zaj\u0105c](https://github.com/zygmuntz)\n  - Zygmunt is the master behind the early stopping feature frequently used by Kagglers.\n* [Ajinkya Kale](https://github.com/ajkl)\n* [Boliang Chen](https://github.com/cblsjtu)\n* [Yangqing Men](https://github.com/yanqingmen)\n  - Yangqing is the creator of XGBoost java package.\n* [Engpeng Yao](https://github.com/yepyao)\n* [Giulio](https://github.com/giuliohome)\n  - Giulio is the creator of Windows project of XGBoost\n* [Jamie Hall](https://github.com/nerdcha)\n  - Jamie is the initial creator of XGBoost scikit-learn module.\n* [Yen-Ying Lee](https://github.com/white1033)\n* [Masaaki Horikoshi](https://github.com/sinhrks)\n  - Masaaki is the initial creator of XGBoost Python plotting module.\n* [daiyl0320](https://github.com/daiyl0320)\n  - daiyl0320 contributed patch to XGBoost distributed version more robust, and scales stably on TB scale datasets.\n* [Huayi Zhang](https://github.com/irachex)\n* [Johan Manders](https://github.com/johanmanders)\n* [yoori](https://github.com/yoori)\n* [Mathias M\u00fcller](https://github.com/far0n)\n* [Sam Thomson](https://github.com/sammthomson)\n* [ganesh-krishnan](https://github.com/ganesh-krishnan)\n* [Damien Carol](https://github.com/damiencarol)\n* [Alex Bain](https://github.com/convexquad)\n* [Baltazar Bieniek](https://github.com/bbieniek)\n* [Adam Pocock](https://github.com/Craigacp)\n* [Gideon Whitehead](https://github.com/gaw89)\n* [Yi-Lin Juang](https://github.com/frankyjuang)\n* [Andrew Hannigan](https://github.com/andrewhannigan)\n* [Andy Adinets](https://github.com/canonizer)\n* [Henry Gouk](https://github.com/henrygouk)\n* [Pierre de Sahb](https://github.com/pdesahb)\n* [liuliang01](https://github.com/liuliang01)\n  - liuliang01 added support for the qid column for LIBSVM input format. This makes ranking task easier in distributed setting.\n* [Andrew Thia](https://github.com/BlueTea88)\n  - Andrew Thia implemented feature interaction constraints\n* [Wei Tian](https://github.com/weitian)\n* [Chen Qin](https://github.com/chenqin)\n* [Sam Wilkinson](https://samwilkinson.io)\n* [Matthew Jones](https://github.com/mt-jones)\n* [Jiaxiang Li](https://github.com/JiaxiangBU)\n* [Bryan Woods](https://github.com/bryan-woods)\n  - Bryan added support for cross-validation for the ranking objective\n* [Haoda Fu](https://github.com/fuhaoda)\n* [Evan Kepner](https://github.com/EvanKepner)\n  - Evan Kepner added support for os.PathLike file paths in Python", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//CONTRIBUTORS.md\nList of Contributors\n--------------------\n* [Full List of Contributors](https://github.com/dmlc/xgboost/graphs/contributors)\n  - To contributors: please add your name to the list when you submit a patch to the project:)\n* [Kailong Chen](https://github.com/kalenhaha)\n  - Kailong is an early contributor of XGBoost, he is creator of ranking objectives in XGBoost.\n* [Skipper Seabold](https://github.com/jseabold)\n  - Skipper is the major contributor to the scikit-learn module of XGBoost.\n* [Zygmunt Zaj\u0105c](https://github.com/zygmuntz)\n  - Zygmunt is the master behind the early stopping feature frequently used by Kagglers.\n* [Ajinkya Kale](https://github.com/ajkl)\n* [Boliang Chen](https://github.com/cblsjtu)\n* [Yangqing Men](https://github.com/yanqingmen)\n  - Yangqing is the creator of XGBoost java package.\n* [Engpeng Yao](https://github.com/yepyao)\n* [Giulio](https://github.com/giuliohome)\n  - Giulio is the creator of Windows project of XGBoost\n* [Jamie Hall](https://github.com/nerdcha)\n  - Jamie is the initial creator of XGBoost scikit-learn module.\n* [Yen-Ying Lee](https://github.com/white1033)\n* [Masaaki Horikoshi](https://github.com/sinhrks)\n  - Masaaki is the initial creator of XGBoost Python plotting module.\n* [daiyl0320](https://github.com/daiyl0320)\n  - daiyl0320 contributed patch to XGBoost distributed version more robust, and scales stably on TB scale datasets.\n* [Huayi Zhang](https://github.com/irachex)\n* [Johan Manders](https://github.com/johanmanders)\n* [yoori](https://github.com/yoori)\n* [Mathias M\u00fcller](https://github.com/far0n)\n* [Sam Thomson](https://github.com/sammthomson)\n* [ganesh-krishnan](https://github.com/ganesh-krishnan)\n* [Damien Carol](https://github.com/damiencarol)\n* [Alex Bain](https://github.com/convexquad)\n* [Baltazar Bieniek](https://github.com/bbieniek)\n* [Adam Pocock](https://github.com/Craigacp)\n* [Gideon Whitehead](https://github.com/gaw89)\n* [Yi-Lin Juang](https://github.com/frankyjuang)\n* [Andrew Hannigan](https://github.com/andrewhannigan)\n* [Andy Adinets](https://github.com/canonizer)\n* [Henry Gouk](https://github.com/henrygouk)\n* [Pierre de Sahb](https://github.com/pdesahb)\n* [liuliang01](https://github.com/liuliang01)\n  - liuliang01 added support for the qid column for LIBSVM input format. This makes ranking task easier in distributed setting.\n* [Andrew Thia](https://github.com/BlueTea88)\n  - Andrew Thia implemented feature interaction constraints\n* [Wei Tian](https://github.com/weitian)\n* [Chen Qin](https://github.com/chenqin)\n* [Sam Wilkinson](https://samwilkinson.io)\n* [Matthew Jones](https://github.com/mt-jones)\n* [Jiaxiang Li](https://github.com/JiaxiangBU)\n* [Bryan Woods](https://github.com/bryan-woods)\n  - Bryan added support for cross-validation for the ranking objective\n* [Haoda Fu](https://github.com/fuhaoda)\n* [Evan Kepner](https://github.com/EvanKepner)\n  - Evan Kepner added support for os.PathLike file paths in Python"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//README.md\n<img src=\"https://xgboost.ai/images/logo/xgboost-logo-trimmed.png\" width=200/> eXtreme Gradient Boosting\n===========\n\n[![Build Status](https://badge.buildkite.com/aca47f40a32735c00a8550540c5eeff6a4c1d246a580cae9b0.svg?branch=master)](https://buildkite.com/xgboost/xgboost-ci)\n[![XGBoost-CI](https://github.com/dmlc/xgboost/workflows/XGBoost-CI/badge.svg?branch=master)](https://github.com/dmlc/xgboost/actions)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org)\n[![GitHub license](https://dmlc.github.io/img/apache2.svg)](./LICENSE)\n[![CRAN Status Badge](https://www.r-pkg.org/badges/version/xgboost)](https://cran.r-project.org/web/packages/xgboost)\n[![PyPI version](https://badge.fury.io/py/xgboost.svg)](https://pypi.python.org/pypi/xgboost/)\n[![Conda version](https://img.shields.io/conda/vn/conda-forge/py-xgboost.svg)](https://anaconda.org/conda-forge/py-xgboost)\n[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)\n[![Twitter](https://img.shields.io/badge/@XGBoostProject--_.svg?style=social&logo=twitter)](https://twitter.com/XGBoostProject)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost/badge)](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-training/xgboost/notebooks/how_to_use_comet_with_xgboost_tutorial.ipynb)\n\n[Community](https://xgboost.ai/community) |\n[Documentation](https://xgboost.readthedocs.org) |\n[Resources](demo/README.md) |\n[Contributors](CONTRIBUTORS.md) |\n[Release Notes](https://xgboost.readthedocs.io/en/latest/changes/index.html)\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***.\nIt implements machine learning algorithms under the [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) framework.\nXGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\nThe same code runs on major distributed environment (Kubernetes, Hadoop, SGE, Dask, Spark, PySpark) and can solve problems beyond billions of examples.\n\nLicense\n-------\n\u00a9 Contributors, 2021. Licensed under an [Apache-2](https://github.com/dmlc/xgboost/blob/master/LICENSE) license.\n\nContribute to XGBoost\n---------------------\nXGBoost has been developed and used by a group of active community members. Your help is very valuable to make the package better for everyone.\nCheckout the [Community Page](https://xgboost.ai/community).\n\nReference\n---------\n- Tianqi Chen and Carlos Guestrin. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754). In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016\n- XGBoost originates from research project at University of Washington.\n\nSponsors\n--------\nBecome a sponsor and get a logo here. See details at [Sponsoring the XGBoost Project](https://xgboost.ai/sponsors). The funds are used to defray the cost of continuous integration and testing infrastructure (https://xgboost-ci.net).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//README.md\n<img src=\"https://xgboost.ai/images/logo/xgboost-logo-trimmed.png\" width=200/> eXtreme Gradient Boosting\n===========\n\n[![Build Status](https://badge.buildkite.com/aca47f40a32735c00a8550540c5eeff6a4c1d246a580cae9b0.svg?branch=master)](https://buildkite.com/xgboost/xgboost-ci)\n[![XGBoost-CI](https://github.com/dmlc/xgboost/workflows/XGBoost-CI/badge.svg?branch=master)](https://github.com/dmlc/xgboost/actions)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org)\n[![GitHub license](https://dmlc.github.io/img/apache2.svg)](./LICENSE)\n[![CRAN Status Badge](https://www.r-pkg.org/badges/version/xgboost)](https://cran.r-project.org/web/packages/xgboost)\n[![PyPI version](https://badge.fury.io/py/xgboost.svg)](https://pypi.python.org/pypi/xgboost/)\n[![Conda version](https://img.shields.io/conda/vn/conda-forge/py-xgboost.svg)](https://anaconda.org/conda-forge/py-xgboost)\n[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)\n[![Twitter](https://img.shields.io/badge/@XGBoostProject--_.svg?style=social&logo=twitter)](https://twitter.com/XGBoostProject)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost/badge)](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-training/xgboost/notebooks/how_to_use_comet_with_xgboost_tutorial.ipynb)\n\n[Community](https://xgboost.ai/community) |\n[Documentation](https://xgboost.readthedocs.org) |\n[Resources](demo/README.md) |\n[Contributors](CONTRIBUTORS.md) |\n[Release Notes](https://xgboost.readthedocs.io/en/latest/changes/index.html)\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***.\nIt implements machine learning algorithms under the [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) framework.\nXGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\nThe same code runs on major distributed environment (Kubernetes, Hadoop, SGE, Dask, Spark, PySpark) and can solve problems beyond billions of examples.\n\nLicense\n-------\n\u00a9 Contributors, 2021. Licensed under an [Apache-2](https://github.com/dmlc/xgboost/blob/master/LICENSE) license.\n\nContribute to XGBoost\n---------------------\nXGBoost has been developed and used by a group of active community members. Your help is very valuable to make the package better for everyone.\nCheckout the [Community Page](https://xgboost.ai/community).\n\nReference\n---------\n- Tianqi Chen and Carlos Guestrin. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754). In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016\n- XGBoost originates from research project at University of Washington.\n\nSponsors\n--------\nBecome a sponsor and get a logo here. See details at [Sponsoring the XGBoost Project](https://xgboost.ai/sponsors). The funds are used to defray the cost of continuous integration and testing infrastructure (https://xgboost-ci.net)."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//README.md\n## Open Source Collective sponsors\n[![Backers on Open Collective](https://opencollective.com/xgboost/backers/badge.svg)](#backers) [![Sponsors on Open Collective](https://opencollective.com/xgboost/sponsors/badge.svg)](#sponsors)\n\n### Sponsors\n[[Become a sponsor](https://opencollective.com/xgboost#sponsor)]\n\n<a href=\"https://www.nvidia.com/en-us/\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/nvidia.jpg\" alt=\"NVIDIA\" width=\"72\" height=\"72\"></a>\n<a href=\"https://www.comet.com/site/?utm_source=xgboost&utm_medium=github&utm_content=readme\" target=\"_blank\"><img src=\"https://cdn.comet.ml/img/notebook_logo.png\" height=\"72\"></a>\n<a href=\"https://opencollective.com/guest-f5ebfc79\" target=\"_blank\"><img src=\"https://images.opencollective.com/guest-f5ebfc79/avatar/256.png\" height=\"72\"></a>\n\n### Backers\n[[Become a backer](https://opencollective.com/xgboost#backer)]\n\n<a href=\"https://opencollective.com/xgboost#backers\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/backers.svg?width=890\"></a>", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//README.md\n## Open Source Collective sponsors\n[![Backers on Open Collective](https://opencollective.com/xgboost/backers/badge.svg)](#backers) [![Sponsors on Open Collective](https://opencollective.com/xgboost/sponsors/badge.svg)](#sponsors)\n\n### Sponsors\n[[Become a sponsor](https://opencollective.com/xgboost#sponsor)]\n\n<a href=\"https://www.nvidia.com/en-us/\" target=\"_blank\"><img src=\"https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/nvidia.jpg\" alt=\"NVIDIA\" width=\"72\" height=\"72\"></a>\n<a href=\"https://www.comet.com/site/?utm_source=xgboost&utm_medium=github&utm_content=readme\" target=\"_blank\"><img src=\"https://cdn.comet.ml/img/notebook_logo.png\" height=\"72\"></a>\n<a href=\"https://opencollective.com/guest-f5ebfc79\" target=\"_blank\"><img src=\"https://images.opencollective.com/guest-f5ebfc79/avatar/256.png\" height=\"72\"></a>\n\n### Backers\n[[Become a backer](https://opencollective.com/xgboost#backer)]\n\n<a href=\"https://opencollective.com/xgboost#backers\" target=\"_blank\"><img src=\"https://opencollective.com/xgboost/backers.svg?width=890\"></a>"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main//SECURITY.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\n<!-- Use this section to tell people about which versions of your project are\ncurrently being supported with security updates. -->\nSecurity updates are applied only to the most recent release.\n\n## Reporting a Vulnerability\n\n<!-- Use this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc. -->\n\nTo report a security issue, please email\n[security@xgboost-ci.net](mailto:security@xgboost-ci.net)\nwith a description of the issue, the steps you took to create the issue,\naffected versions, and, if known, mitigations for the issue.\n\nAll support will be made on the best effort base, so please indicate the \"urgency level\" of the vulnerability as Critical, High, Medium or Low.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main//SECURITY.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\n<!-- Use this section to tell people about which versions of your project are\ncurrently being supported with security updates. -->\nSecurity updates are applied only to the most recent release.\n\n## Reporting a Vulnerability\n\n<!-- Use this section to tell people how to report a vulnerability.\n\nTell them where to go, how often they can expect to get an update on a\nreported vulnerability, what to expect if the vulnerability is accepted or\ndeclined, etc. -->\n\nTo report a security issue, please email\n[security@xgboost-ci.net](mailto:security@xgboost-ci.net)\nwith a description of the issue, the steps you took to create the issue,\naffected versions, and, if known, mitigations for the issue.\n\nAll support will be made on the best effort base, so please indicate the \"urgency level\" of the vulnerability as Critical, High, Medium or Low."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/tests/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/tests/README.md\nThis folder contains test cases for XGBoost c++ core, Python package and some other CI\nfacilities.\n\n# Directories\n  * ci_build:  Test facilities for Jenkins CI and GitHub action.\n  * cli: Basic test for command line executable `xgboost`.  Most of the other command line\n    specific tests are in Python test `test_cli.py`.\n  * cpp: Tests for C++ core, using Google test framework.\n  * python: Tests for Python package, demonstrations and CLI.  For how to setup the\n    dependencies for tests, see conda files in `ci_build`.\n  * python-gpu: Similar to python tests, but for GPU.\n  * travis: CI facilities for Travis.\n  * test_distributed: Test for distributed systems including spark and dask.\n\n# Others\n  * pytest.ini: Describes the `pytest` marker for python tests, some markers are generated\n    by `conftest.py` file.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/tests/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/tests/README.md\nThis folder contains test cases for XGBoost c++ core, Python package and some other CI\nfacilities.\n\n# Directories\n  * ci_build:  Test facilities for Jenkins CI and GitHub action.\n  * cli: Basic test for command line executable `xgboost`.  Most of the other command line\n    specific tests are in Python test `test_cli.py`.\n  * cpp: Tests for C++ core, using Google test framework.\n  * python: Tests for Python package, demonstrations and CLI.  For how to setup the\n    dependencies for tests, see conda files in `ci_build`.\n  * python-gpu: Similar to python tests, but for GPU.\n  * travis: CI facilities for Travis.\n  * test_distributed: Test for distributed systems including spark and dask.\n\n# Others\n  * pytest.ini: Describes the `pytest` marker for python tests, some markers are generated\n    by `conftest.py` file."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/plugin/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/README.md\nXGBoost Plugins Modules\n=======================\n\nThis folder contains plugin modules to xgboost that can be optionally installed.  The\nplugin system helps us to extend xgboost with additional features, and add experimental\nfeatures that may not yet be ready to be included in the main project.\n\nTo include a certain plugin, say ```plugin_a```, you only need to add the following line\nto `xgboost/plugin/CMakeLists.txt`\n``` cmake\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES}\n    ${xgboost_SOURCE_DIR}/plugin/plugin_a.cc PARENT_SCOPE)\n```\nalong with specified source file `plugin_a.cc`.\n\nThen rebuild XGBoost with CMake.\n\nWrite Your Own Plugin\n---------------------\nYou can plugin your own modules to xgboost by adding code to this folder,\nwithout modification to the main code repo.\nThe [example](example) folder provides an example to write a plugin.\n\nList of register functions\n--------------------------\nA plugin has to register a new functionality to xgboost to be able to use it.\nThe register macros available to plugin writers are:\n\n - XGBOOST_REGISTER_METRIC - Register an evaluation metric\n - XGBOOST_REGISTER_GBM - Register a new gradient booster that learns through\n   gradient statistics\n - XGBOOST_REGISTER_OBJECTIVE - Register a new objective function used by xgboost\n - XGBOOST_REGISTER_TREE_UPDATER - Register a new tree-updater which updates\n   the tree given the gradient information\n\nAnd from dmlc-core:\n\n - DMLC_REGISTER_PARAMETER - Register a set of parameter for a specific usecase", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/plugin/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/README.md\nXGBoost Plugins Modules\n=======================\n\nThis folder contains plugin modules to xgboost that can be optionally installed.  The\nplugin system helps us to extend xgboost with additional features, and add experimental\nfeatures that may not yet be ready to be included in the main project.\n\nTo include a certain plugin, say ```plugin_a```, you only need to add the following line\nto `xgboost/plugin/CMakeLists.txt`\n``` cmake\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES}\n    ${xgboost_SOURCE_DIR}/plugin/plugin_a.cc PARENT_SCOPE)\n```\nalong with specified source file `plugin_a.cc`.\n\nThen rebuild XGBoost with CMake.\n\nWrite Your Own Plugin\n---------------------\nYou can plugin your own modules to xgboost by adding code to this folder,\nwithout modification to the main code repo.\nThe [example](example) folder provides an example to write a plugin.\n\nList of register functions\n--------------------------\nA plugin has to register a new functionality to xgboost to be able to use it.\nThe register macros available to plugin writers are:\n\n - XGBOOST_REGISTER_METRIC - Register an evaluation metric\n - XGBOOST_REGISTER_GBM - Register a new gradient booster that learns through\n   gradient statistics\n - XGBOOST_REGISTER_OBJECTIVE - Register a new objective function used by xgboost\n - XGBOOST_REGISTER_TREE_UPDATER - Register a new tree-updater which updates\n   the tree given the gradient information\n\nAnd from dmlc-core:\n\n - DMLC_REGISTER_PARAMETER - Register a set of parameter for a specific usecase"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/plugin/updater_gpu/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/updater_gpu/README.md\n# XGBoost GPU algorithms\n\nGPU algorithms are no longer a plugin and are included in official releases. [See documentation for more details](https://xgboost.readthedocs.io/en/latest/gpu/).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/plugin/updater_gpu/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/updater_gpu/README.md\n# XGBoost GPU algorithms\n\nGPU algorithms are no longer a plugin and are included in official releases. [See documentation for more details](https://xgboost.readthedocs.io/en/latest/gpu/)."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/plugin/example/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/example/README.md\nXGBoost Plugin Example\n======================\nThis folder provides an example of implementing xgboost plugin.\n\nThere are three steps you need to do to add a plugin to xgboost\n- Create your source .cc file, implement a new extension\n  - In this example [custom_obj.cc](custom_obj.cc)\n- Register this extension to xgboost via a registration macro\n  - In this example ```XGBOOST_REGISTER_OBJECTIVE``` in [this line](custom_obj.cc#L78)\n- Add a line to `xgboost/plugin/CMakeLists.txt`:\n```\ntarget_sources(objxgboost PRIVATE ${xgboost_SOURCE_DIR}/plugin/example/custom_obj.cc)\n```\n\nThen you can test this plugin by using ```objective=mylogistic``` parameter.\n\n<!--  LocalWords:  XGBoost\n -->", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/plugin/example/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/example/README.md\nXGBoost Plugin Example\n======================\nThis folder provides an example of implementing xgboost plugin.\n\nThere are three steps you need to do to add a plugin to xgboost\n- Create your source .cc file, implement a new extension\n  - In this example [custom_obj.cc](custom_obj.cc)\n- Register this extension to xgboost via a registration macro\n  - In this example ```XGBOOST_REGISTER_OBJECTIVE``` in [this line](custom_obj.cc#L78)\n- Add a line to `xgboost/plugin/CMakeLists.txt`:\n```\ntarget_sources(objxgboost PRIVATE ${xgboost_SOURCE_DIR}/plugin/example/custom_obj.cc)\n```\n\nThen you can test this plugin by using ```objective=mylogistic``` parameter.\n\n<!--  LocalWords:  XGBoost\n -->"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/plugin/sycl/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/sycl/README.md\n<!--\n******************************************************************************\n* Copyright by Contributors 2017-2023\n*******************************************************************************/-->\n\n# SYCL-based Algorithm for Tree Construction\nThis plugin adds support of SYCL programming model for prediction algorithms to XGBoost.\n\n## Usage\nSpecify the 'device' parameter as described in the table below to offload model training and inference on SYCL device.\n\n### Algorithms\n| device | Description |\n| --- | --- |\nsycl | use default sycl device  |\nsycl:gpu | use default sycl gpu  |\nsycl:cpu | use default sycl cpu  |\nsycl:gpu:N | use sycl gpu number N |\nsycl:cpu:N | use sycl cpu number N |\n\nPython example:\n```python\nparam['device'] = 'sycl:gpu:0'\n```\nNote: 'sycl:cpu' devices have full functional support but can't provide good enough performance. We recommend use 'sycl:cpu' devices only for test purposes.\nNote: if device is specified to be 'sycl', device type will be automatically chosen. In case the system has both sycl GPU and sycl CPU, GPU will on use.\n\n## Dependencies\nTo build and use the plugin, install [Intel\u00ae oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html).\nSee also [Intel\u00ae oneAPI Programming Guide](https://www.intel.com/content/www/us/en/docs/oneapi/programming-guide/2024-0/overview.html).\n\n## Build\nFrom the ``xgboost`` directory, run:\n\n```bash\n$ cmake -B build -S . -DPLUGIN_SYCL=ON\n$ cmake --build build -j\n```", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/plugin/sycl/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/sycl/README.md\n<!--\n******************************************************************************\n* Copyright by Contributors 2017-2023\n*******************************************************************************/-->\n\n# SYCL-based Algorithm for Tree Construction\nThis plugin adds support of SYCL programming model for prediction algorithms to XGBoost.\n\n## Usage\nSpecify the 'device' parameter as described in the table below to offload model training and inference on SYCL device.\n\n### Algorithms\n| device | Description |\n| --- | --- |\nsycl | use default sycl device  |\nsycl:gpu | use default sycl gpu  |\nsycl:cpu | use default sycl cpu  |\nsycl:gpu:N | use sycl gpu number N |\nsycl:cpu:N | use sycl cpu number N |\n\nPython example:\n```python\nparam['device'] = 'sycl:gpu:0'\n```\nNote: 'sycl:cpu' devices have full functional support but can't provide good enough performance. We recommend use 'sycl:cpu' devices only for test purposes.\nNote: if device is specified to be 'sycl', device type will be automatically chosen. In case the system has both sycl GPU and sycl CPU, GPU will on use.\n\n## Dependencies\nTo build and use the plugin, install [Intel\u00ae oneAPI DPC++/C++ Compiler](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html).\nSee also [Intel\u00ae oneAPI Programming Guide](https://www.intel.com/content/www/us/en/docs/oneapi/programming-guide/2024-0/overview.html).\n\n## Build\nFrom the ``xgboost`` directory, run:\n\n```bash\n$ cmake -B build -S . -DPLUGIN_SYCL=ON\n$ cmake --build build -j\n```"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/plugin/federated/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/federated/README.md\nXGBoost Plugin for Federated Learning\n=====================================\n\nThis folder contains the plugin for federated learning.\n\nSee [build instruction](../../doc/build.rst) for how to build the plugin.\n\n\nTest Federated XGBoost\n----------------------\n```shell\n# Under xgboost source tree.\ncd tests/distributed/test_federated\n# This tests both CPU training (`hist`) and GPU training (`gpu_hist`).\n./runtests-federated.sh\n```", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/plugin/federated/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/plugin/federated/README.md\nXGBoost Plugin for Federated Learning\n=====================================\n\nThis folder contains the plugin for federated learning.\n\nSee [build instruction](../../doc/build.rst) for how to build the plugin.\n\n\nTest Federated XGBoost\n----------------------\n```shell\n# Under xgboost source tree.\ncd tests/distributed/test_federated\n# This tests both CPU training (`hist`) and GPU training (`gpu_hist`).\n./runtests-federated.sh\n```"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\nAwesome XGBoost\n===============\nThis page contains a curated list of examples, tutorials, blogs about XGBoost usecases.\nIt is inspired by [awesome-MXNet](https://github.com/dmlc/mxnet/blob/master/example/README.md),\n[awesome-php](https://github.com/ziadoz/awesome-php) and [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning).\n\nPlease send a pull request if you find things that belongs to here.\n\nContents\n--------\n- [Code Examples](#code-examples)\n  - [Features Walkthrough](#features-walkthrough)\n  - [Basic Examples by Tasks](#basic-examples-by-tasks)\n  - [Benchmarks](#benchmarks)\n- [Machine Learning Challenge Winning Solutions](#machine-learning-challenge-winning-solutions)\n- [Tutorials](#tutorials)\n- [Usecases](#usecases)\n- [Tools using XGBoost](#tools-using-xgboost)\n- [Integrations with 3rd party software](#integrations-with-3rd-party-software)\n- [Awards](#awards)\n- [Windows Binaries](#windows-binaries)\n\nCode Examples\n-------------\n### Features Walkthrough\n\n_Note: for the R package, see the in-package examples and vignettes instead_\n\nThis is a list of short codes introducing different functionalities of xgboost packages.\n\n* Basic walkthrough of packages\n  [python](guide-python/basic_walkthrough.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/basic_walkthrough.jl)\n  [PHP](https://github.com/bpachev/xgboost-php/blob/master/demo/titanic_demo.php)\n* Customize loss function, and evaluation metric\n  [python](guide-python/custom_objective.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/custom_objective.jl)\n* Boosting from existing prediction\n  [python](guide-python/boost_from_prediction.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/boost_from_prediction.jl)\n* Predicting using first n trees\n  [python](guide-python/predict_first_ntree.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/predict_first_ntree.jl)\n* Generalized Linear Model\n  [python](guide-python/generalized_linear_model.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/generalized_linear_model.jl)\n* Cross validation\n  [python](guide-python/cross_validation.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/cross_validation.jl)\n* Predicting leaf indices\n  [python](guide-python/predict_leaf_indices.py)\n\n### Basic Examples by Tasks\n\nMost of examples in this section are based on CLI or python version.\nHowever, the parameter settings can be applied to all versions\n\n- [Binary classification](CLI/binary_classification)\n- [Multiclass classification](multiclass_classification)\n- [Regression](CLI/regression)\n- [Learning to Rank](rank)\n\n### Benchmarks\n\n- [Starter script for Kaggle Higgs Boson](kaggle-higgs)\n- [Kaggle Tradeshift winning solution by daxiongshu](https://github.com/daxiongshu/kaggle-tradeshift-winning-solution)\n- [Benchmarking the most commonly used open source tools for binary classification](https://github.com/szilard/benchm-ml#boosting-gradient-boosted-treesgradient-boosting-machines)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\nAwesome XGBoost\n===============\nThis page contains a curated list of examples, tutorials, blogs about XGBoost usecases.\nIt is inspired by [awesome-MXNet](https://github.com/dmlc/mxnet/blob/master/example/README.md),\n[awesome-php](https://github.com/ziadoz/awesome-php) and [awesome-machine-learning](https://github.com/josephmisiti/awesome-machine-learning).\n\nPlease send a pull request if you find things that belongs to here.\n\nContents\n--------\n- [Code Examples](#code-examples)\n  - [Features Walkthrough](#features-walkthrough)\n  - [Basic Examples by Tasks](#basic-examples-by-tasks)\n  - [Benchmarks](#benchmarks)\n- [Machine Learning Challenge Winning Solutions](#machine-learning-challenge-winning-solutions)\n- [Tutorials](#tutorials)\n- [Usecases](#usecases)\n- [Tools using XGBoost](#tools-using-xgboost)\n- [Integrations with 3rd party software](#integrations-with-3rd-party-software)\n- [Awards](#awards)\n- [Windows Binaries](#windows-binaries)\n\nCode Examples\n-------------\n### Features Walkthrough\n\n_Note: for the R package, see the in-package examples and vignettes instead_\n\nThis is a list of short codes introducing different functionalities of xgboost packages.\n\n* Basic walkthrough of packages\n  [python](guide-python/basic_walkthrough.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/basic_walkthrough.jl)\n  [PHP](https://github.com/bpachev/xgboost-php/blob/master/demo/titanic_demo.php)\n* Customize loss function, and evaluation metric\n  [python](guide-python/custom_objective.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/custom_objective.jl)\n* Boosting from existing prediction\n  [python](guide-python/boost_from_prediction.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/boost_from_prediction.jl)\n* Predicting using first n trees\n  [python](guide-python/predict_first_ntree.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/predict_first_ntree.jl)\n* Generalized Linear Model\n  [python](guide-python/generalized_linear_model.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/generalized_linear_model.jl)\n* Cross validation\n  [python](guide-python/cross_validation.py)\n  [Julia](https://github.com/antinucleon/XGBoost.jl/blob/master/demo/cross_validation.jl)\n* Predicting leaf indices\n  [python](guide-python/predict_leaf_indices.py)\n\n### Basic Examples by Tasks\n\nMost of examples in this section are based on CLI or python version.\nHowever, the parameter settings can be applied to all versions\n\n- [Binary classification](CLI/binary_classification)\n- [Multiclass classification](multiclass_classification)\n- [Regression](CLI/regression)\n- [Learning to Rank](rank)\n\n### Benchmarks\n\n- [Starter script for Kaggle Higgs Boson](kaggle-higgs)\n- [Kaggle Tradeshift winning solution by daxiongshu](https://github.com/daxiongshu/kaggle-tradeshift-winning-solution)\n- [Benchmarking the most commonly used open source tools for binary classification](https://github.com/szilard/benchm-ml#boosting-gradient-boosted-treesgradient-boosting-machines)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n## Machine Learning Challenge Winning Solutions\n\nXGBoost is extensively used by machine learning practitioners to create state of art data science solutions,\nthis is a list of machine learning winning solutions with XGBoost.\nPlease send pull requests if you find ones that are missing here.\n\n- Bishwarup Bhattacharjee, 1st place winner of [Allstate Claims Severity](https://www.kaggle.com/competitions/allstate-claims-severity/overview) conducted on December 2016. Link to [discussion](https://www.kaggle.com/competitions/allstate-claims-severity/discussion/26416)\n- Benedikt Schifferer, Gilberto Titericz, Chris Deotte, Christof Henkel, Kazuki Onodera, Jiwei Liu, Bojan Tunguz, Even Oldridge, Gabriel De Souza Pereira Moreira and Ahmet Erdem, 1st place winner of [Twitter RecSys Challenge 2020](https://recsys-twitter.com/) conducted from June,20-August,20. [GPU Accelerated Feature Engineering and Training for Recommender Systems](https://medium.com/rapids-ai/winning-solution-of-recsys2020-challenge-gpu-accelerated-feature-engineering-and-training-for-cd67c5a87b1f)\n- Eugene Khvedchenya,Jessica Fridrich, Jan Butora, Yassine Yousfi 1st place winner in [ALASKA2 Image Steganalysis](https://www.kaggle.com/c/alaska2-image-steganalysis/overview). Link to [discussion](https://www.kaggle.com/c/alaska2-image-steganalysis/discussion/168546)\n- Dan Ofer, Seffi Cohen, Noa Dagan, Nurit, 1st place in WiDS Datathon 2020. Link to [discussion](https://www.kaggle.com/c/widsdatathon2020/discussion/133189)\n- Chris Deotte, Konstantin Yakovlev 1st place in [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview). Link to [discussion](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308)\n- Giba, Lucasz, 1st place winner in [Santander Value Prediction Challenge](https://www.kaggle.com/c/santander-value-prediction-challenge) organized on August,2018. Solution [discussion](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/65272) and [code](https://www.kaggle.com/titericz/winner-model-giba-single-xgb-lb0-5178/comments)\n- Beluga, 2nd place and Evgeny Nekrasov, 3rd place winner in Statoil/C-CORE Iceberg Classifier Challenge'2018. Link to [discussion](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/discussion/48294)\n- Radek Osmulski, 1st place of the [iMaterialist Challenge (Fashion) at FGVC5](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/overview). Link to [the winning solution](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/discussion/57944).\n- Maksims Volkovs, Guangwei Yu and Tomi Poutanen, 1st place of the [2017 ACM RecSys challenge](http://2017.recsyschallenge.com/). Link to [paper](http://www.cs.toronto.edu/~mvolkovs/recsys2017_challenge.pdf).\n- Vlad Sandulescu, Mihai Chiru, 1st place of the [KDD Cup 2016 competition](https://kddcup2016.azurewebsites.net). Link to [the arxiv paper](http://arxiv.org/abs/1609.02728).\n- Marios Michailidis, Mathias M\u00fcller and HJ van Veen, 1st place of the [Dato Truely Native? competition](https://www.kaggle.com/c/dato-native). Link to [the Kaggle interview](http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/).\n- Vlad Mironov, Alexander Guschin, 1st place of the [CERN LHCb experiment Flavour of Physics competition](https://www.kaggle.com/c/flavours-of-physics). Link to [the Kaggle interview](http://blog.kaggle.com/2015/11/30/flavour-of-physics-technical-write-up-1st-place-go-polar-bears/).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n## Machine Learning Challenge Winning Solutions\n\nXGBoost is extensively used by machine learning practitioners to create state of art data science solutions,\nthis is a list of machine learning winning solutions with XGBoost.\nPlease send pull requests if you find ones that are missing here.\n\n- Bishwarup Bhattacharjee, 1st place winner of [Allstate Claims Severity](https://www.kaggle.com/competitions/allstate-claims-severity/overview) conducted on December 2016. Link to [discussion](https://www.kaggle.com/competitions/allstate-claims-severity/discussion/26416)\n- Benedikt Schifferer, Gilberto Titericz, Chris Deotte, Christof Henkel, Kazuki Onodera, Jiwei Liu, Bojan Tunguz, Even Oldridge, Gabriel De Souza Pereira Moreira and Ahmet Erdem, 1st place winner of [Twitter RecSys Challenge 2020](https://recsys-twitter.com/) conducted from June,20-August,20. [GPU Accelerated Feature Engineering and Training for Recommender Systems](https://medium.com/rapids-ai/winning-solution-of-recsys2020-challenge-gpu-accelerated-feature-engineering-and-training-for-cd67c5a87b1f)\n- Eugene Khvedchenya,Jessica Fridrich, Jan Butora, Yassine Yousfi 1st place winner in [ALASKA2 Image Steganalysis](https://www.kaggle.com/c/alaska2-image-steganalysis/overview). Link to [discussion](https://www.kaggle.com/c/alaska2-image-steganalysis/discussion/168546)\n- Dan Ofer, Seffi Cohen, Noa Dagan, Nurit, 1st place in WiDS Datathon 2020. Link to [discussion](https://www.kaggle.com/c/widsdatathon2020/discussion/133189)\n- Chris Deotte, Konstantin Yakovlev 1st place in [IEEE-CIS Fraud Detection](https://www.kaggle.com/c/ieee-fraud-detection/overview). Link to [discussion](https://www.kaggle.com/c/ieee-fraud-detection/discussion/111308)\n- Giba, Lucasz, 1st place winner in [Santander Value Prediction Challenge](https://www.kaggle.com/c/santander-value-prediction-challenge) organized on August,2018. Solution [discussion](https://www.kaggle.com/c/santander-value-prediction-challenge/discussion/65272) and [code](https://www.kaggle.com/titericz/winner-model-giba-single-xgb-lb0-5178/comments)\n- Beluga, 2nd place and Evgeny Nekrasov, 3rd place winner in Statoil/C-CORE Iceberg Classifier Challenge'2018. Link to [discussion](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/discussion/48294)\n- Radek Osmulski, 1st place of the [iMaterialist Challenge (Fashion) at FGVC5](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/overview). Link to [the winning solution](https://www.kaggle.com/c/imaterialist-challenge-fashion-2018/discussion/57944).\n- Maksims Volkovs, Guangwei Yu and Tomi Poutanen, 1st place of the [2017 ACM RecSys challenge](http://2017.recsyschallenge.com/). Link to [paper](http://www.cs.toronto.edu/~mvolkovs/recsys2017_challenge.pdf).\n- Vlad Sandulescu, Mihai Chiru, 1st place of the [KDD Cup 2016 competition](https://kddcup2016.azurewebsites.net). Link to [the arxiv paper](http://arxiv.org/abs/1609.02728).\n- Marios Michailidis, Mathias M\u00fcller and HJ van Veen, 1st place of the [Dato Truely Native? competition](https://www.kaggle.com/c/dato-native). Link to [the Kaggle interview](http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/).\n- Vlad Mironov, Alexander Guschin, 1st place of the [CERN LHCb experiment Flavour of Physics competition](https://www.kaggle.com/c/flavours-of-physics). Link to [the Kaggle interview](http://blog.kaggle.com/2015/11/30/flavour-of-physics-technical-write-up-1st-place-go-polar-bears/)."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n- Josef Slavicek, 3rd place of the [CERN LHCb experiment Flavour of Physics competition](https://www.kaggle.com/c/flavours-of-physics). Link to [the Kaggle interview](http://blog.kaggle.com/2015/11/23/flavour-of-physics-winners-interview-3rd-place-josef-slavicek/).\n- Mario Filho, Josef Feigl, Lucas, Gilberto, 1st place of the [Caterpillar Tube Pricing competition](https://www.kaggle.com/c/caterpillar-tube-pricing). Link to [the Kaggle interview](http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/).\n- Qingchen Wang, 1st place of the [Liberty Mutual Property Inspection](https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction). Link to [the Kaggle interview](http://blog.kaggle.com/2015/09/28/liberty-mutual-property-inspection-winners-interview-qingchen-wang/).\n- Chenglong Chen, 1st place of the [Crowdflower Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance). Link to [the winning solution](https://www.kaggle.com/c/crowdflower-search-relevance/forums/t/15186/1st-place-winner-solution-chenglong-chen/).\n- Alexandre Barachant (\u201cCat\u201d) and Rafa\u0142 Cyco\u0144 (\u201cDog\u201d), 1st place of the [Grasp-and-Lift EEG Detection](https://www.kaggle.com/c/grasp-and-lift-eeg-detection). Link to [the Kaggle interview](http://blog.kaggle.com/2015/10/12/grasp-and-lift-eeg-winners-interview-1st-place-cat-dog/).\n- Halla Yang, 2nd place of the [Recruit Coupon Purchase Prediction Challenge](https://www.kaggle.com/c/coupon-purchase-prediction). Link to [the Kaggle interview](http://blog.kaggle.com/2015/10/21/recruit-coupon-purchase-winners-interview-2nd-place-halla-yang/).\n- Owen Zhang, 1st place of the [Avito Context Ad Clicks competition](https://www.kaggle.com/c/avito-context-ad-clicks). Link to [the Kaggle interview](http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/).\n- Keiichi Kuroyanagi, 2nd place of the [Airbnb New User Bookings](https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings). Link to [the Kaggle interview](http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/).\n- Marios Michailidis, Mathias M\u00fcller and Ning Situ, 1st place [Homesite Quote Conversion](https://www.kaggle.com/c/homesite-quote-conversion). Link to [the Kaggle interview](http://blog.kaggle.com/2016/04/08/homesite-quote-conversion-winners-write-up-1st-place-kazanova-faron-clobber/).\n- Gilberto Titericz, Stanislav Semenov, 1st place in challenge to classify products into the correct category organized by Otto Group in 2015. Link to [challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge). Link to [kaggle winning solution](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)\n- Darius Baru\u0161auskas, 1st place winner in [Predicting Red Hat Business Value](https://www.kaggle.com/c/predicting-red-hat-business-value). Link to [interview](https://medium.com/kaggle-blog/red-hat-business-value-competition-1st-place-winners-interview-darius-baru%C5%A1auskas-646692a2841b). Link to [discussion](https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/23786)\n- David Austin, Weimin Wang, 1st place winner in [Iceberg-classifier-challenge](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/leaderboard) Link to [discussion](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/discussion/48241)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n- Josef Slavicek, 3rd place of the [CERN LHCb experiment Flavour of Physics competition](https://www.kaggle.com/c/flavours-of-physics). Link to [the Kaggle interview](http://blog.kaggle.com/2015/11/23/flavour-of-physics-winners-interview-3rd-place-josef-slavicek/).\n- Mario Filho, Josef Feigl, Lucas, Gilberto, 1st place of the [Caterpillar Tube Pricing competition](https://www.kaggle.com/c/caterpillar-tube-pricing). Link to [the Kaggle interview](http://blog.kaggle.com/2015/09/22/caterpillar-winners-interview-1st-place-gilberto-josef-leustagos-mario/).\n- Qingchen Wang, 1st place of the [Liberty Mutual Property Inspection](https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction). Link to [the Kaggle interview](http://blog.kaggle.com/2015/09/28/liberty-mutual-property-inspection-winners-interview-qingchen-wang/).\n- Chenglong Chen, 1st place of the [Crowdflower Search Results Relevance](https://www.kaggle.com/c/crowdflower-search-relevance). Link to [the winning solution](https://www.kaggle.com/c/crowdflower-search-relevance/forums/t/15186/1st-place-winner-solution-chenglong-chen/).\n- Alexandre Barachant (\u201cCat\u201d) and Rafa\u0142 Cyco\u0144 (\u201cDog\u201d), 1st place of the [Grasp-and-Lift EEG Detection](https://www.kaggle.com/c/grasp-and-lift-eeg-detection). Link to [the Kaggle interview](http://blog.kaggle.com/2015/10/12/grasp-and-lift-eeg-winners-interview-1st-place-cat-dog/).\n- Halla Yang, 2nd place of the [Recruit Coupon Purchase Prediction Challenge](https://www.kaggle.com/c/coupon-purchase-prediction). Link to [the Kaggle interview](http://blog.kaggle.com/2015/10/21/recruit-coupon-purchase-winners-interview-2nd-place-halla-yang/).\n- Owen Zhang, 1st place of the [Avito Context Ad Clicks competition](https://www.kaggle.com/c/avito-context-ad-clicks). Link to [the Kaggle interview](http://blog.kaggle.com/2015/08/26/avito-winners-interview-1st-place-owen-zhang/).\n- Keiichi Kuroyanagi, 2nd place of the [Airbnb New User Bookings](https://www.kaggle.com/c/airbnb-recruiting-new-user-bookings). Link to [the Kaggle interview](http://blog.kaggle.com/2016/03/17/airbnb-new-user-bookings-winners-interview-2nd-place-keiichi-kuroyanagi-keiku/).\n- Marios Michailidis, Mathias M\u00fcller and Ning Situ, 1st place [Homesite Quote Conversion](https://www.kaggle.com/c/homesite-quote-conversion). Link to [the Kaggle interview](http://blog.kaggle.com/2016/04/08/homesite-quote-conversion-winners-write-up-1st-place-kazanova-faron-clobber/).\n- Gilberto Titericz, Stanislav Semenov, 1st place in challenge to classify products into the correct category organized by Otto Group in 2015. Link to [challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge). Link to [kaggle winning solution](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335)\n- Darius Baru\u0161auskas, 1st place winner in [Predicting Red Hat Business Value](https://www.kaggle.com/c/predicting-red-hat-business-value). Link to [interview](https://medium.com/kaggle-blog/red-hat-business-value-competition-1st-place-winners-interview-darius-baru%C5%A1auskas-646692a2841b). Link to [discussion](https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/23786)\n- David Austin, Weimin Wang, 1st place winner in [Iceberg-classifier-challenge](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/leaderboard) Link to [discussion](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge/discussion/48241)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n- Kazuki Onodera, Kazuki Fujikawa, 2nd place winner in [OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction](https://www.kaggle.com/c/stanford-covid-vaccine/overview) Link to [Discussion](https://www.kaggle.com/c/stanford-covid-vaccine/discussion/189709)\n- Prarthana Bhat, 2nd place winner in [DYD Competition](https://datahack.analyticsvidhya.com/contest/date-your-data/). Link to [Solution](https://github.com/analyticsvidhya/DateYourData/blob/master/Prathna_Bhat_Model.R).\n\n## Talks\n- XGBoost: A Scalable Tree Boosting System ([video] (https://www.youtube.com/watch?v=Vly8xGnNiWs) + [slides](https://speakerdeck.com/datasciencela/tianqi-chen-xgboost-overview-and-latest-news-la-meetup-talk)) by Tianqi Chen at the Los Angeles Data Science meetup\n\n## Tutorials\n\n- [XGBoost Training with Dask, using Saturn Cloud](https://www.saturncloud.io/docs/tutorials/xgboost/)\n- [Machine Learning with XGBoost on Qubole Spark Cluster](https://www.qubole.com/blog/machine-learning-xgboost-qubole-spark-cluster/)\n- [XGBoost Official RMarkdown Tutorials](https://xgboost.readthedocs.org/en/latest/R-package/index.html#tutorials)\n- [An Introduction to XGBoost R Package](http://dmlc.ml/rstats/2016/03/10/xgboost.html) by Tong He\n- [Open Source Tools & Data Science Competitions](http://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1) by Owen Zhang - XGBoost parameter tuning tips\n* [Feature Importance Analysis with XGBoost in Tax audit](http://fr.slideshare.net/MichaelBENESTY/feature-importance-analysis-with-xgboost-in-tax-audit)\n* [Winning solution of Kaggle Higgs competition: what a single model can do](http://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/)\n- [XGBoost - eXtreme Gradient Boosting](http://www.slideshare.net/ShangxuanZhang/xgboost) by Tong He\n- [How to use XGBoost algorithm in R in easy steps](http://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/) by TAVISH SRIVASTAVA ([Chinese Translation \u4e2d\u6587\u7ffb\u8bd1](https://segmentfault.com/a/1190000004421821) by [HarryZhu](https://segmentfault.com/u/harryprince))\n- [Kaggle Solution: What\u2019s Cooking ? (Text Mining Competition)](http://www.analyticsvidhya.com/blog/2015/12/kaggle-solution-cooking-text-mining-competition/) by MANISH SARASWAT\n- Better Optimization with Repeated Cross Validation and the XGBoost model - Machine Learning with R) by Manuel Amunategui ([Youtube Link](https://www.youtube.com/watch?v=Og7CGAfSr_Y)) ([GitHub Link](https://github.com/amunategui/BetterCrossValidation))\n- [XGBoost Rossman Parameter Tuning](https://www.kaggle.com/khozzy/rossmann-store-sales/xgboost-parameter-tuning-template/run/90168/notebook) by [Norbert Kozlowski](https://www.kaggle.com/khozzy)\n- [Featurizing log data before XGBoost](http://www.slideshare.net/DataRobot/featurizing-log-data-before-xgboost) by Xavier Conort, Owen Zhang etc\n- [West Nile Virus Competition Benchmarks & Tutorials](http://blog.kaggle.com/2015/07/21/west-nile-virus-competition-benchmarks-tutorials/) by [Anna Montoya](http://blog.kaggle.com/author/annamontoya/)\n- [Ensemble Decision Tree with XGBoost](https://www.kaggle.com/binghsu/predict-west-nile-virus/xgboost-starter-code-python-0-69) by [Bing Xu](https://www.kaggle.com/binghsu)\n- [Notes on eXtreme Gradient Boosting](http://startup.ml/blog/xgboost) by ARSHAK NAVRUZYAN ([iPython Notebook](https://github.com/startupml/koan/blob/master/eXtreme%20Gradient%20Boosting.ipynb))", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n- Kazuki Onodera, Kazuki Fujikawa, 2nd place winner in [OpenVaccine: COVID-19 mRNA Vaccine Degradation Prediction](https://www.kaggle.com/c/stanford-covid-vaccine/overview) Link to [Discussion](https://www.kaggle.com/c/stanford-covid-vaccine/discussion/189709)\n- Prarthana Bhat, 2nd place winner in [DYD Competition](https://datahack.analyticsvidhya.com/contest/date-your-data/). Link to [Solution](https://github.com/analyticsvidhya/DateYourData/blob/master/Prathna_Bhat_Model.R).\n\n## Talks\n- XGBoost: A Scalable Tree Boosting System ([video] (https://www.youtube.com/watch?v=Vly8xGnNiWs) + [slides](https://speakerdeck.com/datasciencela/tianqi-chen-xgboost-overview-and-latest-news-la-meetup-talk)) by Tianqi Chen at the Los Angeles Data Science meetup\n\n## Tutorials\n\n- [XGBoost Training with Dask, using Saturn Cloud](https://www.saturncloud.io/docs/tutorials/xgboost/)\n- [Machine Learning with XGBoost on Qubole Spark Cluster](https://www.qubole.com/blog/machine-learning-xgboost-qubole-spark-cluster/)\n- [XGBoost Official RMarkdown Tutorials](https://xgboost.readthedocs.org/en/latest/R-package/index.html#tutorials)\n- [An Introduction to XGBoost R Package](http://dmlc.ml/rstats/2016/03/10/xgboost.html) by Tong He\n- [Open Source Tools & Data Science Competitions](http://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1) by Owen Zhang - XGBoost parameter tuning tips\n* [Feature Importance Analysis with XGBoost in Tax audit](http://fr.slideshare.net/MichaelBENESTY/feature-importance-analysis-with-xgboost-in-tax-audit)\n* [Winning solution of Kaggle Higgs competition: what a single model can do](http://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/)\n- [XGBoost - eXtreme Gradient Boosting](http://www.slideshare.net/ShangxuanZhang/xgboost) by Tong He\n- [How to use XGBoost algorithm in R in easy steps](http://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/) by TAVISH SRIVASTAVA ([Chinese Translation \u4e2d\u6587\u7ffb\u8bd1](https://segmentfault.com/a/1190000004421821) by [HarryZhu](https://segmentfault.com/u/harryprince))\n- [Kaggle Solution: What\u2019s Cooking ? (Text Mining Competition)](http://www.analyticsvidhya.com/blog/2015/12/kaggle-solution-cooking-text-mining-competition/) by MANISH SARASWAT\n- Better Optimization with Repeated Cross Validation and the XGBoost model - Machine Learning with R) by Manuel Amunategui ([Youtube Link](https://www.youtube.com/watch?v=Og7CGAfSr_Y)) ([GitHub Link](https://github.com/amunategui/BetterCrossValidation))\n- [XGBoost Rossman Parameter Tuning](https://www.kaggle.com/khozzy/rossmann-store-sales/xgboost-parameter-tuning-template/run/90168/notebook) by [Norbert Kozlowski](https://www.kaggle.com/khozzy)\n- [Featurizing log data before XGBoost](http://www.slideshare.net/DataRobot/featurizing-log-data-before-xgboost) by Xavier Conort, Owen Zhang etc\n- [West Nile Virus Competition Benchmarks & Tutorials](http://blog.kaggle.com/2015/07/21/west-nile-virus-competition-benchmarks-tutorials/) by [Anna Montoya](http://blog.kaggle.com/author/annamontoya/)\n- [Ensemble Decision Tree with XGBoost](https://www.kaggle.com/binghsu/predict-west-nile-virus/xgboost-starter-code-python-0-69) by [Bing Xu](https://www.kaggle.com/binghsu)\n- [Notes on eXtreme Gradient Boosting](http://startup.ml/blog/xgboost) by ARSHAK NAVRUZYAN ([iPython Notebook](https://github.com/startupml/koan/blob/master/eXtreme%20Gradient%20Boosting.ipynb))"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n- [Complete Guide to Parameter Tuning in XGBoost](http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) by Aarshay Jain\n- [Practical XGBoost in Python online course](http://education.parrotprediction.teachable.com/courses/practical-xgboost-in-python) by Parrot Prediction\n- [Spark and XGBoost using Scala](http://www.elenacuoco.com/2016/10/10/scala-spark-xgboost-classification/) by Elena Cuoco\n\n## Usecases\nIf you have particular usecase of xgboost that you would like to highlight.\nSend a PR to add a one sentence description:)\n\n- XGBoost is used in [Kaggle Script](https://www.kaggle.com/scripts) to solve data science challenges.\n- Distribute XGBoost as Rest API server from Jupyter notebook with [BentoML](https://github.com/bentoml/bentoml). [Link to notebook](https://github.com/bentoml/BentoML/blob/master/examples/xgboost-predict-titanic-survival/XGBoost-titanic-survival-prediction.ipynb)\n- [Seldon predictive service powered by XGBoost](https://docs.seldon.io/projects/seldon-core/en/latest/servers/xgboost.html)\n- XGBoost Distributed is used in [ODPS Cloud Service by Alibaba](https://yq.aliyun.com/articles/6355) (in Chinese)\n- XGBoost is incoporated as part of [Graphlab Create](https://dato.com/products/create/) for scalable machine learning.\n- [Hanjing Su](https://www.52cs.org) from Tencent data platform team: \"We use distributed XGBoost for click through prediction in wechat shopping and lookalikes. The problems involve hundreds millions of users and thousands of features. XGBoost is cleanly designed and can be easily integrated into our production environment, reducing our cost in developments.\"\n- [CNevd](https://github.com/CNevd) from autohome.com ad platform team: \"Distributed XGBoost is used for click through rate prediction in our display advertising, XGBoost is highly efficient and flexible and can be easily used on our distributed platform, our ctr made a great improvement with hundred millions samples and millions features due to this awesome XGBoost\"\n\n## Tools using XGBoost\n\n- [BayesBoost](https://github.com/mpearmain/BayesBoost) - Bayesian Optimization using xgboost and sklearn API\n- [FLAML](https://github.com/microsoft/FLAML) - An open source AutoML library\ndesigned to automatically produce accurate machine learning models with low computational cost. FLAML includes [XGBoost as one of the default learners](https://github.com/microsoft/FLAML/blob/main/flaml/model.py) and can also be used as a fast hyperparameter tuning tool for XGBoost ([code example](https://microsoft.github.io/FLAML/docs/Examples/AutoML-for-XGBoost)).\n- [gp_xgboost_gridsearch](https://github.com/vatsan/gp_xgboost_gridsearch) - In-database parallel grid-search for XGBoost on [Greenplum](https://github.com/greenplum-db/gpdb) using PL/Python\n- [tpot](https://github.com/rhiever/tpot) - A Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n- [Complete Guide to Parameter Tuning in XGBoost](http://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) by Aarshay Jain\n- [Practical XGBoost in Python online course](http://education.parrotprediction.teachable.com/courses/practical-xgboost-in-python) by Parrot Prediction\n- [Spark and XGBoost using Scala](http://www.elenacuoco.com/2016/10/10/scala-spark-xgboost-classification/) by Elena Cuoco\n\n## Usecases\nIf you have particular usecase of xgboost that you would like to highlight.\nSend a PR to add a one sentence description:)\n\n- XGBoost is used in [Kaggle Script](https://www.kaggle.com/scripts) to solve data science challenges.\n- Distribute XGBoost as Rest API server from Jupyter notebook with [BentoML](https://github.com/bentoml/bentoml). [Link to notebook](https://github.com/bentoml/BentoML/blob/master/examples/xgboost-predict-titanic-survival/XGBoost-titanic-survival-prediction.ipynb)\n- [Seldon predictive service powered by XGBoost](https://docs.seldon.io/projects/seldon-core/en/latest/servers/xgboost.html)\n- XGBoost Distributed is used in [ODPS Cloud Service by Alibaba](https://yq.aliyun.com/articles/6355) (in Chinese)\n- XGBoost is incoporated as part of [Graphlab Create](https://dato.com/products/create/) for scalable machine learning.\n- [Hanjing Su](https://www.52cs.org) from Tencent data platform team: \"We use distributed XGBoost for click through prediction in wechat shopping and lookalikes. The problems involve hundreds millions of users and thousands of features. XGBoost is cleanly designed and can be easily integrated into our production environment, reducing our cost in developments.\"\n- [CNevd](https://github.com/CNevd) from autohome.com ad platform team: \"Distributed XGBoost is used for click through rate prediction in our display advertising, XGBoost is highly efficient and flexible and can be easily used on our distributed platform, our ctr made a great improvement with hundred millions samples and millions features due to this awesome XGBoost\"\n\n## Tools using XGBoost\n\n- [BayesBoost](https://github.com/mpearmain/BayesBoost) - Bayesian Optimization using xgboost and sklearn API\n- [FLAML](https://github.com/microsoft/FLAML) - An open source AutoML library\ndesigned to automatically produce accurate machine learning models with low computational cost. FLAML includes [XGBoost as one of the default learners](https://github.com/microsoft/FLAML/blob/main/flaml/model.py) and can also be used as a fast hyperparameter tuning tool for XGBoost ([code example](https://microsoft.github.io/FLAML/docs/Examples/AutoML-for-XGBoost)).\n- [gp_xgboost_gridsearch](https://github.com/vatsan/gp_xgboost_gridsearch) - In-database parallel grid-search for XGBoost on [Greenplum](https://github.com/greenplum-db/gpdb) using PL/Python\n- [tpot](https://github.com/rhiever/tpot) - A Python tool that automatically creates and optimizes machine learning pipelines using genetic programming."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n## Integrations with 3rd party software\nOpen source integrations with XGBoost:\n* [Neptune.ai](http://neptune.ai/) - Experiment management and collaboration tool for ML/DL/RL specialists. Integration has a form of the [XGBoost callback](https://docs.neptune.ai/integrations/xgboost.html) that automatically logs training and evaluation metrics, as well as saved model (booster), feature importance chart and visualized trees.\n* [Optuna](https://optuna.org/) - An open source hyperparameter optimization framework to automate hyperparameter search. Optuna integrates with XGBoost in the [XGBoostPruningCallback](https://optuna.readthedocs.io/en/stable/reference/integration.html#optuna.integration.XGBoostPruningCallback) that let users easily prune unpromising trials.\n* [dtreeviz](https://github.com/parrt/dtreeviz) - A python library for decision tree visualization and model interpretation. Starting from version 1.0, dtreeviz is able to visualize tree ensembles produced by XGBoost.\n\n## Awards\n- [John Chambers Award](http://stat-computing.org/awards/jmc/winners.html) - 2016 Winner: XGBoost R Package, by Tong He (Simon Fraser University) and Tianqi Chen (University of Washington)\n- [InfoWorld\u2019s 2019 Technology of the Year Award](https://www.infoworld.com/article/3336072/application-development/infoworlds-2019-technology-of-the-year-award-winners.html)\n\n## Windows Binaries\nUnofficial windows binaries and instructions on how to use them are hosted on [Guido Tapia's blog](http://www.picnet.com.au/blogs/guido/post/2016/09/22/xgboost-windows-x64-binaries-for-download/)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/README.md\n## Integrations with 3rd party software\nOpen source integrations with XGBoost:\n* [Neptune.ai](http://neptune.ai/) - Experiment management and collaboration tool for ML/DL/RL specialists. Integration has a form of the [XGBoost callback](https://docs.neptune.ai/integrations/xgboost.html) that automatically logs training and evaluation metrics, as well as saved model (booster), feature importance chart and visualized trees.\n* [Optuna](https://optuna.org/) - An open source hyperparameter optimization framework to automate hyperparameter search. Optuna integrates with XGBoost in the [XGBoostPruningCallback](https://optuna.readthedocs.io/en/stable/reference/integration.html#optuna.integration.XGBoostPruningCallback) that let users easily prune unpromising trials.\n* [dtreeviz](https://github.com/parrt/dtreeviz) - A python library for decision tree visualization and model interpretation. Starting from version 1.0, dtreeviz is able to visualize tree ensembles produced by XGBoost.\n\n## Awards\n- [John Chambers Award](http://stat-computing.org/awards/jmc/winners.html) - 2016 Winner: XGBoost R Package, by Tong He (Simon Fraser University) and Tianqi Chen (University of Washington)\n- [InfoWorld\u2019s 2019 Technology of the Year Award](https://www.infoworld.com/article/3336072/application-development/infoworlds-2019-technology-of-the-year-award-winners.html)\n\n## Windows Binaries\nUnofficial windows binaries and instructions on how to use them are hosted on [Guido Tapia's blog](http://www.picnet.com.au/blogs/guido/post/2016/09/22/xgboost-windows-x64-binaries-for-download/)"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/nvflare/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/nvflare/README.md\n# Experimental Support of Federated XGBoost using NVFlare\n\nThis directory contains a demo of Federated Learning using\n[NVFlare](https://nvidia.github.io/NVFlare/).\n\n## Horizontal Federated XGBoost\n\nFor horizontal federated learning using XGBoost (data is split row-wise), check out the `horizontal` directory\n(see the [README](horizontal/README.md)).\n\n## Vertical Federated XGBoost\n\nFor vertical federated learning using XGBoost (data is split column-wise), check out the `vertical` directory\n(see the [README](vertical/README.md)).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/nvflare/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/nvflare/README.md\n# Experimental Support of Federated XGBoost using NVFlare\n\nThis directory contains a demo of Federated Learning using\n[NVFlare](https://nvidia.github.io/NVFlare/).\n\n## Horizontal Federated XGBoost\n\nFor horizontal federated learning using XGBoost (data is split row-wise), check out the `horizontal` directory\n(see the [README](horizontal/README.md)).\n\n## Vertical Federated XGBoost\n\nFor vertical federated learning using XGBoost (data is split column-wise), check out the `vertical` directory\n(see the [README](vertical/README.md))."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/nvflare/vertical/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/nvflare/vertical/README.md\n# Experimental Support of Vertical Federated XGBoost using NVFlare\n\nThis directory contains a demo of Vertical Federated Learning using\n[NVFlare](https://nvidia.github.io/NVFlare/).\n\n## Training with CPU only\n\nTo run the demo, first build XGBoost with the federated learning plugin enabled (see the\n[README](../../../plugin/federated/README.md)).\n\nInstall NVFlare:\n```shell\npip install nvflare\n```\n\nPrepare the data (note that this step will download the HIGGS dataset, which is 2.6GB compressed, and 7.5GB\nuncompressed, so make sure you have enough disk space and are on a fast internet connection):\n```shell\n./prepare_data.sh\n```\n\nStart the NVFlare federated server:\n```shell\n/tmp/nvflare/poc/server/startup/start.sh\n```\n\nIn another terminal, start the first worker:\n```shell\n/tmp/nvflare/poc/site-1/startup/start.sh\n```\n\nAnd the second worker:\n```shell\n/tmp/nvflare/poc/site-2/startup/start.sh\n```\n\nThen start the admin CLI:\n```shell\n/tmp/nvflare/poc/admin/startup/fl_admin.sh\n```\n\nIn the admin CLI, run the following command:\n```shell\nsubmit_job vertical-xgboost\n```\n\nOnce the training finishes, the model file should be written into\n`/tmp/nvlfare/poc/site-1/run_1/test.model.json` and `/tmp/nvflare/poc/site-2/run_1/test.model.json`\nrespectively.\n\nFinally, shutdown everything from the admin CLI, using `admin` as password:\n```shell\nshutdown client\nshutdown server\n```\n\n## Training with GPUs\n\nTo demo with Vertical Federated Learning using GPUs, make sure your machine has at least 2 GPUs.\nBuild XGBoost with the federated learning plugin enabled along with CUDA\n(see the [README](../../plugin/federated/README.md)).\n\nModify `../config/config_fed_client.json` and set `use_gpus` to `true`, then repeat the steps\nabove.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/nvflare/vertical/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/nvflare/vertical/README.md\n# Experimental Support of Vertical Federated XGBoost using NVFlare\n\nThis directory contains a demo of Vertical Federated Learning using\n[NVFlare](https://nvidia.github.io/NVFlare/).\n\n## Training with CPU only\n\nTo run the demo, first build XGBoost with the federated learning plugin enabled (see the\n[README](../../../plugin/federated/README.md)).\n\nInstall NVFlare:\n```shell\npip install nvflare\n```\n\nPrepare the data (note that this step will download the HIGGS dataset, which is 2.6GB compressed, and 7.5GB\nuncompressed, so make sure you have enough disk space and are on a fast internet connection):\n```shell\n./prepare_data.sh\n```\n\nStart the NVFlare federated server:\n```shell\n/tmp/nvflare/poc/server/startup/start.sh\n```\n\nIn another terminal, start the first worker:\n```shell\n/tmp/nvflare/poc/site-1/startup/start.sh\n```\n\nAnd the second worker:\n```shell\n/tmp/nvflare/poc/site-2/startup/start.sh\n```\n\nThen start the admin CLI:\n```shell\n/tmp/nvflare/poc/admin/startup/fl_admin.sh\n```\n\nIn the admin CLI, run the following command:\n```shell\nsubmit_job vertical-xgboost\n```\n\nOnce the training finishes, the model file should be written into\n`/tmp/nvlfare/poc/site-1/run_1/test.model.json` and `/tmp/nvflare/poc/site-2/run_1/test.model.json`\nrespectively.\n\nFinally, shutdown everything from the admin CLI, using `admin` as password:\n```shell\nshutdown client\nshutdown server\n```\n\n## Training with GPUs\n\nTo demo with Vertical Federated Learning using GPUs, make sure your machine has at least 2 GPUs.\nBuild XGBoost with the federated learning plugin enabled along with CUDA\n(see the [README](../../plugin/federated/README.md)).\n\nModify `../config/config_fed_client.json` and set `use_gpus` to `true`, then repeat the steps\nabove."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/nvflare/horizontal/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/nvflare/horizontal/README.md\n# Experimental Support of Horizontal Federated XGBoost using NVFlare\n\nThis directory contains a demo of Horizontal Federated Learning using\n[NVFlare](https://nvidia.github.io/NVFlare/).\n\n## Training with CPU only\n\nTo run the demo, first build XGBoost with the federated learning plugin enabled (see the\n[README](../../../plugin/federated/README.md)).\n\nInstall NVFlare:\n```shell\npip install nvflare\n```\n\nPrepare the data:\n```shell\n./prepare_data.sh\n```\n\nStart the NVFlare federated server:\n```shell\n/tmp/nvflare/poc/server/startup/start.sh\n```\n\nIn another terminal, start the first worker:\n```shell\n/tmp/nvflare/poc/site-1/startup/start.sh\n```\n\nAnd the second worker:\n```shell\n/tmp/nvflare/poc/site-2/startup/start.sh\n```\n\nThen start the admin CLI:\n```shell\n/tmp/nvflare/poc/admin/startup/fl_admin.sh\n```\n\nIn the admin CLI, run the following command:\n```shell\nsubmit_job horizontal-xgboost\n```\n\nMake a note of the job id:\n```console\nSubmitted job: 28309e77-a7c5-45e6-b2bc-c2e3655122d8\n```\n\nOn both workers, you should see train and eval losses printed:\n```console\n[10:45:41] [0]\teval-logloss:0.22646\ttrain-logloss:0.23316\n[10:45:41] [1]\teval-logloss:0.13776\ttrain-logloss:0.13654\n[10:45:41] [2]\teval-logloss:0.08036\ttrain-logloss:0.08243\n[10:45:41] [3]\teval-logloss:0.05830\ttrain-logloss:0.05645\n[10:45:41] [4]\teval-logloss:0.03825\ttrain-logloss:0.04148\n[10:45:41] [5]\teval-logloss:0.02660\ttrain-logloss:0.02958\n[10:45:41] [6]\teval-logloss:0.01386\ttrain-logloss:0.01918\n[10:45:41] [7]\teval-logloss:0.01018\ttrain-logloss:0.01331\n[10:45:41] [8]\teval-logloss:0.00847\ttrain-logloss:0.01112\n[10:45:41] [9]\teval-logloss:0.00691\ttrain-logloss:0.00662\n[10:45:41] [10]\teval-logloss:0.00543\ttrain-logloss:0.00503\n[10:45:41] [11]\teval-logloss:0.00445\ttrain-logloss:0.00420\n[10:45:41] [12]\teval-logloss:0.00336\ttrain-logloss:0.00355\n[10:45:41] [13]\teval-logloss:0.00277\ttrain-logloss:0.00280\n[10:45:41] [14]\teval-logloss:0.00252\ttrain-logloss:0.00244\n[10:45:41] [15]\teval-logloss:0.00177\ttrain-logloss:0.00193\n[10:45:41] [16]\teval-logloss:0.00156\ttrain-logloss:0.00161\n[10:45:41] [17]\teval-logloss:0.00135\ttrain-logloss:0.00142\n[10:45:41] [18]\teval-logloss:0.00123\ttrain-logloss:0.00125\n[10:45:41] [19]\teval-logloss:0.00106\ttrain-logloss:0.00107\n```\n\nOnce the training finishes, the model file should be written into\n`/tmp/nvlfare/poc/site-1/${job_id}/test.model.json` and `/tmp/nvflare/poc/site-2/${job_id}/test.model.json`\nrespectively, where `job_id` is the UUID printed out when we ran `submit_job`.\n\nFinally, shutdown everything from the admin CLI, using `admin` as password:\n```shell\nshutdown client\nshutdown server\n```\n\n## Training with GPUs\n\nTo demo with Federated Learning using GPUs, make sure your machine has at least 2 GPUs.\nBuild XGBoost with the federated learning plugin enabled along with CUDA\n(see the [README](../../plugin/federated/README.md)).\n\nModify `../config/config_fed_client.json` and set `use_gpus` to `true`, then repeat the steps\nabove.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/nvflare/horizontal/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/nvflare/horizontal/README.md\n# Experimental Support of Horizontal Federated XGBoost using NVFlare\n\nThis directory contains a demo of Horizontal Federated Learning using\n[NVFlare](https://nvidia.github.io/NVFlare/).\n\n## Training with CPU only\n\nTo run the demo, first build XGBoost with the federated learning plugin enabled (see the\n[README](../../../plugin/federated/README.md)).\n\nInstall NVFlare:\n```shell\npip install nvflare\n```\n\nPrepare the data:\n```shell\n./prepare_data.sh\n```\n\nStart the NVFlare federated server:\n```shell\n/tmp/nvflare/poc/server/startup/start.sh\n```\n\nIn another terminal, start the first worker:\n```shell\n/tmp/nvflare/poc/site-1/startup/start.sh\n```\n\nAnd the second worker:\n```shell\n/tmp/nvflare/poc/site-2/startup/start.sh\n```\n\nThen start the admin CLI:\n```shell\n/tmp/nvflare/poc/admin/startup/fl_admin.sh\n```\n\nIn the admin CLI, run the following command:\n```shell\nsubmit_job horizontal-xgboost\n```\n\nMake a note of the job id:\n```console\nSubmitted job: 28309e77-a7c5-45e6-b2bc-c2e3655122d8\n```\n\nOn both workers, you should see train and eval losses printed:\n```console\n[10:45:41] [0]\teval-logloss:0.22646\ttrain-logloss:0.23316\n[10:45:41] [1]\teval-logloss:0.13776\ttrain-logloss:0.13654\n[10:45:41] [2]\teval-logloss:0.08036\ttrain-logloss:0.08243\n[10:45:41] [3]\teval-logloss:0.05830\ttrain-logloss:0.05645\n[10:45:41] [4]\teval-logloss:0.03825\ttrain-logloss:0.04148\n[10:45:41] [5]\teval-logloss:0.02660\ttrain-logloss:0.02958\n[10:45:41] [6]\teval-logloss:0.01386\ttrain-logloss:0.01918\n[10:45:41] [7]\teval-logloss:0.01018\ttrain-logloss:0.01331\n[10:45:41] [8]\teval-logloss:0.00847\ttrain-logloss:0.01112\n[10:45:41] [9]\teval-logloss:0.00691\ttrain-logloss:0.00662\n[10:45:41] [10]\teval-logloss:0.00543\ttrain-logloss:0.00503\n[10:45:41] [11]\teval-logloss:0.00445\ttrain-logloss:0.00420\n[10:45:41] [12]\teval-logloss:0.00336\ttrain-logloss:0.00355\n[10:45:41] [13]\teval-logloss:0.00277\ttrain-logloss:0.00280\n[10:45:41] [14]\teval-logloss:0.00252\ttrain-logloss:0.00244\n[10:45:41] [15]\teval-logloss:0.00177\ttrain-logloss:0.00193\n[10:45:41] [16]\teval-logloss:0.00156\ttrain-logloss:0.00161\n[10:45:41] [17]\teval-logloss:0.00135\ttrain-logloss:0.00142\n[10:45:41] [18]\teval-logloss:0.00123\ttrain-logloss:0.00125\n[10:45:41] [19]\teval-logloss:0.00106\ttrain-logloss:0.00107\n```\n\nOnce the training finishes, the model file should be written into\n`/tmp/nvlfare/poc/site-1/${job_id}/test.model.json` and `/tmp/nvflare/poc/site-2/${job_id}/test.model.json`\nrespectively, where `job_id` is the UUID printed out when we ran `submit_job`.\n\nFinally, shutdown everything from the admin CLI, using `admin` as password:\n```shell\nshutdown client\nshutdown server\n```\n\n## Training with GPUs\n\nTo demo with Federated Learning using GPUs, make sure your machine has at least 2 GPUs.\nBuild XGBoost with the federated learning plugin enabled along with CUDA\n(see the [README](../../plugin/federated/README.md)).\n\nModify `../config/config_fed_client.json` and set `use_gpus` to `true`, then repeat the steps\nabove."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/multiclass_classification/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/multiclass_classification/README.md\nDemonstrating how to use XGBoost accomplish Multi-Class classification task on [UCI Dermatology dataset](https://archive.ics.uci.edu/ml/datasets/Dermatology)\n\nMake sure you make xgboost python module in ../../python\n\n1. Run runexp.sh\n```bash\n./runexp.sh\n```\n\n**R version** please see the `train.R`.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/multiclass_classification/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/multiclass_classification/README.md\nDemonstrating how to use XGBoost accomplish Multi-Class classification task on [UCI Dermatology dataset](https://archive.ics.uci.edu/ml/datasets/Dermatology)\n\nMake sure you make xgboost python module in ../../python\n\n1. Run runexp.sh\n```bash\n./runexp.sh\n```\n\n**R version** please see the `train.R`."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\nBinary Classification\n=====================\nThis is the quick start tutorial for xgboost CLI version.\nHere we demonstrate how to use XGBoost for a binary classification task. Before getting started, make sure you compile xgboost in the root directory of the project by typing ```make```.\nThe script 'runexp.sh' can be used to run the demo. Here we use [mushroom dataset](https://archive.ics.uci.edu/ml/datasets/Mushroom) from UCI machine learning repository.\n\n### Tutorial\n#### Generate Input Data\nXGBoost takes LIBSVM format. An example of faked input data is below:\n```\n1 101:1.2 102:0.03\n0 1:2.1 10001:300 10002:400\n...\n```\nEach line represent a single instance, and in the first line '1' is the instance label,'101' and '102' are feature indices, '1.2' and '0.03' are feature values. In the binary classification case, '1' is used to indicate positive samples, and '0' is used to indicate negative samples. We also support probability values in [0,1] as label, to indicate the probability of the instance being positive.\n\n\nFirst we will transform the dataset into classic LIBSVM format and split the data into training set and test set by running:\n```\npython mapfeat.py\npython mknfold.py agaricus.txt 1\n```\nThe two files, 'agaricus.txt.train' and 'agaricus.txt.test' will be used as training set and test set.\n\n#### Training\nThen we can run the training process:\n```\n../../xgboost mushroom.conf\n```\n\nmushroom.conf is the configuration for both training and testing. Each line containing the [attribute]=[value] configuration:\n\n```conf\n# General Parameters, see comment for each definition\n# can be gbtree or gblinear\nbooster = gbtree\n# choose logistic regression loss function for binary classification\nobjective = binary:logistic\n\n# Tree Booster Parameters\n# step size shrinkage\neta = 1.0\n# minimum loss reduction required to make a further partition\ngamma = 1.0\n# minimum sum of instance weight(hessian) needed in a child\nmin_child_weight = 1\n# maximum depth of a tree\nmax_depth = 3\n\n# Task Parameters\n# the number of round to do boosting\nnum_round = 2\n# 0 means do not save any model except the final round model\nsave_period = 0\n# The path of training data\ndata = \"agaricus.txt.train\"\n# The path of validation data, used to monitor training process, here [test] sets name of the validation set\neval[test] = \"agaricus.txt.test\"\n# The path of test data\ntest:data = \"agaricus.txt.test\"\n```\nWe use the tree booster and logistic regression objective in our setting. This indicates that we accomplish our task using classic gradient boosting regression tree(GBRT), which is a promising method for binary classification.\n\nThe parameters shown in the example gives the most common ones that are needed to use xgboost.\nIf you are interested in more parameter settings, the complete parameter settings and detailed descriptions are [here](https://xgboost.readthedocs.io/en/stable/parameter.html). Besides putting the parameters in the configuration file, we can set them by passing them as arguments as below:\n\n```\n../../xgboost mushroom.conf max_depth=6\n```\nThis means that the parameter max_depth will be set as 6 rather than 3 in the conf file. When you use command line, make sure max_depth=6 is passed in as single argument, i.e. do not contain space in the argument. When a parameter setting is provided in both command line input and  the config file, the command line setting will override the setting in config file.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\nBinary Classification\n=====================\nThis is the quick start tutorial for xgboost CLI version.\nHere we demonstrate how to use XGBoost for a binary classification task. Before getting started, make sure you compile xgboost in the root directory of the project by typing ```make```.\nThe script 'runexp.sh' can be used to run the demo. Here we use [mushroom dataset](https://archive.ics.uci.edu/ml/datasets/Mushroom) from UCI machine learning repository.\n\n### Tutorial\n#### Generate Input Data\nXGBoost takes LIBSVM format. An example of faked input data is below:\n```\n1 101:1.2 102:0.03\n0 1:2.1 10001:300 10002:400\n...\n```\nEach line represent a single instance, and in the first line '1' is the instance label,'101' and '102' are feature indices, '1.2' and '0.03' are feature values. In the binary classification case, '1' is used to indicate positive samples, and '0' is used to indicate negative samples. We also support probability values in [0,1] as label, to indicate the probability of the instance being positive.\n\n\nFirst we will transform the dataset into classic LIBSVM format and split the data into training set and test set by running:\n```\npython mapfeat.py\npython mknfold.py agaricus.txt 1\n```\nThe two files, 'agaricus.txt.train' and 'agaricus.txt.test' will be used as training set and test set.\n\n#### Training\nThen we can run the training process:\n```\n../../xgboost mushroom.conf\n```\n\nmushroom.conf is the configuration for both training and testing. Each line containing the [attribute]=[value] configuration:\n\n```conf\n# General Parameters, see comment for each definition\n# can be gbtree or gblinear\nbooster = gbtree\n# choose logistic regression loss function for binary classification\nobjective = binary:logistic\n\n# Tree Booster Parameters\n# step size shrinkage\neta = 1.0\n# minimum loss reduction required to make a further partition\ngamma = 1.0\n# minimum sum of instance weight(hessian) needed in a child\nmin_child_weight = 1\n# maximum depth of a tree\nmax_depth = 3\n\n# Task Parameters\n# the number of round to do boosting\nnum_round = 2\n# 0 means do not save any model except the final round model\nsave_period = 0\n# The path of training data\ndata = \"agaricus.txt.train\"\n# The path of validation data, used to monitor training process, here [test] sets name of the validation set\neval[test] = \"agaricus.txt.test\"\n# The path of test data\ntest:data = \"agaricus.txt.test\"\n```\nWe use the tree booster and logistic regression objective in our setting. This indicates that we accomplish our task using classic gradient boosting regression tree(GBRT), which is a promising method for binary classification.\n\nThe parameters shown in the example gives the most common ones that are needed to use xgboost.\nIf you are interested in more parameter settings, the complete parameter settings and detailed descriptions are [here](https://xgboost.readthedocs.io/en/stable/parameter.html). Besides putting the parameters in the configuration file, we can set them by passing them as arguments as below:\n\n```\n../../xgboost mushroom.conf max_depth=6\n```\nThis means that the parameter max_depth will be set as 6 rather than 3 in the conf file. When you use command line, make sure max_depth=6 is passed in as single argument, i.e. do not contain space in the argument. When a parameter setting is provided in both command line input and  the config file, the command line setting will override the setting in config file."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\nIn this example, we use tree booster for gradient boosting. If you would like to use linear booster for regression, you can keep all the parameters except booster and the tree booster parameters as below:\n```conf\n# General Parameters\n# choose the linear booster\nbooster = gblinear\n...\n\n# Change Tree Booster Parameters into Linear Booster Parameters\n# L2 regularization term on weights, default 0\nlambda = 0.01\n# L1 regularization term on weights, default 0\nalpha = 0.01\n# L2 regularization term on bias, default 0\nlambda_bias = 0.01\n\n# Regression Parameters\n...\n```\n\n#### Get Predictions\nAfter training, we can use the output model to get the prediction of the test data:\n```\n../../xgboost mushroom.conf task=pred model_in=0002.model\n```\nFor binary classification, the output predictions are probability confidence scores in [0,1], corresponds to the probability of the label to be positive.\n\n#### Dump Model\nThis is a preliminary feature, so only tree models support text dump. XGBoost can display the tree models in text or JSON files, and we can scan the model in an easy way:\n```\n../../xgboost mushroom.conf task=dump model_in=0002.model name_dump=dump.raw.txt\n../../xgboost mushroom.conf task=dump model_in=0002.model fmap=featmap.txt name_dump=dump.nice.txt\n```\n\nIn this demo, the tree boosters obtained will be printed in dump.raw.txt and dump.nice.txt, and the latter one is easier to understand because of usage of feature mapping featmap.txt\n\nFormat of ```featmap.txt: <featureid> <featurename> <q or i or int>\\n ```:\n  - Feature id must be from 0 to number of features, in sorted order.\n  - i means this feature is binary indicator feature\n  - q means this feature is a quantitative value, such as age, time, can be missing\n  - int means this feature is integer value (when int is hinted, the decision boundary will be integer)\n\n#### Monitoring Progress\nWhen you run training we can find there are messages displayed on screen\n```\ntree train end, 1 roots, 12 extra nodes, 0 pruned nodes ,max_depth=3\n[0]  test-error:0.016139\nboosting round 1, 0 sec elapsed\n\ntree train end, 1 roots, 10 extra nodes, 0 pruned nodes ,max_depth=3\n[1]  test-error:0.000000\n```\nThe messages for evaluation are printed into stderr, so if you want only to log the evaluation progress, simply type\n```\n../../xgboost mushroom.conf 2>log.txt\n```\nThen you can find the following content in log.txt\n```\n[0]     test-error:0.016139\n[1]     test-error:0.000000\n```\nWe can also monitor both training and test statistics, by adding following lines to configure\n```conf\neval[test] = \"agaricus.txt.test\"\neval[trainname] = \"agaricus.txt.train\"\n```\nRun the command again, we can find the log file becomes\n```\n[0]     test-error:0.016139     trainname-error:0.014433\n[1]     test-error:0.000000     trainname-error:0.001228\n```\nThe rule is eval[name-printed-in-log] = filename, then the file will be added to monitoring process, and evaluated each round.\n\nxgboost also supports monitoring multiple metrics, suppose we also want to monitor average log-likelihood of each prediction during training, simply add ```eval_metric=logloss``` to configure. Run again, we can find the log file becomes\n```\n[0]     test-error:0.016139     test-negllik:0.029795   trainname-error:0.014433        trainname-negllik:0.027023\n[1]     test-error:0.000000     test-negllik:0.000000   trainname-error:0.001228        trainname-negllik:0.002457\n```", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\nIn this example, we use tree booster for gradient boosting. If you would like to use linear booster for regression, you can keep all the parameters except booster and the tree booster parameters as below:\n```conf\n# General Parameters\n# choose the linear booster\nbooster = gblinear\n...\n\n# Change Tree Booster Parameters into Linear Booster Parameters\n# L2 regularization term on weights, default 0\nlambda = 0.01\n# L1 regularization term on weights, default 0\nalpha = 0.01\n# L2 regularization term on bias, default 0\nlambda_bias = 0.01\n\n# Regression Parameters\n...\n```\n\n#### Get Predictions\nAfter training, we can use the output model to get the prediction of the test data:\n```\n../../xgboost mushroom.conf task=pred model_in=0002.model\n```\nFor binary classification, the output predictions are probability confidence scores in [0,1], corresponds to the probability of the label to be positive.\n\n#### Dump Model\nThis is a preliminary feature, so only tree models support text dump. XGBoost can display the tree models in text or JSON files, and we can scan the model in an easy way:\n```\n../../xgboost mushroom.conf task=dump model_in=0002.model name_dump=dump.raw.txt\n../../xgboost mushroom.conf task=dump model_in=0002.model fmap=featmap.txt name_dump=dump.nice.txt\n```\n\nIn this demo, the tree boosters obtained will be printed in dump.raw.txt and dump.nice.txt, and the latter one is easier to understand because of usage of feature mapping featmap.txt\n\nFormat of ```featmap.txt: <featureid> <featurename> <q or i or int>\\n ```:\n  - Feature id must be from 0 to number of features, in sorted order.\n  - i means this feature is binary indicator feature\n  - q means this feature is a quantitative value, such as age, time, can be missing\n  - int means this feature is integer value (when int is hinted, the decision boundary will be integer)\n\n#### Monitoring Progress\nWhen you run training we can find there are messages displayed on screen\n```\ntree train end, 1 roots, 12 extra nodes, 0 pruned nodes ,max_depth=3\n[0]  test-error:0.016139\nboosting round 1, 0 sec elapsed\n\ntree train end, 1 roots, 10 extra nodes, 0 pruned nodes ,max_depth=3\n[1]  test-error:0.000000\n```\nThe messages for evaluation are printed into stderr, so if you want only to log the evaluation progress, simply type\n```\n../../xgboost mushroom.conf 2>log.txt\n```\nThen you can find the following content in log.txt\n```\n[0]     test-error:0.016139\n[1]     test-error:0.000000\n```\nWe can also monitor both training and test statistics, by adding following lines to configure\n```conf\neval[test] = \"agaricus.txt.test\"\neval[trainname] = \"agaricus.txt.train\"\n```\nRun the command again, we can find the log file becomes\n```\n[0]     test-error:0.016139     trainname-error:0.014433\n[1]     test-error:0.000000     trainname-error:0.001228\n```\nThe rule is eval[name-printed-in-log] = filename, then the file will be added to monitoring process, and evaluated each round.\n\nxgboost also supports monitoring multiple metrics, suppose we also want to monitor average log-likelihood of each prediction during training, simply add ```eval_metric=logloss``` to configure. Run again, we can find the log file becomes\n```\n[0]     test-error:0.016139     test-negllik:0.029795   trainname-error:0.014433        trainname-negllik:0.027023\n[1]     test-error:0.000000     test-negllik:0.000000   trainname-error:0.001228        trainname-negllik:0.002457\n```"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\n### Saving Progress Models\nIf you want to save model every two round, simply set save_period=2. You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.\n\n#### Continue from Existing Model\nIf you want to continue boosting from existing model, say 0002.model, use\n```\n../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model\n```\nxgboost will load from 0002.model continue boosting for 2 rounds, and save output to continue.model. However, beware that the training and evaluation data specified in mushroom.conf should not change when you use this function.\n#### Use Multi-Threading\nWhen you are working with a large dataset, you may want to take advantage of parallelism. If your compiler supports OpenMP, xgboost is naturally multi-threaded, to set number of parallel running add ```nthread``` parameter to your configuration.\nEg. ```nthread=10```\n\nSet nthread to be the number of your real cpu (On Unix, this can be found using ```lscpu```)\nSome systems will have ```Thread(s) per core = 2```, for example, a 4 core cpu with 8 threads, in such case set ```nthread=4``` and not 8.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/binary_classification/README.md\n### Saving Progress Models\nIf you want to save model every two round, simply set save_period=2. You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.\n\n#### Continue from Existing Model\nIf you want to continue boosting from existing model, say 0002.model, use\n```\n../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model\n```\nxgboost will load from 0002.model continue boosting for 2 rounds, and save output to continue.model. However, beware that the training and evaluation data specified in mushroom.conf should not change when you use this function.\n#### Use Multi-Threading\nWhen you are working with a large dataset, you may want to take advantage of parallelism. If your compiler supports OpenMP, xgboost is naturally multi-threaded, to set number of parallel running add ```nthread``` parameter to your configuration.\nEg. ```nthread=10```\n\nSet nthread to be the number of your real cpu (On Unix, this can be found using ```lscpu```)\nSome systems will have ```Thread(s) per core = 2```, for example, a 4 core cpu with 8 threads, in such case set ```nthread=4``` and not 8."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/CLI/yearpredMSD/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/yearpredMSD/README.md\nDemonstrating how to use XGBoost on [Year Prediction task of Million Song Dataset](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)\n\n1. Run runexp.sh\n```bash\n./runexp.sh\n```\n\nYou can also use the script to prepare LIBSVM format, and run the [Distributed Version](../../multi-node).\nNote that though that normally you only need to use single machine for dataset at this scale, and use distributed version for larger scale dataset.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/CLI/yearpredMSD/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/yearpredMSD/README.md\nDemonstrating how to use XGBoost on [Year Prediction task of Million Song Dataset](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD)\n\n1. Run runexp.sh\n```bash\n./runexp.sh\n```\n\nYou can also use the script to prepare LIBSVM format, and run the [Distributed Version](../../multi-node).\nNote that though that normally you only need to use single machine for dataset at this scale, and use distributed version for larger scale dataset."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/CLI/regression/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/regression/README.md\nRegression\n====\nUsing XGBoost for regression is very similar to using it for binary classification. We suggest that you can refer to the [binary classification demo](../binary_classification) first. In XGBoost if we use negative log likelihood as the loss function for regression, the training procedure is same as training binary classifier of XGBoost.\n\n### Tutorial\nThe dataset we used is the [computer hardware dataset from UCI repository](https://archive.ics.uci.edu/ml/datasets/Computer+Hardware). The demo for regression is almost the same as the [binary classification demo](../binary_classification), except a little difference in general parameter:\n```\n# General parameter\n# this is the only difference with classification, use reg:squarederror to do linear regression\n# when labels are in [0,1] we can also use reg:logistic\nobjective = reg:squarederror\n...\n\n```\n\nThe input format is same as binary classification, except that the label is now the target regression values. We use linear regression here, if we want use objective = reg:logistic logistic regression, the label needed to be pre-scaled into [0,1].", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/CLI/regression/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/regression/README.md\nRegression\n====\nUsing XGBoost for regression is very similar to using it for binary classification. We suggest that you can refer to the [binary classification demo](../binary_classification) first. In XGBoost if we use negative log likelihood as the loss function for regression, the training procedure is same as training binary classifier of XGBoost.\n\n### Tutorial\nThe dataset we used is the [computer hardware dataset from UCI repository](https://archive.ics.uci.edu/ml/datasets/Computer+Hardware). The demo for regression is almost the same as the [binary classification demo](../binary_classification), except a little difference in general parameter:\n```\n# General parameter\n# this is the only difference with classification, use reg:squarederror to do linear regression\n# when labels are in [0,1] we can also use reg:logistic\nobjective = reg:squarederror\n...\n\n```\n\nThe input format is same as binary classification, except that the label is now the target regression values. We use linear regression here, if we want use objective = reg:logistic logistic regression, the label needed to be pre-scaled into [0,1]."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/CLI/distributed-training/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/distributed-training/README.md\nDistributed XGBoost Training\n============================\nThis is an tutorial of Distributed XGBoost Training.\nCurrently xgboost supports distributed training via CLI program with the configuration file.\nThere is also plan push distributed python and other language bindings, please open an issue\nif you are interested in contributing.\n\nBuild XGBoost with Distributed Filesystem Support\n-------------------------------------------------\nTo use distributed xgboost, you only need to turn the options on to build\nwith distributed filesystems(HDFS or S3) in cmake.\n\n```\ncmake <path/to/xgboost> -DUSE_HDFS=ON -DUSE_S3=ON -DUSE_AZURE=ON\n```\n\n\nStep by Step Tutorial on AWS\n----------------------------\nCheckout [this tutorial](https://xgboost.readthedocs.org/en/latest/tutorials/aws_yarn.html) for running distributed xgboost.\n\n\nModel Analysis\n--------------\nXGBoost is exchangeable across all bindings and platforms.\nThis means you can use python or R to analyze the learnt model and do prediction.\nFor example, you can use the [plot_model.ipynb](plot_model.ipynb) to visualize the learnt model.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/CLI/distributed-training/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/CLI/distributed-training/README.md\nDistributed XGBoost Training\n============================\nThis is an tutorial of Distributed XGBoost Training.\nCurrently xgboost supports distributed training via CLI program with the configuration file.\nThere is also plan push distributed python and other language bindings, please open an issue\nif you are interested in contributing.\n\nBuild XGBoost with Distributed Filesystem Support\n-------------------------------------------------\nTo use distributed xgboost, you only need to turn the options on to build\nwith distributed filesystems(HDFS or S3) in cmake.\n\n```\ncmake <path/to/xgboost> -DUSE_HDFS=ON -DUSE_S3=ON -DUSE_AZURE=ON\n```\n\n\nStep by Step Tutorial on AWS\n----------------------------\nCheckout [this tutorial](https://xgboost.readthedocs.org/en/latest/tutorials/aws_yarn.html) for running distributed xgboost.\n\n\nModel Analysis\n--------------\nXGBoost is exchangeable across all bindings and platforms.\nThis means you can use python or R to analyze the learnt model and do prediction.\nFor example, you can use the [plot_model.ipynb](plot_model.ipynb) to visualize the learnt model."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/data/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/data/README.md\nThis folder contains processed example dataset used by the demos.\nCopyright of the dataset belongs to the original copyright holder", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/data/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/data/README.md\nThis folder contains processed example dataset used by the demos.\nCopyright of the dataset belongs to the original copyright holder"}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/kaggle-higgs/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/kaggle-higgs/README.md\nHighlights\n=====\nHiggs challenge ends recently, xgboost is being used by many users. This list highlights the xgboost solutions of players\n* Blogpost by phunther: [Winning solution of Kaggle Higgs competition: what a single model can do](http://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/)\n* The solution by Tianqi Chen and Tong He [Link](https://github.com/hetong007/higgsml)\n\nGuide for Kaggle Higgs Challenge\n=====\n\nThis is the folder giving example of how to use XGBoost Python Module  to run Kaggle Higgs competition\n\nThis script will achieve about 3.600 AMS score in public leaderboard. To get start, you need do following step:\n\n1. Compile the XGBoost python lib\n```bash\ncd ../..\nmake\n```\n\n2. Put training.csv test.csv on folder './data' (you can create a symbolic link)\n\n3. Run ./run.sh\n\nSpeed\n=====\nspeedtest.py compares xgboost's speed on this dataset with sklearn.GBM\n\n\nUsing R module\n=====\n* Alternatively, you can run using R, higgs-train.R and higgs-pred.R.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/kaggle-higgs/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/kaggle-higgs/README.md\nHighlights\n=====\nHiggs challenge ends recently, xgboost is being used by many users. This list highlights the xgboost solutions of players\n* Blogpost by phunther: [Winning solution of Kaggle Higgs competition: what a single model can do](http://no2147483647.wordpress.com/2014/09/17/winning-solution-of-kaggle-higgs-competition-what-a-single-model-can-do/)\n* The solution by Tianqi Chen and Tong He [Link](https://github.com/hetong007/higgsml)\n\nGuide for Kaggle Higgs Challenge\n=====\n\nThis is the folder giving example of how to use XGBoost Python Module  to run Kaggle Higgs competition\n\nThis script will achieve about 3.600 AMS score in public leaderboard. To get start, you need do following step:\n\n1. Compile the XGBoost python lib\n```bash\ncd ../..\nmake\n```\n\n2. Put training.csv test.csv on folder './data' (you can create a symbolic link)\n\n3. Run ./run.sh\n\nSpeed\n=====\nspeedtest.py compares xgboost's speed on this dataset with sklearn.GBM\n\n\nUsing R module\n=====\n* Alternatively, you can run using R, higgs-train.R and higgs-pred.R."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/c-api/basic/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/basic/README.md\nC-APIs\n===\n\n**XGBoost** implements a C API originally designed for various language\nbindings.  For detailed reference, please check xgboost/c_api.h.  Here is a\ndemonstration of using the API.\n\n# CMake\nIf you use **CMake** for your project, you can either install **XGBoost**\nsomewhere in your system and tell CMake to find it by calling\n`find_package(xgboost)`, or put **XGBoost** inside your project's source tree\nand call **CMake** command: `add_subdirectory(xgboost)`.  To use\n`find_package()`, put the following in your **CMakeLists.txt**:\n\n``` CMake\nfind_package(xgboost REQUIRED)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost::xgboost)\n```\n\nIf you want to put XGBoost inside your project (like git submodule), use this\ninstead:\n``` CMake\nadd_subdirectory(xgboost)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost)\n```\n\n# make\nYou can start by modifying the makefile in this directory to fit your need.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/c-api/basic/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/basic/README.md\nC-APIs\n===\n\n**XGBoost** implements a C API originally designed for various language\nbindings.  For detailed reference, please check xgboost/c_api.h.  Here is a\ndemonstration of using the API.\n\n# CMake\nIf you use **CMake** for your project, you can either install **XGBoost**\nsomewhere in your system and tell CMake to find it by calling\n`find_package(xgboost)`, or put **XGBoost** inside your project's source tree\nand call **CMake** command: `add_subdirectory(xgboost)`.  To use\n`find_package()`, put the following in your **CMakeLists.txt**:\n\n``` CMake\nfind_package(xgboost REQUIRED)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost::xgboost)\n```\n\nIf you want to put XGBoost inside your project (like git submodule), use this\ninstead:\n``` CMake\nadd_subdirectory(xgboost)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost)\n```\n\n# make\nYou can start by modifying the makefile in this directory to fit your need."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md\nDefining a Custom Data Iterator to Load Data from External Memory\n=================================================================\n\nA simple demo for using custom data iterator with XGBoost.  The feature is still\n**experimental** and not ready for production use.  If you are not familiar with C API,\nplease read its introduction in our tutorials and visit the basic demo first.\n\nDefining Data Iterator\n----------------------\n\nIn the example, we define a custom data iterator with 2 methods: `reset` and `next`.  The\n`next` method passes data into XGBoost and tells XGBoost whether the iterator has reached\nits end, and the `reset` method resets iterations. One important detail when using the C\nAPI for data iterator is users need to make sure that the data passed into `next` method\nmust be kept in memory until the next iteration or `reset` is called.  The external memory\nDMatrix is not limited to training, but also valid for other features like prediction.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/demo/c-api/external-memory/README.md\nDefining a Custom Data Iterator to Load Data from External Memory\n=================================================================\n\nA simple demo for using custom data iterator with XGBoost.  The feature is still\n**experimental** and not ready for production use.  If you are not familiar with C API,\nplease read its introduction in our tutorials and visit the basic demo first.\n\nDefining Data Iterator\n----------------------\n\nIn the example, we define a custom data iterator with 2 methods: `reset` and `next`.  The\n`next` method passes data into XGBoost and tells XGBoost whether the iterator has reached\nits end, and the `reset` method resets iterations. One important detail when using the C\nAPI for data iterator is users need to make sure that the data passed into `next` method\nmust be kept in memory until the next iteration or `reset` is called.  The external memory\nDMatrix is not limited to training, but also valid for other features like prediction."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/R-package/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/R-package/README.md\nXGBoost R Package\n=================\n\n[![CRAN Status Badge](http://www.r-pkg.org/badges/version/xgboost)](https://cran.r-project.org/web/packages/xgboost)\n[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/xgboost)](https://cran.rstudio.com/web/packages/xgboost/index.html)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org/en/latest/R-package/index.html)\n\nResources\n---------\n* [XGBoost R Package Online Documentation](https://xgboost.readthedocs.org/en/stable/R-package/index.html)\n  - Check this out for detailed documents, examples and tutorials.\n\nInstallation\n------------\n\nWe are [on CRAN](https://cran.r-project.org/web/packages/xgboost/index.html) now. For stable/pre-compiled(for Windows and OS X) version, please install from CRAN:\n\n```r\ninstall.packages('xgboost')\n```\n\nFor more detailed installation instructions, please see [here](https://xgboost.readthedocs.io/en/stable/install.html).\n\nDevelopment\n-----------\n\n* See the [R Package section](https://xgboost.readthedocs.io/en/latest/contrib/coding_guide.html#r-coding-guideline) of the contributors guide.", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/R-package/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/R-package/README.md\nXGBoost R Package\n=================\n\n[![CRAN Status Badge](http://www.r-pkg.org/badges/version/xgboost)](https://cran.r-project.org/web/packages/xgboost)\n[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/xgboost)](https://cran.rstudio.com/web/packages/xgboost/index.html)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org/en/latest/R-package/index.html)\n\nResources\n---------\n* [XGBoost R Package Online Documentation](https://xgboost.readthedocs.org/en/stable/R-package/index.html)\n  - Check this out for detailed documents, examples and tutorials.\n\nInstallation\n------------\n\nWe are [on CRAN](https://cran.r-project.org/web/packages/xgboost/index.html) now. For stable/pre-compiled(for Windows and OS X) version, please install from CRAN:\n\n```r\ninstall.packages('xgboost')\n```\n\nFor more detailed installation instructions, please see [here](https://xgboost.readthedocs.io/en/stable/install.html).\n\nDevelopment\n-----------\n\n* See the [R Package section](https://xgboost.readthedocs.io/en/latest/contrib/coding_guide.html#r-coding-guideline) of the contributors guide."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/jvm-packages/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/jvm-packages/README.md\n# XGBoost4J: Distributed XGBoost for Scala/Java\n[![Build Status](https://badge.buildkite.com/aca47f40a32735c00a8550540c5eeff6a4c1d246a580cae9b0.svg?branch=master)](https://buildkite.com/xgboost/xgboost-ci)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org/en/latest/jvm/index.html)\n[![GitHub license](http://dmlc.github.io/img/apache2.svg)](../LICENSE)\n\n[Documentation](https://xgboost.readthedocs.org/en/stable/jvm/index.html) |\n[Resources](../demo/README.md) |\n[Release Notes](../NEWS.md)\n\nXGBoost4J is the JVM package of xgboost. It brings all the optimizations and power xgboost\ninto JVM ecosystem.\n\n- Train XGBoost models in scala and java with easy customization.\n- Run distributed xgboost natively on jvm frameworks such as Apache Flink and Apache\nSpark.\n\nYou can find more about XGBoost on [Documentation](https://xgboost.readthedocs.org/en/stable/jvm/index.html) and [Resource Page](../demo/README.md).", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/jvm-packages/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/jvm-packages/README.md\n# XGBoost4J: Distributed XGBoost for Scala/Java\n[![Build Status](https://badge.buildkite.com/aca47f40a32735c00a8550540c5eeff6a4c1d246a580cae9b0.svg?branch=master)](https://buildkite.com/xgboost/xgboost-ci)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org/en/latest/jvm/index.html)\n[![GitHub license](http://dmlc.github.io/img/apache2.svg)](../LICENSE)\n\n[Documentation](https://xgboost.readthedocs.org/en/stable/jvm/index.html) |\n[Resources](../demo/README.md) |\n[Release Notes](../NEWS.md)\n\nXGBoost4J is the JVM package of xgboost. It brings all the optimizations and power xgboost\ninto JVM ecosystem.\n\n- Train XGBoost models in scala and java with easy customization.\n- Run distributed xgboost natively on jvm frameworks such as Apache Flink and Apache\nSpark.\n\nYou can find more about XGBoost on [Documentation](https://xgboost.readthedocs.org/en/stable/jvm/index.html) and [Resource Page](../demo/README.md)."}, {"codebase": "xgboost", "reference": "https://github.com/dmlc/xgboost/blob/main/jvm-packages/xgboost4j-example/README.md", "text": "xgboost\nhttps://github.com/dmlc/xgboost/blob/main/jvm-packages/xgboost4j-example/README.md\nXGBoost4J Code Examples\n=======================\n\n## Java API\n* [Basic walkthrough of wrappers](src/main/java/ml/dmlc/xgboost4j/java/example/BasicWalkThrough.java)\n* [Customize loss function, and evaluation metric](src/main/java/ml/dmlc/xgboost4j/java/example/CustomObjective.java)\n* [Boosting from existing prediction](src/main/java/ml/dmlc/xgboost4j/java/example/BoostFromPrediction.java)\n* [Predicting using first n trees](src/main/java/ml/dmlc/xgboost4j/java/example/PredictFirstNtree.java)\n* [Generalized Linear Model](src/main/java/ml/dmlc/xgboost4j/java/example/GeneralizedLinearModel.java)\n* [Cross validation](src/main/java/ml/dmlc/xgboost4j/java/example/CrossValidation.java)\n* [Predicting leaf indices](src/main/java/ml/dmlc/xgboost4j/java/example/PredictLeafIndices.java)\n* [External Memory](src/main/java/ml/dmlc/xgboost4j/java/example/ExternalMemory.java)\n* [Early Stopping](src/main/java/ml/dmlc/xgboost4j/java/example/EarlyStopping.java)\n\n## Scala API\n\n* [Basic walkthrough of wrappers](src/main/scala/ml/dmlc/xgboost4j/scala/example/BasicWalkThrough.scala)\n* [Customize loss function, and evaluation metric](src/main/scala/ml/dmlc/xgboost4j/scala/example/CustomObjective.scala)\n* [Boosting from existing prediction](src/main/scala/ml/dmlc/xgboost4j/scala/example/BoostFromPrediction.scala)\n* [Predicting using first n trees](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictFirstNTree.scala)\n* [Generalized Linear Model](src/main/scala/ml/dmlc/xgboost4j/scala/example/GeneralizedLinearModel.scala)\n* [Cross validation](src/main/scala/ml/dmlc/xgboost4j/scala/example/CrossValidation.scala)\n* [Predicting leaf indices](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictLeafIndices.scala)\n* [External Memory](src/main/scala/ml/dmlc/xgboost4j/scala/example/ExternalMemory.scala)\n\n## Spark API\n* [Distributed Training with Spark](src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/SparkMLlibPipeline.scala)\n\n## Flink API\n* [Distributed Training with Flink](src/main/scala/ml/dmlc/xgboost4j/scala/example/flink/DistTrainWithFlink.scala)", "enriched_text": "[Codebase: xgboost] [File: https://github.com/dmlc/xgboost/blob/main/jvm-packages/xgboost4j-example/README.md]\n\nxgboost\nhttps://github.com/dmlc/xgboost/blob/main/jvm-packages/xgboost4j-example/README.md\nXGBoost4J Code Examples\n=======================\n\n## Java API\n* [Basic walkthrough of wrappers](src/main/java/ml/dmlc/xgboost4j/java/example/BasicWalkThrough.java)\n* [Customize loss function, and evaluation metric](src/main/java/ml/dmlc/xgboost4j/java/example/CustomObjective.java)\n* [Boosting from existing prediction](src/main/java/ml/dmlc/xgboost4j/java/example/BoostFromPrediction.java)\n* [Predicting using first n trees](src/main/java/ml/dmlc/xgboost4j/java/example/PredictFirstNtree.java)\n* [Generalized Linear Model](src/main/java/ml/dmlc/xgboost4j/java/example/GeneralizedLinearModel.java)\n* [Cross validation](src/main/java/ml/dmlc/xgboost4j/java/example/CrossValidation.java)\n* [Predicting leaf indices](src/main/java/ml/dmlc/xgboost4j/java/example/PredictLeafIndices.java)\n* [External Memory](src/main/java/ml/dmlc/xgboost4j/java/example/ExternalMemory.java)\n* [Early Stopping](src/main/java/ml/dmlc/xgboost4j/java/example/EarlyStopping.java)\n\n## Scala API\n\n* [Basic walkthrough of wrappers](src/main/scala/ml/dmlc/xgboost4j/scala/example/BasicWalkThrough.scala)\n* [Customize loss function, and evaluation metric](src/main/scala/ml/dmlc/xgboost4j/scala/example/CustomObjective.scala)\n* [Boosting from existing prediction](src/main/scala/ml/dmlc/xgboost4j/scala/example/BoostFromPrediction.scala)\n* [Predicting using first n trees](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictFirstNTree.scala)\n* [Generalized Linear Model](src/main/scala/ml/dmlc/xgboost4j/scala/example/GeneralizedLinearModel.scala)\n* [Cross validation](src/main/scala/ml/dmlc/xgboost4j/scala/example/CrossValidation.scala)\n* [Predicting leaf indices](src/main/scala/ml/dmlc/xgboost4j/scala/example/PredictLeafIndices.scala)\n* [External Memory](src/main/scala/ml/dmlc/xgboost4j/scala/example/ExternalMemory.scala)\n\n## Spark API\n* [Distributed Training with Spark](src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/SparkMLlibPipeline.scala)\n\n## Flink API\n* [Distributed Training with Flink](src/main/scala/ml/dmlc/xgboost4j/scala/example/flink/DistTrainWithFlink.scala)"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfounders@getzep.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\nfounders@getzep.com.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CODE_OF_CONDUCT.md\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CLAUDE.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CLAUDE.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n# CLAUDE.md\n\nThis file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n## Project Overview\n\nGraphiti is a Python framework for building temporally-aware knowledge graphs designed for AI agents. It enables real-time incremental updates to knowledge graphs without batch recomputation, making it suitable for dynamic environments.\n\nKey features:\n\n- Bi-temporal data model with explicit tracking of event occurrence times\n- Hybrid retrieval combining semantic embeddings, keyword search (BM25), and graph traversal\n- Support for custom entity definitions via Pydantic models\n- Integration with Neo4j and FalkorDB as graph storage backends\n\n## Development Commands\n\n### Main Development Commands (run from project root)\n\n```bash\n# Install dependencies\nuv sync --extra dev\n\n# Format code (ruff import sorting + formatting)\nmake format\n\n# Lint code (ruff + pyright type checking)\nmake lint\n\n# Run tests\nmake test\n\n# Run all checks (format, lint, test)\nmake check\n```\n\n### Server Development (run from server/ directory)\n\n```bash\ncd server/\n# Install server dependencies\nuv sync --extra dev\n\n# Run server in development mode\nuvicorn graph_service.main:app --reload\n\n# Format, lint, test server code\nmake format\nmake lint\nmake test\n```\n\n### MCP Server Development (run from mcp_server/ directory)\n\n```bash\ncd mcp_server/\n# Install MCP server dependencies\nuv sync\n\n# Run with Docker Compose\ndocker-compose up\n```\n\n## Code Architecture\n\n### Core Library (`graphiti_core/`)\n\n- **Main Entry Point**: `graphiti.py` - Contains the main `Graphiti` class that orchestrates all functionality\n- **Graph Storage**: `driver/` - Database drivers for Neo4j and FalkorDB\n- **LLM Integration**: `llm_client/` - Clients for OpenAI, Anthropic, Gemini, Groq\n- **Embeddings**: `embedder/` - Embedding clients for various providers\n- **Graph Elements**: `nodes.py`, `edges.py` - Core graph data structures\n- **Search**: `search/` - Hybrid search implementation with configurable strategies\n- **Prompts**: `prompts/` - LLM prompts for entity extraction, deduplication, summarization\n- **Utilities**: `utils/` - Maintenance operations, bulk processing, datetime handling\n\n### Server (`server/`)\n\n- **FastAPI Service**: `graph_service/main.py` - REST API server\n- **Routers**: `routers/` - API endpoints for ingestion and retrieval\n- **DTOs**: `dto/` - Data transfer objects for API contracts\n\n### MCP Server (`mcp_server/`)\n\n- **MCP Implementation**: `graphiti_mcp_server.py` - Model Context Protocol server for AI assistants\n- **Docker Support**: Containerized deployment with Neo4j\n\n## Testing\n\n- **Unit Tests**: `tests/` - Comprehensive test suite using pytest\n- **Integration Tests**: Tests marked with `_int` suffix require database connections\n- **Evaluation**: `tests/evals/` - End-to-end evaluation scripts"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CLAUDE.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = \"basic\"`, server uses `typeCheckingMode = \"standard\"`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k \"_int\"`\n- Run only unit tests: `pytest tests/ -k \"not _int\"`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CLAUDE.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CLAUDE.md\n## Configuration\n\n### Environment Variables\n\n- `OPENAI_API_KEY` - Required for LLM inference and embeddings\n- `USE_PARALLEL_RUNTIME` - Optional boolean for Neo4j parallel runtime (enterprise only)\n- Provider-specific keys: `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `GROQ_API_KEY`, `VOYAGE_API_KEY`\n\n### Database Setup\n\n- **Neo4j**: Version 5.26+ required, available via Neo4j Desktop\n  - Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n  - Override by passing `database` parameter to driver constructor\n- **FalkorDB**: Version 1.1.2+ as alternative backend\n  - Database name defaults to `default_db` (hardcoded in FalkorDriver)\n  - Override by passing `database` parameter to driver constructor\n\n## Development Guidelines\n\n### Code Style\n\n- Use Ruff for formatting and linting (configured in pyproject.toml)\n- Line length: 100 characters\n- Quote style: single quotes\n- Type checking with Pyright is enforced\n- Main project uses `typeCheckingMode = \"basic\"`, server uses `typeCheckingMode = \"standard\"`\n\n### Testing Requirements\n\n- Run tests with `make test` or `pytest`\n- Integration tests require database connections and are marked with `_int` suffix\n- Use `pytest-xdist` for parallel test execution\n- Run specific test files: `pytest tests/test_specific_file.py`\n- Run specific test methods: `pytest tests/test_file.py::test_method_name`\n- Run only integration tests: `pytest tests/ -k \"_int\"`\n- Run only unit tests: `pytest tests/ -k \"not _int\"`\n\n### LLM Provider Support\n\nThe codebase supports multiple LLM providers but works best with services supporting structured output (OpenAI, Gemini). Other providers may cause schema validation issues, especially with smaller models.\n\n### MCP Server Usage Guidelines\n\nWhen working with the MCP server, follow the patterns established in `mcp_server/cursor_rules.md`:\n\n- Always search for existing knowledge before adding new information\n- Use specific entity type filters (`Preference`, `Procedure`, `Requirement`)\n- Store new information immediately using `add_memory`\n- Follow discovered procedures and respect established preferences"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=\"center\">\n  <a href=\"https://www.getzep.com/\">\n    <img src=\"https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73\" width=\"150\" alt=\"Zep Logo\">\n  </a>\n</p>\n\n<h1 align=\"center\">\nGraphiti\n</h1>\n<h2 align=\"center\"> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=\"center\">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=\"center\">\n\n<a href=\"https://trendshift.io/repositories/12986\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/12986\" alt=\"getzep%2Fgraphiti | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=\"center\">\n    <img src=\"images/graphiti-graph-intro.gif\" alt=\"Graphiti temporal walkthrough\" width=\"700px\">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _\"Kendra loves Adidas shoes.\"_ Each fact is a \"triplet\" represented by two entities, or\nnodes (\"Kendra\", \"Adidas shoes\"), and their relationship, or edge (\"loves\"). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n<p align=\"center\">\n  <a href=\"https://www.getzep.com/\">\n    <img src=\"https://github.com/user-attachments/assets/119c5682-9654-4257-8922-56b7cb8ffd73\" width=\"150\" alt=\"Zep Logo\">\n  </a>\n</p>\n\n<h1 align=\"center\">\nGraphiti\n</h1>\n<h2 align=\"center\"> Build Real-Time Knowledge Graphs for AI Agents</h2>\n<div align=\"center\">\n\n[![Lint](https://github.com/getzep/Graphiti/actions/workflows/lint.yml/badge.svg?style=flat)](https://github.com/getzep/Graphiti/actions/workflows/lint.yml)\n[![Unit Tests](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/unit_tests.yml)\n[![MyPy Check](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml/badge.svg)](https://github.com/getzep/Graphiti/actions/workflows/typecheck.yml)\n\n![GitHub Repo stars](https://img.shields.io/github/stars/getzep/graphiti)\n[![Discord](https://dcbadge.vercel.app/api/server/W8Kw6bsgXQ?style=flat)](https://discord.com/invite/W8Kw6bsgXQ)\n[![arXiv](https://img.shields.io/badge/arXiv-2501.13956-b31b1b.svg?style=flat)](https://arxiv.org/abs/2501.13956)\n[![Release](https://img.shields.io/github/v/release/getzep/graphiti?style=flat&label=Release&color=limegreen)](https://github.com/getzep/graphiti/releases)\n\n</div>\n<div align=\"center\">\n\n<a href=\"https://trendshift.io/repositories/12986\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/12986\" alt=\"getzep%2Fgraphiti | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n</div>\n\n:star: _Help us reach more developers and grow the Graphiti community. Star this repo!_\n\n<br />\n\n> [!TIP]\n> Check out the new [MCP server for Graphiti](mcp_server/README.md)! Give Claude, Cursor, and other MCP clients powerful Knowledge Graph-based memory.\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents operating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti continuously integrates user interactions, structured and unstructured enterprise data, and external information into a coherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical queries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI applications.\n\nUse Graphiti to:\n\n- Integrate and maintain dynamic user interactions and business data.\n- Facilitate state-based reasoning and task automation for agents.\n- Query complex, evolving data with semantic, keyword, and graph-based search methods.\n\n<br />\n\n<p align=\"center\">\n    <img src=\"images/graphiti-graph-intro.gif\" alt=\"Graphiti temporal walkthrough\" width=\"700px\">   \n</p>\n\n<br />\n\nA knowledge graph is a network of interconnected facts, such as _\"Kendra loves Adidas shoes.\"_ Each fact is a \"triplet\" represented by two entities, or\nnodes (\"Kendra\", \"Adidas shoes\"), and their relationship, or edge (\"loves\"). Knowledge Graphs have been explored\nextensively for information retrieval. What makes Graphiti unique is its ability to autonomously build a knowledge graph\nwhile handling changing relationships and maintaining historical context."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep's memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we've demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2501.13956\"><img src=\"images/arxiv-screenshot.png\" alt=\"Zep: A Temporal Knowledge Graph Architecture for Agent Memory\" width=\"700px\"></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=\"center\">\n    <img src=\"/images/graphiti-intro-slides-stock-2.gif\" alt=\"Graphiti structured + unstructured demo\" width=\"700px\">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Graphiti and Zep Memory\n\nGraphiti powers the core of [Zep's memory layer](https://www.getzep.com) for AI Agents.\n\nUsing Graphiti, we've demonstrated Zep is\nthe [State of the Art in Agent Memory](https://blog.getzep.com/state-of-the-art-agent-memory/).\n\nRead our paper: [Zep: A Temporal Knowledge Graph Architecture for Agent Memory](https://arxiv.org/abs/2501.13956).\n\nWe're excited to open-source Graphiti, believing its potential reaches far beyond AI memory applications.\n\n<p align=\"center\">\n    <a href=\"https://arxiv.org/abs/2501.13956\"><img src=\"images/arxiv-screenshot.png\" alt=\"Zep: A Temporal Knowledge Graph Architecture for Agent Memory\" width=\"700px\"></a>\n</p>\n\n## Why Graphiti?\n\nTraditional RAG approaches often rely on batch processing and static data summarization, making them inefficient for frequently changing data. Graphiti addresses these challenges by providing:\n\n- **Real-Time Incremental Updates:** Immediate integration of new data episodes without batch recomputation.\n- **Bi-Temporal Data Model:** Explicit tracking of event occurrence and ingestion times, allowing accurate point-in-time queries.\n- **Efficient Hybrid Retrieval:** Combines semantic embeddings, keyword (BM25), and graph traversal to achieve low-latency queries without reliance on LLM summarization.\n- **Custom Entity Definitions:** Flexible ontology creation and support for developer-defined entities through straightforward Pydantic models.\n- **Scalability:** Efficiently manages large datasets with parallel processing, suitable for enterprise environments.\n\n<p align=\"center\">\n    <img src=\"/images/graphiti-intro-slides-stock-2.gif\" alt=\"Graphiti structured + unstructured demo\" width=\"700px\">   \n</p>\n\n## Graphiti vs. GraphRAG\n\n| Aspect                     | GraphRAG                              | Graphiti                                         |\n| -------------------------- | ------------------------------------- | ------------------------------------------------ |\n| **Primary Use**            | Static document summarization         | Dynamic data management                          |\n| **Data Handling**          | Batch-oriented processing             | Continuous, incremental updates                  |\n| **Knowledge Structure**    | Entity clusters & community summaries | Episodic data, semantic entities, communities    |\n| **Retrieval Method**       | Sequential LLM summarization          | Hybrid semantic, keyword, and graph-based search |\n| **Adaptability**           | Low                                   | High                                             |\n| **Temporal Handling**      | Basic timestamp tracking              | Explicit bi-temporal tracking                    |\n| **Contradiction Handling** | LLM-driven summarization judgments    | Temporal edge invalidation                       |\n| **Query Latency**          | Seconds to tens of seconds            | Typically sub-second latency                     |\n| **Custom Entity Types**    | No                                    | Yes, customizable                                |\n| **Scalability**            | Moderate                              | High, optimized for large datasets               |\n\nGraphiti is specifically designed to address the challenges of dynamic and frequently updated datasets, making it particularly suitable for applications requiring real-time interaction and precise historical queries."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md).", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Installation\n\nRequirements:\n\n- Python 3.10 or higher\n- Neo4j 5.26 / FalkorDB 1.1.2 or higher (serves as the embeddings storage backend)\n- OpenAI API key (Graphiti defaults to OpenAI for LLM inference and embedding)\n\n> [!IMPORTANT]\n> Graphiti works best with LLM services that support Structured Output (such as OpenAI and Gemini).\n> Using other services may result in incorrect output schemas and ingestion failures. This is particularly\n> problematic when using smaller models.\n\nOptional:\n\n- Google Gemini, Anthropic, or Groq API key (for alternative LLM providers)\n\n> [!TIP]\n> The simplest way to install Neo4j is via [Neo4j Desktop](https://neo4j.com/download/). It provides a user-friendly\n> interface to manage Neo4j instances and databases.\n> Alternatively, you can use FalkorDB on-premises via Docker and instantly start with the quickstart example:\n\n```bash\ndocker run -p 6379:6379 -p 3000:3000 -it --rm falkordb/falkordb:latest\n\n```\n\n```bash\npip install graphiti-core\n```\n\nor\n\n```bash\nuv add graphiti-core\n```\n\n### Installing with FalkorDB Support\n\nIf you plan to use FalkorDB as your graph database backend, install with the FalkorDB extra:\n\n```bash\npip install graphiti-core[falkordb]\n\n# or with uv\nuv add graphiti-core[falkordb]\n```\n\n### You can also install optional LLM providers as extras:\n\n```bash\n# Install with Anthropic support\npip install graphiti-core[anthropic]\n\n# Install with Groq support\npip install graphiti-core[groq]\n\n# Install with Google Gemini support\npip install graphiti-core[google-genai]\n\n# Install with multiple providers\npip install graphiti-core[anthropic,groq,google-genai]\n\n# Install with FalkorDB and LLM providers\npip install graphiti-core[falkordb,anthropic,google-genai]\n```\n\n## Quick Start\n\n> [!IMPORTANT]\n> Graphiti defaults to using OpenAI for LLM inference and embedding. Ensure that an `OPENAI_API_KEY` is set in your environment.\n> Support for Anthropic and Groq LLM inferences is available, too. Other LLM providers may be supported via OpenAI\n> compatible APIs.\n\nFor a complete working example, see the [Quickstart Example](./examples/quickstart/README.md) in the examples directory. The quickstart demonstrates:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph (both text and structured JSON)\n4. Searching for relationships (edges) using hybrid search\n5. Reranking search results using graph distance\n6. Searching for nodes using predefined search recipes\n\nThe example is fully documented with clear explanations of each functionality and includes a comprehensive README with setup instructions and next steps.\n\n## MCP Server\n\nThe `mcp_server` directory contains a Model Context Protocol (MCP) server implementation for Graphiti. This server allows AI assistants to interact with Graphiti's knowledge graph capabilities through the MCP protocol.\n\nKey features of the MCP server include:\n\n- Episode management (add, retrieve, delete)\n- Entity management and relationship handling\n- Semantic and hybrid search capabilities\n- Group management for organizing related data\n- Graph maintenance operations\n\nThe MCP server can be deployed using Docker with Neo4j, making it easy to integrate Graphiti into your AI assistant workflows.\n\nFor detailed setup instructions and usage examples, see the [MCP server README](./mcp_server/README.md)."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=\"bolt://localhost:7687\",\n    user=\"neo4j\",\n    password=\"password\",\n    database=\"my_custom_database\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=\"localhost\",\n    port=6379,\n    username=\"falkor_user\",  # Optional\n    password=\"falkor_password\",  # Optional\n    database=\"my_custom_graph\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j's parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = \"<your-api-key>\"\napi_version = \"<your-api-version>\"\nllm_endpoint = \"<your-llm-endpoint>\"  # e.g., \"https://your-llm-resource.openai.azure.com/\"\nembedding_endpoint = \"<your-embedding-endpoint>\"  # e.g., \"https://your-embedding-resource.openai.azure.com/\"\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=\"gpt-4.1-nano\",\n    model=\"gpt-4.1-mini\",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=\"text-embedding-3-small-deployment\"  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## REST Service\n\nThe `server` directory contains an API service for interacting with the Graphiti API. It is built using FastAPI.\n\nPlease see the [server README](./server/README.md) for more information.\n\n## Optional Environment Variables\n\nIn addition to the Neo4j and OpenAi-compatible credentials, Graphiti also has a few optional environment variables.\nIf you are using one of our supported models, such as Anthropic or Voyage models, the necessary environment variables\nmust be set.\n\n### Database Configuration\n\nDatabase names are configured directly in the driver constructors:\n\n- **Neo4j**: Database name defaults to `neo4j` (hardcoded in Neo4jDriver)\n- **FalkorDB**: Database name defaults to `default_db` (hardcoded in FalkorDriver)\n\nAs of v0.17.0, if you need to customize your database configuration, you can instantiate a database driver and pass it to the Graphiti constructor using the `graph_driver` parameter.\n\n#### Neo4j with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.neo4j_driver import Neo4jDriver\n\n# Create a Neo4j driver with custom database name\ndriver = Neo4jDriver(\n    uri=\"bolt://localhost:7687\",\n    user=\"neo4j\",\n    password=\"password\",\n    database=\"my_custom_database\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n#### FalkorDB with Custom Database Name\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.driver.falkordb_driver import FalkorDriver\n\n# Create a FalkorDB driver with custom database name\ndriver = FalkorDriver(\n    host=\"localhost\",\n    port=6379,\n    username=\"falkor_user\",  # Optional\n    password=\"falkor_password\",  # Optional\n    database=\"my_custom_graph\"  # Custom database name\n)\n\n# Pass the driver to Graphiti\ngraphiti = Graphiti(graph_driver=driver)\n```\n\n\n### Performance Configuration\n\n`USE_PARALLEL_RUNTIME` is an optional boolean variable that can be set to true if you wish\nto enable Neo4j's parallel runtime feature for several of our search queries.\nNote that this feature is not supported for Neo4j Community edition or for smaller AuraDB instances,\nas such this feature is off by default.\n\n## Using Graphiti with Azure OpenAI\n\nGraphiti supports Azure OpenAI for both LLM inference and embeddings. Azure deployments often require different endpoints for LLM and embedding services, and separate deployments for default and small models.\n\n```python\nfrom openai import AsyncAzureOpenAI\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client import LLMConfig, OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Azure OpenAI configuration - use separate endpoints for different services\napi_key = \"<your-api-key>\"\napi_version = \"<your-api-version>\"\nllm_endpoint = \"<your-llm-endpoint>\"  # e.g., \"https://your-llm-resource.openai.azure.com/\"\nembedding_endpoint = \"<your-embedding-endpoint>\"  # e.g., \"https://your-embedding-resource.openai.azure.com/\"\n\n# Create separate Azure OpenAI clients for different services\nllm_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=llm_endpoint\n)\n\nembedding_client_azure = AsyncAzureOpenAI(\n    api_key=api_key,\n    api_version=api_version,\n    azure_endpoint=embedding_endpoint\n)\n\n# Create LLM Config with your Azure deployment names\nazure_llm_config = LLMConfig(\n    small_model=\"gpt-4.1-nano\",\n    model=\"gpt-4.1-mini\",\n)\n\n# Initialize Graphiti with Azure OpenAI clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=OpenAIClient(\n        llm_config=azure_llm_config,\n        client=llm_client_azure\n    ),\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            embedding_model=\"text-embedding-3-small-deployment\"  # Your Azure embedding deployment name\n        ),\n        client=embedding_client_azure\n    ),\n    cross_encoder=OpenAIRerankerClient(\n        llm_config=LLMConfig(\n            model=azure_llm_config.small_model  # Use small model for reranking\n        ),\n        client=llm_client_azure\n    )\n)\n\n# Now you can use Graphiti with Azure OpenAI\n```"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add \"graphiti-core[google-genai]\"\n\n# or\n\npip install \"graphiti-core[google-genai]\"\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = \"<your-google-api-key>\"\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.0-flash\"\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=\"embedding-001\"\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.5-flash-lite-preview-06-17\"\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini's log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=\"abc\",  # Ollama doesn't require a real API key\n    model=\"deepseek-r1:7b\",\n    small_model=\"deepseek-r1:7b\",\n    base_url=\"http://localhost:11434/v1\", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"abc\",\n            embedding_model=\"nomic-embed-text\",\n            embedding_dim=768,\n            base_url=\"http://localhost:11434/v1\",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\nMake sure to replace the placeholder values with your actual Azure OpenAI credentials and deployment names that match your Azure OpenAI service configuration.\n\n## Using Graphiti with Google Gemini\n\nGraphiti supports Google's Gemini models for LLM inference, embeddings, and cross-encoding/reranking. To use Gemini, you'll need to configure the LLM client, embedder, and the cross-encoder with your Google API key.\n\nInstall Graphiti:\n\n```bash\nuv add \"graphiti-core[google-genai]\"\n\n# or\n\npip install \"graphiti-core[google-genai]\"\n```\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.gemini_client import GeminiClient, LLMConfig\nfrom graphiti_core.embedder.gemini import GeminiEmbedder, GeminiEmbedderConfig\nfrom graphiti_core.cross_encoder.gemini_reranker_client import GeminiRerankerClient\n\n# Google API key configuration\napi_key = \"<your-google-api-key>\"\n\n# Initialize Graphiti with Gemini clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=GeminiClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.0-flash\"\n        )\n    ),\n    embedder=GeminiEmbedder(\n        config=GeminiEmbedderConfig(\n            api_key=api_key,\n            embedding_model=\"embedding-001\"\n        )\n    ),\n    cross_encoder=GeminiRerankerClient(\n        config=LLMConfig(\n            api_key=api_key,\n            model=\"gemini-2.5-flash-lite-preview-06-17\"\n        )\n    )\n)\n\n# Now you can use Graphiti with Google Gemini for all components\n```\n\nThe Gemini reranker uses the `gemini-2.5-flash-lite-preview-06-17` model by default, which is optimized for cost-effective and low-latency classification tasks. It uses the same boolean classification approach as the OpenAI reranker, leveraging Gemini's log probabilities feature to rank passage relevance.\n\n## Using Graphiti with Ollama (Local LLM)\n\nGraphiti supports Ollama for running local LLMs and embedding models via Ollama's OpenAI-compatible API. This is ideal for privacy-focused applications or when you want to avoid API costs.\n\nInstall the models:\nollama pull deepseek-r1:7b # LLM\nollama pull nomic-embed-text # embeddings\n\n```python\nfrom graphiti_core import Graphiti\nfrom graphiti_core.llm_client.config import LLMConfig\nfrom graphiti_core.llm_client.openai_client import OpenAIClient\nfrom graphiti_core.embedder.openai import OpenAIEmbedder, OpenAIEmbedderConfig\nfrom graphiti_core.cross_encoder.openai_reranker_client import OpenAIRerankerClient\n\n# Configure Ollama LLM client\nllm_config = LLMConfig(\n    api_key=\"abc\",  # Ollama doesn't require a real API key\n    model=\"deepseek-r1:7b\",\n    small_model=\"deepseek-r1:7b\",\n    base_url=\"http://localhost:11434/v1\", # Ollama provides this port\n)\n\nllm_client = OpenAIClient(config=llm_config)\n\n# Initialize Graphiti with Ollama clients\ngraphiti = Graphiti(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\",\n    llm_client=llm_client,\n    embedder=OpenAIEmbedder(\n        config=OpenAIEmbedderConfig(\n            api_key=\"abc\",\n            embedding_model=\"nomic-embed-text\",\n            embedding_dim=768,\n            base_url=\"http://localhost:11434/v1\",\n        )\n    ),\n    cross_encoder=OpenAIRerankerClient(client=llm_client, config=llm_config),\n)\n\n# Now you can use Graphiti with local Ollama models\n```\n\nEnsure Ollama is running (`ollama serve`) and that you have pulled the models you want to use."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Documentation\n\n- [Guides and API documentation](https://help.getzep.com/graphiti).\n- [Quick Start](https://help.getzep.com/graphiti/graphiti/quick-start)\n- [Building an agent with LangChain's LangGraph and Graphiti](https://help.getzep.com/graphiti/graphiti/lang-graph-agent)\n\n## Telemetry\n\nGraphiti collects anonymous usage statistics to help us understand how the framework is being used and improve it for everyone. We believe transparency is important, so here's exactly what we collect and why.\n\n### What We Collect\n\nWhen you initialize a Graphiti instance, we collect:\n\n- **Anonymous identifier**: A randomly generated UUID stored locally in `~/.cache/graphiti/telemetry_anon_id`\n- **System information**: Operating system, Python version, and system architecture\n- **Graphiti version**: The version you're using\n- **Configuration choices**:\n  - LLM provider type (OpenAI, Azure, Anthropic, etc.)\n  - Database backend (Neo4j, FalkorDB)\n  - Embedder provider (OpenAI, Azure, Voyage, etc.)\n\n### What We Don't Collect\n\nWe are committed to protecting your privacy. We **never** collect:\n\n- Personal information or identifiers\n- API keys or credentials\n- Your actual data, queries, or graph content\n- IP addresses or hostnames\n- File paths or system-specific information\n- Any content from your episodes, nodes, or edges\n\n### Why We Collect This Data\n\nThis information helps us:\n\n- Understand which configurations are most popular to prioritize support and testing\n- Identify which LLM and database providers to focus development efforts on\n- Track adoption patterns to guide our roadmap\n- Ensure compatibility across different Python versions and operating systems\n\nBy sharing this anonymous information, you help us make Graphiti better for everyone in the community.\n\n### View the Telemetry Code\n\nThe Telemetry code [may be found here](graphiti_core/telemetry/telemetry.py).\n\n### How to Disable Telemetry\n\nTelemetry is **opt-out** and can be disabled at any time. To disable telemetry collection:\n\n**Option 1: Environment Variable**\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\n**Option 2: Set in your shell profile**\n\n```bash\n# For bash users (~/.bashrc or ~/.bash_profile)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.bashrc\n\n# For zsh users (~/.zshrc)\necho 'export GRAPHITI_TELEMETRY_ENABLED=false' >> ~/.zshrc\n```\n\n**Option 3: Set for a specific Python session**\n\n```python\nimport os\nos.environ['GRAPHITI_TELEMETRY_ENABLED'] = 'false'\n\n# Then initialize Graphiti as usual\nfrom graphiti_core import Graphiti\ngraphiti = Graphiti(...)\n```\n\nTelemetry is automatically disabled during test runs (when `pytest` is detected).\n\n### Technical Details\n\n- Telemetry uses PostHog for anonymous analytics collection\n- All telemetry operations are designed to fail silently - they will never interrupt your application or affect Graphiti functionality\n- The anonymous ID is stored locally and is not tied to any personal information"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//README.md\n## Status and Roadmap\n\nGraphiti is under active development. We aim to maintain API stability while working on:\n\n- [x] Supporting custom graph schemas:\n  - Allow developers to provide their own defined node and edge classes when ingesting episodes\n  - Enable more flexible knowledge representation tailored to specific use cases\n- [x] Enhancing retrieval capabilities with more robust and configurable options\n- [x] Graphiti MCP Server\n- [ ] Expanding test coverage to ensure reliability and catch edge cases\n\n## Contributing\n\nWe encourage and appreciate all forms of contributions, whether it's code, documentation, addressing GitHub Issues, or\nanswering questions in the Graphiti Discord channel. For detailed guidelines on code contributions, please refer\nto [CONTRIBUTING](CONTRIBUTING.md).\n\n## Support\n\nJoin the [Zep Discord server](https://discord.com/invite/W8Kw6bsgXQ) and make your way to the **#Graphiti** channel!"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//SECURITY.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n|---------|--------------------|\n| 0.x     | :white_check_mark: |\n\n\n## Reporting a Vulnerability\n\nPlease use GitHub's Private Vulnerability Reporting mechanism found in the Security section of this repo.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//SECURITY.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//SECURITY.md\n# Security Policy\n\n## Supported Versions\n\nUse this section to tell people about which versions of your project are\ncurrently being supported with security updates.\n\n| Version | Supported          |\n|---------|--------------------|\n| 0.x     | :white_check_mark: |\n\n\n## Reporting a Vulnerability\n\nPlease use GitHub's Private Vulnerability Reporting mechanism found in the Security section of this repo."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//Zep-CLA.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//Zep-CLA.md\n# Contributor License Agreement (CLA)\n\nIn order to clarify the intellectual property license granted with Contributions from any person or entity, Zep Software, Inc. (\"Zep\") must have a Contributor License Agreement (\"CLA\") on file that has been signed by each Contributor, indicating agreement to the license terms below. This license is for your protection as a Contributor as well as the protection of Zep; it does not change your rights to use your own Contributions for any other purpose.\n\nYou accept and agree to the following terms and conditions for Your present and future Contributions submitted to Zep. Except for the license granted herein to Zep and recipients of software distributed by Zep, You reserve all right, title, and interest in and to Your Contributions.\n\n## Definitions\n\n**\"You\" (or \"Your\")** shall mean the copyright owner or legal entity authorized by the copyright owner that is making this Agreement with Zep. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means:\n\ni. the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or\nii. ownership of fifty percent (50%) or more of the outstanding shares, or\niii. beneficial ownership of such entity.\n\n**\"Contribution\"** shall mean any original work of authorship, including any modifications or additions to an existing work, that is intentionally submitted by You to Zep for inclusion in, or documentation of, any of the products owned or managed by Zep (the \"Work\"). For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to Zep or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Zep for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"\n\n## Grant of Copyright License\n\nSubject to the terms and conditions of this Agreement, You hereby grant to Zep and to recipients of software distributed by Zep a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works.\n\n## Grant of Patent License\n\nSubject to the terms and conditions of this Agreement, You hereby grant to Zep and to recipients of software distributed by Zep a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) was submitted. If any entity institutes patent litigation against You or any other entity (including a cross-claim or counterclaim in a lawsuit) alleging that your Contribution, or the Work to which you have contributed, constitutes direct or contributory patent infringement, then any patent licenses granted to that entity under this Agreement for that Contribution or Work shall terminate as of the date such litigation is filed.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//Zep-CLA.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//Zep-CLA.md\n# Contributor License Agreement (CLA)\n\nIn order to clarify the intellectual property license granted with Contributions from any person or entity, Zep Software, Inc. (\"Zep\") must have a Contributor License Agreement (\"CLA\") on file that has been signed by each Contributor, indicating agreement to the license terms below. This license is for your protection as a Contributor as well as the protection of Zep; it does not change your rights to use your own Contributions for any other purpose.\n\nYou accept and agree to the following terms and conditions for Your present and future Contributions submitted to Zep. Except for the license granted herein to Zep and recipients of software distributed by Zep, You reserve all right, title, and interest in and to Your Contributions.\n\n## Definitions\n\n**\"You\" (or \"Your\")** shall mean the copyright owner or legal entity authorized by the copyright owner that is making this Agreement with Zep. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means:\n\ni. the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or\nii. ownership of fifty percent (50%) or more of the outstanding shares, or\niii. beneficial ownership of such entity.\n\n**\"Contribution\"** shall mean any original work of authorship, including any modifications or additions to an existing work, that is intentionally submitted by You to Zep for inclusion in, or documentation of, any of the products owned or managed by Zep (the \"Work\"). For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to Zep or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Zep for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\"\n\n## Grant of Copyright License\n\nSubject to the terms and conditions of this Agreement, You hereby grant to Zep and to recipients of software distributed by Zep a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works.\n\n## Grant of Patent License\n\nSubject to the terms and conditions of this Agreement, You hereby grant to Zep and to recipients of software distributed by Zep a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) was submitted. If any entity institutes patent litigation against You or any other entity (including a cross-claim or counterclaim in a lawsuit) alleging that your Contribution, or the Work to which you have contributed, constitutes direct or contributory patent infringement, then any patent licenses granted to that entity under this Agreement for that Contribution or Work shall terminate as of the date such litigation is filed."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//Zep-CLA.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//Zep-CLA.md\n## Representations\n\nYou represent that you are legally entitled to grant the above license. If your employer(s) has rights to intellectual property that you create that includes your Contributions, you represent that you have received permission to make Contributions on behalf of that employer, that your employer has waived such rights for your Contributions to Zep, or that your employer has executed a separate Corporate CLA with Zep.\n\nYou represent that each of Your Contributions is Your original creation (see section 7 for submissions on behalf of others). You represent that Your Contribution submissions include complete details of any third-party license or other restriction (including, but not limited to, related patents and trademarks) of which you are personally aware and which are associated with any part of Your Contributions.\n\n## Support\n\nYou are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. Unless required by applicable law or agreed to in writing, You provide Your Contributions on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.\n\n## Third-Party Submissions\n\nShould You wish to submit work that is not Your original creation, You may submit it to Zep separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which you are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\".\n\n## Notifications\n\nYou agree to notify Zep of any facts or circumstances of which you become aware that would make these representations inaccurate in any respect.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//Zep-CLA.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//Zep-CLA.md\n## Representations\n\nYou represent that you are legally entitled to grant the above license. If your employer(s) has rights to intellectual property that you create that includes your Contributions, you represent that you have received permission to make Contributions on behalf of that employer, that your employer has waived such rights for your Contributions to Zep, or that your employer has executed a separate Corporate CLA with Zep.\n\nYou represent that each of Your Contributions is Your original creation (see section 7 for submissions on behalf of others). You represent that Your Contribution submissions include complete details of any third-party license or other restriction (including, but not limited to, related patents and trademarks) of which you are personally aware and which are associated with any part of Your Contributions.\n\n## Support\n\nYou are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. Unless required by applicable law or agreed to in writing, You provide Your Contributions on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE.\n\n## Third-Party Submissions\n\nShould You wish to submit work that is not Your original creation, You may submit it to Zep separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which you are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\".\n\n## Notifications\n\nYou agree to notify Zep of any facts or circumstances of which you become aware that would make these representations inaccurate in any respect."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe're thrilled you're interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we're committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random \"good first issue,\" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe've restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with \"help wanted\" and \"good first issue.\" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don't need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you're trying to accomplish. What are you working on? What's getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a \"Feature Request\" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn't code. If you're using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you've found an issue tagged with \"good first issue\" or \"help wanted,\" or prepared an example to share, here's how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We've included more detailed technical instructions below; be open to feedback during review.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n# Contributing to Graphiti\n\nWe're thrilled you're interested in contributing to Graphiti! As firm believers in the power of open source collaboration, we're committed to building not just a tool, but a vibrant community where developers of all experience levels can make meaningful contributions.\n\nWhen I first joined this project, I was overwhelmed trying to figure out where to start. Someone eventually pointed me to a random \"good first issue,\" but I later discovered there were multiple ways I could have contributed that would have better matched my skills and interests.\n\nWe've restructured our contribution paths to solve this problem:\n\n# Four Ways to Get Involved\n\n### Pick Up Existing Issues\n\nOur developers regularly tag issues with \"help wanted\" and \"good first issue.\" These are pre-vetted tasks with clear scope and someone ready to help you if you get stuck.\n\n### Create Your Own Tickets\n\nSee something that needs fixing? Have an idea for an improvement? You don't need permission to identify problems. The people closest to the pain are often best positioned to describe the solution.\n\nFor **feature requests**, tell us the story of what you're trying to accomplish. What are you working on? What's getting in your way? What would make your life easier? Submit these through our [GitHub issue tracker](https://github.com/getzep/graphiti/issues) with a \"Feature Request\" label.\n\nFor **bug reports**, we need enough context to reproduce the problem. Use the [GitHub issue tracker](https://github.com/getzep/graphiti/issues) and include:\n\n- A clear title that summarizes the specific problem\n- What you were trying to do when you encountered the bug\n- What you expected to happen\n- What actually happened\n- A code sample or test case that demonstrates the issue\n\n### Share Your Use Cases\n\nSometimes the most valuable contribution isn't code. If you're using our project in an interesting way, add it to the [examples](https://github.com/getzep/graphiti/tree/main/examples) folder. This helps others discover new possibilities and counts as a meaningful contribution. We regularly feature compelling examples in our blog posts and videos - your work might be showcased to the broader community!\n\n### Help Others in Discord\n\nJoin our [Discord server](https://discord.com/invite/W8Kw6bsgXQ) community and pitch in at the helpdesk. Answering questions and helping troubleshoot issues is an incredibly valuable contribution that benefits everyone. The knowledge you share today saves someone hours of frustration tomorrow.\n\n## What happens next?\n\nOnce you've found an issue tagged with \"good first issue\" or \"help wanted,\" or prepared an example to share, here's how to turn that into a contribution:\n\n1. Share your approach in the issue discussion or [Discord](https://discord.com/invite/W8Kw6bsgXQ) before diving deep into code. This helps ensure your solution adheres to the architecture of Graphiti from the start and saves you from potential rework.\n\n2. Fork the repo, make your changes in a branch, and submit a PR. We've included more detailed technical instructions below; be open to feedback during review."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m \"Your detailed commit message\"\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you're changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [\"your-package>=1.0.0\"]\n   dev = [\n       # ... existing dev dependencies\n       \"your-package>=1.0.0\",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               'your-package is required for YourServiceClient. '\n               'Install it with: pip install graphiti-core[your-service]'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n## Setup\n\n1. Fork the repository on GitHub.\n2. Clone your fork locally:\n   ```\n   git clone https://github.com/getzep/graphiti\n   cd graphiti\n   ```\n3. Set up your development environment:\n\n   - Ensure you have Python 3.10+ installed.\n   - Install uv: https://docs.astral.sh/uv/getting-started/installation/\n   - Install project dependencies:\n     ```\n     make install\n     ```\n   - To run integration tests, set the appropriate environment variables\n\n     ```\n     export TEST_OPENAI_API_KEY=...\n     export TEST_OPENAI_MODEL=...\n     export TEST_ANTHROPIC_API_KEY=...\n\n     # For Neo4j\n     export TEST_URI=neo4j://...\n     export TEST_USER=...\n     export TEST_PASSWORD=...\n     ```\n\n## Making Changes\n\n1. Create a new branch for your changes:\n   ```\n   git checkout -b your-branch-name\n   ```\n2. Make your changes in the codebase.\n3. Write or update tests as necessary.\n4. Run the tests to ensure they pass:\n   ```\n   make test\n   ```\n5. Format your code:\n   ```\n   make format\n   ```\n6. Run linting checks:\n   ```\n   make lint\n   ```\n\n## Submitting Changes\n\n1. Commit your changes:\n   ```\n   git commit -m \"Your detailed commit message\"\n   ```\n2. Push to your fork:\n   ```\n   git push origin your-branch-name\n   ```\n3. Submit a pull request through the GitHub website to https://github.com/getzep/graphiti.\n\n## Pull Request Guidelines\n\n- Provide a clear title and description of your changes.\n- Include any relevant issue numbers in the PR description.\n- Ensure all tests pass and there are no linting errors.\n- Update documentation if you're changing functionality.\n\n## Code Style and Quality\n\nWe use several tools to maintain code quality:\n\n- Ruff for linting and formatting\n- Pyright for static type checking\n- Pytest for testing\n\nBefore submitting a pull request, please run:\n\n```\nmake check\n```\n\nThis command will format your code, run linting checks, and execute tests.\n\n## Third-Party Integrations\n\nWhen contributing integrations for third-party services (LLM providers, embedding services, databases, etc.), please follow these patterns:\n\n### Optional Dependencies\n\nAll third-party integrations must be optional dependencies to keep the core library lightweight. Follow this pattern:\n\n1. **Add to `pyproject.toml`**: Define your dependency as an optional extra AND include it in the dev extra:\n   ```toml\n   [project.optional-dependencies]\n   your-service = [\"your-package>=1.0.0\"]\n   dev = [\n       # ... existing dev dependencies\n       \"your-package>=1.0.0\",  # Include all optional extras here\n       # ... other dependencies\n   ]\n   ```\n\n2. **Use TYPE_CHECKING pattern**: In your integration module, import dependencies conditionally:\n   ```python\n   from typing import TYPE_CHECKING\n   \n   if TYPE_CHECKING:\n       import your_package\n       from your_package import SomeType\n   else:\n       try:\n           import your_package\n           from your_package import SomeType\n       except ImportError:\n           raise ImportError(\n               'your-package is required for YourServiceClient. '\n               'Install it with: pip install graphiti-core[your-service]'\n           ) from None\n   ```\n\n3. **Benefits of this pattern**:\n   - Fast startup times (no import overhead during type checking)\n   - Clear error messages with installation instructions\n   - Proper type hints for development\n   - Consistent user experience\n\n4. **Do NOT**:"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main//CONTRIBUTING.md\n- Add optional imports to `__init__.py` files\n   - Use direct imports without error handling\n   - Include optional dependencies in the main `dependencies` list\n\n### Integration Structure\n\n- Place LLM clients in `graphiti_core/llm_client/`\n- Place embedding clients in `graphiti_core/embedder/`\n- Place database drivers in `graphiti_core/driver/`\n- Follow existing naming conventions (e.g., `your_service_client.py`)\n\n### Testing\n\n- Add comprehensive tests in the appropriate `tests/` subdirectory\n- Mark integration tests with `_int` suffix if they require external services\n- Include both unit tests and integration tests where applicable\n\n# Questions?\n\nStuck on a contribution or have a half-formed idea? Come say hello in our [Discord server](https://discord.com/invite/W8Kw6bsgXQ). Whether you're ready to contribute or just want to learn more, we're happy to have you! It's faster than GitHub issues and you'll find both maintainers and fellow contributors ready to help.\n\nThank you for contributing to Graphiti!"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result's source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### \"Graph not found: default_db\" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn't exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=\"your_db_name\")\n```", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n# Graphiti Quickstart Example\n\nThis example demonstrates the basic functionality of Graphiti, including:\n\n1. Connecting to a Neo4j or FalkorDB database\n2. Initializing Graphiti indices and constraints\n3. Adding episodes to the graph\n4. Searching the graph with semantic and keyword matching\n5. Exploring graph-based search with reranking using the top search result's source node UUID\n6. Performing node search using predefined search recipes\n\n## Prerequisites\n\n- Python 3.9+  \n- OpenAI API key (set as `OPENAI_API_KEY` environment variable)  \n- **For Neo4j**:\n  - Neo4j Desktop installed and running  \n  - A local DBMS created and started in Neo4j Desktop  \n- **For FalkorDB**:\n  - FalkorDB server running (see [FalkorDB documentation](https://falkordb.com/docs/) for setup)\n\n\n## Setup Instructions\n\n1. Install the required dependencies:\n\n```bash\npip install graphiti-core\n```\n\n2. Set up environment variables:\n\n```bash\n# Required for LLM and embedding\nexport OPENAI_API_KEY=your_openai_api_key\n\n# Optional Neo4j connection parameters (defaults shown)\nexport NEO4J_URI=bolt://localhost:7687\nexport NEO4J_USER=neo4j\nexport NEO4J_PASSWORD=password\n\n# Optional FalkorDB connection parameters (defaults shown)\nexport FALKORDB_URI=falkor://localhost:6379\n\n# To use a different database, modify the driver constructor in the script\n```\n\n3. Run the example:\n\n```bash\npython quickstart_neo4j.py\n\n# For FalkorDB\npython quickstart_falkordb.py\n```\n\n## What This Example Demonstrates\n\n- **Graph Initialization**: Setting up the Graphiti indices and constraints in Neo4j or FalkorDB\n- **Adding Episodes**: Adding text content that will be analyzed and converted into knowledge graph nodes and edges\n- **Edge Search Functionality**: Performing hybrid searches that combine semantic similarity and BM25 retrieval to find relationships (edges)\n- **Graph-Aware Search**: Using the source node UUID from the top search result to rerank additional search results based on graph distance\n- **Node Search Using Recipes**: Using predefined search configurations like NODE_HYBRID_SEARCH_RRF to directly search for nodes rather than edges\n- **Result Processing**: Understanding the structure of search results including facts, nodes, and temporal metadata\n\n## Next Steps\n\nAfter running this example, you can:\n\n1. Modify the episode content to add your own information\n2. Try different search queries to explore the knowledge extraction\n3. Experiment with different center nodes for graph-distance-based reranking\n4. Try other predefined search recipes from `graphiti_core.search.search_config_recipes`\n5. Explore the more advanced examples in the other directories\n\n## Troubleshooting\n\n### \"Graph not found: default_db\" Error\n\nIf you encounter the error `Neo.ClientError.Database.DatabaseNotFound: Graph not found: default_db`, this occurs when the driver is trying to connect to a database that doesn't exist.\n\n**Solution:**\nThe Neo4j driver defaults to using `neo4j` as the database name. If you need to use a different database, modify the driver constructor in the script:\n\n```python\n# In quickstart_neo4j.py, change:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password)\n\n# To specify a different database:\ndriver = Neo4jDriver(uri=neo4j_uri, user=neo4j_user, password=neo4j_password, database=\"your_db_name\")\n```"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/examples/quickstart/README.md\n## Understanding the Output\n\n### Edge Search Results\n\nThe edge search results include EntityEdge objects with:\n\n- UUID: Unique identifier for the edge\n- Fact: The extracted fact from the episode\n- Valid at/invalid at: Time period during which the fact was true (if available)\n- Source/target node UUIDs: Connections between entities in the knowledge graph\n\n### Node Search Results\n\nThe node search results include EntityNode objects with:\n\n- UUID: Unique identifier for the node\n- Name: The name of the entity\n- Content Summary: A summary of the node's content\n- Node Labels: The types of the node (e.g., Person, Organization)\n- Created At: When the node was created\n- Attributes: Additional properties associated with the node"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/mcp_server/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n# Graphiti MCP Server\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nThis is an experimental Model Context Protocol (MCP) server implementation for Graphiti. The MCP server exposes\nGraphiti's key functionality through the MCP protocol, allowing AI assistants to interact with Graphiti's knowledge\ngraph capabilities.\n\n## Features\n\nThe Graphiti MCP server exposes the following key high-level functions of Graphiti:\n\n- **Episode Management**: Add, retrieve, and delete episodes (text, messages, or JSON data)\n- **Entity Management**: Search and manage entity nodes and relationships in the knowledge graph\n- **Search Capabilities**: Search for facts (edges) and node summaries using semantic and hybrid search\n- **Group Management**: Organize and manage groups of related data with group_id filtering\n- **Graph Maintenance**: Clear the graph and rebuild indices\n\n## Quick Start\n\n### Clone the Graphiti GitHub repo\n\n```bash\ngit clone https://github.com/getzep/graphiti.git\n```\n\nor\n\n```bash\ngh repo clone getzep/graphiti\n```\n\n### For Claude Desktop and other `stdio` only clients\n\n1. Note the full path to this directory.\n\n```\ncd graphiti && pwd\n```\n\n2. Install the [Graphiti prerequisites](#prerequisites).\n\n3. Configure Claude, Cursor, or other MCP client to use [Graphiti with a `stdio` transport](#integrating-with-mcp-clients). See the client documentation on where to find their MCP configuration files.\n\n### For Cursor and other `sse`-enabled clients\n\n1. Change directory to the `mcp_server` directory\n\n`cd graphiti/mcp_server`\n\n2. Start the service using Docker Compose\n\n`docker compose up`\n\n3. Point your MCP client to `http://localhost:8000/sse`\n\n## Installation\n\n### Prerequisites\n\n1. Ensure you have Python 3.10 or higher installed.\n2. A running Neo4j database (version 5.26 or later required)\n3. OpenAI API key for LLM operations\n\n### Setup\n\n1. Clone the repository and navigate to the mcp_server directory\n2. Use `uv` to create a virtual environment and install dependencies:\n\n```bash\n# Install uv if you don't have it already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a virtual environment and install dependencies in one step\nuv sync\n```\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `NEO4J_URI`: URI for the Neo4j database (default: `bolt://localhost:7687`)\n- `NEO4J_USER`: Neo4j username (default: `neo4j`)\n- `NEO4J_PASSWORD`: Neo4j password (default: `demodemo`)\n- `OPENAI_API_KEY`: OpenAI API key (required for LLM operations)\n- `OPENAI_BASE_URL`: Optional base URL for OpenAI API\n- `MODEL_NAME`: OpenAI model name to use for LLM operations.\n- `SMALL_MODEL_NAME`: OpenAI model name to use for smaller LLM operations.\n- `LLM_TEMPERATURE`: Temperature for LLM responses (0.0-2.0).\n- `AZURE_OPENAI_ENDPOINT`: Optional Azure OpenAI LLM endpoint URL", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/mcp_server/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n# Graphiti MCP Server\n\nGraphiti is a framework for building and querying temporally-aware knowledge graphs, specifically tailored for AI agents\noperating in dynamic environments. Unlike traditional retrieval-augmented generation (RAG) methods, Graphiti\ncontinuously integrates user interactions, structured and unstructured enterprise data, and external information into a\ncoherent, queryable graph. The framework supports incremental data updates, efficient retrieval, and precise historical\nqueries without requiring complete graph recomputation, making it suitable for developing interactive, context-aware AI\napplications.\n\nThis is an experimental Model Context Protocol (MCP) server implementation for Graphiti. The MCP server exposes\nGraphiti's key functionality through the MCP protocol, allowing AI assistants to interact with Graphiti's knowledge\ngraph capabilities.\n\n## Features\n\nThe Graphiti MCP server exposes the following key high-level functions of Graphiti:\n\n- **Episode Management**: Add, retrieve, and delete episodes (text, messages, or JSON data)\n- **Entity Management**: Search and manage entity nodes and relationships in the knowledge graph\n- **Search Capabilities**: Search for facts (edges) and node summaries using semantic and hybrid search\n- **Group Management**: Organize and manage groups of related data with group_id filtering\n- **Graph Maintenance**: Clear the graph and rebuild indices\n\n## Quick Start\n\n### Clone the Graphiti GitHub repo\n\n```bash\ngit clone https://github.com/getzep/graphiti.git\n```\n\nor\n\n```bash\ngh repo clone getzep/graphiti\n```\n\n### For Claude Desktop and other `stdio` only clients\n\n1. Note the full path to this directory.\n\n```\ncd graphiti && pwd\n```\n\n2. Install the [Graphiti prerequisites](#prerequisites).\n\n3. Configure Claude, Cursor, or other MCP client to use [Graphiti with a `stdio` transport](#integrating-with-mcp-clients). See the client documentation on where to find their MCP configuration files.\n\n### For Cursor and other `sse`-enabled clients\n\n1. Change directory to the `mcp_server` directory\n\n`cd graphiti/mcp_server`\n\n2. Start the service using Docker Compose\n\n`docker compose up`\n\n3. Point your MCP client to `http://localhost:8000/sse`\n\n## Installation\n\n### Prerequisites\n\n1. Ensure you have Python 3.10 or higher installed.\n2. A running Neo4j database (version 5.26 or later required)\n3. OpenAI API key for LLM operations\n\n### Setup\n\n1. Clone the repository and navigate to the mcp_server directory\n2. Use `uv` to create a virtual environment and install dependencies:\n\n```bash\n# Install uv if you don't have it already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a virtual environment and install dependencies in one step\nuv sync\n```\n\n## Configuration\n\nThe server uses the following environment variables:\n\n- `NEO4J_URI`: URI for the Neo4j database (default: `bolt://localhost:7687`)\n- `NEO4J_USER`: Neo4j username (default: `neo4j`)\n- `NEO4J_PASSWORD`: Neo4j password (default: `demodemo`)\n- `OPENAI_API_KEY`: OpenAI API key (required for LLM operations)\n- `OPENAI_BASE_URL`: Optional base URL for OpenAI API\n- `MODEL_NAME`: OpenAI model name to use for LLM operations.\n- `SMALL_MODEL_NAME`: OpenAI model name to use for smaller LLM operations.\n- `LLM_TEMPERATURE`: Temperature for LLM responses (0.0-2.0).\n- `AZURE_OPENAI_ENDPOINT`: Optional Azure OpenAI LLM endpoint URL"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/mcp_server/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to \"default\".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti's ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it's optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/mcp_server/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n- `AZURE_OPENAI_DEPLOYMENT_NAME`: Optional Azure OpenAI LLM deployment name\n- `AZURE_OPENAI_API_VERSION`: Optional Azure OpenAI LLM API version\n- `AZURE_OPENAI_EMBEDDING_API_KEY`: Optional Azure OpenAI Embedding deployment key (if other than `OPENAI_API_KEY`)\n- `AZURE_OPENAI_EMBEDDING_ENDPOINT`: Optional Azure OpenAI Embedding endpoint URL\n- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME`: Optional Azure OpenAI embedding deployment name\n- `AZURE_OPENAI_EMBEDDING_API_VERSION`: Optional Azure OpenAI API version\n- `AZURE_OPENAI_USE_MANAGED_IDENTITY`: Optional use Azure Managed Identities for authentication\n- `SEMAPHORE_LIMIT`: Episode processing concurrency. See [Concurrency and LLM Provider 429 Rate Limit Errors](#concurrency-and-llm-provider-429-rate-limit-errors)\n\nYou can set these variables in a `.env` file in the project directory.\n\n## Running the Server\n\nTo run the Graphiti MCP server directly using `uv`:\n\n```bash\nuv run graphiti_mcp_server.py\n```\n\nWith options:\n\n```bash\nuv run graphiti_mcp_server.py --model gpt-4.1-mini --transport sse\n```\n\nAvailable arguments:\n\n- `--model`: Overrides the `MODEL_NAME` environment variable.\n- `--small-model`: Overrides the `SMALL_MODEL_NAME` environment variable.\n- `--temperature`: Overrides the `LLM_TEMPERATURE` environment variable.\n- `--transport`: Choose the transport method (sse or stdio, default: sse)\n- `--group-id`: Set a namespace for the graph (optional). If not provided, defaults to \"default\".\n- `--destroy-graph`: If set, destroys all Graphiti graphs on startup.\n- `--use-custom-entities`: Enable entity extraction using the predefined ENTITY_TYPES\n\n### Concurrency and LLM Provider 429 Rate Limit Errors\n\nGraphiti's ingestion pipelines are designed for high concurrency, controlled by the `SEMAPHORE_LIMIT` environment variable.\nBy default, `SEMAPHORE_LIMIT` is set to `10` concurrent operations to help prevent `429` rate limit errors from your LLM provider. If you encounter such errors, try lowering this value.\n\nIf your LLM provider allows higher throughput, you can increase `SEMAPHORE_LIMIT` to boost episode ingestion performance.\n\n### Docker Deployment\n\nThe Graphiti MCP server can be deployed using Docker. The Dockerfile uses `uv` for package management, ensuring\nconsistent dependency installation.\n\n#### Environment Configuration\n\nBefore running the Docker Compose setup, you need to configure the environment variables. You have two options:\n\n1. **Using a .env file** (recommended):\n\n   - Copy the provided `.env.example` file to create a `.env` file:\n     ```bash\n     cp .env.example .env\n     ```\n   - Edit the `.env` file to set your OpenAI API key and other configuration options:\n     ```\n     # Required for LLM operations\n     OPENAI_API_KEY=your_openai_api_key_here\n     MODEL_NAME=gpt-4.1-mini\n     # Optional: OPENAI_BASE_URL only needed for non-standard OpenAI endpoints\n     # OPENAI_BASE_URL=https://api.openai.com/v1\n     ```\n   - The Docker Compose setup is configured to use this file if it exists (it's optional)\n\n2. **Using environment variables directly**:\n   - You can also set the environment variables when running the Docker Compose command:\n     ```bash\n     OPENAI_API_KEY=your_key MODEL_NAME=gpt-4.1-mini docker compose up\n     ```"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/mcp_server/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n#### Neo4j Configuration\n\nThe Docker Compose setup includes a Neo4j container with the following default configuration:\n\n- Username: `neo4j`\n- Password: `demodemo`\n- URI: `bolt://neo4j:7687` (from within the Docker network)\n- Memory settings optimized for development use\n\n#### Running with Docker Compose\n\nA Graphiti MCP container is available at: `zepai/knowledge-graph-mcp`. The latest build of this container is used by the Compose setup below.\n\nStart the services using Docker Compose:\n\n```bash\ndocker compose up\n```\n\nOr if you're using an older version of Docker Compose:\n\n```bash\ndocker-compose up\n```\n\nThis will start both the Neo4j database and the Graphiti MCP server. The Docker setup:\n\n- Uses `uv` for package management and running the server\n- Installs dependencies from the `pyproject.toml` file\n- Connects to the Neo4j container using the environment variables\n- Exposes the server on port 8000 for HTTP-based SSE transport\n- Includes a healthcheck for Neo4j to ensure it's fully operational before starting the MCP server\n\n## Integrating with MCP Clients\n\n### Configuration\n\nTo use the Graphiti MCP server with an MCP-compatible client, configure it to connect to the server:\n\n> [!IMPORTANT]\n> You will need the Python package manager, `uv` installed. Please refer to the [`uv` install instructions](https://docs.astral.sh/uv/getting-started/installation/).\n>\n> Ensure that you set the full path to the `uv` binary and your Graphiti project folder.\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"transport\": \"stdio\",\n      \"command\": \"/Users/<user>/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--isolated\",\n        \"--directory\",\n        \"/Users/<user>>/dev/zep/graphiti/mcp_server\",\n        \"--project\",\n        \".\",\n        \"graphiti_mcp_server.py\",\n        \"--transport\",\n        \"stdio\"\n      ],\n      \"env\": {\n        \"NEO4J_URI\": \"bolt://localhost:7687\",\n        \"NEO4J_USER\": \"neo4j\",\n        \"NEO4J_PASSWORD\": \"password\",\n        \"OPENAI_API_KEY\": \"sk-XXXXXXXX\",\n        \"MODEL_NAME\": \"gpt-4.1-mini\"\n      }\n    }\n  }\n}\n```\n\nFor SSE transport (HTTP-based), you can use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe Graphiti MCP server exposes the following tools:\n\n- `add_episode`: Add an episode to the knowledge graph (supports text, JSON, and message formats)\n- `search_nodes`: Search the knowledge graph for relevant node summaries\n- `search_facts`: Search the knowledge graph for relevant facts (edges between entities)\n- `delete_entity_edge`: Delete an entity edge from the knowledge graph\n- `delete_episode`: Delete an episode from the knowledge graph\n- `get_entity_edge`: Get an entity edge by its UUID\n- `get_episodes`: Get the most recent episodes for a specific group\n- `clear_graph`: Clear all data from the knowledge graph and rebuild indices\n- `get_status`: Get the status of the Graphiti MCP server and Neo4j connection", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/mcp_server/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n#### Neo4j Configuration\n\nThe Docker Compose setup includes a Neo4j container with the following default configuration:\n\n- Username: `neo4j`\n- Password: `demodemo`\n- URI: `bolt://neo4j:7687` (from within the Docker network)\n- Memory settings optimized for development use\n\n#### Running with Docker Compose\n\nA Graphiti MCP container is available at: `zepai/knowledge-graph-mcp`. The latest build of this container is used by the Compose setup below.\n\nStart the services using Docker Compose:\n\n```bash\ndocker compose up\n```\n\nOr if you're using an older version of Docker Compose:\n\n```bash\ndocker-compose up\n```\n\nThis will start both the Neo4j database and the Graphiti MCP server. The Docker setup:\n\n- Uses `uv` for package management and running the server\n- Installs dependencies from the `pyproject.toml` file\n- Connects to the Neo4j container using the environment variables\n- Exposes the server on port 8000 for HTTP-based SSE transport\n- Includes a healthcheck for Neo4j to ensure it's fully operational before starting the MCP server\n\n## Integrating with MCP Clients\n\n### Configuration\n\nTo use the Graphiti MCP server with an MCP-compatible client, configure it to connect to the server:\n\n> [!IMPORTANT]\n> You will need the Python package manager, `uv` installed. Please refer to the [`uv` install instructions](https://docs.astral.sh/uv/getting-started/installation/).\n>\n> Ensure that you set the full path to the `uv` binary and your Graphiti project folder.\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"transport\": \"stdio\",\n      \"command\": \"/Users/<user>/.local/bin/uv\",\n      \"args\": [\n        \"run\",\n        \"--isolated\",\n        \"--directory\",\n        \"/Users/<user>>/dev/zep/graphiti/mcp_server\",\n        \"--project\",\n        \".\",\n        \"graphiti_mcp_server.py\",\n        \"--transport\",\n        \"stdio\"\n      ],\n      \"env\": {\n        \"NEO4J_URI\": \"bolt://localhost:7687\",\n        \"NEO4J_USER\": \"neo4j\",\n        \"NEO4J_PASSWORD\": \"password\",\n        \"OPENAI_API_KEY\": \"sk-XXXXXXXX\",\n        \"MODEL_NAME\": \"gpt-4.1-mini\"\n      }\n    }\n  }\n}\n```\n\nFor SSE transport (HTTP-based), you can use this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"transport\": \"sse\",\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n## Available Tools\n\nThe Graphiti MCP server exposes the following tools:\n\n- `add_episode`: Add an episode to the knowledge graph (supports text, JSON, and message formats)\n- `search_nodes`: Search the knowledge graph for relevant node summaries\n- `search_facts`: Search the knowledge graph for relevant facts (edges between entities)\n- `delete_entity_edge`: Delete an entity edge from the knowledge graph\n- `delete_episode`: Delete an episode from the knowledge graph\n- `get_entity_edge`: Get an entity edge by its UUID\n- `get_episodes`: Get the most recent episodes for a specific group\n- `clear_graph`: Clear all data from the knowledge graph and rebuild indices\n- `get_status`: Get the status of the Graphiti MCP server and Neo4j connection"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/mcp_server/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=\"json\"`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=\"Customer Profile\",\nepisode_body=\"{\\\"company\\\": {\\\"name\\\": \\\"Acme Technologies\\\"}, \\\"products\\\": [{\\\"id\\\": \\\"P001\\\", \\\"name\\\": \\\"CloudSync\\\"}, {\\\"id\\\": \\\"P002\\\", \\\"name\\\": \\\"DataMiner\\\"}]}\",\nsource=\"json\",\nsource_description=\"CRM data\"\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use \"default\" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor's User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti's knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you'll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"graphiti-memory\": {\n          // You can choose a different name if you prefer\n          \"command\": \"npx\", // Or the full path to mcp-remote if npx is not in your PATH\n          \"args\": [\n            \"mcp-remote\",\n            \"http://localhost:8000/sse\" // Ensure this matches your Graphiti server's SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/mcp_server/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Working with JSON Data\n\nThe Graphiti MCP server can process structured JSON data through the `add_episode` tool with `source=\"json\"`. This\nallows you to automatically extract entities and relationships from structured data:\n\n```\n\nadd_episode(\nname=\"Customer Profile\",\nepisode_body=\"{\\\"company\\\": {\\\"name\\\": \\\"Acme Technologies\\\"}, \\\"products\\\": [{\\\"id\\\": \\\"P001\\\", \\\"name\\\": \\\"CloudSync\\\"}, {\\\"id\\\": \\\"P002\\\", \\\"name\\\": \\\"DataMiner\\\"}]}\",\nsource=\"json\",\nsource_description=\"CRM data\"\n)\n\n```\n\n## Integrating with the Cursor IDE\n\nTo integrate the Graphiti MCP Server with the Cursor IDE, follow these steps:\n\n1. Run the Graphiti MCP server using the SSE transport:\n\n```bash\npython graphiti_mcp_server.py --transport sse --use-custom-entities --group-id <your_group_id>\n```\n\nHint: specify a `group_id` to namespace graph data. If you do not specify a `group_id`, the server will use \"default\" as the group_id.\n\nor\n\n```bash\ndocker compose up\n```\n\n2. Configure Cursor to connect to the Graphiti MCP server.\n\n```json\n{\n  \"mcpServers\": {\n    \"graphiti-memory\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n3. Add the Graphiti rules to Cursor's User Rules. See [cursor_rules.md](cursor_rules.md) for details.\n\n4. Kick off an agent session in Cursor.\n\nThe integration enables AI assistants in Cursor to maintain persistent memory through Graphiti's knowledge graph\ncapabilities.\n\n## Integrating with Claude Desktop (Docker MCP Server)\n\nThe Graphiti MCP Server container uses the SSE MCP transport. Claude Desktop does not natively support SSE, so you'll need to use a gateway like `mcp-remote`.\n\n1.  **Run the Graphiti MCP server using SSE transport**:\n\n    ```bash\n    docker compose up\n    ```\n\n2.  **(Optional) Install `mcp-remote` globally**:\n    If you prefer to have `mcp-remote` installed globally, or if you encounter issues with `npx` fetching the package, you can install it globally. Otherwise, `npx` (used in the next step) will handle it for you.\n\n    ```bash\n    npm install -g mcp-remote\n    ```\n\n3.  **Configure Claude Desktop**:\n    Open your Claude Desktop configuration file (usually `claude_desktop_config.json`) and add or modify the `mcpServers` section as follows:\n\n    ```json\n    {\n      \"mcpServers\": {\n        \"graphiti-memory\": {\n          // You can choose a different name if you prefer\n          \"command\": \"npx\", // Or the full path to mcp-remote if npx is not in your PATH\n          \"args\": [\n            \"mcp-remote\",\n            \"http://localhost:8000/sse\" // Ensure this matches your Graphiti server's SSE endpoint\n          ]\n        }\n      }\n    }\n    ```\n\n    If you already have an `mcpServers` entry, add `graphiti-memory` (or your chosen name) as a new key within it.\n\n4.  **Restart Claude Desktop** for the changes to take effect.\n\n## Requirements\n\n- Python 3.10 or higher\n- Neo4j database (version 5.26 or later required)\n- OpenAI API key (for LLM operations and embeddings)\n- MCP-compatible client"}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/mcp_server/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Telemetry\n\nThe Graphiti MCP server uses the Graphiti core library, which includes anonymous telemetry collection. When you initialize the Graphiti MCP server, anonymous usage statistics are collected to help improve the framework.\n\n### What's Collected\n\n- Anonymous identifier and system information (OS, Python version)\n- Graphiti version and configuration choices (LLM provider, database backend, embedder type)\n- **No personal data, API keys, or actual graph content is ever collected**\n\n### How to Disable\n\nTo disable telemetry in the MCP server, set the environment variable:\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\nOr add it to your `.env` file:\n\n```\nGRAPHITI_TELEMETRY_ENABLED=false\n```\n\nFor complete details about what's collected and why, see the [Telemetry section in the main Graphiti README](../README.md#telemetry).\n\n## License\n\nThis project is licensed under the same license as the parent Graphiti project.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/mcp_server/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/README.md\n## Telemetry\n\nThe Graphiti MCP server uses the Graphiti core library, which includes anonymous telemetry collection. When you initialize the Graphiti MCP server, anonymous usage statistics are collected to help improve the framework.\n\n### What's Collected\n\n- Anonymous identifier and system information (OS, Python version)\n- Graphiti version and configuration choices (LLM provider, database backend, embedder type)\n- **No personal data, API keys, or actual graph content is ever collected**\n\n### How to Disable\n\nTo disable telemetry in the MCP server, set the environment variable:\n\n```bash\nexport GRAPHITI_TELEMETRY_ENABLED=false\n```\n\nOr add it to your `.env` file:\n\n```\nGRAPHITI_TELEMETRY_ENABLED=false\n```\n\nFor complete details about what's collected and why, see the [Telemetry section in the main Graphiti README](../README.md#telemetry).\n\n## License\n\nThis project is licensed under the same license as the parent Graphiti project."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/mcp_server/cursor_rules.md\n## Instructions for Using Graphiti's MCP Tools for Agent Memory\n\n### Before Starting Any Task\n\n- **Always search first:** Use the `search_nodes` tool to look for relevant preferences and procedures before beginning work.\n- **Search for facts too:** Use the `search_facts` tool to discover relationships and factual information that may be relevant to your task.\n- **Filter by entity type:** Specify `Preference`, `Procedure`, or `Requirement` in your node search to get targeted results.\n- **Review all matches:** Carefully examine any preferences, procedures, or facts that match your current task.\n\n### Always Save New or Updated Information\n\n- **Capture requirements and preferences immediately:** When a user expresses a requirement or preference, use `add_memory` to store it right away.\n  - _Best practice:_ Split very long requirements into shorter, logical chunks.\n- **Be explicit if something is an update to existing knowledge.** Only add what's changed or new to the graph.\n- **Document procedures clearly:** When you discover how a user wants things done, record it as a procedure.\n- **Record factual relationships:** When you learn about connections between entities, store these as facts.\n- **Be specific with categories:** Label preferences and procedures with clear categories for better retrieval later.\n\n### During Your Work\n\n- **Respect discovered preferences:** Align your work with any preferences you've found.\n- **Follow procedures exactly:** If you find a procedure for your current task, follow it step by step.\n- **Apply relevant facts:** Use factual information to inform your decisions and recommendations.\n- **Stay consistent:** Maintain consistency with previously identified preferences, procedures, and facts.\n\n### Best Practices\n\n- **Search before suggesting:** Always check if there's established knowledge before making recommendations.\n- **Combine node and fact searches:** For complex tasks, search both nodes and facts to build a complete picture.\n- **Use `center_node_uuid`:** When exploring related information, center your search around a specific node.\n- **Prioritize specific matches:** More specific information takes precedence over general information.\n- **Be proactive:** If you notice patterns in user behavior, consider storing them as preferences or procedures.\n\n**Remember:** The knowledge graph is your memory. Use it consistently to provide personalized assistance that respects the user's established preferences, procedures, and factual context."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/tests/evals/data/longmemeval_data/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/tests/evals/data/longmemeval_data/README.md\nThe `longmemeval_oracle` dataset is an open-source dataset that we are using.\nWe did not create this dataset and it can be found\nhere: https://huggingface.co/datasets/xiaowu0162/longmemeval/blob/main/longmemeval_oracle.", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/tests/evals/data/longmemeval_data/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/tests/evals/data/longmemeval_data/README.md\nThe `longmemeval_oracle` dataset is an open-source dataset that we are using.\nWe did not create this dataset and it can be found\nhere: https://huggingface.co/datasets/xiaowu0162/longmemeval/blob/main/longmemeval_oracle."}, {"codebase": "graphiti", "reference": "https://github.com/getzep/graphiti/blob/main/server/README.md", "text": "graphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: '3.8'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - \"8000:8000\"\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - \"7474:7474\"  # HTTP\n            - \"${NEO4J_PORT}:${NEO4J_PORT}\"  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using).", "enriched_text": "[Codebase: graphiti] [File: https://github.com/getzep/graphiti/blob/main/server/README.md]\n\ngraphiti\nhttps://github.com/getzep/graphiti/blob/main/server/README.md\n# graph-service\n\nGraph service is a fast api server implementing the [graphiti](https://github.com/getzep/graphiti) package.\n\n\n## Running Instructions\n\n1. Ensure you have Docker and Docker Compose installed on your system.\n\n2. Add `zepai/graphiti:latest` to your service setup\n\n3. Make sure to pass the following environment variables to the service\n\n   ```\n   OPENAI_API_KEY=your_openai_api_key\n   NEO4J_USER=your_neo4j_user\n   NEO4J_PASSWORD=your_neo4j_password\n   NEO4J_PORT=your_neo4j_port\n   ```\n\n4. This service depends on having access to a neo4j instance, you may wish to add a neo4j image to your service setup as well. Or you may wish to use neo4j cloud or a desktop version if running this locally.\n\n   An example of docker compose setup may look like this:\n\n   ```yml\n      version: '3.8'\n\n      services:\n      graph:\n         image: zepai/graphiti:latest\n         ports:\n            - \"8000:8000\"\n         \n         environment:\n            - OPENAI_API_KEY=${OPENAI_API_KEY}\n            - NEO4J_URI=bolt://neo4j:${NEO4J_PORT}\n            - NEO4J_USER=${NEO4J_USER}\n            - NEO4J_PASSWORD=${NEO4J_PASSWORD}\n      neo4j:\n         image: neo4j:5.22.0\n         \n         ports:\n            - \"7474:7474\"  # HTTP\n            - \"${NEO4J_PORT}:${NEO4J_PORT}\"  # Bolt\n         volumes:\n            - neo4j_data:/data\n         environment:\n            - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}\n\n      volumes:\n      neo4j_data:\n   ```\n\n5. Once you start the service, it will be available at `http://localhost:8000` (or the port you have specified in the docker compose file).\n\n6. You may access the swagger docs at `http://localhost:8000/docs`. You may also access redocs at `http://localhost:8000/redoc`.\n\n7. You may also access the neo4j browser at `http://localhost:7474` (the port depends on the neo4j instance you are using)."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//README.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//README.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//README.md\n# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`.\nNote that `tensorflow` is required for using certain Keras 3 features: certain preprocessing layers\nas well as `tf.data` pipelines.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `\"tensorflow\"`, `\"jax\"`, `\"torch\"`, `\"openvino\"`. Example:\n\n```\nexport KERAS_BACKEND=\"jax\"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after \nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//README.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//README.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//README.md\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you're\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you're using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/)."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//SECURITY.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant\u2019s identity.", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//SECURITY.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n# Security Policy\n\n - [**Using Keras Securely**](#using-keras-securely)\n   - [Untrusted inputs](#untrusted-inputs)\n   - [Data privacy](#data-privacy)\n   - [Untrusted environments or networks](#untrusted-environments-or-networks)\n   - [Multi-Tenant environments](#multi-tenant-environments)\n - [**Reporting a Vulnerability**](#reporting-a-vulnerability)\n\n## Using Keras Securely\n\n### Untrusted inputs\n\nSome models accept various input formats (text, images, audio, etc.). The libraries converting these inputs have varying security levels, so it's crucial to isolate the model and carefully pre-process inputs to mitigate script injection risks.\n\nFor maximum security when handling untrusted inputs, you may need to employ the following:\n\n* Sandboxing: Isolate the model process.\n* Pre-analysis: check how the model performs by default when exposed to prompt injection (e.g. using [fuzzing for prompt injection](https://github.com/FonduAI/awesome-prompt-injection?tab=readme-ov-file#tools)). This will give you leads on how hard you will have to work on the next topics.\n* Updates: Keep your model and libraries updated with the latest security patches.\n* Input Sanitation: Before feeding data to the model, sanitize inputs rigorously. This involves techniques such as:\n    * Validation: Enforce strict rules on allowed characters and data types.\n    * Filtering: Remove potentially malicious scripts or code fragments.\n    * Encoding: Convert special characters into safe representations.\n    * Verification: Run tooling that identifies potential script injections (e.g. [models that detect prompt injection attempts](https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)). \n\n### Data privacy\nTo protect sensitive data from potential leaks or unauthorized access, it is essential to sandbox the model execution. This means running the model in a secure, isolated environment, which helps mitigate many attack vectors.\n\nWhen training the model with sensitive data, expose your newly-trained model to tests to identify potential sensitive data leaks.\n\n### Untrusted environments or networks\n\nIf you can't run your models in a secure and isolated environment or if it must be exposed to an untrusted network, make sure to take the following security precautions:\n* Confirm the hash of  any downloaded artifact (i.e. pre-trained model weights) matches a known-good value\n* Encrypt your data while sending it over the network.\n\n### Multi-Tenant environments\n\nIf you intend to run multiple models in parallel with shared memory, it is your responsibility to ensure the models do not interact or access each other's data. The primary areas of concern are tenant isolation, resource allocation, model sharing and hardware attacks.\n\n#### Tenant Isolation\n\nYou must make sure that models run separately. Since models can run code, it's important to use strong isolation methods to prevent unwanted access to the data from other tenants.\n\nSeparating networks is also a big part of isolation. If you keep model network traffic separate, you not only prevent unauthorized access to data or models, but also prevent malicious users or tenants sending graphs to execute under another tenant\u2019s identity."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//SECURITY.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n#### Resource Allocation\n\nA denial of service caused by one model can impact the overall system health. Implement safeguards like rate limits, access controls, and health monitoring.\n\n#### Model Sharing\n\nIn a multitenant design that allows sharing models, make sure that tenants and users fully understand the potential security risks involved. They must be aware that they will essentially be running code provided by other users. Unfortunately, there are no reliable methods available to detect malicious models, graphs, or checkpoints. To mitigate this risk, the recommended approach is to sandbox the model execution, effectively isolating it from the rest of the system.\n\n#### Hardware Attacks\n\nBesides the virtual environment, the hardware (GPUs or TPUs) can also be attacked. [Research](https://scholar.google.com/scholar?q=gpu+side+channel) has shown that side channel attacks on GPUs are possible, which can make data leak from other models or processes running on the same system at the same time.\n\n## Reporting a Vulnerability\n\nBeware that none of the topics under [Using Keras Securely](#using-keras-securely) are considered vulnerabilities of Keras.\n\nIf you have discovered a security vulnerability in this project, please report it\nprivately. **Do not disclose it as a public issue.** This gives us time to work with you\nto fix the issue before public exposure, reducing the chance that the exploit will be\nused before a patch is released.\n\nYou may submit the report in the following ways:\n\n- send an email to francois.chollet@gmail.com; and/or\n- send a [private vulnerability report](https://github.com/keras-team/keras/security/advisories/new)\n\nPlease provide the following information in your report:\n\n- A description of the vulnerability and its impact\n- How to reproduce the issue\n\nThis project is maintained by volunteers on a reasonable-effort basis. As such,\nplease give us 90 days to work on a fix before public exposure.", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//SECURITY.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//SECURITY.md\n#### Resource Allocation\n\nA denial of service caused by one model can impact the overall system health. Implement safeguards like rate limits, access controls, and health monitoring.\n\n#### Model Sharing\n\nIn a multitenant design that allows sharing models, make sure that tenants and users fully understand the potential security risks involved. They must be aware that they will essentially be running code provided by other users. Unfortunately, there are no reliable methods available to detect malicious models, graphs, or checkpoints. To mitigate this risk, the recommended approach is to sandbox the model execution, effectively isolating it from the rest of the system.\n\n#### Hardware Attacks\n\nBesides the virtual environment, the hardware (GPUs or TPUs) can also be attacked. [Research](https://scholar.google.com/scholar?q=gpu+side+channel) has shown that side channel attacks on GPUs are possible, which can make data leak from other models or processes running on the same system at the same time.\n\n## Reporting a Vulnerability\n\nBeware that none of the topics under [Using Keras Securely](#using-keras-securely) are considered vulnerabilities of Keras.\n\nIf you have discovered a security vulnerability in this project, please report it\nprivately. **Do not disclose it as a public issue.** This gives us time to work with you\nto fix the issue before public exposure, reducing the chance that the exploit will be\nused before a patch is released.\n\nYou may submit the report in the following ways:\n\n- send an email to francois.chollet@gmail.com; and/or\n- send a [private vulnerability report](https://github.com/keras-team/keras/security/advisories/new)\n\nPlease provide the following information in your report:\n\n- A description of the vulnerability and its impact\n- How to reproduce the issue\n\nThis project is maintained by volunteers on a reasonable-effort basis. As such,\nplease give us 90 days to work on a fix before public exposure."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//CONTRIBUTING.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you'll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn't already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n\"Setup environment\".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven't signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer).", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//CONTRIBUTING.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\nKeras 3 is a high-velocity open-source project. We welcome contributions!\n\nContributions can be made in a variety of ways, including coding, enriching documentation, refining docstrings, and providing code examples.\n\n\n## Current items open for contributions\nAt [this link](https://github.com/keras-team/keras/issues/18442), you'll find a list of items where you help is needed!\n\n\n## How to contribute code\n\nFollow these steps to submit your code contribution.\n\n### Step 1. Open an issue\n\nBefore making any changes, we recommend opening an issue (if one doesn't already\nexist) and discussing your proposed changes. This way, we can give you feedback\nand validate the proposed changes.\n\nIf the changes are minor (simple bug fix or documentation fix), then feel free\nto open a Pull Request (PR) without discussion.\n\n### Step 2. Make code changes\n\nTo make code changes, you need to fork the repository. You will need to setup a\ndevelopment environment and run the unit tests. This is covered in the section\n\"Setup environment\".\n\n### Step 3. Create a pull request\n\nOnce the change is ready, open a pull request from your branch in your fork to\nthe master branch in [keras-team/keras](https://github.com/keras-team/keras).\n\n### Step 4. Sign the Contributor License Agreement\n\nAfter creating the pull request, the `cla/google` check will be performed and,\nif you haven't signed the Contributor License Agreement (CLA), it will fail with\ninstructions on how to do so. Please follow the instructions to sign the CLA and\nthe check will pass.\n\n![CLA signed](https://github.com/keras-team/keras/assets/1091026/71c26353-e3b5-4135-8bae-64693c717775)\n\n\n### Step 5. Code review\n\nIf the tests fail, look into the error messages and try to fix them.\n\n![CI tests](https://github.com/keras-team/keras/assets/1091026/6f6c17ef-6bd7-4e95-9fbc-1906cde37380)\n\nA reviewer will review the pull request and provide comments. There may be\nseveral rounds of comments and code changes before the pull request gets\napproved by the reviewer.\n\n![Approval from reviewer](https://github.com/keras-team/keras/assets/1091026/8d28f74c-21e9-4146-b0ff-62d649a552a8)\n\n### Step 6. Merging\n\nOnce the pull request is approved, a `ready to pull` tag will be added to the\npull request. A team member will take care of the merging.\n\n![Ready to pull and merged](https://github.com/keras-team/keras/assets/1091026/c3908345-d7ae-44ee-a428-01f3b448b46b)\n\nHere is an [example pull request](https://github.com/keras-team/keras/pull/18848)\nfor your reference.\n\n## Setup environment\n\nWe provide two ways of setting up a development environment. One is to use a\ndev container, and the other one is to set up a local environment by installing\nthe dev tools needed.\n\n### Option 1: GitHub Codespace or dev container\n\nWe support GitHub Codespaces, Visual Studio Code dev containers and JetBrain dev\ncontainers. Please see the\n[Dev container documentation](https://github.com/keras-team/keras/tree/master/.devcontainer)."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//CONTRIBUTING.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n### Option 2: Set up a local environment\n\nTo set up your local dev environment, you will need the following tools.\n\n1.  [git](https://github.com/) for code repository management.\n2.  [python](https://www.python.org/) to build and code in Keras.\n\nThe following commands check the tools above are successfully installed. Note\nthat Keras requires at least Python 3.10 to run.\n\n```shell\ngit --version\npython --version\n```\n\nClone your forked repo to your local machine. Go to the cloned directory to\ninstall the dependencies.\n\n```shell\ngit clone https://github.com/YOUR_GITHUB_USERNAME/keras.git\ncd keras\npip install -r requirements.txt\n```\n\nYou then need to configure the backend to use, see the\n[Configuring your backend](https://github.com/keras-team/keras/blob/master/README.md#configuring-your-backend)\nsection of the README.\n\nYou can also add GPU support to your environment, see the\n[Adding GPU support](https://github.com/keras-team/keras/blob/master/README.md#adding-gpu-support)\nsection of the README.\n\n## Generating public API and formatting the code\n\nFor the first time you are setting up the repo, please run `pre-commit install`.\nNote that this needs to be done only once at the beginning.\n\nNow, whenever you run `git commit -m \"<message>\"`, three things are\nautomatically done:\n\n- Public API generation\n- Code formatting\n- Code linting\n\nIf there's any error, the commit will not go through. Please fix the error (\nmost of the times, the error is fixed automatically by the formatter/linter) and\nre-run the following:\n\n```\ngit add .\ngit commit -m \"<message>\" # This will not get logged as a duplicate commit.\n```\n\nIn case you want to run the above manually on all files, you can do the\nfollowing:\n\n```\npre-commit run --all-files\n```\n\nKerasHub uses [Ruff](https://docs.astral.sh/ruff/) to format the code.\n\n### Docstrings\n\nWe do not have an automated way to check docstring style, so if you write\nor edit any docstring, please make sure to check them manually.\nKeras docstrings follow the conventions below:\n\nA **class docstring** may contain the following items:\n\n* A one-line description of the class.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for arguments in `__init__()`.\n* If it's a layer:\n    * `Call arguments` section for arguments in `Layer.call()`.\n    * `Returns` section for the return values of `Layer.call()`.\n    * Optional `Raises` section for possible errors.\n\nYou can check out `MultiHeadAttention` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/layers/attention/multi_head_attention.py#L20).\n\nA **function docstring** may contain the following items:\n\n* One-line description of the function.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for the function arguments.\n* `Returns` section for the return values.\n* Optional `Raises` section for possible errors.\n\nYou can check out `text_dataset_from_directory` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/utils/text_dataset_utils.py#L27).", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//CONTRIBUTING.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n### Option 2: Set up a local environment\n\nTo set up your local dev environment, you will need the following tools.\n\n1.  [git](https://github.com/) for code repository management.\n2.  [python](https://www.python.org/) to build and code in Keras.\n\nThe following commands check the tools above are successfully installed. Note\nthat Keras requires at least Python 3.10 to run.\n\n```shell\ngit --version\npython --version\n```\n\nClone your forked repo to your local machine. Go to the cloned directory to\ninstall the dependencies.\n\n```shell\ngit clone https://github.com/YOUR_GITHUB_USERNAME/keras.git\ncd keras\npip install -r requirements.txt\n```\n\nYou then need to configure the backend to use, see the\n[Configuring your backend](https://github.com/keras-team/keras/blob/master/README.md#configuring-your-backend)\nsection of the README.\n\nYou can also add GPU support to your environment, see the\n[Adding GPU support](https://github.com/keras-team/keras/blob/master/README.md#adding-gpu-support)\nsection of the README.\n\n## Generating public API and formatting the code\n\nFor the first time you are setting up the repo, please run `pre-commit install`.\nNote that this needs to be done only once at the beginning.\n\nNow, whenever you run `git commit -m \"<message>\"`, three things are\nautomatically done:\n\n- Public API generation\n- Code formatting\n- Code linting\n\nIf there's any error, the commit will not go through. Please fix the error (\nmost of the times, the error is fixed automatically by the formatter/linter) and\nre-run the following:\n\n```\ngit add .\ngit commit -m \"<message>\" # This will not get logged as a duplicate commit.\n```\n\nIn case you want to run the above manually on all files, you can do the\nfollowing:\n\n```\npre-commit run --all-files\n```\n\nKerasHub uses [Ruff](https://docs.astral.sh/ruff/) to format the code.\n\n### Docstrings\n\nWe do not have an automated way to check docstring style, so if you write\nor edit any docstring, please make sure to check them manually.\nKeras docstrings follow the conventions below:\n\nA **class docstring** may contain the following items:\n\n* A one-line description of the class.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for arguments in `__init__()`.\n* If it's a layer:\n    * `Call arguments` section for arguments in `Layer.call()`.\n    * `Returns` section for the return values of `Layer.call()`.\n    * Optional `Raises` section for possible errors.\n\nYou can check out `MultiHeadAttention` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/layers/attention/multi_head_attention.py#L20).\n\nA **function docstring** may contain the following items:\n\n* One-line description of the function.\n* Paragraph(s) of more detailed information.\n* Optional `Examples` section.\n* `Args` section for the function arguments.\n* `Returns` section for the return values.\n* Optional `Raises` section for possible errors.\n\nYou can check out `text_dataset_from_directory` as an example\n[(link)](https://github.com/keras-team/keras/blob/v3.0.0/keras/utils/text_dataset_utils.py#L27)."}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main//CONTRIBUTING.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main//CONTRIBUTING.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main//CONTRIBUTING.md\n## Run tests\n\nWe use [pytest](https://pytest.org/) to run the tests.\n\n### Run a test file\n\nTo run the tests in `keras/src/losses/losses_test.py`, use the following command\nat the root directory of the repo.\n\n```shell\npytest keras/src/losses/losses_test.py\n```\n\n### Run a single test case\n\nYou can specify a single test class to run within a file.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest\n```\n\nYou can also specify a single test method to run within a class.\n\n```shell\npytest keras/src/losses/losses_test.py::MeanSquaredErrorTest::test_sample_weighted\n```\n\n### Run all tests\n\nYou can run all the tests locally by running the following command in the repo\nroot directory.\n\n```shell\npytest keras\n```\n\nNote that you can skip the Keras applications tests using the\n`SKIP_APPLICATIONS_TESTS` environment variable. This will cut down the testing\ntime significantly.\n\n```shell\nSKIP_APPLICATIONS_TESTS=True pytest keras\n```\n\nTo run all tests using a different backend, you can simply specify it on the\ncommand line.\n\n```shell\nKERAS_BACKEND=jax SKIP_APPLICATIONS_TESTS=True pytest keras\n```"}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/layer_benchmark/README.md\n# Benchmark the layer performance\n\nThis directory contains benchmarks to compare the performance of\n`keras.layers.XXX` and `tf.keras.layers.XXX`. We compare the performance of\nboth the forward pass and train step (forward & backward pass). \n\nTo run the benchmark, use the command below and change the flags according to\nyour target:\n\n```shell\npython3 -m benchmarks.layer_benchmark.conv_benchmark \\\n    --benchmark_name=benchmark_conv2D \\\n    --num_samples=2048 \\\n    --batch_size=256 \\\n    --jit_compile=True\n```"}, {"codebase": "keras", "reference": "https://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md", "text": "keras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```", "enriched_text": "[Codebase: keras] [File: https://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md]\n\nkeras\nhttps://github.com/keras-team/keras/blob/main/benchmarks/torch_ctl_benchmark/README.md\n# Benchmark the performance of torch custom training loop\n\nThis directory contains benchmarks to compare the performance of a Keras model\nand a equivalent Torch model while using the same Torch custom training loop.\n\nThe benchmark purpose is to understand the performance diff resulting from the\nmodeling API choice (Keras or Torch).\n\nTo run the benchmark, use the command below and change to your target:\n\n```shell\npython3 -m benchmarks.torch_ctl_benchmark.conv_model_benchmark\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//CODE_OF_CONDUCT.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//CODE_OF_CONDUCT.md\n# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//CODE_OF_CONDUCT.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//CODE_OF_CONDUCT.md\n# Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\n\nResources:\n\n- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)\n- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\n- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What's NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=\"https://github.com/microsoft/RD-Agent\"><img src=\"docs/_static/img/rdagent_logo.png\" alt=\"RD_Agent\" style=\"height: 2em\"></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**\ud83d\udce2, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star\ud83c\udf1f!\n\nTo learn more, please visit our [\u267e\ufe0fDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (\u4e2d\u6587) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- \ud83d\udcc3**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- \ud83d\udc7e**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | \ud83d\udcc8Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What's NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href=\"https://github.com/microsoft/RD-Agent\"><img src=\"docs/_static/img/rdagent_logo.png\" alt=\"RD_Agent\" style=\"height: 2em\"></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**\ud83d\udce2, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your star\ud83c\udf1f!\n\nTo learn more, please visit our [\u267e\ufe0fDemo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (\u4e2d\u6587) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- \ud83d\udcc3**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- \ud83d\udc7e**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | \ud83d\udcc8Coming soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n| \ud83d\udd25LLM-driven Auto Quant Factory\ud83d\udd25 | \ud83d\ude80 Released in [\u267e\ufe0fRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |\n| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |\n| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |\n| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|\n| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |\n| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | \ud83d\udcd6 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | \n| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |\n| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |\n| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |\n| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | \n| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | \n| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |\n| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |\n| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |\n| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |\n| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |\n| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |\n| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |\n| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |\n| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |\n| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | \n| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n| \ud83d\udd25LLM-driven Auto Quant Factory\ud83d\udd25 | \ud83d\ude80 Released in [\u267e\ufe0fRD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |\n| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |\n| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |\n| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|\n| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |\n| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | \ud83d\udcd6 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | \n| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |\n| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |\n| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |\n| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | \n| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | \n| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |\n| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |\n| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |\n| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |\n| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |\n| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |\n| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |\n| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |\n| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |\n| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | \n| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |\n| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | \n| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |\n| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |\n\nFeatures released before 2021 are not listed here.\n\n<p align=\"center\">\n  <img src=\"docs/_static/img/logo/1.png\" />\n</p>\n\nQlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.\n\nAn increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.\n\nIt contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. \nFor more details, please refer to our paper [\"Qlib: An AI-oriented Quantitative Investment Platform\"](https://arxiv.org/abs/2009.11189).\n\n\n<table>\n  <tbody>\n    <tr>\n      <th>Frameworks, Tutorial, Data & DevOps</th>\n      <th>Main Challenges & Solutions in Quant Research</th>\n    </tr>\n    <tr>\n      <td>\n        <li><a href=\"#plans\"><strong>Plans</strong></a></li>\n        <li><a href=\"#framework-of-qlib\">Framework of Qlib</a></li>\n        <li><a href=\"#quick-start\">Quick Start</a></li>\n          <ul dir=\"auto\">\n            <li type=\"circle\"><a href=\"#installation\">Installation</a> </li>\n            <li type=\"circle\"><a href=\"#data-preparation\">Data Preparation</a></li>\n            <li type=\"circle\"><a href=\"#auto-quant-research-workflow\">Auto Quant Research Workflow</a></li>\n            <li type=\"circle\"><a href=\"#building-customized-quant-research-workflow-by-code\">Building Customized Quant Research Workflow by Code</a></li></ul>\n        <li><a href=\"#quant-dataset-zoo\"><strong>Quant Dataset Zoo</strong></a></li>\n        <li><a href=\"#learning-framework\">Learning Framework</a></li>\n        <li><a href=\"#more-about-qlib\">More About Qlib</a></li>\n        <li><a href=\"#offline-mode-and-online-mode\">Offline Mode and Online Mode</a>\n        <ul>\n          <li type=\"circle\"><a href=\"#performance-of-qlib-data-server\">Performance of Qlib Data Server</a></li></ul>\n        <li><a href=\"#related-reports\">Related Reports</a></li>\n        <li><a href=\"#contact-us\">Contact Us</a></li>\n        <li><a href=\"#contributing\">Contributing</a></li>\n      </td>\n      <td valign=\"baseline\">", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |\n| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | \n| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |\n| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |\n\nFeatures released before 2021 are not listed here.\n\n<p align=\"center\">\n  <img src=\"docs/_static/img/logo/1.png\" />\n</p>\n\nQlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.\n\nAn increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.\n\nIt contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. \nFor more details, please refer to our paper [\"Qlib: An AI-oriented Quantitative Investment Platform\"](https://arxiv.org/abs/2009.11189).\n\n\n<table>\n  <tbody>\n    <tr>\n      <th>Frameworks, Tutorial, Data & DevOps</th>\n      <th>Main Challenges & Solutions in Quant Research</th>\n    </tr>\n    <tr>\n      <td>\n        <li><a href=\"#plans\"><strong>Plans</strong></a></li>\n        <li><a href=\"#framework-of-qlib\">Framework of Qlib</a></li>\n        <li><a href=\"#quick-start\">Quick Start</a></li>\n          <ul dir=\"auto\">\n            <li type=\"circle\"><a href=\"#installation\">Installation</a> </li>\n            <li type=\"circle\"><a href=\"#data-preparation\">Data Preparation</a></li>\n            <li type=\"circle\"><a href=\"#auto-quant-research-workflow\">Auto Quant Research Workflow</a></li>\n            <li type=\"circle\"><a href=\"#building-customized-quant-research-workflow-by-code\">Building Customized Quant Research Workflow by Code</a></li></ul>\n        <li><a href=\"#quant-dataset-zoo\"><strong>Quant Dataset Zoo</strong></a></li>\n        <li><a href=\"#learning-framework\">Learning Framework</a></li>\n        <li><a href=\"#more-about-qlib\">More About Qlib</a></li>\n        <li><a href=\"#offline-mode-and-online-mode\">Offline Mode and Online Mode</a>\n        <ul>\n          <li type=\"circle\"><a href=\"#performance-of-qlib-data-server\">Performance of Qlib Data Server</a></li></ul>\n        <li><a href=\"#related-reports\">Related Reports</a></li>\n        <li><a href=\"#contact-us\">Contact Us</a></li>\n        <li><a href=\"#contributing\">Contributing</a></li>\n      </td>\n      <td valign=\"baseline\">"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=\"#main-challenges--solutions-in-quant-research\">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=\"circle\"><a href=\"#forecasting-finding-valuable-signalspatterns\">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=\"disc\"><a href=\"#quant-model-paper-zoo\"><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=\"circle\"><a href=\"#run-a-single-model\">Run a Single Model</a></li>\n                    <li type=\"circle\"><a href=\"#run-multiple-models\">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=\"circle\"><a href=\"#adapting-to-market-dynamics\">Adapting to Market Dynamics</a></li>\n          <li type=\"circle\"><a href=\"#reinforcement-learning-modeling-continuous-decisions\">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=\"align: center\">\n<img src=\"docs/_static/img/framework-abstract.jpg\" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib's design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n<li><a href=\"#main-challenges--solutions-in-quant-research\">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type=\"circle\"><a href=\"#forecasting-finding-valuable-signalspatterns\">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type=\"disc\"><a href=\"#quant-model-paper-zoo\"><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type=\"circle\"><a href=\"#run-a-single-model\">Run a Single Model</a></li>\n                    <li type=\"circle\"><a href=\"#run-multiple-models\">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type=\"circle\"><a href=\"#adapting-to-market-dynamics\">Adapting to Market Dynamics</a></li>\n          <li type=\"circle\"><a href=\"#reinforcement-learning-modeling-continuous-decisions\">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style=\"align: center\">\n<img src=\"docs/_static/img/framework-abstract.jpg\" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib's design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Quick Start\n\nThis quick start guide tries to demonstrate\n1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.\n2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.\n\nHere is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).\n\n\n## Installation\n\nThis table demonstrates the supported Python version of `Qlib`:\n|               | install with pip      | install from source  |        plot        |\n| ------------- |:---------------------:|:--------------------:|:------------------:|\n| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n\n**Note**: \n1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.\n2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`'s Python to install ``Qlib`` from source.\n\n### Install with pip\nUsers can easily install ``Qlib`` by pip according to the following command.\n\n```bash\n  pip install pyqlib\n```\n\n**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.\n\n### Install from source\nAlso, users can install the latest dev version ``Qlib`` by the source code according to the following steps:\n\n* Before installing ``Qlib`` from source, users need to install some dependencies:\n\n  ```bash\n  pip install numpy\n  pip install --upgrade cython\n  ```\n\n* Clone the repository and install ``Qlib`` as follows.\n    ```bash\n    git clone https://github.com/microsoft/qlib.git && cd qlib\n    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst\n    ```\n\n**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.\n\n**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Quick Start\n\nThis quick start guide tries to demonstrate\n1. It's very easy to build a complete Quant research workflow and try your ideas with _Qlib_.\n2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.\n\nHere is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).\n\n\n## Installation\n\nThis table demonstrates the supported Python version of `Qlib`:\n|               | install with pip      | install from source  |        plot        |\n| ------------- |:---------------------:|:--------------------:|:------------------:|\n| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n\n**Note**: \n1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.\n2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`'s Python to install ``Qlib`` from source.\n\n### Install with pip\nUsers can easily install ``Qlib`` by pip according to the following command.\n\n```bash\n  pip install pyqlib\n```\n\n**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.\n\n### Install from source\nAlso, users can install the latest dev version ``Qlib`` by the source code according to the following steps:\n\n* Before installing ``Qlib`` from source, users need to install some dependencies:\n\n  ```bash\n  pip install numpy\n  pip install --upgrade cython\n  ```\n\n* Clone the repository and install ``Qlib`` as follows.\n    ```bash\n    git clone https://github.com/microsoft/qlib.git && cd qlib\n    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst\n    ```\n\n**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.\n\n**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n\u2757 Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the \"qlib\" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Data Preparation\n\u2757 Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.run.get_data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the \"qlib\" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = \"~/.qlib/qlib_data/cn_data\"  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments('csi500')\n  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = ['SH600000']\n  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']\n  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = \"~/.qlib/qlib_data/cn_data\"  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time='2010-01-01', end_time='2017-12-31', freq='day')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments('csi500')\n  print(D.list_instruments(instruments=instruments, start_time='2010-01-01', end_time='2017-12-31', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = ['SH600000']\n  fields = ['$close', '$volume', 'Ref($close, 1)', 'Mean($close, 3)', '$high-$low']\n  print(D.features(instruments, fields, start_time='2010-01-01', end_time='2017-12-31', freq='day').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app qlib_image_stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\nIf users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/workflow/cli.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    'The following are analysis results of the excess return without cost.'\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    'The following are analysis results of the excess return with cost.'\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Building Customized Quant Research Workflow by Code\nThe automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.\n\n# Main Challenges & Solutions in Quant Research\nQuant investment is a very unique scenario with lots of key challenges to be solved.\nCurrently, Qlib provides some solutions for several of them.\n\n## Forecasting: Finding Valuable Signals/Patterns\nAccurate forecasting of the stock price trend is a very important part to construct profitable portfolios.\nHowever, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.\n\nAn increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`\n\n\n### [Quant Model (Paper) Zoo](examples/benchmarks)\n\nHere is a list of models built on `Qlib`.\n- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)\n- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)\n- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)\n- [MLP based on pytorch](examples/benchmarks/MLP/)\n- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)\n- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)\n- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)\n- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)\n- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)\n- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)\n- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)\n- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)\n- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)\n- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)\n- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)\n- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)\n- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)\n- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)\n- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)\n- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)\n- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)\n- [KRNN based on pytorch](examples/benchmarks/KRNN/)\n- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)\n\nYour PR of new Quant models is highly welcomed.\n\nThe performance of each model on the `Alpha158` and `Alpha360` datasets can be found [here](examples/benchmarks/README.md).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## Building Customized Quant Research Workflow by Code\nThe automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.\n\n# Main Challenges & Solutions in Quant Research\nQuant investment is a very unique scenario with lots of key challenges to be solved.\nCurrently, Qlib provides some solutions for several of them.\n\n## Forecasting: Finding Valuable Signals/Patterns\nAccurate forecasting of the stock price trend is a very important part to construct profitable portfolios.\nHowever, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.\n\nAn increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`\n\n\n### [Quant Model (Paper) Zoo](examples/benchmarks)\n\nHere is a list of models built on `Qlib`.\n- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)\n- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)\n- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)\n- [MLP based on pytorch](examples/benchmarks/MLP/)\n- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)\n- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)\n- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)\n- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)\n- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)\n- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)\n- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)\n- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)\n- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)\n- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)\n- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)\n- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)\n- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)\n- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)\n- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)\n- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)\n- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)\n- [KRNN based on pytorch](examples/benchmarks/KRNN/)\n- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)\n\nYour PR of new Quant models is highly welcomed.\n\nThe performance of each model on the `Alpha158` and `Alpha360` datasets can be found [here](examples/benchmarks/README.md)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model's workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file's [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn't support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file's [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn't guarantee that some programmes will run correctly, including:\n* qlib\\examples\\rl_order_execution\\scripts\\gen_training_orders.py\n* qlib\\examples\\benchmarks\\TRA\\src\\dataset.MTSDatasetH.py\n* qlib\\examples\\benchmarks\\TFT\\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: \"An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization\", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: \"Universal Trading for Order Execution with Oracle Policy Distillation\", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  \u221a        |  \u221a           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  \u221a        |  \u221a           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib's RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It's worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: \"An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization\", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: \"Universal Trading for Order Execution with Oracle Policy Distillation\", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  \u221a        |  \u221a           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  \u221a        |  \u221a           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib's RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It's worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Offline Mode and Online Mode\nThe data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.\n\nUnder `Offline` mode, the data will be deployed locally. \n\nUnder `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).\n\n## Performance of Qlib Data Server\nThe performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we\ncompare it with several other data storage solutions. \n\nWe evaluate the performance of several storage solutions by finishing the same task,\nwhich creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.\n\n|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |\n| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |\n| Total (1CPU) (seconds)  | 184.4\u00b13.7 | 365.3\u00b17.5 | 253.6\u00b16.7 | 368.2\u00b13.6 | 147.0\u00b18.8   | 47.6\u00b11.0     | **7.4\u00b10.3** |\n| Total (64CPU) (seconds) |           |           |           |           | 8.8\u00b10.6     | **4.2\u00b10.2**  |             |\n* `+(-)E` indicates with (out) `ExpressionCache`\n* `+(-)D` indicates with (out) `DatasetCache`\n\nMost general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.\nSuch overheads greatly slow down the data loading process.\nQlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.\n\n# Related Reports\n- [Guide To Qlib: Microsoft\u2019s AI Investment Platform](https://analyticsindiamag.com/qlib/)\n- [\u5fae\u8f6f\u4e5f\u641eAI\u91cf\u5316\u5e73\u53f0\uff1f\u8fd8\u662f\u5f00\u6e90\u7684\uff01](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)\n- [\u5fae\u77ffQlib\uff1a\u4e1a\u5185\u9996\u4e2aAI\u91cf\u5316\u6295\u8d44\u5f00\u6e90\u5e73\u53f0](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)\n\n# Contact Us\n- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).\n- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). \n- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).\n  - We are recruiting new members(both FTEs and interns), your resumes are welcome!\n\nJoin IM discussion groups:\n|[Gitter](https://gitter.im/Microsoft/qlib)|\n|----|\n|![image](https://github.com/microsoft/qlib/blob/main/docs/_static/img/qrcode/gitter_qr.png)|", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Offline Mode and Online Mode\nThe data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.\n\nUnder `Offline` mode, the data will be deployed locally. \n\nUnder `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).\n\n## Performance of Qlib Data Server\nThe performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we\ncompare it with several other data storage solutions. \n\nWe evaluate the performance of several storage solutions by finishing the same task,\nwhich creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.\n\n|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |\n| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |\n| Total (1CPU) (seconds)  | 184.4\u00b13.7 | 365.3\u00b17.5 | 253.6\u00b16.7 | 368.2\u00b13.6 | 147.0\u00b18.8   | 47.6\u00b11.0     | **7.4\u00b10.3** |\n| Total (64CPU) (seconds) |           |           |           |           | 8.8\u00b10.6     | **4.2\u00b10.2**  |             |\n* `+(-)E` indicates with (out) `ExpressionCache`\n* `+(-)D` indicates with (out) `DatasetCache`\n\nMost general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.\nSuch overheads greatly slow down the data loading process.\nQlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.\n\n# Related Reports\n- [Guide To Qlib: Microsoft\u2019s AI Investment Platform](https://analyticsindiamag.com/qlib/)\n- [\u5fae\u8f6f\u4e5f\u641eAI\u91cf\u5316\u5e73\u53f0\uff1f\u8fd8\u662f\u5f00\u6e90\u7684\uff01](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)\n- [\u5fae\u77ffQlib\uff1a\u4e1a\u5185\u9996\u4e2aAI\u91cf\u5316\u6295\u8d44\u5f00\u6e90\u5e73\u53f0](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)\n\n# Contact Us\n- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).\n- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). \n- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).\n  - We are recruiting new members(both FTEs and interns), your resumes are welcome!\n\nJoin IM discussion groups:\n|[Gitter](https://gitter.im/Microsoft/qlib)|\n|----|\n|![image](https://github.com/microsoft/qlib/blob/main/docs/_static/img/qrcode/gitter_qr.png)|"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href=\"https://github.com/microsoft/qlib/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=microsoft/qlib\" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.\n<p align=\"center\">\n  <img src=\"https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif\" />\n</p>\n\nIf you don't know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg 'TODO|FIXME' qlib`\n \nIf you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href=\"https://github.com/microsoft/qlib/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=microsoft/qlib\" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib's document/code, you can follow the steps in the figure below.\n<p align=\"center\">\n  <img src=\"https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif\" />\n</p>\n\nIf you don't know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg 'TODO|FIXME' qlib`\n \nIf you would like to become one of Qlib's maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## License\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe right to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//README.md\n## License\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe right to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main//SECURITY.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main//SECURITY.md\n<!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main//SECURITY.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main//SECURITY.md\n<!-- BEGIN MICROSOFT SECURITY.MD V0.0.5 BLOCK -->\n\n## Security\n\nMicrosoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).\n\nIf you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://docs.microsoft.com/en-us/previous-versions/tn-archive/cc751383(v=technet.10)), please report it to us as described below.\n\n## Reporting Security Issues\n\n**Please do not report security vulnerabilities through public GitHub issues.**\n\nInstead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://msrc.microsoft.com/create-report).\n\nIf you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://www.microsoft.com/en-us/msrc/pgp-key-msrc).\n\nYou should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). \n\nPlease include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:\n\n  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)\n  * Full paths of source file(s) related to the manifestation of the issue\n  * The location of the affected source code (tag/branch/commit or direct URL)\n  * Any special configuration required to reproduce the issue\n  * Step-by-step instructions to reproduce the issue\n  * Proof-of-concept or exploit code (if possible)\n  * Impact of the issue, including how an attacker might exploit the issue\n\nThis information will help us triage your report more quickly.\n\nIf you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://microsoft.com/msrc/bounty) page for more details about our active programs.\n\n## Preferred Languages\n\nWe prefer all communications to be in English.\n\n## Policy\n\nMicrosoft follows the principle of [Coordinated Vulnerability Disclosure](https://www.microsoft.com/en-us/msrc/cvd).\n\n<!-- END MICROSOFT SECURITY.MD BLOCK -->"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/README.md\n# Requirements\n\nHere is the minimal hardware requirements to run the `workflow_by_code` example.\n- Memory: 16G\n- Free Disk: 5G\n\n\n# NOTE\nThe results will slightly vary on different OSs(the variance of annualized return will be less than 2%).\nThe evaluation results in the `README.md` page are from Linux OS.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/README.md\n# Requirements\n\nHere is the minimal hardware requirements to run the `workflow_by_code` example.\n- Memory: 16G\n- Free Disk: 5G\n\n\n# NOTE\nThe results will slightly vary on different OSs(the variance of annualized return will be less than 2%).\nThe evaluation results in the `README.md` page are from Linux OS."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/highfreq/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/highfreq/README.md\n# Introduction\nThis folder contains 2 examples\n- A high-frequency dataset example\n- An example of predicting the price trend in high-frequency data\n\n## High-Frequency Dataset\n\nThis dataset is an example for RL high frequency trading.\n\n### Get High-Frequency Data\n\nGet high-frequency data by running the following command:\n```bash\n    python workflow.py get_data\n```\n\n### Dump & Reload & Reinitialize the Dataset\n\n\nThe High-Frequency Dataset is implemented as `qlib.data.dataset.DatasetH` in the `workflow.py`. `DatatsetH` is the subclass of [`qlib.utils.serial.Serializable`](https://qlib.readthedocs.io/en/latest/advanced/serial.html), whose state can be dumped in or loaded from disk in `pickle` format.\n\n### About Reinitialization\n\nAfter reloading `Dataset` from disk, `Qlib` also support reinitializing the dataset. It means that users can reset some states of `Dataset` or `DataHandler` such as `instruments`, `start_time`, `end_time` and `segments`, etc.,  and generate new data according to the states.\n\nThe example is given in `workflow.py`, users can run the code as follows.\n\n### Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py dump_and_load_dataset\n```\n\n## Benchmarks Performance (predicting the price trend in high-frequency data)\n\nHere are the results of models for predicting the price trend in high-frequency data. We will keep updating benchmark models in future.\n\n| Model Name | Dataset | IC | ICIR | Rank IC | Rank ICIR | Long precision| Short Precision | Long-Short Average Return | Long-Short Average Sharpe |\n|---|---|---|---|---|---|---|---|---|---|\n| LightGBM | Alpha158 | 0.0349\u00b10.00 | 0.3805\u00b10.00| 0.0435\u00b10.00 | 0.4724\u00b10.00 | 0.5111\u00b10.00 | 0.5428\u00b10.00 | 0.000074\u00b10.00 | 0.2677\u00b10.00 |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/highfreq/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/highfreq/README.md\n# Introduction\nThis folder contains 2 examples\n- A high-frequency dataset example\n- An example of predicting the price trend in high-frequency data\n\n## High-Frequency Dataset\n\nThis dataset is an example for RL high frequency trading.\n\n### Get High-Frequency Data\n\nGet high-frequency data by running the following command:\n```bash\n    python workflow.py get_data\n```\n\n### Dump & Reload & Reinitialize the Dataset\n\n\nThe High-Frequency Dataset is implemented as `qlib.data.dataset.DatasetH` in the `workflow.py`. `DatatsetH` is the subclass of [`qlib.utils.serial.Serializable`](https://qlib.readthedocs.io/en/latest/advanced/serial.html), whose state can be dumped in or loaded from disk in `pickle` format.\n\n### About Reinitialization\n\nAfter reloading `Dataset` from disk, `Qlib` also support reinitializing the dataset. It means that users can reset some states of `Dataset` or `DataHandler` such as `instruments`, `start_time`, `end_time` and `segments`, etc.,  and generate new data according to the states.\n\nThe example is given in `workflow.py`, users can run the code as follows.\n\n### Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py dump_and_load_dataset\n```\n\n## Benchmarks Performance (predicting the price trend in high-frequency data)\n\nHere are the results of models for predicting the price trend in high-frequency data. We will keep updating benchmark models in future.\n\n| Model Name | Dataset | IC | ICIR | Rank IC | Rank ICIR | Long precision| Short Precision | Long-Short Average Return | Long-Short Average Sharpe |\n|---|---|---|---|---|---|---|---|---|---|\n| LightGBM | Alpha158 | 0.0349\u00b10.00 | 0.3805\u00b10.00| 0.0435\u00b10.00 | 0.4724\u00b10.00 | 0.5111\u00b10.00 | 0.5428\u00b10.00 | 0.000074\u00b10.00 | 0.2677\u00b10.00 |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/hyperparameter/LightGBM/Readme.md\n# LightGBM hyperparameter\n\n## Alpha158\nFirst terminal\n```\noptuna create-study --study LGBM_158 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_158.py\n```\n\n## Alpha360\nFirst terminal\n```\noptuna create-study --study LGBM_360 --storage sqlite:///db.sqlite3\noptuna-dashboard --port 5000 --host 0.0.0.0 sqlite:///db.sqlite3\n```\nSecond terminal\n```\npython hyperparameter_360.py\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/README.md\n# Nested Decision Execution\n\nThis workflow is an example for nested decision execution in backtesting. Qlib supports nested decision execution in backtesting. It means that users can use different strategies to make trade decision in different frequencies.\n\n## Weekly Portfolio Generation and Daily Order Execution\n\nThis workflow provides an example that uses a DropoutTopkStrategy (a strategy based on the daily frequency Lightgbm model) in weekly frequency for portfolio generation and uses SBBStrategyEMA (a rule-based strategy that uses EMA for decision-making) to execute orders in daily frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest\n```\n\nStart collecting data by running the following command:\n```bash\n    python workflow.py collect_data\n```\n\n## Daily Portfolio Generation and Minutely Order Execution\n\nThis workflow also provides a high-frequency example that uses a DropoutTopkStrategy for portfolio generation in daily frequency and uses SBBStrategyEMA to execute orders in minutely frequency. \n\n### Usage\n\nStart backtesting by running the following command:\n```bash\n    python workflow.py backtest_highfreq\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rolling_process_data/README.md\n# Rolling Process Data\n\nThis workflow is an example for `Rolling Process Data`.\n\n## Background\n\nWhen rolling train the models, data also needs to be generated in the different rolling windows. When the rolling window moves, the training data will change, and the processor's learnable state (such as standard deviation, mean, etc.) will also change. \n\nIn order to avoid regenerating data, this example uses the `DataHandler-based DataLoader` to load the raw features that are not related to the rolling window, and then used Processors to generate processed-features related to the rolling window.\n\n\n## Run the Code\n\nRun the example by running the following command:\n```bash\n    python workflow.py rolling_process\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/orderbook_data/README.md\n# Introduction\n\nThis example tries to demonstrate how Qlib supports data without fixed shared frequency.\n\nFor example,\n- Daily prices volume data are fixed-frequency data. The data comes in a fixed frequency (i.e. daily)\n- Orders are not fixed data and they may come at any time point\n\nTo support such non-fixed-frequency, Qlib implements an Arctic-based backend.\nHere is an example to import and query data based on this backend.\n\n# Installation\n\nPlease refer to [the installation docs](https://docs.mongodb.com/manual/installation/) of mongodb.\nCurrent version of script with default value tries to connect localhost **via default port without authentication**.\n\nRun following command to install necessary libraries\n```\npip install pytest coverage gdown\npip install arctic  # NOTE: pip may fail to resolve the right package dependency !!! Please make sure the dependency are satisfied.\n```\n\n# Importing example data\n\n\n1. (Optional) Please follow the first part of [this section](https://github.com/microsoft/qlib#data-preparation) to **get 1min data** of Qlib.\n2. Please follow following steps to download example data\n```bash\ncd examples/orderbook_data/\ngdown https://drive.google.com/uc?id=15nZF7tFT_eKVZAcMFL1qPS4jGyJflH7e  # Proxies may be necessary here.\npython ../../scripts/get_data.py _unzip --file_path highfreq_orderbook_example_data.zip --target_dir .\n```\n\n3. Please import the example data to your mongo db\n```bash\npython create_dataset.py initialize_library  # Initialization Libraries\npython create_dataset.py import_data  # Initialization Libraries\n```\n\n# Query Examples\n\nAfter importing these data, you run `example.py` to create some high-frequency features.\n```bash\npytest -s --disable-warnings example.py   # If you want run all examples\npytest -s --disable-warnings example.py::TestClass::test_exp_10  # If you want to run specific example\n```\n\n\n# Known limitations\nExpression computing between different frequencies are not supported yet"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 orders\n\u2514\u2500\u2500 pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper \"[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)\".\n- **OPDS**: Method proposed by AAAI 2021 paper \"[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)\".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n# RL Example for Order Execution\n\nThis folder comprises an example of Reinforcement Learning (RL) workflows for order execution scenario, including both training workflows and backtest workflows.\n\n## Data Processing\n\n### Get Data\n\n```\npython -m qlib.run.get_data qlib_data qlib_data --target_dir ./data/bin --region hs300 --interval 5min\n```\n\n### Generate Pickle-Style Data\n\nTo run codes in this example, we need data in pickle format. To achieve this, run following commands (might need a few minutes to finish):\n\n[//]: # (TODO: Instead of dumping dataframe with different format &#40;like `_gen_dataset` and `_gen_day_dataset` in `qlib/contrib/data/highfreq_provider.py`&#41;, we encourage to implement different subclass of `Dataset` and `DataHandler`. This will keep the workflow cleaner and interfaces more consistent, and move all the complexity to the subclass.)\n\n```\npython scripts/gen_pickle_data.py -c scripts/pickle_data_config.yml\npython scripts/gen_training_orders.py\npython scripts/merge_orders.py\n```\n\nWhen finished, the structure under `data/` should be:\n\n```\ndata\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 orders\n\u2514\u2500\u2500 pickle\n```\n\n## Training\n\nEach training task is specified by a config file. The config file for task `TASKNAME` is `exp_configs/train_TASKNAME.yml`. This example provides two training tasks:\n\n- **PPO**: Method proposed by IJCAL 2020 paper \"[An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization](https://www.ijcai.org/proceedings/2020/0627.pdf)\".\n- **OPDS**: Method proposed by AAAI 2021 paper \"[Universal Trading for Order Execution with Oracle Policy Distillation](https://arxiv.org/abs/2103.10860)\".\n\nThe main differece between these two methods is their reward functions. Please see their config files for details.\n\nTake OPDS as an example, to run the training workflow, run:\n\n```\npython -m qlib.rl.contrib.train_onpolicy --config_path exp_configs/train_opds.yml --run_backtest\n```\n\nMetrics, logs, and checkpoints will be stored under `outputs/opds` (configured by `exp_configs/train_opds.yml`). \n\n## Backtest\n\nOnce the training workflow has completed, the trained model can be used for the backtesting workflow. Still taking OPDS as an example, once training is finished, the latest checkpoint of the model can be found at `outputs/opds/checkpoints/latest.pth`. To run backtest workflow:\n\n1. Uncomment the `weight_file` parameter in `exp_configs/train_opds.yml` (it is commented by default). While it is possible to run the backtesting workflow without setting a checkpoint, this will lead to randomly initialized model results, thus making them meaningless.\n2. Run `python -m qlib.rl.contrib.backtest --config_path exp_configs/backtest_opds.yml`.\n\nThe backtest result is stored in `outputs/checkpoints/backtest_result.csv`.\n\nIn addition to OPDS and PPO, we also provide TWAP ([Time-weighted average price](https://en.wikipedia.org/wiki/Time-weighted_average_price)) as a weak baseline. The config file for TWAP is `exp_configs/backtest_twap.yml`."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline's testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it's best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use \"Price Advantage (PA)\" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 \u00b1 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 \u00b1 0.5780      |\n| PPO                         | -1.0935 \u00b1 0.0922      |\n| TWAP                        |   \u2248 0.0 \u00b1 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last \"half hour\"). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/rl_order_execution/README.md\n### Gap between backtest and training pipeline's testing\n\nIt is worthy to notice that the results of the backtesting process may differ from the results of the testing process used during training.\nThis is because different simulators are used to simulate market conditions during training and backtesting.\nIn training pipeline, the simplified simulator called `SingleAssetOrderExecutionSimple` is used for efficiency reasons. \n`SingleAssetOrderExecutionSimple` makes no restriction to trading amounts. \nNo matter what the amount of the order is, it can be completely executed.\nHowever, during backtesting, a more realistic simulator called `SingleAssetOrderExecution` is used. \nIt takes into account practical constraints in more real-world scenarios (for example, the trading volume must be a multiple of the smallest trading unit).\nAs a result, the amount of an order that is actually executed during backtesting may differ from the amount expected to be executed.\n\nIf you would like to obtain results that are exactly the same as those obtained during testing in the training pipeline, you could run training pipeline with only backtest phrase.\nIn order to do this:\n- Modify the training config. Add the path of the checkpoint you want to use (see following for an example).\n- Run `python -m qlib.rl.contrib.train_onpolicy --config_path PATH/TO/CONFIG --run_backtest --no_training`\n\n```yaml\n...\npolicy:\n  class: PPO  # PPO, DQN\n  kwargs:\n    lr: 0.0001\n    weight_file: PATH/TO/CHECKPOINT\n  module_path: qlib.rl.order_execution.policy\n...\n```\n\n## Benchmarks (TBD)\n\nTo accurately evaluate the performance of models using Reinforcement Learning algorithms, it's best to run experiments multiple times and compute the average performance across all trials. However, given the time-consuming nature of model training, this is not always feasible. An alternative approach is to run each training task only once, selecting the 10 checkpoints with the highest validation performance to simulate multiple trials. In this example, we use \"Price Advantage (PA)\" as the metric for selecting these checkpoints. The average performance of these 10 checkpoints on the testing set is as follows:\n\n| **Model**                   | **PA mean with std.** |\n|-----------------------------|-----------------------|\n| OPDS (with PPO policy)      |  0.4785 \u00b1 0.7815      |\n| OPDS (with DQN policy)      | -0.0114 \u00b1 0.5780      |\n| PPO                         | -1.0935 \u00b1 0.0922      |\n| TWAP                        |   \u2248 0.0 \u00b1 0.0         |\n\nThe table above also includes TWAP as a rule-based baseline. The ideal PA of TWAP should be 0.0, however, in this example, the order execution is divided into two steps: first, the order is split equally among each half hour, and then each five minutes within each half hour. Since trading is forbidden during the last five minutes of the day, this approach may slightly differ from traditional TWAP over the course of a full day (as there are 5 minutes missing in the last \"half hour\"). Therefore, the PA of TWAP can be considered as a number that is close to 0.0. To verify this, you may run a TWAP backtest and check the results."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/README.md\n# Introduction\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nThe table below shows the performances of different solutions on different forecasting models.\n\n## Alpha158 Dataset\nHere is the [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\nrm -f qlib_bin.tar.gz\n```\n\n| Model Name       | Dataset | IC | ICIR | Rank IC | Rank ICIR | Annualized Return | Information Ratio | Max Drawdown |\n|------------------|---------|------|------|---------|-----------|-------------------|-------------------|--------------|\n| RR[Linear]       |Alpha158 |0.0945|0.5989|0.1069   |0.6495     |0.0857             |1.3682             |-0.0986       |\n| DDG-DA[Linear]   |Alpha158 |0.0983|0.6157|0.1108   |0.6646     |0.0764             |1.1904             |-0.0769       |\n| RR[LightGBM]     |Alpha158 |0.0816|0.5887|0.0912   |0.6263     |0.0771             |1.3196             |-0.0909       |\n| DDG-DA[LightGBM] |Alpha158 |0.0878|0.6185|0.0975   |0.6524     |0.1261             |2.0096             |-0.0744       |\n\n- The label horizon of the `Alpha158` dataset is set to 20.\n- The rolling time intervals are set to 20 trading days.\n- The test rolling periods are from January 2017 to August 2020.\n- The results are based on the crowd-sourced version. The Yahoo version of qlib data does not contain `VWAP`, so all related factors are missing and filled with 0, which leads to a rank-deficient matrix (a matrix does not have full rank) and makes lower-level optimization of DDG-DA can not be solved.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/README.md\n# Introduction\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies' performance.\n\nThe table below shows the performances of different solutions on different forecasting models.\n\n## Alpha158 Dataset\nHere is the [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\nrm -f qlib_bin.tar.gz\n```\n\n| Model Name       | Dataset | IC | ICIR | Rank IC | Rank ICIR | Annualized Return | Information Ratio | Max Drawdown |\n|------------------|---------|------|------|---------|-----------|-------------------|-------------------|--------------|\n| RR[Linear]       |Alpha158 |0.0945|0.5989|0.1069   |0.6495     |0.0857             |1.3682             |-0.0986       |\n| DDG-DA[Linear]   |Alpha158 |0.0983|0.6157|0.1108   |0.6646     |0.0764             |1.1904             |-0.0769       |\n| RR[LightGBM]     |Alpha158 |0.0816|0.5887|0.0912   |0.6263     |0.0771             |1.3196             |-0.0909       |\n| DDG-DA[LightGBM] |Alpha158 |0.0878|0.6185|0.0975   |0.6524     |0.1261             |2.0096             |-0.0744       |\n\n- The label horizon of the `Alpha158` dataset is set to 20.\n- The rolling time intervals are set to 20 trading days.\n- The test rolling periods are from January 2017 to August 2020.\n- The results are based on the crowd-sourced version. The Yahoo version of qlib data does not contain `VWAP`, so all related factors are missing and filled with 0, which leads to a rank-deficient matrix (a matrix does not have full rank) and makes lower-level optimization of DDG-DA can not be solved."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/DDG-DA/README.md\n# Introduction\nThis is the implementation of `DDG-DA` based on `Meta Controller` component provided by `Qlib`.\n\nPlease refer to the paper for more details: *DDG-DA: Data Distribution Generation for Predictable Concept Drift Adaptation* [[arXiv](https://arxiv.org/abs/2201.04038)]\n\n\n# Background\nIn many real-world scenarios, we often deal with streaming data that is sequentially collected over time. Due to the non-stationary nature of the environment, the streaming data distribution may change in unpredictable ways, which is known as concept drift. To handle concept drift, previous methods first detect when/where the concept drift happens and then adapt models to fit the distribution of the latest data. However, there are still many cases that some underlying factors of environment evolution are predictable, making it possible to model the future concept drift trend of the streaming data, while such cases are not fully explored in previous work.\n\nTherefore, we propose a novel method `DDG-DA`, that can effectively forecast the evolution of data distribution and improve the performance of models. Specifically, we first train a predictor to estimate the future data distribution, then leverage it to generate training samples, and finally train models on the generated data.\n\n# Dataset\nThe data in the paper are private. So we conduct experiments on Qlib's public dataset.\nThough the dataset is different, the conclusion remains the same. By applying `DDG-DA`, users can see rising trends at the test phase both in the proxy models' ICs and the performances of the forecasting models.\n\n# Run the Code\nUsers can try `DDG-DA` by running the following command:\n```bash\n    python workflow.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `forecast_model` parameter when `DDG-DA` initializes. For example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python workflow.py --conf_path=../workflow_config_lightgbm_Alpha158.yaml run\n```\n\n# Results\nThe results of related methods in Qlib's public dataset can be found [here](../)\n\n# Requirements\nHere are the minimal hardware requirements to run the ``workflow.py`` of DDG-DA.\n* Memory: 45G\n* Disk: 4G\n\nPytorch with CPU & RAM will be enough for this example."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/baseline/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/baseline/README.md\n# Introduction\n\nThis is the framework of periodically Rolling Retrain (RR) forecasting models. RR adapts to market dynamics by utilizing the up-to-date data periodically.\n\n## Run the Code\nUsers can try RR by running the following command:\n```bash\n    python rolling_benchmark.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `model_type` parameter.\nFor example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python rolling_benchmark.py --conf_path=workflow_config_lightgbm_Alpha158.yaml run\n\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/baseline/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks_dynamic/baseline/README.md\n# Introduction\n\nThis is the framework of periodically Rolling Retrain (RR) forecasting models. RR adapts to market dynamics by utilizing the up-to-date data periodically.\n\n## Run the Code\nUsers can try RR by running the following command:\n```bash\n    python rolling_benchmark.py run\n```\n\nThe default forecasting models are `Linear`. Users can choose other forecasting models by changing the `model_type` parameter.\nFor example, users can try `LightGBM` forecasting models by running the following command:\n```bash\n    python rolling_benchmark.py --conf_path=workflow_config_lightgbm_Alpha158.yaml run\n\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/portfolio/README.md\n# Portfolio Optimization Strategy\n\n## Introduction\n\nIn `qlib/examples/benchmarks` we have various **alpha** models that predict\nthe stock returns. We also use a simple rule based `TopkDropoutStrategy` to\nevaluate the investing performance of these models. However, such a strategy\nis too simple to control the portfolio risk like correlation and volatility.\n\nTo this end, an optimization based strategy should be used to for the\ntrade-off between return and risk. In this doc, we will show how to use\n`EnhancedIndexingStrategy` to maximize portfolio return while minimizing\ntracking error relative to a benchmark.\n\n\n## Preparation\n\nWe use China stock market data for our example.\n\n1. Prepare CSI300 weight:\n\n   ```bash\n   wget https://github.com/SunsetWolf/qlib_dataset/releases/download/v0/csi300_weight.zip\n   unzip -d ~/.qlib/qlib_data/cn_data csi300_weight.zip\n   rm -f csi300_weight.zip\n   ```\n   NOTE:  We don't find any public free resource to get the weight in the benchmark. To run the example, we manually create this weight data.\n\n2. Prepare risk model data:\n\n   ```bash\n   python prepare_riskdata.py\n   ```\n\nHere we use a **Statistical Risk Model** implemented in `qlib.model.riskmodel`.\nHowever users are strongly recommended to use other risk models for better quality:\n* **Fundamental Risk Model** like MSCI BARRA\n* [Deep Risk Model](https://arxiv.org/abs/2107.05201)\n\n\n## End-to-End Workflow\n\nYou can finish workflow with `EnhancedIndexingStrategy` by running\n`qrun config_enhanced_indexing.yaml`.\n\nIn this config, we mainly changed the strategy section compared to\n`qlib/examples/benchmarks/workflow_config_lightgbm_Alpha158.yaml`."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n# Benchmarks Performance\nThis page lists a batch of methods designed for alpha seeking. Each method tries to give scores/predictions for all stocks each day(e.g. forecasting the future excess return of stocks). The scores/predictions of the models will be used as the mined alpha. Investing in stocks with higher scores is expected to yield more profit.  \n\nThe alpha is evaluated in two ways.\n1. The correlation between the alpha and future return.\n1. Constructing portfolio based on the alpha and evaluating the final total return.\n   - The explanation of metrics can be found [here](https://qlib.readthedocs.io/en/latest/component/report.html#id4)\n\nHere are the results of each benchmark model running on Qlib's `Alpha360` and `Alpha158` dataset with China's A shared-stock & CSI300 data respectively. The values of each metric are the mean and std calculated based on 20 runs with different random seeds.\n\nThe numbers shown below demonstrate the performance of the entire `workflow` of each model. We will update the `workflow` as well as models in the near future for better results.\n<!-- \n> If you need to reproduce the results below, please use the **v1** dataset: `python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --version v1`\n>\n> In the new version of qlib, the default dataset is **v2**. Since the data is collected from the YahooFinance API (which is not very stable), the results of *v2* and *v1* may differ -->\n\n> NOTE:\n> The backtest start from 0.8.0 is quite different from previous version. Please check out the changelog for the difference.\n\n> NOTE:\n> We have very limited resources to implement and finetune the models. We tried our best effort to fairly compare these models.  But some models may have greater potential than what it looks like in the table below.  Your contribution is highly welcomed to explore their potential.\n\n## Results on CSI300\n\n### Alpha158 dataset\n\n| Model Name                               | Dataset                             | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------------------------------------|-------------------------------------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| TCN(Shaojie Bai, et al.)                 | Alpha158                            | 0.0279\u00b10.00 | 0.2181\u00b10.01 | 0.0421\u00b10.00 | 0.3429\u00b10.01 | 0.0262\u00b10.02       | 0.4133\u00b10.25       | -0.1090\u00b10.03 |\n| TabNet(Sercan O. Arik, et al.)           | Alpha158                            | 0.0204\u00b10.01 | 0.1554\u00b10.07 | 0.0333\u00b10.00 | 0.2552\u00b10.05 | 0.0227\u00b10.04       | 0.3676\u00b10.54       | -0.1089\u00b10.08 |\n| Transformer(Ashish Vaswani, et al.)      | Alpha158                            | 0.0264\u00b10.00 | 0.2053\u00b10.02 | 0.0407\u00b10.00 | 0.3273\u00b10.02 | 0.0273\u00b10.02       | 0.3970\u00b10.26       | -0.1101\u00b10.02 |\n| GRU(Kyunghyun Cho, et al.)               | Alpha158(with selected 20 features) | 0.0315\u00b10.00 | 0.2450\u00b10.04 | 0.0428\u00b10.00 | 0.3440\u00b10.03 | 0.0344\u00b10.02       | 0.5160\u00b10.25       | -0.1017\u00b10.02 |\n| LSTM(Sepp Hochreiter, et al.)            | Alpha158(with selected 20 features) | 0.0318\u00b10.00 | 0.2367\u00b10.04 | 0.0435\u00b10.00 | 0.3389\u00b10.03 | 0.0381\u00b10.03       | 0.5561\u00b10.46       | -0.1207\u00b10.04 |\n| Localformer(Juyong Jiang, et al.)        | Alpha158                            | 0.0356\u00b10.00 | 0.2756\u00b10.03 | 0.0468\u00b10.00 | 0.3784\u00b10.03 | 0.0438\u00b10.02       | 0.6600\u00b10.33       | -0.0952\u00b10.02 |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n# Benchmarks Performance\nThis page lists a batch of methods designed for alpha seeking. Each method tries to give scores/predictions for all stocks each day(e.g. forecasting the future excess return of stocks). The scores/predictions of the models will be used as the mined alpha. Investing in stocks with higher scores is expected to yield more profit.  \n\nThe alpha is evaluated in two ways.\n1. The correlation between the alpha and future return.\n1. Constructing portfolio based on the alpha and evaluating the final total return.\n   - The explanation of metrics can be found [here](https://qlib.readthedocs.io/en/latest/component/report.html#id4)\n\nHere are the results of each benchmark model running on Qlib's `Alpha360` and `Alpha158` dataset with China's A shared-stock & CSI300 data respectively. The values of each metric are the mean and std calculated based on 20 runs with different random seeds.\n\nThe numbers shown below demonstrate the performance of the entire `workflow` of each model. We will update the `workflow` as well as models in the near future for better results.\n<!-- \n> If you need to reproduce the results below, please use the **v1** dataset: `python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn --version v1`\n>\n> In the new version of qlib, the default dataset is **v2**. Since the data is collected from the YahooFinance API (which is not very stable), the results of *v2* and *v1* may differ -->\n\n> NOTE:\n> The backtest start from 0.8.0 is quite different from previous version. Please check out the changelog for the difference.\n\n> NOTE:\n> We have very limited resources to implement and finetune the models. We tried our best effort to fairly compare these models.  But some models may have greater potential than what it looks like in the table below.  Your contribution is highly welcomed to explore their potential.\n\n## Results on CSI300\n\n### Alpha158 dataset\n\n| Model Name                               | Dataset                             | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------------------------------------|-------------------------------------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| TCN(Shaojie Bai, et al.)                 | Alpha158                            | 0.0279\u00b10.00 | 0.2181\u00b10.01 | 0.0421\u00b10.00 | 0.3429\u00b10.01 | 0.0262\u00b10.02       | 0.4133\u00b10.25       | -0.1090\u00b10.03 |\n| TabNet(Sercan O. Arik, et al.)           | Alpha158                            | 0.0204\u00b10.01 | 0.1554\u00b10.07 | 0.0333\u00b10.00 | 0.2552\u00b10.05 | 0.0227\u00b10.04       | 0.3676\u00b10.54       | -0.1089\u00b10.08 |\n| Transformer(Ashish Vaswani, et al.)      | Alpha158                            | 0.0264\u00b10.00 | 0.2053\u00b10.02 | 0.0407\u00b10.00 | 0.3273\u00b10.02 | 0.0273\u00b10.02       | 0.3970\u00b10.26       | -0.1101\u00b10.02 |\n| GRU(Kyunghyun Cho, et al.)               | Alpha158(with selected 20 features) | 0.0315\u00b10.00 | 0.2450\u00b10.04 | 0.0428\u00b10.00 | 0.3440\u00b10.03 | 0.0344\u00b10.02       | 0.5160\u00b10.25       | -0.1017\u00b10.02 |\n| LSTM(Sepp Hochreiter, et al.)            | Alpha158(with selected 20 features) | 0.0318\u00b10.00 | 0.2367\u00b10.04 | 0.0435\u00b10.00 | 0.3389\u00b10.03 | 0.0381\u00b10.03       | 0.5561\u00b10.46       | -0.1207\u00b10.04 |\n| Localformer(Juyong Jiang, et al.)        | Alpha158                            | 0.0356\u00b10.00 | 0.2756\u00b10.03 | 0.0468\u00b10.00 | 0.3784\u00b10.03 | 0.0438\u00b10.02       | 0.6600\u00b10.33       | -0.0952\u00b10.02 |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379\u00b10.00 | 0.2959\u00b10.04 | 0.0464\u00b10.00 | 0.3825\u00b10.04 | 0.0465\u00b10.02       | 0.5672\u00b10.29       | -0.1282\u00b10.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362\u00b10.01 | 0.2789\u00b10.06 | 0.0463\u00b10.01 | 0.3661\u00b10.05 | 0.0470\u00b10.03       | 0.6992\u00b10.47       | -0.1072\u00b10.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349\u00b10.00 | 0.2511\u00b10.01 | 0.0462\u00b10.00 | 0.3564\u00b10.01 | 0.0497\u00b10.01       | 0.7338\u00b10.19       | -0.0777\u00b10.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404\u00b10.00 | 0.3197\u00b10.05 | 0.0490\u00b10.00 | 0.4047\u00b10.04 | 0.0649\u00b10.02       | 1.0091\u00b10.30       | -0.0860\u00b10.02 |\n| Linear                                   | Alpha158                            | 0.0397\u00b10.00 | 0.3000\u00b10.00 | 0.0472\u00b10.00 | 0.3531\u00b10.00 | 0.0692\u00b10.00       | 0.9209\u00b10.00       | -0.1509\u00b10.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440\u00b10.00 | 0.3535\u00b10.05 | 0.0540\u00b10.00 | 0.4451\u00b10.03 | 0.0718\u00b10.02       | 1.0835\u00b10.35       | -0.0760\u00b10.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481\u00b10.00 | 0.3366\u00b10.00 | 0.0454\u00b10.00 | 0.3311\u00b10.00 | 0.0765\u00b10.00       | 0.8032\u00b10.01       | -0.1092\u00b10.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498\u00b10.00 | 0.3779\u00b10.00 | 0.0505\u00b10.00 | 0.4131\u00b10.00 | 0.0780\u00b10.00       | 0.9070\u00b10.00       | -0.1168\u00b10.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358\u00b10.00 | 0.2160\u00b10.03 | 0.0116\u00b10.01 | 0.0720\u00b10.03 | 0.0847\u00b10.02       | 0.8131\u00b10.19       | -0.1824\u00b10.03 |\n| MLP                                      | Alpha158                            | 0.0376\u00b10.00 | 0.2846\u00b10.02 | 0.0429\u00b10.00 | 0.3220\u00b10.01 | 0.0895\u00b10.02       | 1.1408\u00b10.23       | -0.1103\u00b10.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448\u00b10.00 | 0.3660\u00b10.00 | 0.0469\u00b10.00 | 0.3877\u00b10.00 | 0.0901\u00b10.00       | 1.0164\u00b10.00       | -0.1038\u00b10.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521\u00b10.00 | 0.4223\u00b10.01 | 0.0502\u00b10.00 | 0.4117\u00b10.01 | 0.1158\u00b10.01       | 1.3432\u00b10.11       | -0.0920\u00b10.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114\u00b10.00 | 0.0716\u00b10.03 | 0.0327\u00b10.00 | 0.2248\u00b10.02 | -0.0270\u00b10.03      | -0.3378\u00b10.37      | -0.1653\u00b10.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099\u00b10.00 | 0.0593\u00b10.00 | 0.0290\u00b10.00 | 0.1887\u00b10.00 | -0.0369\u00b10.00      | -0.3892\u00b10.00      | -0.2145\u00b10.00 |\n| MLP                                       | Alpha360 | 0.0273\u00b10.00 | 0.1870\u00b10.02 | 0.0396\u00b10.00 | 0.2910\u00b10.02 | 0.0029\u00b10.02       | 0.0274\u00b10.23       | -0.1385\u00b10.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404\u00b10.00 | 0.2932\u00b10.04 | 0.0542\u00b10.00 | 0.4110\u00b10.03 | 0.0246\u00b10.02       | 0.3211\u00b10.21       | -0.1095\u00b10.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378\u00b10.00 | 0.2714\u00b10.00 | 0.0467\u00b10.00 | 0.3659\u00b10.00 | 0.0292\u00b10.00       | 0.3781\u00b10.00       | -0.0862\u00b10.00 |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| SFM(Liheng Zhang, et al.)                | Alpha158                            | 0.0379\u00b10.00 | 0.2959\u00b10.04 | 0.0464\u00b10.00 | 0.3825\u00b10.04 | 0.0465\u00b10.02       | 0.5672\u00b10.29       | -0.1282\u00b10.03 |\n| ALSTM (Yao Qin, et al.)                  | Alpha158(with selected 20 features) | 0.0362\u00b10.01 | 0.2789\u00b10.06 | 0.0463\u00b10.01 | 0.3661\u00b10.05 | 0.0470\u00b10.03       | 0.6992\u00b10.47       | -0.1072\u00b10.03 |\n| GATs (Petar Velickovic, et al.)          | Alpha158(with selected 20 features) | 0.0349\u00b10.00 | 0.2511\u00b10.01 | 0.0462\u00b10.00 | 0.3564\u00b10.01 | 0.0497\u00b10.01       | 0.7338\u00b10.19       | -0.0777\u00b10.02 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158(with selected 20 features) | 0.0404\u00b10.00 | 0.3197\u00b10.05 | 0.0490\u00b10.00 | 0.4047\u00b10.04 | 0.0649\u00b10.02       | 1.0091\u00b10.30       | -0.0860\u00b10.02 |\n| Linear                                   | Alpha158                            | 0.0397\u00b10.00 | 0.3000\u00b10.00 | 0.0472\u00b10.00 | 0.3531\u00b10.00 | 0.0692\u00b10.00       | 0.9209\u00b10.00       | -0.1509\u00b10.00 |\n| TRA(Hengxu Lin, et al.)                  | Alpha158                            | 0.0440\u00b10.00 | 0.3535\u00b10.05 | 0.0540\u00b10.00 | 0.4451\u00b10.03 | 0.0718\u00b10.02       | 1.0835\u00b10.35       | -0.0760\u00b10.02 |\n| CatBoost(Liudmila Prokhorenkova, et al.) | Alpha158                            | 0.0481\u00b10.00 | 0.3366\u00b10.00 | 0.0454\u00b10.00 | 0.3311\u00b10.00 | 0.0765\u00b10.00       | 0.8032\u00b10.01       | -0.1092\u00b10.00 |\n| XGBoost(Tianqi Chen, et al.)             | Alpha158                            | 0.0498\u00b10.00 | 0.3779\u00b10.00 | 0.0505\u00b10.00 | 0.4131\u00b10.00 | 0.0780\u00b10.00       | 0.9070\u00b10.00       | -0.1168\u00b10.00 |\n| TFT (Bryan Lim, et al.)                  | Alpha158(with selected 20 features) | 0.0358\u00b10.00 | 0.2160\u00b10.03 | 0.0116\u00b10.01 | 0.0720\u00b10.03 | 0.0847\u00b10.02       | 0.8131\u00b10.19       | -0.1824\u00b10.03 |\n| MLP                                      | Alpha158                            | 0.0376\u00b10.00 | 0.2846\u00b10.02 | 0.0429\u00b10.00 | 0.3220\u00b10.01 | 0.0895\u00b10.02       | 1.1408\u00b10.23       | -0.1103\u00b10.02 |\n| LightGBM(Guolin Ke, et al.)              | Alpha158                            | 0.0448\u00b10.00 | 0.3660\u00b10.00 | 0.0469\u00b10.00 | 0.3877\u00b10.00 | 0.0901\u00b10.00       | 1.0164\u00b10.00       | -0.1038\u00b10.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)    | Alpha158                            | 0.0521\u00b10.00 | 0.4223\u00b10.01 | 0.0502\u00b10.00 | 0.4117\u00b10.01 | 0.1158\u00b10.01       | 1.3432\u00b10.11       | -0.0920\u00b10.01 |\n\n### Alpha360 dataset\n\n| Model Name                                | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|-------------------------------------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Transformer(Ashish Vaswani, et al.)       | Alpha360 | 0.0114\u00b10.00 | 0.0716\u00b10.03 | 0.0327\u00b10.00 | 0.2248\u00b10.02 | -0.0270\u00b10.03      | -0.3378\u00b10.37      | -0.1653\u00b10.05 |\n| TabNet(Sercan O. Arik, et al.)            | Alpha360 | 0.0099\u00b10.00 | 0.0593\u00b10.00 | 0.0290\u00b10.00 | 0.1887\u00b10.00 | -0.0369\u00b10.00      | -0.3892\u00b10.00      | -0.2145\u00b10.00 |\n| MLP                                       | Alpha360 | 0.0273\u00b10.00 | 0.1870\u00b10.02 | 0.0396\u00b10.00 | 0.2910\u00b10.02 | 0.0029\u00b10.02       | 0.0274\u00b10.23       | -0.1385\u00b10.03 |\n| Localformer(Juyong Jiang, et al.)         | Alpha360 | 0.0404\u00b10.00 | 0.2932\u00b10.04 | 0.0542\u00b10.00 | 0.4110\u00b10.03 | 0.0246\u00b10.02       | 0.3211\u00b10.21       | -0.1095\u00b10.02 |\n| CatBoost((Liudmila Prokhorenkova, et al.) | Alpha360 | 0.0378\u00b10.00 | 0.2714\u00b10.00 | 0.0467\u00b10.00 | 0.3659\u00b10.00 | 0.0292\u00b10.00       | 0.3781\u00b10.00       | -0.0862\u00b10.00 |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394\u00b10.00 | 0.2909\u00b10.00 | 0.0448\u00b10.00 | 0.3679\u00b10.00 | 0.0344\u00b10.00       | 0.4527\u00b10.02       | -0.1004\u00b10.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390\u00b10.00 | 0.2946\u00b10.01 | 0.0486\u00b10.00 | 0.3836\u00b10.01 | 0.0462\u00b10.01       | 0.6151\u00b10.18       | -0.0915\u00b10.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400\u00b10.00 | 0.3037\u00b10.00 | 0.0499\u00b10.00 | 0.4042\u00b10.00 | 0.0558\u00b10.00       | 0.7632\u00b10.00       | -0.0659\u00b10.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441\u00b10.00 | 0.3301\u00b10.02 | 0.0519\u00b10.00 | 0.4130\u00b10.01 | 0.0604\u00b10.02       | 0.8295\u00b10.34       | -0.1018\u00b10.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497\u00b10.00 | 0.3829\u00b10.04 | 0.0599\u00b10.00 | 0.4736\u00b10.03 | 0.0626\u00b10.02       | 0.8651\u00b10.31       | -0.0994\u00b10.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448\u00b10.00 | 0.3474\u00b10.04 | 0.0549\u00b10.00 | 0.4366\u00b10.03 | 0.0647\u00b10.03       | 0.8963\u00b10.39       | -0.0875\u00b10.02 |\n| ADD                                       | Alpha360 | 0.0430\u00b10.00 | 0.3188\u00b10.04 | 0.0559\u00b10.00 | 0.4301\u00b10.03 | 0.0667\u00b10.02       | 0.8992\u00b10.34       | -0.0855\u00b10.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493\u00b10.00 | 0.3772\u00b10.04 | 0.0584\u00b10.00 | 0.4638\u00b10.03 | 0.0720\u00b10.02       | 0.9730\u00b10.33       | -0.0821\u00b10.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464\u00b10.01 | 0.3619\u00b10.08 | 0.0539\u00b10.01 | 0.4287\u00b10.06 | 0.0753\u00b10.03       | 1.0200\u00b10.40       | -0.0936\u00b10.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476\u00b10.00 | 0.3508\u00b10.02 | 0.0598\u00b10.00 | 0.4604\u00b10.01 | 0.0824\u00b10.02       | 1.1079\u00b10.26       | -0.0894\u00b10.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508\u00b10.00 | 0.3931\u00b10.04 | 0.0599\u00b10.00 | 0.4756\u00b10.03 | 0.0893\u00b10.03       | 1.2256\u00b10.36       | -0.0857\u00b10.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485\u00b10.00 | 0.3787\u00b10.03 | 0.0587\u00b10.00 | 0.4756\u00b10.03 | 0.0920\u00b10.03       | 1.2789\u00b10.42       | -0.0834\u00b10.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480\u00b10.00 | 0.3589\u00b10.02 | 0.0606\u00b10.00 | 0.4773\u00b10.01 | 0.0946\u00b10.02       | 1.3509\u00b10.25       | -0.0716\u00b10.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522\u00b10.00 | 0.3530\u00b10.01 | 0.0667\u00b10.00 | 0.4576\u00b10.01 | 0.0987\u00b10.02       | 1.3726\u00b10.27       | -0.0681\u00b10.01 |\n| KRNN                                      | Alpha360 | 0.0173\u00b10.01 | 0.1210\u00b10.06 | 0.0270\u00b10.01 | 0.2018\u00b10.04 | -0.0465\u00b10.05      | -0.5415\u00b10.62      | -0.2919\u00b10.13 |\n| Sandwich                                  | Alpha360 | 0.0258\u00b10.00 | 0.1924\u00b10.04 | 0.0337\u00b10.00 | 0.2624\u00b10.03 | 0.0005\u00b10.03       | 0.0001\u00b10.33       | -0.1752\u00b10.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| XGBoost(Tianqi Chen, et al.)              | Alpha360 | 0.0394\u00b10.00 | 0.2909\u00b10.00 | 0.0448\u00b10.00 | 0.3679\u00b10.00 | 0.0344\u00b10.00       | 0.4527\u00b10.02       | -0.1004\u00b10.00 |\n| DoubleEnsemble(Chuheng Zhang, et al.)     | Alpha360 | 0.0390\u00b10.00 | 0.2946\u00b10.01 | 0.0486\u00b10.00 | 0.3836\u00b10.01 | 0.0462\u00b10.01       | 0.6151\u00b10.18       | -0.0915\u00b10.01 |\n| LightGBM(Guolin Ke, et al.)               | Alpha360 | 0.0400\u00b10.00 | 0.3037\u00b10.00 | 0.0499\u00b10.00 | 0.4042\u00b10.00 | 0.0558\u00b10.00       | 0.7632\u00b10.00       | -0.0659\u00b10.00 |\n| TCN(Shaojie Bai, et al.)                  | Alpha360 | 0.0441\u00b10.00 | 0.3301\u00b10.02 | 0.0519\u00b10.00 | 0.4130\u00b10.01 | 0.0604\u00b10.02       | 0.8295\u00b10.34       | -0.1018\u00b10.03 |\n| ALSTM (Yao Qin, et al.)                   | Alpha360 | 0.0497\u00b10.00 | 0.3829\u00b10.04 | 0.0599\u00b10.00 | 0.4736\u00b10.03 | 0.0626\u00b10.02       | 0.8651\u00b10.31       | -0.0994\u00b10.03 |\n| LSTM(Sepp Hochreiter, et al.)             | Alpha360 | 0.0448\u00b10.00 | 0.3474\u00b10.04 | 0.0549\u00b10.00 | 0.4366\u00b10.03 | 0.0647\u00b10.03       | 0.8963\u00b10.39       | -0.0875\u00b10.02 |\n| ADD                                       | Alpha360 | 0.0430\u00b10.00 | 0.3188\u00b10.04 | 0.0559\u00b10.00 | 0.4301\u00b10.03 | 0.0667\u00b10.02       | 0.8992\u00b10.34       | -0.0855\u00b10.02 |\n| GRU(Kyunghyun Cho, et al.)                | Alpha360 | 0.0493\u00b10.00 | 0.3772\u00b10.04 | 0.0584\u00b10.00 | 0.4638\u00b10.03 | 0.0720\u00b10.02       | 0.9730\u00b10.33       | -0.0821\u00b10.02 |\n| AdaRNN(Yuntao Du, et al.)                 | Alpha360 | 0.0464\u00b10.01 | 0.3619\u00b10.08 | 0.0539\u00b10.01 | 0.4287\u00b10.06 | 0.0753\u00b10.03       | 1.0200\u00b10.40       | -0.0936\u00b10.03 |\n| GATs (Petar Velickovic, et al.)           | Alpha360 | 0.0476\u00b10.00 | 0.3508\u00b10.02 | 0.0598\u00b10.00 | 0.4604\u00b10.01 | 0.0824\u00b10.02       | 1.1079\u00b10.26       | -0.0894\u00b10.03 |\n| TCTS(Xueqing Wu, et al.)                  | Alpha360 | 0.0508\u00b10.00 | 0.3931\u00b10.04 | 0.0599\u00b10.00 | 0.4756\u00b10.03 | 0.0893\u00b10.03       | 1.2256\u00b10.36       | -0.0857\u00b10.02 |\n| TRA(Hengxu Lin, et al.)                   | Alpha360 | 0.0485\u00b10.00 | 0.3787\u00b10.03 | 0.0587\u00b10.00 | 0.4756\u00b10.03 | 0.0920\u00b10.03       | 1.2789\u00b10.42       | -0.0834\u00b10.02 |\n| IGMTF(Wentao Xu, et al.)                  | Alpha360 | 0.0480\u00b10.00 | 0.3589\u00b10.02 | 0.0606\u00b10.00 | 0.4773\u00b10.01 | 0.0946\u00b10.02       | 1.3509\u00b10.25       | -0.0716\u00b10.02 |\n| HIST(Wentao Xu, et al.)                   | Alpha360 | 0.0522\u00b10.00 | 0.3530\u00b10.01 | 0.0667\u00b10.00 | 0.4576\u00b10.01 | 0.0987\u00b10.02       | 1.3726\u00b10.27       | -0.0681\u00b10.01 |\n| KRNN                                      | Alpha360 | 0.0173\u00b10.01 | 0.1210\u00b10.06 | 0.0270\u00b10.01 | 0.2018\u00b10.04 | -0.0465\u00b10.05      | -0.5415\u00b10.62      | -0.2919\u00b10.13 |\n| Sandwich                                  | Alpha360 | 0.0258\u00b10.00 | 0.1924\u00b10.04 | 0.0337\u00b10.00 | 0.2624\u00b10.03 | 0.0005\u00b10.03       | 0.0001\u00b10.33       | -0.1752\u00b10.05 |\n\n\n- The selected 20 features are based on the feature importance of a lightgbm-based model.\n- The base model of DoubleEnsemble is LGBM.\n- The base model of TCTS is GRU.\n- About the datasets\n  - Alpha158 is a tabular dataset. There are less spatial relationships between different features. Each feature are carefully designed by human (a.k.a feature engineering)\n  - Alpha360 contains raw price and volue data without much feature engineering. There are strong strong spatial relationships between the features in the time dimension.\n- The metrics can be categorized into two\n   - Signal-based evaluation:  IC, ICIR, Rank IC, Rank ICIR\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7Bcorr%7D%28%5Ctextbf%7Bx%7D%2C%5Ctextbf%7By%7D%29%3D%5Cfrac%7B%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%28y_i-%5Cbar%7By%7D%29%7D%7B%5Csqrt%7B%5Csum_i%28x_i-%5Cbar%7Bx%7D%29%5E2%5Csum_i%28y_i-%5Cbar%7By%7D%29%5E2%7D%7D)"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR's for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i \"s/csi300/csi500/g\"  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i \"s/SH000300/SH000905/g\"  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332\u00b10.00 | 0.3044\u00b10.00 | 0.0462\u00b10.00 | 0.4326\u00b10.00 | 0.0382\u00b10.00       | 0.1723\u00b10.00       | -0.4876\u00b10.00 |\n| MLP        | Alpha158 | 0.0229\u00b10.01 | 0.2181\u00b10.05 | 0.0360\u00b10.00 | 0.3409\u00b10.02 | 0.0043\u00b10.02       | 0.0602\u00b10.27       | -0.2184\u00b10.04 |\n| LightGBM   | Alpha158 | 0.0399\u00b10.00 | 0.4065\u00b10.00 | 0.0482\u00b10.00 | 0.5101\u00b10.00 | 0.1284\u00b10.00       | 1.5650\u00b10.00       | -0.0635\u00b10.00 |\n| CatBoost   | Alpha158 | 0.0345\u00b10.00 | 0.2855\u00b10.00 | 0.0417\u00b10.00 | 0.3740\u00b10.00 | 0.0496\u00b10.00       | 0.5977\u00b10.00       | -0.1496\u00b10.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380\u00b10.00 | 0.3659\u00b10.00 | 0.0442\u00b10.00 | 0.4324\u00b10.00 | 0.0382\u00b10.00       | 0.1723\u00b10.00       | -0.4876\u00b10.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258\u00b10.00 | 0.2021\u00b10.02 | 0.0426\u00b10.00 | 0.3840\u00b10.02 | 0.0022\u00b10.02       | 0.0301\u00b10.26       | -0.2064\u00b10.02 |\n| LightGBM   | Alpha360 | 0.0400\u00b10.00 | 0.3605\u00b10.00 | 0.0536\u00b10.00 | 0.5431\u00b10.00 | 0.0505\u00b10.00       | 0.7658\u00b10.02       | -0.1880\u00b10.00 |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n- ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BIC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%2C%20%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BIC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BIC%7D%29%7D)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20IC%7D%5E%7B%28t%29%7D%20%3D%20%5Ctext%7Bcorr%7D%28%5Ctext%7Brank%7D%28%5Chat%7B%5Ctextbf%7By%7D%7D%5E%7B%28t%29%7D%29%2C%20%5Ctext%7Brank%7D%28%5Ctextbf%7Bret%7D%5E%7B%28t%29%7D%29%29)\n      - ![equation](https://latex.codecogs.com/gif.latex?%5Ctext%7BRank%20ICIR%7D%20%3D%20%5Cfrac%20%7B%5Ctext%7Bmean%7D%28%5Ctextbf%7BRank%20IC%7D%29%7D%20%7B%5Ctext%7Bstd%7D%28%5Ctextbf%7BRankIC%7D%29%7D)\n   - Portfolio-based metrics:  Annualized Return, Information Ratio, Max Drawdown\n\n## Results on CSI500\nThe results on CSI500 is not complete. PR's for models on csi500 are welcome!\n\nTransfer previous models in CSI300 to CSI500 is quite easy.  You can try models with just a few commands below.\n```\ncd examples/benchmarks/LightGBM\npip install -r requirements.txt\n\n# create new config and set the benchmark to csi500\ncp workflow_config_lightgbm_Alpha158.yaml workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i \"s/csi300/csi500/g\"  workflow_config_lightgbm_Alpha158_csi500.yaml\nsed -i \"s/SH000300/SH000905/g\"  workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# you can either run the model once\nqrun workflow_config_lightgbm_Alpha158_csi500.yaml\n\n# or run it for multiple times automatically and get the summarized results.\ncd  ../../\npython run_all_model.py run 3 lightgbm Alpha158 csi500  # for models with randomness.  please run it for 20 times.\n```\n\n### Alpha158 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| Linear     | Alpha158 | 0.0332\u00b10.00 | 0.3044\u00b10.00 | 0.0462\u00b10.00 | 0.4326\u00b10.00 | 0.0382\u00b10.00       | 0.1723\u00b10.00       | -0.4876\u00b10.00 |\n| MLP        | Alpha158 | 0.0229\u00b10.01 | 0.2181\u00b10.05 | 0.0360\u00b10.00 | 0.3409\u00b10.02 | 0.0043\u00b10.02       | 0.0602\u00b10.27       | -0.2184\u00b10.04 |\n| LightGBM   | Alpha158 | 0.0399\u00b10.00 | 0.4065\u00b10.00 | 0.0482\u00b10.00 | 0.5101\u00b10.00 | 0.1284\u00b10.00       | 1.5650\u00b10.00       | -0.0635\u00b10.00 |\n| CatBoost   | Alpha158 | 0.0345\u00b10.00 | 0.2855\u00b10.00 | 0.0417\u00b10.00 | 0.3740\u00b10.00 | 0.0496\u00b10.00       | 0.5977\u00b10.00       | -0.1496\u00b10.00 |\n| DoubleEnsemble  | Alpha158 | 0.0380\u00b10.00 | 0.3659\u00b10.00 | 0.0442\u00b10.00 | 0.4324\u00b10.00 | 0.0382\u00b10.00       | 0.1723\u00b10.00       | -0.4876\u00b10.00 |\n\n### Alpha360 dataset\n| Model Name | Dataset  | IC          | ICIR        | Rank IC     | Rank ICIR   | Annualized Return | Information Ratio | Max Drawdown |\n|------------|----------|-------------|-------------|-------------|-------------|-------------------|-------------------|--------------|\n| MLP        | Alpha360 | 0.0258\u00b10.00 | 0.2021\u00b10.02 | 0.0426\u00b10.00 | 0.3840\u00b10.02 | 0.0022\u00b10.02       | 0.0301\u00b10.26       | -0.2064\u00b10.02 |\n| LightGBM   | Alpha360 | 0.0400\u00b10.00 | 0.3605\u00b10.00 | 0.0536\u00b10.00 | 0.5431\u00b10.00 | 0.0505\u00b10.00       | 0.7658\u00b10.02       | -0.1880\u00b10.00 |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382\u00b10.00 | 0.3229\u00b10.00 | 0.0489\u00b10.00 | 0.4649\u00b10.00 | 0.0297\u00b10.00       | 0.4227\u00b10.02       | -0.1499\u00b10.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361\u00b10.00 | 0.3092\u00b10.00 | 0.0499\u00b10.00 | 0.4793\u00b10.00 | 0.0382\u00b10.00       | 0.1723\u00b10.02       | -0.4876\u00b10.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/README.md\n| CatBoost   | Alpha360 | 0.0382\u00b10.00 | 0.3229\u00b10.00 | 0.0489\u00b10.00 | 0.4649\u00b10.00 | 0.0297\u00b10.00       | 0.4227\u00b10.02       | -0.1499\u00b10.01 |\n| DoubleEnsemble  | Alpha360 | 0.0361\u00b10.00 | 0.3092\u00b10.00 | 0.0499\u00b10.00 | 0.4793\u00b10.00 | 0.0382\u00b10.00       | 0.1723\u00b10.02       | -0.4876\u00b10.00 |\n\n# Contributing\n\nYour contributions to new models are highly welcome!\n\nIf you want to contribute your new models, you can follow the steps below.\n1. Create a folder for your model\n2. The folder contains following items(you can refer to [this example](https://github.com/microsoft/qlib/tree/main/examples/benchmarks/TCTS)).\n    - `requirements.txt`: required dependencies.\n    - `README.md`: a brief introduction to your models\n    - `workflow_config_<model name>_<dataset>.yaml`: a configuration which can read by `qrun`. You are encouraged to run your model in all datasets.\n3. You can integrate your model as a module [in this folder](https://github.com/microsoft/qlib/tree/main/qlib/contrib/model).\n4. Please update your results in the above **Benchmark Tables**, e.g. [Alpha360](#alpha158-dataset), [Alpha158](#alpha158-dataset)(the values of each metric are the mean and std calculated based on **20 Runs** with different random seeds. You can accomplish the above operations through the automated [script](https://github.com/microsoft/qlib/blob/main/examples/run_all_model.py) provided by Qlib, and get the final result in the .md file. if you don't have enough computational resource, you can ask for help in the PR).\n5. Update the info in the index page in the [news list](https://github.com/microsoft/qlib#newspaper-whats-new----sparkling_heart) and [model list](https://github.com/microsoft/qlib#quant-model-paper-zoo).\n\nFinally, you can send PR for review. ([here is an example](https://github.com/microsoft/qlib/pull/1040))\n\n\n# FAQ\n\nQ: What's the difference between models with name `*.py` and `*_ts.py`?\n\nA: Models with name `*_ts.py` are designed for `TSDatasetH` (`TSDatasetH` will create time-series automatically from tabular data).  Models with name `*.py` are designed for `DatasetH` (`DatasetH` is usually used in tabular data.  But users still can apply time-series models on tabular datasets if the columns has time-series relationships)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/LightGBM/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/LightGBM/README.md\n# LightGBM\n* Code: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)\n* Paper: LightGBM: A Highly Efficient Gradient Boosting\nDecision Tree. [https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf).\n\n\n# Introductions about the settings/configs.\n\n`workflow_config_lightgbm_multi_freq.yaml`\n- It uses data sources of different frequencies (i.e. multiple frequencies) for daily prediction.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/LightGBM/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/LightGBM/README.md\n# LightGBM\n* Code: [https://github.com/microsoft/LightGBM](https://github.com/microsoft/LightGBM)\n* Paper: LightGBM: A Highly Efficient Gradient Boosting\nDecision Tree. [https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf).\n\n\n# Introductions about the settings/configs.\n\n`workflow_config_lightgbm_multi_freq.yaml`\n- It uses data sources of different frequencies (i.e. multiple frequencies) for daily prediction."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/ALSTM/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ALSTM/README.md\n# ALSTM\n\n- ALSTM contains a temporal attentive aggregation layer based on normal LSTM.\n\n- Paper: A dual-stage attention-based recurrent neural network for time series prediction.\n\n  [https://www.ijcai.org/Proceedings/2017/0366.pdf](https://www.ijcai.org/Proceedings/2017/0366.pdf)\n\n- NOTE: Current version of implementation is just a simplified version of ALSTM. It is an LSTM with attention.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/ALSTM/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ALSTM/README.md\n# ALSTM\n\n- ALSTM contains a temporal attentive aggregation layer based on normal LSTM.\n\n- Paper: A dual-stage attention-based recurrent neural network for time series prediction.\n\n  [https://www.ijcai.org/Proceedings/2017/0366.pdf](https://www.ijcai.org/Proceedings/2017/0366.pdf)\n\n- NOTE: Current version of implementation is just a simplified version of ALSTM. It is an LSTM with attention."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/DoubleEnsemble/README.md\n# DoubleEnsemble\n* DoubleEnsemble is an ensemble framework leveraging learning trajectory based sample reweighting and shuffling based feature selection, to solve both the low signal-to-noise ratio and increasing number of features problems. They identify the key samples based on the training dynamics on each sample and elicit key features based on the ablation impact of each feature via shuffling. The model is applicable to a wide range of base models, capable of extracting complex patterns, while mitigating the overfitting and instability issues for financial market prediction.\n* This code used in Qlib is implemented by ourselves.\n* Paper: DoubleEnsemble: A New Ensemble Method Based on Sample Reweighting and Feature Selection for Financial Data Analysis [https://arxiv.org/pdf/2010.01265.pdf](https://arxiv.org/pdf/2010.01265.pdf)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. \"Temporal fusion transformers for interpretable multi-horizon time series forecasting.\" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TFT/README.md\n# Temporal Fusion Transformers Benchmark\n## Source\n**Reference**: Lim, Bryan, et al. \"Temporal fusion transformers for interpretable multi-horizon time series forecasting.\" arXiv preprint arXiv:1912.09363 (2019).\n\n**GitHub**: https://github.com/google-research/google-research/tree/master/tft\n\n## Run the Workflow\nUsers can follow the ``workflow_by_code_tft.py`` to run the benchmark. \n\n### Notes\n1. Please be **aware** that this script can only support `Python 3.6 - 3.7`.\n2. If the CUDA version on your machine is not 10.0, please remember to run the following commands `conda install anaconda cudatoolkit=10.0` and `conda install cudnn` on your machine.\n3. The model must run in GPU, or an error will be raised.\n4. New datasets should be registered in ``data_formatters``, for detail please visit the source."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TabNet/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TabNet/README.md\n# TabNet\n* Code: [https://github.com/dreamquark-ai/tabnet](https://github.com/dreamquark-ai/tabnet)\n* Paper: [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/pdf/1908.07442.pdf).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TabNet/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TabNet/README.md\n# TabNet\n* Code: [https://github.com/dreamquark-ai/tabnet](https://github.com/dreamquark-ai/tabnet)\n* Paper: [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/pdf/1908.07442.pdf)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n# Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport\n\nTemporal Routing Adaptor (TRA) is designed to capture multiple trading patterns in the stock market data. Please refer to [our paper](http://arxiv.org/abs/2106.12950) for more details.\n\nIf you find our work useful in your research, please cite:\n```\n@inproceedings{HengxuKDD2021,\n author = {Hengxu Lin and Dong Zhou and Weiqing Liu and Jiang Bian},\n title = {Learning Multiple Stock Trading Patterns with Temporal Routing Adaptor and Optimal Transport},\n booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \\& Data Mining},\n series = {KDD '21},\n year = {2021},\n publisher = {ACM},\n}\n\n@article{yang2020qlib,\n  title={Qlib: An AI-oriented Quantitative Investment Platform},\n  author={Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},\n  journal={arXiv preprint arXiv:2009.11189},\n  year={2020}\n}\n```\n\n## Usage (Recommended)\n\n**Update**: `TRA` has been moved to `qlib.contrib.model.pytorch_tra` to support other `Qlib` components like  `qlib.workflow` and `Alpha158/Alpha360` dataset.\n\nPlease follow the official [doc](https://qlib.readthedocs.io/en/latest/component/workflow.html) to use `TRA` with `workflow`. Here we also provide several example config files:\n\n- `workflow_config_tra_Alpha360.yaml`: running `TRA` with `Alpha360` dataset\n- `workflow_config_tra_Alpha158.yaml`: running `TRA` with `Alpha158` dataset (with feature subsampling)\n- `workflow_config_tra_Alpha158_full.yaml`: running `TRA` with `Alpha158` dataset (without feature subsampling)\n\nThe performances of `TRA` are reported in [Benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).\n\n## Usage (Not Maintained)\n\nThis section is used to reproduce the results in the paper.\n\n### Running\n\nWe attach our running scripts for the paper in `run.sh`.\n\nAnd here are two ways to run the model:\n\n* Running from scripts with default parameters\n\n  You can directly run from Qlib command `qrun`:\n  ```\n  qrun configs/config_alstm.yaml\n  ```\n\n* Running from code with self-defined parameters\n\n  Setting different parameters is also allowed. See codes in `example.py`:\n  ```\n  python example.py --config_file configs/config_alstm.yaml\n  ```\n\nHere we trained TRA on a pretrained backbone model. Therefore we run `*_init.yaml` before TRA's scripts.\n\n### Results\n\nAfter running the scripts, you can find result files in path `./output`:\n\n* `info.json` - config settings and result metrics.\n* `log.csv` - running logs.\n* `model.bin` - the model parameter dictionary.\n* `pred.pkl` - the prediction scores and output for inference.\n\nEvaluation metrics reported in the paper:\nThis result is generated by qlib==0.7.1.\n\n| Methods | MSE| MAE| IC | ICIR | AR | AV | SR | MDD |\n|-------|-------|------|-----|-----|-----|-----|-----|-----|\n|Linear|0.163|0.327|0.020|0.132|-3.2%|16.8%|-0.191|32.1%|\n|LightGBM|0.160(0.000)|0.323(0.000)|0.041|0.292|7.8%|15.5%|0.503|25.7%|\n|MLP|0.160(0.002)|0.323(0.003)|0.037|0.273|3.7%|15.3%|0.264|26.2%|\n|SFM|0.159(0.001)\t|0.321(0.001)\t|0.047\t|0.381\t|7.1%\t|14.3%\t|0.497\t|22.9%|\n|ALSTM|0.158(0.001)\t|0.320(0.001)\t|0.053\t|0.419\t|12.3%\t|13.7%\t|0.897\t|20.2%|\n|Trans.|0.158(0.001)\t|0.322(0.001)\t|0.051\t|0.400\t|14.5%\t|14.2%\t|1.028\t|22.5%|\n|ALSTM+TS|0.160(0.002)\t|0.321(0.002)\t|0.039\t|0.291\t|6.7%\t|14.6%\t|0.480|22.3%|\n|Trans.+TS|0.160(0.004)\t|0.324(0.005)\t|0.037\t|0.278\t|10.4%\t|14.7%\t|0.722\t|23.7%|"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/README.md\n|ALSTM+TRA(Ours)|0.157(0.000)\t|0.318(0.000)\t|0.059\t|0.460\t|12.4%\t|14.0%\t|0.885\t|20.4%|\n|Trans.+TRA(Ours)|0.157(0.000)\t|0.320(0.000)\t|0.056\t|0.442\t|16.1%\t|14.2%\t|1.133\t|23.1%|\n\nA more detailed demo for our experiment results in the paper can be found in `Report.ipynb`.\n\n## Common Issues\n\nFor help or issues using TRA, please submit a GitHub issue.\n\nSometimes we might encounter situation where the loss is `NaN`, please check the `epsilon` parameter in the sinkhorn algorithm, adjusting the `epsilon` according to input's scale is important."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TRA/data/README.md\nData Link: https://drive.google.com/drive/folders/1fMqZYSeLyrHiWmVzygeI4sw3vp5Gt8cY?usp=sharing"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/IGMTF/README.md\n# IGMTF\n* Code: [https://github.com/Wentao-Xu/IGMTF](https://github.com/Wentao-Xu/IGMTF)\n* Paper: [IGMTF: An Instance-wise Graph-based Framework for\nMultivariate Time Series Forecasting](https://arxiv.org/abs/2109.06489)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/Sandwich/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Sandwich/README.md\n# Sandwich\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py](https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\nmake use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.version==1.12.1", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/Sandwich/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Sandwich/README.md\n# Sandwich\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py](https://github.com/microsoft/FOST/blob/main/fostool/model/sandwich.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\nmake use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.version==1.12.1"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/XGBoost/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/XGBoost/README.md\n# XGBoost\n* Code: [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)\n* Paper: XGBoost: A Scalable Tree Boosting System. [https://dl.acm.org/doi/pdf/10.1145/2939672.2939785](https://dl.acm.org/doi/pdf/10.1145/2939672.2939785).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/XGBoost/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/XGBoost/README.md\n# XGBoost\n* Code: [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)\n* Paper: XGBoost: A Scalable Tree Boosting System. [https://dl.acm.org/doi/pdf/10.1145/2939672.2939785](https://dl.acm.org/doi/pdf/10.1145/2939672.2939785)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GeneralPtNN/README.md\n# Introduction\n\nWhat is GeneralPtNN\n- Fix previous design that fail to support both Time-series and tabular data\n- Now you can just replace the Pytorch model structure to run a NN model.\n\nWe provide an example to demonstrate the effectiveness of the current design.\n- `workflow_config_gru.yaml` align with previous results [GRU(Kyunghyun Cho, et al.)](../README.md#Alpha158-dataset)\n  - `workflow_config_gru2mlp.yaml` to demonstrate we can convert config from time-series to tabular data with minimal changes\n    - You only have to change the net & dataset class to make the conversion.\n- `workflow_config_mlp.yaml` achieved similar functionality with [MLP](../README.md#Alpha158-dataset)\n\n# TODO\n\n- We will align existing models to current design.\n\n- The result of `workflow_config_mlp.yaml` is different with the result of [MLP](../README.md#Alpha158-dataset) since GeneralPtNN has a different stopping method compared to previous implementations. Specificly, GeneralPtNN controls training according to epoches, whereas previous methods controlled by max_steps."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADARNN/README.md\n# AdaRNN\n* Code: [https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn](https://github.com/jindongwang/transferlearning/tree/master/code/deep/adarnn)\n* Paper: [AdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/pdf/2108.04443.pdf)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods\u2019 features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GATs/README.md\n# GATs\n* Graph Attention Networks(GATs) leverage masked self-attentional layers on graph-structured data. The nodes in stacked layers have different weights and they are able to attend over their\nneighborhoods\u2019 features, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront.\n* This code used in Qlib is implemented with PyTorch by ourselves.\n* Paper: Graph Attention Networks https://arxiv.org/pdf/1710.10903.pdf"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=\"center\"> \n<img src=\"workflow.png\"/>\n</p>\n\nAt step <img src=\"https://latex.codecogs.com/png.latex?s\" title=\"s\" />, with training data <img src=\"https://latex.codecogs.com/png.latex?x_s,y_s\" title=\"x_s,y_s\" />, the scheduler <img src=\"https://latex.codecogs.com/png.latex?\\varphi\" title=\"\\varphi\" /> chooses a suitable task <img src=\"https://latex.codecogs.com/png.latex?T_{i_s}\" title=\"T_{i_s}\" /> (green solid lines) to update the model <img src=\"https://latex.codecogs.com/png.latex?f\" title=\"f\" /> (blue solid lines). After <img src=\"https://latex.codecogs.com/png.latex?S\" title=\"S\" /> steps, we evaluate the model <img src=\"https://latex.codecogs.com/png.latex?f\" title=\"f\" /> on the validation set and update the scheduler <img src=\"https://latex.codecogs.com/png.latex?\\varphi\" title=\"\\varphi\" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n# Temporally Correlated Task Scheduling for Sequence Learning\n### Background\nSequence learning has attracted much research attention from the machine learning community in recent years. In many applications, a sequence learning task is usually associated with multiple temporally correlated auxiliary tasks, which are different in terms of how much input information to use or which future step to predict. In stock trend forecasting, as demonstrated in Figure1, one can predict the price of a stock in different future days (e.g., tomorrow, the day after tomorrow). In this paper, we propose a framework to make use of those temporally correlated tasks to help each other. \n\n### Method\nGiven that there are usually multiple temporally correlated tasks, the key challenge lies in which tasks to use and when to use them in the training process. This work introduces a learnable task scheduler for sequence learning, which adaptively selects temporally correlated tasks during the training process. The scheduler accesses the model status and the current training data (e.g., in the current minibatch) and selects the best auxiliary task to help the training of the main task. The scheduler and the model for the main task are jointly trained through bi-level optimization: the scheduler is trained to maximize the validation performance of the model, and the model is trained to minimize the training loss guided by the scheduler. The process is demonstrated in Figure2.\n\n<p align=\"center\"> \n<img src=\"workflow.png\"/>\n</p>\n\nAt step <img src=\"https://latex.codecogs.com/png.latex?s\" title=\"s\" />, with training data <img src=\"https://latex.codecogs.com/png.latex?x_s,y_s\" title=\"x_s,y_s\" />, the scheduler <img src=\"https://latex.codecogs.com/png.latex?\\varphi\" title=\"\\varphi\" /> chooses a suitable task <img src=\"https://latex.codecogs.com/png.latex?T_{i_s}\" title=\"T_{i_s}\" /> (green solid lines) to update the model <img src=\"https://latex.codecogs.com/png.latex?f\" title=\"f\" /> (blue solid lines). After <img src=\"https://latex.codecogs.com/png.latex?S\" title=\"S\" /> steps, we evaluate the model <img src=\"https://latex.codecogs.com/png.latex?f\" title=\"f\" /> on the validation set and update the scheduler <img src=\"https://latex.codecogs.com/png.latex?\\varphi\" title=\"\\varphi\" /> (green dashed lines).\n\n### Experiments\nDue to different data versions and different Qlib versions, the original data and data preprocessing methods of the experimental settings in the paper are different from those experimental settings in the existing Qlib version. Therefore, we provide two versions of the code according to the two kinds of settings, 1) the [code](https://github.com/lwwang1995/tcts) that can be used to reproduce the experimental results and 2) the [code](https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tcts.py) in the current Qlib baseline."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=\"https://latex.codecogs.com/png.latex?T_k\" title=\"T_k\" /> refers to forecasting return of stock <img src=\"https://latex.codecogs.com/png.latex?i\" title=\"i\" /> as following,\n<div align=center>\n<img src=\"https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1\" title=\"r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1\" />\n</div>\n\n* Temporally correlated task sets <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}\" title=\"\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}\" />, in this paper, <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_3\" title=\"\\mathcal{T}_3\" />, <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_5\" title=\"\\mathcal{T}_5\" /> and <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}\" title=\"\\mathcal{T}_{10}\" /> are used in <img src=\"https://latex.codecogs.com/png.latex?T_1\" title=\"T_1\" />, <img src=\"https://latex.codecogs.com/png.latex?T_2\" title=\"T_2\" />, and <img src=\"https://latex.codecogs.com/png.latex?T_3\" title=\"T_3\" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=\"https://latex.codecogs.com/png.latex?T_k\" title=\"T_k\" /> refers to forecasting return of stock <img src=\"https://latex.codecogs.com/png.latex?i\" title=\"i\" /> as following,\n<div align=center>\n<img src=\"https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1\" title=\"r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1\" />\n</div>\n\n* In Qlib baseline, <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_3\" title=\"\\mathcal{T}_3\" />, is used in  <img src=\"https://latex.codecogs.com/png.latex?T_1\" title=\"T_1\" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCTS/README.md\n#### Setting1\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2013), validation (01/01/2014-12/31/2015), and test sets (01/01/2016-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=\"https://latex.codecogs.com/png.latex?T_k\" title=\"T_k\" /> refers to forecasting return of stock <img src=\"https://latex.codecogs.com/png.latex?i\" title=\"i\" /> as following,\n<div align=center>\n<img src=\"https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;k}}{price_i^{t&plus;k-1}}-1\" title=\"r_{i}^{t,k} = \\frac{price_i^{t+k}}{price_i^{t+k-1}}-1\" />\n</div>\n\n* Temporally correlated task sets <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_k&space;=&space;\\{T_1,&space;T_2,&space;...&space;,&space;T_k\\}\" title=\"\\mathcal{T}_k = \\{T_1, T_2, ... , T_k\\}\" />, in this paper, <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_3\" title=\"\\mathcal{T}_3\" />, <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_5\" title=\"\\mathcal{T}_5\" /> and <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_{10}\" title=\"\\mathcal{T}_{10}\" /> are used in <img src=\"https://latex.codecogs.com/png.latex?T_1\" title=\"T_1\" />, <img src=\"https://latex.codecogs.com/png.latex?T_2\" title=\"T_2\" />, and <img src=\"https://latex.codecogs.com/png.latex?T_3\" title=\"T_3\" />.\n\n#### Setting2\n* Dataset: We use the historical transaction data for 300 stocks on [CSI300](http://www.csindex.com.cn/en/indices/index-detail/000300) from 01/01/2008 to 08/01/2020. We split the data into training (01/01/2008-12/31/2014), validation (01/01/2015-12/31/2016), and test sets (01/01/2017-08/01/2020) based on the transaction time. \n\n* The main tasks <img src=\"https://latex.codecogs.com/png.latex?T_k\" title=\"T_k\" /> refers to forecasting return of stock <img src=\"https://latex.codecogs.com/png.latex?i\" title=\"i\" /> as following,\n<div align=center>\n<img src=\"https://latex.codecogs.com/png.image?\\dpi{110}&space;r_{i}^{t,k}&space;=&space;\\frac{price_i^{t&plus;1&plus;k}}{price_i^{t&plus;1}}-1\" title=\"r_{i}^{t,k} = \\frac{price_i^{t+1+k}}{price_i^{t+1}}-1\" />\n</div>\n\n* In Qlib baseline, <img src=\"https://latex.codecogs.com/png.latex?\\mathcal{T}_3\" title=\"\\mathcal{T}_3\" />, is used in  <img src=\"https://latex.codecogs.com/png.latex?T_1\" title=\"T_1\" />.\n\n### Experimental Result\nYou can find the experimental result of setting1 in the [paper](http://proceedings.mlr.press/v139/wu21e/wu21e.pdf) and the experimental result of setting2 in this [page](https://github.com/microsoft/qlib/tree/main/examples/benchmarks)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md\n# HIST\n* Code: [https://github.com/Wentao-Xu/HIST](https://github.com/Wentao-Xu/HIST)\n* Paper: [HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared InformationAdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/abs/2110.13716).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/HIST/README.md\n# HIST\n* Code: [https://github.com/Wentao-Xu/HIST](https://github.com/Wentao-Xu/HIST)\n* Paper: [HIST: A Graph-based Framework for Stock Trend Forecasting via Mining Concept-Oriented Shared InformationAdaRNN: Adaptive Learning and Forecasting for Time Series](https://arxiv.org/abs/2110.13716)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/SFM/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/SFM/README.md\n# State-Frequency-Memory\n- State Frequency Memory (SFM) is a novel recurrent network that uses Discrete Fourier Transform to decompose the hidden states of memory cells and capture the multi-frequency trading patterns from past market data to make stock price predictions. \n- Paper: Stock Price Prediction via Discovering Multi-Frequency Trading Patterns. [http://www.eecs.ucf.edu/~gqi/publications/kdd2017_stock.pdf.](http://www.eecs.ucf.edu/~gqi/publications/kdd2017_stock.pdf)", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/SFM/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/SFM/README.md\n# State-Frequency-Memory\n- State Frequency Memory (SFM) is a novel recurrent network that uses Discrete Fourier Transform to decompose the hidden states of memory cells and capture the multi-frequency trading patterns from past market data to make stock price predictions. \n- Paper: Stock Price Prediction via Discovering Multi-Frequency Trading Patterns. [http://www.eecs.ucf.edu/~gqi/publications/kdd2017_stock.pdf.](http://www.eecs.ucf.edu/~gqi/publications/kdd2017_stock.pdf)"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Localformer/README.md\n# Localformer"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/KRNN/README.md\n# KRNN\n* Code: [https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py](https://github.com/microsoft/FOST/blob/main/fostool/model/krnn.py)\n\n\n# Introductions about the settings/configs.\n* Torch_geometric is used in the original model in FOST, but we didn't use it.\n* make use your CUDA version matches the torch version to allow the usage of GPU, we use CUDA==10.2 and torch.__version__==1.12.1"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md\n# ADD\n* Paper: [ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting](https://arxiv.org/abs/2012.06289).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/ADD/README.md\n# ADD\n* Paper: [ADD: Augmented Disentanglement Distillation Framework for Improving Stock Trend Forecasting](https://arxiv.org/abs/2012.06289)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/CatBoost/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/CatBoost/README.md\n# CatBoost\n* Code: [https://github.com/catboost/catboost](https://github.com/catboost/catboost)\n* Paper: CatBoost: unbiased boosting with categorical features. [https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/CatBoost/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/CatBoost/README.md\n# CatBoost\n* Code: [https://github.com/catboost/catboost](https://github.com/catboost/catboost)\n* Paper: CatBoost: unbiased boosting with categorical features. [https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/14491b756b3a51daac41c24863285549-Paper.pdf)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/Transformer/README.md\n# Transformer\n* Code: [https://github.com/tensorflow/tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n* Paper: [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/MLP/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/MLP/README.md\n# Multi-Layer Perceptron (MLP)", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/MLP/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/MLP/README.md\n# Multi-Layer Perceptron (MLP)"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/GRU/README.md\n# Gated Recurrent Unit (GRU)\n* Paper: [Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation](https://aclanthology.org/D14-1179.pdf)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/LSTM/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/LSTM/README.md\n# Long Short-Term Memory (LSTM)\n* Paper: [Long Short-Term Memory](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/LSTM/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/LSTM/README.md\n# Long Short-Term Memory (LSTM)\n* Paper: [Long Short-Term Memory](https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271).", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/benchmarks/TCN/README.md\n# TCN\n* Code: [https://github.com/locuslab/TCN](https://github.com/locuslab/TCN)\n* Paper: [An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling](https://arxiv.org/abs/1803.01271)."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/examples/data_demo/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/examples/data_demo/README.md\n# Introduction\nThe examples in this folder try to demonstrate some common usage of data-related modules of Qlib", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/examples/data_demo/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/examples/data_demo/README.md\n# Introduction\nThe examples in this folder try to demonstrate some common usage of data-related modules of Qlib"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dataset_tests/README.md\n# About dataset tests\nTests in this folder are for testing the prepared dataset from Yahoo"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/tests/data_mid_layer_tests/README.md\n# Introduction\nThe middle layers of data, which mainly includes\n- Handler\n    - processors\n- Datasets"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid.", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/tests/dependency_tests/README.md\nSome implementations of Qlib depend on some assumptions of its dependencies.\n\nSo some tests are requried to ensure that these assumptions are valid."}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = \"~/.qlib/qlib_data/us_data\"  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = \"~/.qlib/qlib_data/cn_data\"  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/README.md\n- [Download Qlib Data](#Download-Qlib-Data)\n  - [Download CN Data](#Download-CN-Data)\n  - [Download US Data](#Download-US-Data)\n  - [Download CN Simple Data](#Download-CN-Simple-Data)\n  - [Help](#Help)\n- [Using in Qlib](#Using-in-Qlib)\n  - [US data](#US-data)\n  - [CN data](#CN-data)\n\n\n## Download Qlib Data\n\n\n### Download CN Data\n\n```bash\n# daily data\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n# 1min  data (Optional for running non-high-frequency strategies)\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n```\n\n### Download US Data\n\n\n```bash\npython get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us\n```\n\n### Download CN Simple Data\n\n```bash\npython get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --region cn\n```\n\n### Help\n\n```bash\npython get_data.py qlib_data --help\n```\n\n## Using in Qlib\n> For more information: https://qlib.readthedocs.io/en/latest/start/initialization.html\n\n\n### US data\n\n> Need to download data first: [Download US Data](#Download-US-Data)\n\n```python\nimport qlib\nfrom qlib.config import REG_US\nprovider_uri = \"~/.qlib/qlib_data/us_data\"  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_US)\n```\n\n### CN data\n\n> Need to download data first: [Download CN Data](#Download-CN-Data)\n\n```python\nimport qlib\nfrom qlib.constant import REG_CN\n\nprovider_uri = \"~/.qlib/qlib_data/cn_data\"  # target_dir\nqlib.init(provider_uri=provider_uri, region=REG_CN)\n```\n\n## Use Crowd Sourced Data\nThe is also a [crowd sourced version of qlib data](data_collector/crowd_source/README.md): https://github.com/chenditc/investment_data/releases\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/README.md\n# Data Collector\n\n## Introduction\n\nScripts for data collection\n\n- yahoo: get *US/CN* stock data from *Yahoo Finance*\n- fund: get fund data from *http://fund.eastmoney.com*\n- cn_index: get *CN index* from *http://www.csindex.com.cn*, *CSI300*/*CSI100*\n- us_index: get *US index* from *https://en.wikipedia.org/wiki*, *SP500*/*NASDAQ100*/*DJIA*/*SP400*\n- contrib: scripts for some auxiliary functions\n\n\n## Custom Data Collection\n\n> Specific implementation reference: https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo\n\n1. Create a dataset code directory in the current directory\n2. Add `collector.py`\n   - add collector class:\n     ```python\n     CUR_DIR = Path(__file__).resolve().parent\n     sys.path.append(str(CUR_DIR.parent.parent))\n     from data_collector.base import BaseCollector, BaseNormalize, BaseRun\n     class UserCollector(BaseCollector):\n         ...\n     ```\n   - add normalize class:\n     ```python\n     class UserNormalzie(BaseNormalize):\n         ...\n     ```\n   - add `CLI` class:\n     ```python\n     class Run(BaseRun):\n         ...\n     ```\n3. add `README.md`\n4. add `requirements.txt`\n\n\n## Description of dataset\n\n  |             | Basic data                                                                                                       |\n  |------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------|\n  | Features    | **Price/Volume**: <br>&nbsp;&nbsp; - $close/$open/$low/$high/$volume/$change/$factor                             |\n  | Calendar    | **\\<freq>.txt**: <br>&nbsp;&nbsp; - day.txt<br>&nbsp;&nbsp;  - 1min.txt                                          |\n  | Instruments | **\\<market>.txt**: <br>&nbsp;&nbsp; - required: **all.txt**; <br>&nbsp;&nbsp;  - csi300.txt/csi500.txt/sp500.txt |\n\n  - `Features`: data, **digital**\n    - if not **adjusted**, **factor=1**\n\n### Data-dependent component\n\n> To make the component running correctly, the dependent data are required\n\n  | Component      | required data                                     |\n  |---------------------------------------------------|--------------------------------|\n  | Data retrieval | Features, Calendar, Instrument                    |\n  | Backtest       | **Features[Price/Volume]**, Calendar, Instruments |"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/us_index/README.md\n# NASDAQ100/SP500/SP400/DJIA History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name SP500 --qlib_dir ~/.qlib/qlib_data/us_data --method save_new_companies\n\n# index_name support: SP500, NASDAQ100, DJIA, SP400\n# help\npython collector.py --help\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/future_trading_date_collector/README.md\n# Get future trading days\n\n> `D.calendar(future=True)` will be used\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython future_trading_date_collector.py --qlib_dir ~/.qlib/qlib_data/cn_data --freq day\n```\n\n## Parameters\n\n- qlib_dir: qlib data directory\n- freq: value from [`day`, `1min`], default `day`"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/contrib/fill_cn_1min_data/README.md\n# Use 1d data to fill in the missing symbols relative to 1min\n\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## fill 1min data\n\n```bash\npython fill_cn_1min_data.py --data_1min_dir ~/.qlib/csv_data/cn_data_1min --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data\n```\n\n## Parameters\n\n- data_1min_dir: csv data\n- qlib_data_1d_dir: qlib data directory\n- max_workers: `ThreadPoolExecutor(max_workers=max_workers)`, by default *16*\n- date_field_name: date field name, by default *date*\n- symbol_field_name: symbol field name, by default *symbol*"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md\n# iBOVESPA History Companies Collection\n\n## Requirements\n\n- Install the libs from the file `requirements.txt`\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n- `requirements.txt` file was generated using python3.8\n\n## For the ibovespa (IBOV) index, we have:\n\n<hr/>\n\n### Method `get_new_companies`\n\n#### <b>Index start date</b>\n\n- The ibovespa index started on 2 January 1968 ([wiki](https://en.wikipedia.org/wiki/%C3%8Dndice_Bovespa)).  In order to use this start date in our `bench_start_date(self)` method, two conditions must be satisfied:\n    1) APIs used to download brazilian stocks (B3) historical prices must keep track of such historic data since 2 January 1968\n\n    2) Some website or API must provide, from that date, the historic index composition. In other words, the companies used to build the index .\n\n    As a consequence, the method `bench_start_date(self)` inside `collector.py` was implemented using `pd.Timestamp(\"2003-01-03\")` due to two reasons\n\n    1) The earliest ibov composition that have been found was from the first quarter of 2003. More informations about such composition can be seen on the sections below.\n\n    2) Yahoo finance, one of the libraries used to download symbols historic prices, keeps track from this date forward.\n\n- Within the `get_new_companies` method, a logic was implemented to get, for each ibovespa component stock, the start date that yahoo finance keeps track of.\n\n#### <b>Code Logic</b>\n\nThe code does a web scrapping into the B3's [website](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br), which keeps track of the ibovespa stocks composition on the current day. \n\nOther approaches, such as `request` and `Beautiful Soup` could have been used. However, the website shows the table with the stocks with some delay, since it uses a script inside of it to obtain such compositions.\nAlternatively, `selenium` was used to download this stocks' composition in order to overcome this problem.\n\nFuthermore, the data downloaded from the selenium script  was preprocessed so it could be saved into the `csv` format stablished by `scripts/data_collector/index.py`.\n\n<hr/>\n\n### Method `get_changes` \n\nNo suitable data source that keeps track of ibovespa's history stocks composition has been found. Except from this [repository](https://github.com/igor17400/IBOV-HCI) which provide such information have been used, however it only provides the data from the 1st quarter of 2003 to 3rd quarter of 2021.\n\nWith that reference, the index's composition can be compared quarter by quarter and year by year and then generate a file that keeps track of which stocks have been removed and which have been added each quarter and year.\n\n<hr/>\n\n### Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method save_new_companies\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/br_index/README.md\n# iBOVESPA History Companies Collection\n\n## Requirements\n\n- Install the libs from the file `requirements.txt`\n\n    ```bash\n    pip install -r requirements.txt\n    ```\n- `requirements.txt` file was generated using python3.8\n\n## For the ibovespa (IBOV) index, we have:\n\n<hr/>\n\n### Method `get_new_companies`\n\n#### <b>Index start date</b>\n\n- The ibovespa index started on 2 January 1968 ([wiki](https://en.wikipedia.org/wiki/%C3%8Dndice_Bovespa)).  In order to use this start date in our `bench_start_date(self)` method, two conditions must be satisfied:\n    1) APIs used to download brazilian stocks (B3) historical prices must keep track of such historic data since 2 January 1968\n\n    2) Some website or API must provide, from that date, the historic index composition. In other words, the companies used to build the index .\n\n    As a consequence, the method `bench_start_date(self)` inside `collector.py` was implemented using `pd.Timestamp(\"2003-01-03\")` due to two reasons\n\n    1) The earliest ibov composition that have been found was from the first quarter of 2003. More informations about such composition can be seen on the sections below.\n\n    2) Yahoo finance, one of the libraries used to download symbols historic prices, keeps track from this date forward.\n\n- Within the `get_new_companies` method, a logic was implemented to get, for each ibovespa component stock, the start date that yahoo finance keeps track of.\n\n#### <b>Code Logic</b>\n\nThe code does a web scrapping into the B3's [website](https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV?language=pt-br), which keeps track of the ibovespa stocks composition on the current day. \n\nOther approaches, such as `request` and `Beautiful Soup` could have been used. However, the website shows the table with the stocks with some delay, since it uses a script inside of it to obtain such compositions.\nAlternatively, `selenium` was used to download this stocks' composition in order to overcome this problem.\n\nFuthermore, the data downloaded from the selenium script  was preprocessed so it could be saved into the `csv` format stablished by `scripts/data_collector/index.py`.\n\n<hr/>\n\n### Method `get_changes` \n\nNo suitable data source that keeps track of ibovespa's history stocks composition has been found. Except from this [repository](https://github.com/igor17400/IBOV-HCI) which provide such information have been used, however it only provides the data from the 1st quarter of 2003 to 3rd quarter of 2021.\n\nWith that reference, the index's composition can be compared quarter by quarter and year by year and then generate a file that keeps track of which stocks have been removed and which have been added each quarter and year.\n\n<hr/>\n\n### Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name IBOV --qlib_dir ~/.qlib/qlib_data/br_data --method save_new_companies\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex \"^(600519|000725).*\"\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/pit/README.md\n# Collect Point-in-Time Data\n\n> *Please pay **ATTENTION** that the data is collected from [baostock](http://baostock.com) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### Download Quarterly CN Data\n\n```bash\ncd qlib/scripts/data_collector/pit/\n# download from baostock.com\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly\n```\n\nDownloading all data from the stock is very time-consuming. If you just want to run a quick test on a few stocks,  you can run the command below\n```bash\npython collector.py download_data --source_dir ~/.qlib/stock_data/source/pit --start 2000-01-01 --end 2020-01-01 --interval quarterly --symbol_regex \"^(600519|000725).*\"\n```\n\n\n### Normalize Data\n```bash\npython collector.py normalize_data --interval quarterly --source_dir ~/.qlib/stock_data/source/pit --normalize_dir ~/.qlib/stock_data/source/pit_normalized\n```\n\n\n\n### Dump Data into PIT Format\n\n```bash\ncd qlib/scripts\npython dump_pit.py dump --csv_path ~/.qlib/stock_data/source/pit_normalized --qlib_dir ~/.qlib/qlib_data/cn_data --interval quarterly\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- [Collector Data](#collector-data)\n  - [Get Qlib data](#get-qlib-databin-file)\n  - [Collector *YahooFinance* data to qlib](#collector-yahoofinance-data-to-qlib)\n  - [Automatic update of daily frequency data](#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n- [Using qlib data](#using-qlib-data)\n\n\n# Collect Data From Yahoo Finance\n\n> *Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n**NOTE**:  Yahoo! Finance has blocked the access from China. Please change your network if you want to use the Yahoo data crawler.\n\n>  **Examples of abnormal data**\n\n- [SH000661](https://finance.yahoo.com/quote/000661.SZ/history?period1=1558310400&period2=1590796800&interval=1d&filter=history&frequency=1d)\n- [SZ300144](https://finance.yahoo.com/quote/300144.SZ/history?period1=1557446400&period2=1589932800&interval=1d&filter=history&frequency=1d)\n\nWe have considered **STOCK PRICE ADJUSTMENT**, but some price series seem still very abnormal.\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n### Get Qlib data(`bin file`)\n  > `qlib-data` from *YahooFinance*, is the data that has been dumped and can be used directly in `qlib`.\n  > This ready-made qlib-data is not updated regularly. If users want the latest data, please follow [these steps](#collector-yahoofinance-data-to-qlib) download the latest data. \n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data*\n    - `version`: dataset version, value from [`v1`, `v2`], by default `v1`\n      - `v2` end date is *2021-06*, `v1` end date is *2020-09*\n      - If users want to incrementally update data, they need to use yahoo collector to [collect data from scratch](#collector-yahoofinance-data-to-qlib).\n      - **the [benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks) for qlib use `v1`**, *due to the unstable access to historical data by YahooFinance, there are some differences between `v2` and `v1`*\n    - `interval`: `1d` or `1min`, by default `1d`\n    - `region`: `cn` or `us` or `in`, by default `cn`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # cn 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n    # cn 1min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n    # us 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us --interval 1d\n    ```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- [Collector Data](#collector-data)\n  - [Get Qlib data](#get-qlib-databin-file)\n  - [Collector *YahooFinance* data to qlib](#collector-yahoofinance-data-to-qlib)\n  - [Automatic update of daily frequency data](#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n- [Using qlib data](#using-qlib-data)\n\n\n# Collect Data From Yahoo Finance\n\n> *Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n**NOTE**:  Yahoo! Finance has blocked the access from China. Please change your network if you want to use the Yahoo data crawler.\n\n>  **Examples of abnormal data**\n\n- [SH000661](https://finance.yahoo.com/quote/000661.SZ/history?period1=1558310400&period2=1590796800&interval=1d&filter=history&frequency=1d)\n- [SZ300144](https://finance.yahoo.com/quote/300144.SZ/history?period1=1557446400&period2=1589932800&interval=1d&filter=history&frequency=1d)\n\nWe have considered **STOCK PRICE ADJUSTMENT**, but some price series seem still very abnormal.\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n### Get Qlib data(`bin file`)\n  > `qlib-data` from *YahooFinance*, is the data that has been dumped and can be used directly in `qlib`.\n  > This ready-made qlib-data is not updated regularly. If users want the latest data, please follow [these steps](#collector-yahoofinance-data-to-qlib) download the latest data. \n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data*\n    - `version`: dataset version, value from [`v1`, `v2`], by default `v1`\n      - `v2` end date is *2021-06*, `v1` end date is *2020-09*\n      - If users want to incrementally update data, they need to use yahoo collector to [collect data from scratch](#collector-yahoofinance-data-to-qlib).\n      - **the [benchmarks](https://github.com/microsoft/qlib/tree/main/examples/benchmarks) for qlib use `v1`**, *due to the unstable access to historical data by YahooFinance, there are some differences between `v2` and `v1`*\n    - `interval`: `1d` or `1min`, by default `1d`\n    - `region`: `cn` or `us` or `in`, by default `cn`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # cn 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n    # cn 1min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n    # us 1d\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/us_data --region us --interval 1d\n    ```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month's data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *\"2000-01-01\"*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *\"failed\"* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Collector *YahooFinance* data to qlib\n> collector *YahooFinance* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/yahoo/collector.py download_data`\n     \n     This will download the raw data such as high, low, open, close, adjclose price from yahoo to a local directory. One file per symbol.\n\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `1d` or `1min`, by default `1d`\n            > **due to the limitation of the *YahooFinance API*, only the last month's data is available in `1min`**\n          - `region`: `CN` or `US` or `IN` or `BR`, by default `CN`\n          - `delay`: `time.sleep(delay)`, by default *0.5*\n          - `start`: start datetime, by default *\"2000-01-01\"*; *closed interval(including start)*\n          - `end`: end datetime, by default `pd.Timestamp(datetime.datetime.now() + pd.Timedelta(days=1))`; *open interval(excluding end)*\n          - `max_workers`: get the number of concurrent symbols, it is not recommended to change this parameter in order to maintain the integrity of the symbol data, by default *1*\n          - `check_data_length`: check the number of rows per *symbol*, by default `None`\n            > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n          - `max_collector_count`: number of *\"failed\"* symbol retries, by default 2\n     - examples:\n          ```bash\n          # cn 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region CN\n          # cn 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --delay 1 --interval 1min --region CN\n\n          # us 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region US\n          # us 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/us_data_1min --delay 1 --interval 1min --region US\n\n          # in 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data --start 2020-01-01 --end 2020-12-31 --delay 1 --interval 1d --region IN\n          # in 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/in_data_1min --delay 1 --interval 1min --region IN\n\n          # br 1d data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data --start 2003-01-03 --end 2022-03-01 --delay 1 --interval 1d --region BR\n          # br 1min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/br_data_1min --delay 1 --interval 1min --region BR\n          ```\n  2. normalize data: `python scripts/data_collector/yahoo/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n\n     - parameters:\n          - `source_dir`: csv directory"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `\"\"`\n       - `exclude_fields`: fields not dumped, by default `\"\"\"\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n- `normalize_dir`: result directory\n          - `max_workers`: number of concurrent, by default *1*\n          - `interval`: `1d` or `1min`, by default `1d`\n            > if **`interval == 1min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `CN` or `US` or `IN`, by default `CN`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            ```\n            if interval==1min, qlib_data_1d_dir cannot be None, normalize 1min needs to use 1d data;\n        \n                qlib_data_1d can be obtained like this:\n                    $ python scripts/get_data.py qlib_data --target_dir <qlib_data_1d_dir> --interval 1d\n                    $ python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <qlib_data_1d_dir> --end_date <end_date>\n                or:\n                    download 1d data from YahooFinance\n            \n            ```\n      - examples:\n        ```bash\n        # normalize 1d cn\n        python collector.py normalize_data --source_dir ~/.qlib/stock_data/source/cn_data --normalize_dir ~/.qlib/stock_data/source/cn_1d_nor --region CN --interval 1d\n\n        # normalize 1min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/cn_data_1min --normalize_dir ~/.qlib/stock_data/source/cn_1min_nor --region CN --interval 1min\n\n        # normalize 1d br\n        python scripts/data_collector/yahoo/collector.py normalize_data --source_dir ~/.qlib/stock_data/source/br_data --normalize_dir ~/.qlib/stock_data/source/br_1d_nor --region BR --interval 1d\n\n        # normalize 1min br\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/br_data --source_dir ~/.qlib/stock_data/source/br_data_1min --normalize_dir ~/.qlib/stock_data/source/br_1min_nor --region BR --interval 1min\n        ```\n  3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 1mih: 1min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `\"\"`\n       - `exclude_fields`: fields not dumped, by default `\"\"\"\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 1d cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_data --freq day --exclude_fields date,symbol\n       # dump 1min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/cn_1min_nor --qlib_dir ~/.qlib/qlib_data/cn_data_1min --freq 1min --exclude_fields date,symbol\n       ```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Automatic update of daily frequency data(from yahoo finance)\n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n\n  * Automatic update of data to the \"qlib\" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --end_date <end date>\n      ```\n      * `end_date`: end of trading day(not included)\n      * `check_data_length`: check the number of rows per *symbol*, by default `None`\n        > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n\n  * `scripts/data_collector/yahoo/collector.py update_data_to_bin` parameters:\n      * `source_dir`: The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n      * `normalize_dir`: Directory for normalize data, default \"Path(__file__).parent/normalize\"\n      * `qlib_data_1d_dir`: the qlib data to be updated for yahoo, usually from: [download qlib data](https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)\n      * `end_date`: end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end)\n      * `region`: region, value from [\"CN\", \"US\"], default \"CN\"\n      * `interval`: interval, default \"1d\"(Currently only supports 1d data)\n      * `exists_skip`: exists skip, by default False\n\n## Using qlib data\n\n  ```python\n  import qlib\n  from qlib.data import D\n\n  # 1d data cn\n  # freq=day, freq default day\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/cn_data\", region=\"cn\")\n  df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"day\")\n\n  # 1min data cn\n  # freq=1min\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/cn_data_1min\", region=\"cn\")\n  inst = D.list_instruments(D.instruments(\"all\"), freq=\"1min\", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [\"$close\"], freq=\"1min\")\n  # get all symbol data\n  # df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"1min\")\n\n  # 1d data us\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/us_data\", region=\"us\")\n  df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"day\")\n\n  # 1min data us\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/us_data_1min\", region=\"cn\")\n  inst = D.list_instruments(D.instruments(\"all\"), freq=\"1min\", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [\"$close\"], freq=\"1min\")\n  # get all symbol data\n  # df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"1min\")\n  ```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/yahoo/README.md\n### Automatic update of daily frequency data(from yahoo finance)\n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can't incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n\n  * Automatic update of data to the \"qlib\" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --end_date <end date>\n      ```\n      * `end_date`: end of trading day(not included)\n      * `check_data_length`: check the number of rows per *symbol*, by default `None`\n        > if `len(symbol_df) < check_data_length`, it will be re-fetched, with the number of re-fetches coming from the `max_collector_count` parameter\n\n  * `scripts/data_collector/yahoo/collector.py update_data_to_bin` parameters:\n      * `source_dir`: The directory where the raw data collected from the Internet is saved, default \"Path(__file__).parent/source\"\n      * `normalize_dir`: Directory for normalize data, default \"Path(__file__).parent/normalize\"\n      * `qlib_data_1d_dir`: the qlib data to be updated for yahoo, usually from: [download qlib data](https://github.com/microsoft/qlib/tree/main/scripts#download-cn-data)\n      * `end_date`: end datetime, default ``pd.Timestamp(trading_date + pd.Timedelta(days=1))``; open interval(excluding end)\n      * `region`: region, value from [\"CN\", \"US\"], default \"CN\"\n      * `interval`: interval, default \"1d\"(Currently only supports 1d data)\n      * `exists_skip`: exists skip, by default False\n\n## Using qlib data\n\n  ```python\n  import qlib\n  from qlib.data import D\n\n  # 1d data cn\n  # freq=day, freq default day\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/cn_data\", region=\"cn\")\n  df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"day\")\n\n  # 1min data cn\n  # freq=1min\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/cn_data_1min\", region=\"cn\")\n  inst = D.list_instruments(D.instruments(\"all\"), freq=\"1min\", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [\"$close\"], freq=\"1min\")\n  # get all symbol data\n  # df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"1min\")\n\n  # 1d data us\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/us_data\", region=\"us\")\n  df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"day\")\n\n  # 1min data us\n  qlib.init(provider_uri=\"~/.qlib/qlib_data/us_data_1min\", region=\"cn\")\n  inst = D.list_instruments(D.instruments(\"all\"), freq=\"1min\", as_list=True)\n  # get 100 symbols\n  df = D.features(inst[:100], [\"$close\"], freq=\"1min\")\n  # get all symbol data\n  # df = D.features(D.instruments(\"all\"), [\"$close\"], freq=\"1min\")\n  ```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/cn_index/README.md\n# CSI300/CSI100/CSI500 History Companies Collection\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n```bash\n# parse instruments, using in qlib/instruments.\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method parse_instruments\n\n# parse new companies\npython collector.py --index_name CSI300 --qlib_dir ~/.qlib/qlib_data/cn_data --method save_new_companies\n\n# index_name support: CSI300, CSI100, CSI500\n# help\npython collector.py --help\n```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [\u5929\u5929\u57fa\u91d1\u7f51](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=\"~/.qlib/qlib_data/cn_fund_data\")\ndf = D.features(D.instruments(market=\"all\"), [\"$DWJZ\", \"$LJJZ\"], freq=\"day\")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## \u514d\u8d23\u58f0\u660e\n\n\u672c\u9879\u76ee\u4ec5\u4f9b\u5b66\u4e60\u7814\u7a76\u4f7f\u7528\uff0c\u4e0d\u4f5c\u4e3a\u4efb\u4f55\u884c\u4e3a\u7684\u6307\u5bfc\u548c\u5efa\u8bae\uff0c\u7531\u6b64\u800c\u5f15\u53d1\u4efb\u4f55\u4e89\u8bae\u548c\u7ea0\u7eb7\uff0c\u4e0e\u672c\u9879\u76ee\u65e0\u4efb\u4f55\u5173\u7cfb", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/fund/README.md\n# Collect Fund Data\n\n> *Please pay **ATTENTION** that the data is collected from [\u5929\u5929\u57fa\u91d1\u7f51](https://fund.eastmoney.com/) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Collector Data\n\n\n### CN Data\n\n#### 1d from East Money\n\n```bash\n\n# download from eastmoney.com\npython collector.py download_data --source_dir ~/.qlib/fund_data/source/cn_data --region CN --start 2020-11-01 --end 2020-11-10 --delay 0.1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/fund_data/source/cn_data --normalize_dir ~/.qlib/fund_data/source/cn_1d_nor --region CN --interval 1d --date_field_name FSRQ\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/fund_data/source/cn_1d_nor --qlib_dir ~/.qlib/qlib_data/cn_fund_data --freq day --date_field_name FSRQ --include_fields DWJZ,LJJZ\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=\"~/.qlib/qlib_data/cn_fund_data\")\ndf = D.features(D.instruments(market=\"all\"), [\"$DWJZ\", \"$LJJZ\"], freq=\"day\")\n```\n\n\n### Help\n```bash\npythono collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- region: CN\n\n## \u514d\u8d23\u58f0\u660e\n\n\u672c\u9879\u76ee\u4ec5\u4f9b\u5b66\u4e60\u7814\u7a76\u4f7f\u7528\uff0c\u4e0d\u4f5c\u4e3a\u4efb\u4f55\u884c\u4e3a\u7684\u6307\u5bfc\u548c\u5efa\u8bae\uff0c\u7531\u6b64\u800c\u5f15\u53d1\u4efb\u4f55\u4e89\u8bae\u548c\u7ea0\u7eb7\uff0c\u4e0e\u672c\u9879\u76ee\u65e0\u4efb\u4f55\u5173\u7cfb"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/crypto/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crypto/README.md\n# Collect Crypto Data\n\n> *Please pay **ATTENTION** that the data is collected from [Coingecko](https://www.coingecko.com/en/api) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage of the dataset\n> *Crypto dataset only support Data retrieval function but not support backtest function due to the lack of OHLC data.*\n\n## Collector Data\n\n\n### Crypto Data\n\n#### 1d from Coingecko\n\n```bash\n\n# download from https://api.coingecko.com/api/v3/\npython collector.py download_data --source_dir ~/.qlib/crypto_data/source/1d --start 2015-01-01 --end 2021-11-30 --delay 1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/crypto_data/source/1d --normalize_dir ~/.qlib/crypto_data/source/1d_nor --interval 1d --date_field_name date\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/crypto_data/source/1d_nor --qlib_dir ~/.qlib/qlib_data/crypto_data --freq day --date_field_name date --include_fields prices,total_volumes,market_caps\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=\"~/.qlib/qlib_data/crypto_data\")\ndf = D.features(D.instruments(market=\"all\"), [\"$prices\", \"$total_volumes\",\"$market_caps\"], freq=\"day\")\n```\n\n\n### Help\n```bash\npython collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- delay: 1", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/crypto/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crypto/README.md\n# Collect Crypto Data\n\n> *Please pay **ATTENTION** that the data is collected from [Coingecko](https://www.coingecko.com/en/api) and the data might not be perfect. We recommend users to prepare their own data if they have high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*\n\n## Requirements\n\n```bash\npip install -r requirements.txt\n```\n\n## Usage of the dataset\n> *Crypto dataset only support Data retrieval function but not support backtest function due to the lack of OHLC data.*\n\n## Collector Data\n\n\n### Crypto Data\n\n#### 1d from Coingecko\n\n```bash\n\n# download from https://api.coingecko.com/api/v3/\npython collector.py download_data --source_dir ~/.qlib/crypto_data/source/1d --start 2015-01-01 --end 2021-11-30 --delay 1 --interval 1d\n\n# normalize\npython collector.py normalize_data --source_dir ~/.qlib/crypto_data/source/1d --normalize_dir ~/.qlib/crypto_data/source/1d_nor --interval 1d --date_field_name date\n\n# dump data\ncd qlib/scripts\npython dump_bin.py dump_all --csv_path ~/.qlib/crypto_data/source/1d_nor --qlib_dir ~/.qlib/qlib_data/crypto_data --freq day --date_field_name date --include_fields prices,total_volumes,market_caps\n\n```\n\n### using data\n\n```python\nimport qlib\nfrom qlib.data import D\n\nqlib.init(provider_uri=\"~/.qlib/qlib_data/crypto_data\")\ndf = D.features(D.instruments(market=\"all\"), [\"$prices\", \"$total_volumes\",\"$market_caps\"], freq=\"day\")\n```\n\n\n### Help\n```bash\npython collector.py collector_data --help\n```\n\n## Parameters\n\n- interval: 1d\n- delay: 1"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n## Collector Data\n\n### Get Qlib data(`bin file`)\n\n  - get data: `python scripts/get_data.py qlib_data`\n  - parameters:\n    - `target_dir`: save dir, by default *~/.qlib/qlib_data/cn_data_5min*\n    - `version`: dataset version, value from [`v2`], by default `v2`\n      - `v2` end date is *2022-12*\n    - `interval`: `5min`\n    - `region`: `hs300`\n    - `delete_old`: delete existing data from `target_dir`(*features, calendars, instruments, dataset_cache, features_cache*), value from [`True`, `False`], by default `True`\n    - `exists_skip`: traget_dir data already exists, skip `get_data`, value from [`True`, `False`], by default `False`\n  - examples:\n    ```bash\n    # hs300 5min\n    python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/hs300_data_5min --region hs300 --interval 5min\n    ```\n    \n### Collector *Baostock high frequency* data to qlib\n> collector *Baostock high frequency* data and *dump* into `qlib` format.\n> If the above ready-made data can't meet users' requirements,  users can follow this section to crawl the latest data and convert it to qlib-data.\n  1. download data to csv: `python scripts/data_collector/baostock_5min/collector.py download_data`\n     \n     This will download the raw data such as date, symbol, open, high, low, close, volume, amount, adjustflag from baostock to a local directory. One file per symbol.\n     - parameters:\n          - `source_dir`: save the directory\n          - `interval`: `5min`\n          - `region`: `HS300`\n          - `start`: start datetime, by default *None*\n          - `end`: end datetime, by default *None*\n     - examples:\n          ```bash\n          # cn 5min data\n          python collector.py download_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --start 2022-01-01 --end 2022-01-30 --interval 5min --region HS300\n          ```\n  2. normalize data: `python scripts/data_collector/baostock_5min/collector.py normalize_data`\n     \n     This will:\n     1. Normalize high, low, close, open price using adjclose.\n     2. Normalize the high, low, close, open price so that the first valid trading date's close price is 1. \n     - parameters:\n          - `source_dir`: csv directory\n          - `normalize_dir`: result directory\n          - `interval`: `5min`\n            > if **`interval == 5min`**, `qlib_data_1d_dir` cannot be `None`\n          - `region`: `HS300`\n          - `date_field_name`: column *name* identifying time in csv files, by default `date`\n          - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n          - `end_date`: if not `None`, normalize the last date saved (*including end_date*); if `None`, it will ignore this parameter; by default `None`\n          - `qlib_data_1d_dir`: qlib directory(1d data)\n            if interval==5min, qlib_data_1d_dir cannot be None, normalize 5min needs to use 1d data;\n            ```\n                # qlib_data_1d can be obtained like this:\n                python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn --version v3\n            ```\n      - examples:\n        ```bash\n        # normalize 5min cn\n        python collector.py normalize_data --qlib_data_1d_dir ~/.qlib/qlib_data/cn_data --source_dir ~/.qlib/stock_data/source/hs300_5min_original --normalize_dir ~/.qlib/stock_data/source/hs300_5min_nor --region HS300 --interval 5min\n        ```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `\"\"`\n       - `exclude_fields`: fields not dumped, by default `\"\"\"\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/baostock_5min/README.md\n3. dump data: `python scripts/dump_bin.py dump_all`\n    \n     This will convert the normalized csv in `feature` directory as numpy array and store the normalized data one file per column and one symbol per directory. \n    \n     - parameters:\n       - `csv_path`: stock data path or directory, **normalize result(normalize_dir)**\n       - `qlib_dir`: qlib(dump) data director\n       - `freq`: transaction frequency, by default `day`\n         > `freq_map = {1d:day, 5mih: 5min}`\n       - `max_workers`: number of threads, by default *16*\n       - `include_fields`: dump fields, by default `\"\"`\n       - `exclude_fields`: fields not dumped, by default `\"\"\"\n         > dump_fields = `include_fields if include_fields else set(symbol_df.columns) - set(exclude_fields) exclude_fields else symbol_df.columns`\n       - `symbol_field_name`: column *name* identifying symbol in csv files, by default `symbol`\n       - `date_field_name`: column *name* identifying time in csv files, by default `date`\n     - examples:\n       ```bash\n       # dump 5min cn\n       python dump_bin.py dump_all --csv_path ~/.qlib/stock_data/source/hs300_5min_nor --qlib_dir ~/.qlib/qlib_data/hs300_5min_bin --freq 5min --exclude_fields date,symbol\n       ```"}, {"codebase": "qlib", "reference": "https://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md", "text": "qlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md\n# Crowd Source Data\n\n## Initiative\nPublic data source like yahoo is flawed, it might miss data for stock which is delisted and it might have data which is wrong. This can introduce survivorship bias into our training process.\n\nThe Crowd Source Data is introduced to merged data from multiple data source and cross validate against each other, so that:\n1. We will have a more complete history record.\n2. We can identify the anomaly data and apply correction when necessary.\n\n## Related Repo\nThe raw data is hosted on dolthub repo: https://www.dolthub.com/repositories/chenditc/investment_data\n\nThe processing script and sql is hosted on github repo: https://github.com/chenditc/investment_data\n\nThe packaged docker runtime is hosted on dockerhub: https://hub.docker.com/repository/docker/chenditc/investment_data\n\n## How to use it in qlib\n### Option 1: Download release bin data\nUser can download data in qlib bin format and use it directly: https://github.com/chenditc/investment_data/releases/latest\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```\n\n### Option 2: Generate qlib data from dolthub\nDolthub data will be update daily, so that if user wants to get up to date data, they can dump qlib bin using docker:\n```\ndocker run -v /<some output directory>:/output -it --rm chenditc/investment_data bash dump_qlib_bin.sh && cp ./qlib_bin.tar.gz /output/\n```\n\n## FAQ and other info\nSee: https://github.com/chenditc/investment_data/blob/main/README.md", "enriched_text": "[Codebase: qlib] [File: https://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md]\n\nqlib\nhttps://github.com/microsoft/qlib/blob/main/scripts/data_collector/crowd_source/README.md\n# Crowd Source Data\n\n## Initiative\nPublic data source like yahoo is flawed, it might miss data for stock which is delisted and it might have data which is wrong. This can introduce survivorship bias into our training process.\n\nThe Crowd Source Data is introduced to merged data from multiple data source and cross validate against each other, so that:\n1. We will have a more complete history record.\n2. We can identify the anomaly data and apply correction when necessary.\n\n## Related Repo\nThe raw data is hosted on dolthub repo: https://www.dolthub.com/repositories/chenditc/investment_data\n\nThe processing script and sql is hosted on github repo: https://github.com/chenditc/investment_data\n\nThe packaged docker runtime is hosted on dockerhub: https://hub.docker.com/repository/docker/chenditc/investment_data\n\n## How to use it in qlib\n### Option 1: Download release bin data\nUser can download data in qlib bin format and use it directly: https://github.com/chenditc/investment_data/releases/latest\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=2\n```\n\n### Option 2: Generate qlib data from dolthub\nDolthub data will be update daily, so that if user wants to get up to date data, they can dump qlib bin using docker:\n```\ndocker run -v /<some output directory>:/output -it --rm chenditc/investment_data bash dump_qlib_bin.sh && cp ./qlib_bin.tar.gz /output/\n```\n\n## FAQ and other info\nSee: https://github.com/chenditc/investment_data/blob/main/README.md"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement by emailing <NAME> at <EMAIL>.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md\n# Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n- Demonstrating empathy and kindness toward other people\n- Being respectful of differing opinions, viewpoints, and experiences\n- Giving and gracefully accepting constructive feedback\n- Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n- Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n- The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n- Trolling, insulting or derogatory comments, and personal or political attacks\n- Public or private harassment\n- Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n- Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement by emailing <NAME> at <EMAIL>.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//CODE_OF_CONDUCT.md\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## \ud83d\udcab Contributors\n\nThanks to our amazing contributors! \ud83d\udc96\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=topoteretes/cognee\" />\n</a>\n\n## \ud83c\udfc6 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTORS.md\n## \ud83d\udcab Contributors\n\nThanks to our amazing contributors! \ud83d\udc96\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=topoteretes/cognee\" />\n</a>\n\n## \ud83c\udfc6 Top Contributors\n\n| Rank | Contributor | Contributions |\n|------|------------|---------------|"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//README.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - Memory for AI Agents in 6 lines of code\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demo</a>\n  .\n  <a href=\"https://cognee.ai\">Learn more</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Join Discord</a>\n  \u00b7\n  <a href=\"https://www.reddit.com/r/AIMemory/\">Join r/AIMemory</a>\n  .\n  <a href=\"https://docs.cognee.ai/\">Docs</a>\n  .\n  <a href=\"https://github.com/topoteretes/cognee-community\">cognee community repo</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n  <a href=\"https://github.com/sponsors/topoteretes\"><img src=\"https://img.shields.io/badge/Sponsor-\u2764\ufe0f-ff69b4.svg\" alt=\"Sponsor\"></a>\n\n<p>\n  <a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\" style=\"display:inline-block; margin-right:10px;\">\n    <img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" width=\"250\" height=\"54\" />\n  </a>\n\n  <a href=\"https://trendshift.io/repositories/13955\" target=\"_blank\" style=\"display:inline-block;\">\n    <img src=\"https://trendshift.io/api/badge/repositories/13955\" alt=\"topoteretes%2Fcognee | Trendshift\" width=\"250\" height=\"55\" />\n  </a>\n</p>\n\n\n\n\n\nBuild dynamic memory for Agents and replace RAG using scalable, modular ECL (Extract, Cognify, Load) pipelines.\n\n  <p align=\"center\">\n  \ud83c\udf10 Available Languages\n  :\n  <!-- Keep these links. Translations will automatically update with the README. -->\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=de\">Deutsch</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=es\">Espa\u00f1ol</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=fr\">fran\u00e7ais</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ja\">\u65e5\u672c\u8a9e</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ko\">\ud55c\uad6d\uc5b4</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=pt\">Portugu\u00eas</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ru\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//README.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - Memory for AI Agents in 6 lines of code\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demo</a>\n  .\n  <a href=\"https://cognee.ai\">Learn more</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Join Discord</a>\n  \u00b7\n  <a href=\"https://www.reddit.com/r/AIMemory/\">Join r/AIMemory</a>\n  .\n  <a href=\"https://docs.cognee.ai/\">Docs</a>\n  .\n  <a href=\"https://github.com/topoteretes/cognee-community\">cognee community repo</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n  <a href=\"https://github.com/sponsors/topoteretes\"><img src=\"https://img.shields.io/badge/Sponsor-\u2764\ufe0f-ff69b4.svg\" alt=\"Sponsor\"></a>\n\n<p>\n  <a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\" style=\"display:inline-block; margin-right:10px;\">\n    <img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" width=\"250\" height=\"54\" />\n  </a>\n\n  <a href=\"https://trendshift.io/repositories/13955\" target=\"_blank\" style=\"display:inline-block;\">\n    <img src=\"https://trendshift.io/api/badge/repositories/13955\" alt=\"topoteretes%2Fcognee | Trendshift\" width=\"250\" height=\"55\" />\n  </a>\n</p>\n\n\n\n\n\nBuild dynamic memory for Agents and replace RAG using scalable, modular ECL (Extract, Cognify, Load) pipelines.\n\n  <p align=\"center\">\n  \ud83c\udf10 Available Languages\n  :\n  <!-- Keep these links. Translations will automatically update with the README. -->\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=de\">Deutsch</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=es\">Espa\u00f1ol</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=fr\">fran\u00e7ais</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ja\">\u65e5\u672c\u8a9e</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ko\">\ud55c\uad6d\uc5b4</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=pt\">Portugu\u00eas</a> |\n  <a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=ru\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//README.md\n<a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=zh\">\u4e2d\u6587</a>\n  </p>\n\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"Why cognee?\" width=\"50%\" />\n</div>\n</div>\n\n\n\n## Get Started\n\nGet started quickly with a Google Colab  <a href=\"https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing\">notebook</a> , <a href=\"https://deepnote.com/workspace/cognee-382213d0-0444-4c89-8265-13770e333c02/project/cognee-demo-78ffacb9-5832-4611-bb1a-560386068b30/notebook/Notebook-1-75b24cda566d4c24ab348f7150792601?utm_source=share-modal&utm_medium=product-shared-content&utm_campaign=notebook&utm_content=78ffacb9-5832-4611-bb1a-560386068b30\">Deepnote notebook</a> or  <a href=\"https://github.com/topoteretes/cognee/tree/main/cognee-starter-kit\">starter repo</a>\n\n\n## About cognee\n\ncognee works locally and stores your data on your device.\nOur hosted solution is just our deployment of OSS cognee on Modal, with the goal of making development and productionization easier.\n\nSelf-hosted package:\n\n- Interconnects any kind of documents: past conversations, files, images, and audio transcriptions\n- Replaces RAG systems with a memory layer based on graphs and vectors\n- Reduces developer effort and cost, while increasing quality and precision\n- Provides Pythonic data pipelines that manage data ingestion from 30+ data sources\n- Is highly customizable with custom tasks, pipelines, and a set of built-in search endpoints\n\nHosted platform:\n- Includes a managed UI and a [hosted solution](https://www.cognee.ai)\n\n\n\n## Self-Hosted (Open Source)\n\n\n### \ud83d\udce6 Installation\n\nYou can install Cognee using either **pip**, **poetry**, **uv** or any other python package manager..\n\nCognee supports Python 3.10 to 3.12\n\n#### With uv\n\n```bash\nuv pip install cognee\n```\n\nDetailed instructions can be found in our [docs](https://docs.cognee.ai/getting-started/installation#environment-configuration)\n\n### \ud83d\udcbb Basic Usage\n\n#### Setup\n\n```\nimport os\nos.environ[\"LLM_API_KEY\"] = \"YOUR OPENAI_API_KEY\"\n\n```\n\nYou can also set the variables by creating .env file, using our <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">template.</a>\nTo use different LLM providers, for more info check out our <a href=\"https://docs.cognee.ai/setup-configuration/llm-providers\">documentation</a>\n\n\n#### Simple example\n\n\n\n##### Python\n\nThis script will run the default pipeline:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Add text to cognee\n    await cognee.add(\"Cognee turns documents into AI memory.\")\n\n    # Generate the knowledge graph\n    await cognee.cognify()\n\n    # Add memory algorithms to the graph\n    await cognee.memify()\n\n    # Query the knowledge graph\n    results = await cognee.search(\"What does cognee do?\")\n\n    # Display the results\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\nExample output:\n```\n  Cognee turns documents into AI memory.\n\n```", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//README.md\n<a href=\"https://www.readme-i18n.com/topoteretes/cognee?lang=zh\">\u4e2d\u6587</a>\n  </p>\n\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"Why cognee?\" width=\"50%\" />\n</div>\n</div>\n\n\n\n## Get Started\n\nGet started quickly with a Google Colab  <a href=\"https://colab.research.google.com/drive/12Vi9zID-M3fpKpKiaqDBvkk98ElkRPWy?usp=sharing\">notebook</a> , <a href=\"https://deepnote.com/workspace/cognee-382213d0-0444-4c89-8265-13770e333c02/project/cognee-demo-78ffacb9-5832-4611-bb1a-560386068b30/notebook/Notebook-1-75b24cda566d4c24ab348f7150792601?utm_source=share-modal&utm_medium=product-shared-content&utm_campaign=notebook&utm_content=78ffacb9-5832-4611-bb1a-560386068b30\">Deepnote notebook</a> or  <a href=\"https://github.com/topoteretes/cognee/tree/main/cognee-starter-kit\">starter repo</a>\n\n\n## About cognee\n\ncognee works locally and stores your data on your device.\nOur hosted solution is just our deployment of OSS cognee on Modal, with the goal of making development and productionization easier.\n\nSelf-hosted package:\n\n- Interconnects any kind of documents: past conversations, files, images, and audio transcriptions\n- Replaces RAG systems with a memory layer based on graphs and vectors\n- Reduces developer effort and cost, while increasing quality and precision\n- Provides Pythonic data pipelines that manage data ingestion from 30+ data sources\n- Is highly customizable with custom tasks, pipelines, and a set of built-in search endpoints\n\nHosted platform:\n- Includes a managed UI and a [hosted solution](https://www.cognee.ai)\n\n\n\n## Self-Hosted (Open Source)\n\n\n### \ud83d\udce6 Installation\n\nYou can install Cognee using either **pip**, **poetry**, **uv** or any other python package manager..\n\nCognee supports Python 3.10 to 3.12\n\n#### With uv\n\n```bash\nuv pip install cognee\n```\n\nDetailed instructions can be found in our [docs](https://docs.cognee.ai/getting-started/installation#environment-configuration)\n\n### \ud83d\udcbb Basic Usage\n\n#### Setup\n\n```\nimport os\nos.environ[\"LLM_API_KEY\"] = \"YOUR OPENAI_API_KEY\"\n\n```\n\nYou can also set the variables by creating .env file, using our <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">template.</a>\nTo use different LLM providers, for more info check out our <a href=\"https://docs.cognee.ai/setup-configuration/llm-providers\">documentation</a>\n\n\n#### Simple example\n\n\n\n##### Python\n\nThis script will run the default pipeline:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Add text to cognee\n    await cognee.add(\"Cognee turns documents into AI memory.\")\n\n    # Generate the knowledge graph\n    await cognee.cognify()\n\n    # Add memory algorithms to the graph\n    await cognee.memify()\n\n    # Query the knowledge graph\n    results = await cognee.search(\"What does cognee do?\")\n\n    # Display the results\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\nExample output:\n```\n  Cognee turns documents into AI memory.\n\n```"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//README.md\n##### Via CLI\n\nLet's get the basics covered\n\n```\ncognee-cli add \"Cognee turns documents into AI memory.\"\n\ncognee-cli cognify\n\ncognee-cli search \"What does cognee do?\"\ncognee-cli delete --all\n\n```\nor run\n```\ncognee-cli -ui\n```\n\n\n</div>\n\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [cogwit](https://www.cognee.ai)\n2. Add your API key to local UI and sync your data to Cogwit\n\n\n\n\n## Demos\n\n1. Cogwit Beta demo:\n\n[Cogwit Beta](https://github.com/user-attachments/assets/fa520cd2-2913-4246-a444-902ea5242cb0)\n\n2. Simple GraphRAG demo\n\n[Simple GraphRAG demo](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. cognee with Ollama\n\n[cognee with local models](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)\n\n\n## Contributing\nYour contributions are at the core of making this a true open source project. Any contributions you make are **greatly appreciated**. See [`CONTRIBUTING.md`](CONTRIBUTING.md) for more information.\n\n\n## Code of Conduct\n\nWe are committed to making open source an enjoyable and respectful experience for our community. See <a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> for more information.\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@misc{markovic2025optimizinginterfaceknowledgegraphs,\n      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning},\n      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},\n      year={2025},\n      eprint={2505.24478},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2505.24478},\n}\n```", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//README.md\n##### Via CLI\n\nLet's get the basics covered\n\n```\ncognee-cli add \"Cognee turns documents into AI memory.\"\n\ncognee-cli cognify\n\ncognee-cli search \"What does cognee do?\"\ncognee-cli delete --all\n\n```\nor run\n```\ncognee-cli -ui\n```\n\n\n</div>\n\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [cogwit](https://www.cognee.ai)\n2. Add your API key to local UI and sync your data to Cogwit\n\n\n\n\n## Demos\n\n1. Cogwit Beta demo:\n\n[Cogwit Beta](https://github.com/user-attachments/assets/fa520cd2-2913-4246-a444-902ea5242cb0)\n\n2. Simple GraphRAG demo\n\n[Simple GraphRAG demo](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. cognee with Ollama\n\n[cognee with local models](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)\n\n\n## Contributing\nYour contributions are at the core of making this a true open source project. Any contributions you make are **greatly appreciated**. See [`CONTRIBUTING.md`](CONTRIBUTING.md) for more information.\n\n\n## Code of Conduct\n\nWe are committed to making open source an enjoyable and respectful experience for our community. See <a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> for more information.\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@misc{markovic2025optimizinginterfaceknowledgegraphs,\n      title={Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning},\n      author={Vasilije Markovic and Lazar Obradovic and Laszlo Hajdu and Jovan Pavlovic},\n      year={2025},\n      eprint={2505.24478},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2505.24478},\n}\n```"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//SECURITY.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//SECURITY.md\n# Reporting Security Issues\nThe Cognee team takes security issues seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.\n\nTo report a security issue, email [info@topoteretes.com](mailto:info@topoteretes.com) and include the word \"SECURITY\" in the subject line.\n\nWe'll endeavor to respond quickly, and will keep you updated throughout the process.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//SECURITY.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//SECURITY.md\n# Reporting Security Issues\nThe Cognee team takes security issues seriously. We appreciate your efforts to responsibly disclose your findings, and will make every effort to acknowledge your contributions.\n\nTo report a security issue, email [info@topoteretes.com](mailto:info@topoteretes.com) and include the word \"SECURITY\" in the subject line.\n\nWe'll endeavor to respond quickly, and will keep you updated throughout the process."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//NOTICE.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//NOTICE.md\ntopoteretes - cognee\nCopyright \u00a9 2024 Topoteretes UG. (haftungsbeschr\u00e4nkt), Schonehauser Allee 163, Berlin.\n\nThis project includes software developed at Topoteretes UG. (https://www.cognee.ai/).\n\n<!-- Add software from external sources that you used here. e.g.\n\nThis project redistributes code originally from <website>.\n\n -->", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//NOTICE.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//NOTICE.md\ntopoteretes - cognee\nCopyright \u00a9 2024 Topoteretes UG. (haftungsbeschr\u00e4nkt), Schonehauser Allee 163, Berlin.\n\nThis project includes software developed at Topoteretes UG. (https://www.cognee.ai/).\n\n<!-- Add software from external sources that you used here. e.g.\n\nThis project redistributes code originally from <website>.\n\n -->"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//DCO.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//DCO.md\nDeveloper Certificate of Origin Version 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//DCO.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//DCO.md\nDeveloper Certificate of Origin Version 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n\n(c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n\n(d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n> [!IMPORTANT]\n> **Note for contributors:** When branching out, create a new branch from the `dev` branch.\n\n# \ud83c\udf89 Welcome to **cognee**! \n\nWe're excited that you're interested in contributing to our project! \nWe want to ensure that every user and contributor feels welcome, included and supported to participate in cognee community. \nThis guide will help you get started and ensure your contributions can be efficiently integrated into the project.\n\n## \ud83c\udf1f Quick Links\n\n- [Code of Conduct](CODE_OF_CONDUCT.md)\n- [Discord Community](https://discord.gg/bcy8xFAtfd)  \n- [Issue Tracker](https://github.com/topoteretes/cognee/issues)\n- [Cognee Docs](https://docs.cognee.ai)\n\n## 1. \ud83d\ude80 Ways to Contribute\n\nYou can contribute to **cognee** in many ways:\n\n- \ud83d\udcdd Submitting bug reports or feature requests\n- \ud83d\udca1 Improving documentation\n- \ud83d\udd0d Reviewing pull requests\n- \ud83d\udee0\ufe0f Contributing code or tests\n- \ud83c\udf10 Helping other users\n\n## \ud83d\udceb Get in Touch\n\nThere are several ways to connect with the **cognee** team and community:\n\n### GitHub Collaboration\n- [Open an issue](https://github.com/topoteretes/cognee/issues) for bug reports, feature requests, or discussions\n- Submit pull requests to contribute code or documentation\n- Join ongoing discussions in existing issues and PRs\n\n### Community Channels\n- Join our [Discord community](https://discord.gg/bcy8xFAtfd) for real-time discussions\n- Participate in community events and discussions\n- Get help from other community members\n\n### Direct Contact\n- Email: vasilije@cognee.ai\n- For business inquiries or sensitive matters, please reach out via email\n- For general questions, prefer public channels like GitHub issues or Discord\n\nWe aim to respond to all communications within 2 business days. For faster responses, consider using our Discord channel where the whole community can help!\n\n## Issue Labels\n\nTo help you find the most appropriate issues to work on, we use the following labels:\n\n- `good first issue` - Perfect for newcomers to the project\n- `bug` - Something isn't working as expected\n- `documentation` - Improvements or additions to documentation\n- `enhancement` - New features or improvements\n- `help wanted` - Extra attention or assistance needed\n- `question` - Further information is requested\n- `wontfix` - This will not be worked on\n\nLooking for a place to start? Try filtering for [good first issues](https://github.com/topoteretes/cognee/labels/good%20first%20issue)!\n\n\n## 2. \ud83d\udee0\ufe0f Development Setup\n\n### Fork and Clone\n\n1. Fork the [**cognee**](https://github.com/topoteretes/cognee) repository\n2. Clone your fork:\n```shell\ngit clone https://github.com/<your-github-username>/cognee.git\ncd cognee\n```\nIn case you are working on Vector and Graph Adapters\n1. Fork the [**cognee**](https://github.com/topoteretes/cognee-community) repository\n2. Clone your fork:\n```shell\ngit clone https://github.com/<your-github-username>/cognee-community.git\ncd cognee-community\n```", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n> [!IMPORTANT]\n> **Note for contributors:** When branching out, create a new branch from the `dev` branch.\n\n# \ud83c\udf89 Welcome to **cognee**! \n\nWe're excited that you're interested in contributing to our project! \nWe want to ensure that every user and contributor feels welcome, included and supported to participate in cognee community. \nThis guide will help you get started and ensure your contributions can be efficiently integrated into the project.\n\n## \ud83c\udf1f Quick Links\n\n- [Code of Conduct](CODE_OF_CONDUCT.md)\n- [Discord Community](https://discord.gg/bcy8xFAtfd)  \n- [Issue Tracker](https://github.com/topoteretes/cognee/issues)\n- [Cognee Docs](https://docs.cognee.ai)\n\n## 1. \ud83d\ude80 Ways to Contribute\n\nYou can contribute to **cognee** in many ways:\n\n- \ud83d\udcdd Submitting bug reports or feature requests\n- \ud83d\udca1 Improving documentation\n- \ud83d\udd0d Reviewing pull requests\n- \ud83d\udee0\ufe0f Contributing code or tests\n- \ud83c\udf10 Helping other users\n\n## \ud83d\udceb Get in Touch\n\nThere are several ways to connect with the **cognee** team and community:\n\n### GitHub Collaboration\n- [Open an issue](https://github.com/topoteretes/cognee/issues) for bug reports, feature requests, or discussions\n- Submit pull requests to contribute code or documentation\n- Join ongoing discussions in existing issues and PRs\n\n### Community Channels\n- Join our [Discord community](https://discord.gg/bcy8xFAtfd) for real-time discussions\n- Participate in community events and discussions\n- Get help from other community members\n\n### Direct Contact\n- Email: vasilije@cognee.ai\n- For business inquiries or sensitive matters, please reach out via email\n- For general questions, prefer public channels like GitHub issues or Discord\n\nWe aim to respond to all communications within 2 business days. For faster responses, consider using our Discord channel where the whole community can help!\n\n## Issue Labels\n\nTo help you find the most appropriate issues to work on, we use the following labels:\n\n- `good first issue` - Perfect for newcomers to the project\n- `bug` - Something isn't working as expected\n- `documentation` - Improvements or additions to documentation\n- `enhancement` - New features or improvements\n- `help wanted` - Extra attention or assistance needed\n- `question` - Further information is requested\n- `wontfix` - This will not be worked on\n\nLooking for a place to start? Try filtering for [good first issues](https://github.com/topoteretes/cognee/labels/good%20first%20issue)!\n\n\n## 2. \ud83d\udee0\ufe0f Development Setup\n\n### Fork and Clone\n\n1. Fork the [**cognee**](https://github.com/topoteretes/cognee) repository\n2. Clone your fork:\n```shell\ngit clone https://github.com/<your-github-username>/cognee.git\ncd cognee\n```\nIn case you are working on Vector and Graph Adapters\n1. Fork the [**cognee**](https://github.com/topoteretes/cognee-community) repository\n2. Clone your fork:\n```shell\ngit clone https://github.com/<your-github-username>/cognee-community.git\ncd cognee-community\n```"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n### Create a Branch\n\nCreate a new branch for your work:\n```shell\ngit checkout -b feature/your-feature-name\n```\n\n## 3. \ud83c\udfaf Making Changes\n\n1. **Code Style**: Follow the project's coding standards\n2. **Documentation**: Update relevant documentation\n3. **Tests**: Add tests for new features\n4. **Commits**: Write clear commit messages\n\n### Running Tests\n```shell\npython cognee/cognee/tests/test_library.py\n```\n\n## 4. \ud83d\udce4 Submitting Changes\n\n1. Install ruff on your system\n2. Run ```ruff format .``` and ``` ruff check ``` and fix the issues\n3. Push your changes:\n```shell\ngit add .\ngit commit -s -m \"Description of your changes\"\ngit push origin feature/your-feature-name\n```\n\n2. Create a Pull Request:\n   - Go to the [**cognee** repository](https://github.com/topoteretes/cognee) or [cognee community repository](https://github.com/topoteretes/cognee-community)\n   - Click \"Compare & Pull Request\" and open a PR against dev branch\n   - Fill in the PR template with details about your changes\n\n## 5. \ud83d\udcdc Developer Certificate of Origin (DCO)\n\nAll contributions must be signed-off to indicate agreement with our DCO:\n\n```shell\ngit config alias.cos \"commit -s\"  # Create alias for signed commits\n```\n\nWhen your PR is ready, please include:\n> \"I affirm that all code in every commit of this pull request conforms to the terms of the Topoteretes Developer Certificate of Origin\"\n\n## 6. \ud83e\udd1d Community Guidelines\n\n- Be respectful and inclusive\n- Help others learn and grow\n- Follow our [Code of Conduct](CODE_OF_CONDUCT.md)\n- Provide constructive feedback\n- Ask questions when unsure\n\n## 7. \ud83d\udceb Getting Help\n\n- Open an [issue](https://github.com/topoteretes/cognee/issues)\n- Join our Discord community\n- Check existing documentation\n\nThank you for contributing to **cognee**! \ud83c\udf1f", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main//CONTRIBUTING.md\n### Create a Branch\n\nCreate a new branch for your work:\n```shell\ngit checkout -b feature/your-feature-name\n```\n\n## 3. \ud83c\udfaf Making Changes\n\n1. **Code Style**: Follow the project's coding standards\n2. **Documentation**: Update relevant documentation\n3. **Tests**: Add tests for new features\n4. **Commits**: Write clear commit messages\n\n### Running Tests\n```shell\npython cognee/cognee/tests/test_library.py\n```\n\n## 4. \ud83d\udce4 Submitting Changes\n\n1. Install ruff on your system\n2. Run ```ruff format .``` and ``` ruff check ``` and fix the issues\n3. Push your changes:\n```shell\ngit add .\ngit commit -s -m \"Description of your changes\"\ngit push origin feature/your-feature-name\n```\n\n2. Create a Pull Request:\n   - Go to the [**cognee** repository](https://github.com/topoteretes/cognee) or [cognee community repository](https://github.com/topoteretes/cognee-community)\n   - Click \"Compare & Pull Request\" and open a PR against dev branch\n   - Fill in the PR template with details about your changes\n\n## 5. \ud83d\udcdc Developer Certificate of Origin (DCO)\n\nAll contributions must be signed-off to indicate agreement with our DCO:\n\n```shell\ngit config alias.cos \"commit -s\"  # Create alias for signed commits\n```\n\nWhen your PR is ready, please include:\n> \"I affirm that all code in every commit of this pull request conforms to the terms of the Topoteretes Developer Certificate of Origin\"\n\n## 6. \ud83e\udd1d Community Guidelines\n\n- Be respectful and inclusive\n- Help others learn and grow\n- Follow our [Code of Conduct](CODE_OF_CONDUCT.md)\n- Provide constructive feedback\n- Ask questions when unsure\n\n## 7. \ud83d\udceb Getting Help\n\n- Open an [issue](https://github.com/topoteretes/cognee/issues)\n- Join our Discord community\n- Check existing documentation\n\nThank you for contributing to **cognee**! \ud83c\udf1f"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    \"\"\"Return x doubled.\"\"\"\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = \"black\"\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/pep_style_guide.md\n# PEP 8 Style Guide: Essentials\n\n## Code Layout\n- Indentation: 4 spaces per level\n- Line length: 79 for code (88/100 acceptable by team), 72 for comments/docstrings\n- Blank lines: 2 around top-level defs/classes, 1 between methods\n\n```python\n# Hanging indent for long calls\nfoo = long_function_name(\n    var_one, var_two,\n    var_three, var_four,\n)\n```\n\n## Imports\n- One import per line\n- Group: stdlib, third-party, local\n- Prefer absolute imports; avoid wildcard imports\n\n```python\nimport os\nimport sys\nfrom subprocess import Popen, PIPE\n\nimport requests\n\nfrom myproject.models import User\n```\n\n## Whitespace\n- No space inside brackets or before commas/semicolons\n- Spaces around binary operators\n\n```python\nx = 1\nhypot2 = x * x + y * y\n```\n\n## Naming\n- snake_case: functions, variables\n- PascalCase: classes\n- SCREAMING_SNAKE_CASE: constants\n\n## Comments & Docstrings\n- Use complete sentences; keep up to date\n- Triple-double quotes for public modules, classes, functions\n```python\ndef f(x: int) -> int:\n    \"\"\"Return x doubled.\"\"\"\n    return x * 2\n```\n\n## Type Hints\n- Space after colon; arrow for returns\n```python\ndef munge(s: str) -> str: ...\n```\n\n## Tooling\n- Black, isort, Flake8 (or Ruff) to automate style\n- Example pyproject.toml excerpt:\n```toml\n[tool.black]\nline-length = 88\n\n[tool.isort]\nprofile = \"black\"\n```\n\n## Common Violations\n- E501: line too long -> break with parentheses\n- E225: missing whitespace around operator\n- E402: module import not at top of file"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/notebooks/data/zen_principles.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/zen_principles.md\n# The Zen of Python: Practical Guide\n\n## Overview\nThe Zen of Python (Tim Peters, import this) captures Python's philosophy. Use these principles as a checklist during design, coding, and reviews.\n\n## Key Principles With Guidance\n\n### 1. Beautiful is better than ugly\nPrefer descriptive names, clear structure, and consistent formatting.\n\n### 2. Explicit is better than implicit\nBe clear about behavior, imports, and types.\n```python\nfrom datetime import datetime, timedelta\n\ndef get_future_date(days_ahead: int) -> datetime:\n    return datetime.now() + timedelta(days=days_ahead)\n```\n\n### 3. Simple is better than complex\nChoose straightforward solutions first.\n\n### 4. Complex is better than complicated\nWhen complexity is needed, organize it with clear abstractions.\n\n### 5. Flat is better than nested\nUse early returns to reduce indentation.\n\n### 6. Sparse is better than dense\nGive code room to breathe with whitespace.\n\n### 7. Readability counts\nOptimize for human readers; add docstrings for nontrivial code.\n\n### 8. Special cases aren't special enough to break the rules\nStay consistent; exceptions should be rare and justified.\n\n### 9. Although practicality beats purity\nPrefer practical solutions that teams can maintain.\n\n### 10. Errors should never pass silently\nHandle exceptions explicitly; log with context.\n\n### 11. Unless explicitly silenced\nSilence only specific, acceptable errors and document why.\n\n### 12. In the face of ambiguity, refuse the temptation to guess\nRequire explicit inputs and behavior.\n\n### 13. There should be one obvious way to do it\nPrefer standard library patterns and idioms.\n\n### 14. Although that way may not be obvious at first\nLearn Python idioms; embrace clarity over novelty.\n\n### 15. Now is better than never; 16. Never is often better than right now\nIterate, but don't rush broken code.\n\n### 17/18. Hard to explain is bad; easy to explain is good\nPrefer designs you can explain simply.\n\n### 19. Namespaces are one honking great idea\nUse modules/packages to separate concerns; avoid wildcard imports.\n\n## Modern Python Tie-ins\n- Type hints reinforce explicitness\n- Context managers enforce safe resource handling\n- Dataclasses improve readability for data containers\n\n## Quick Review Checklist\n- Is it readable and explicit?\n- Is this the simplest working solution?\n- Are errors explicit and logged?\n- Are modules/namespaces used appropriately?", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/notebooks/data/zen_principles.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/zen_principles.md\n# The Zen of Python: Practical Guide\n\n## Overview\nThe Zen of Python (Tim Peters, import this) captures Python's philosophy. Use these principles as a checklist during design, coding, and reviews.\n\n## Key Principles With Guidance\n\n### 1. Beautiful is better than ugly\nPrefer descriptive names, clear structure, and consistent formatting.\n\n### 2. Explicit is better than implicit\nBe clear about behavior, imports, and types.\n```python\nfrom datetime import datetime, timedelta\n\ndef get_future_date(days_ahead: int) -> datetime:\n    return datetime.now() + timedelta(days=days_ahead)\n```\n\n### 3. Simple is better than complex\nChoose straightforward solutions first.\n\n### 4. Complex is better than complicated\nWhen complexity is needed, organize it with clear abstractions.\n\n### 5. Flat is better than nested\nUse early returns to reduce indentation.\n\n### 6. Sparse is better than dense\nGive code room to breathe with whitespace.\n\n### 7. Readability counts\nOptimize for human readers; add docstrings for nontrivial code.\n\n### 8. Special cases aren't special enough to break the rules\nStay consistent; exceptions should be rare and justified.\n\n### 9. Although practicality beats purity\nPrefer practical solutions that teams can maintain.\n\n### 10. Errors should never pass silently\nHandle exceptions explicitly; log with context.\n\n### 11. Unless explicitly silenced\nSilence only specific, acceptable errors and document why.\n\n### 12. In the face of ambiguity, refuse the temptation to guess\nRequire explicit inputs and behavior.\n\n### 13. There should be one obvious way to do it\nPrefer standard library patterns and idioms.\n\n### 14. Although that way may not be obvious at first\nLearn Python idioms; embrace clarity over novelty.\n\n### 15. Now is better than never; 16. Never is often better than right now\nIterate, but don't rush broken code.\n\n### 17/18. Hard to explain is bad; easy to explain is good\nPrefer designs you can explain simply.\n\n### 19. Namespaces are one honking great idea\nUse modules/packages to separate concerns; avoid wildcard imports.\n\n## Modern Python Tie-ins\n- Type hints reinforce explicitness\n- Context managers enforce safe resource handling\n- Dataclasses improve readability for data containers\n\n## Quick Review Checklist\n- Is it readable and explicit?\n- Is this the simplest working solution?\n- Are errors explicit and logged?\n- Are modules/namespaces used appropriately?"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md\nAssistant Guidelines\nThese rules are absolutely imperative to adhere to. Comply with them precisely as they are outlined.\n\nThe agent must use sequential thinking MCP tool to work out problems.\n\nCore Behavior Guidelines\n\nRespond only to explicit requests. Do not add files, code, tests, or comments unless asked.\n\nFollow instructions precisely. No assumptions or speculative additions.\n\nUse provided context accurately.\n\nAvoid extra output. No debugging logs or test harnesses unless requested.\n\nProduce clean, optimized code when code is requested. Respect existing style.\n\nDeliver complete, standalone solutions. No placeholders.\n\nLimit file creation. Only create new files when necessary.\n\nIf you modify the model in a user's code, you must confirm with the user and never be sneaky. Always tell the user exactly what you are doing.\n\nCommunication & Delivery\n\n9. Don't explain unless asked. Do not expose reasoning in outputs.\n10. If unsure, say \"I don't know.\" Avoid hallucinated content.\n11. Maintain consistency across sessions. Refer to project memory and documentation.\n12. Respect privacy and permissions. Never leak or infer secure data.\n13. Prioritize targeted edits over full rewrites.\n14. Optimize incrementally. Avoid unnecessary overhauls.\n\nSpec.md Requirement\n\nYou must maintain a file named Spec.md. This file acts as the single source of truth for the project.\n\nRules:\n\nBefore starting any implementation, check if Spec.md already exists.\n\nIf it does not exist, create one using the template provided below.\n\nAlways update Spec.md before and after any major change.\n\nUse the contents of Spec.md to guide logic, structure, and implementation decisions.\n\nWhen updating a section, condense previous content to keep the document concise.\n\nSpec.md Starter Template (Plain Text Format)\n\nTitle: Spec.md \u2013 Project Specification\n\nSection: Purpose\nDescribe the main goal of this feature, tool, or system.\n\nSection: Core Functionality\nList the key features, expected behaviors, and common use cases.\n\nSection: Architecture Overview\nSummarize the technical setup, frameworks used, and main modules or services.\n\nSection: Input and Output Contracts\nList all inputs and outputs in a table-like format:\n\nInput: describe the input data, its format, and where it comes from.\n\nOutput: describe the output data, its format, and its destination.\n\nSection: Edge Cases and Constraints\nList known limitations, special scenarios, and fallback behaviors.\n\nSection: File and Module Map\nList all important files or modules and describe what each one is responsible for.\n\nSection: Open Questions or TODOs\nCreate a checklist of unresolved decisions, logic that needs clarification, or tasks that are still pending.\n\nSection: Last Updated\nInclude the most recent update date and who made the update.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/notebooks/data/my_developer_rules.md\nAssistant Guidelines\nThese rules are absolutely imperative to adhere to. Comply with them precisely as they are outlined.\n\nThe agent must use sequential thinking MCP tool to work out problems.\n\nCore Behavior Guidelines\n\nRespond only to explicit requests. Do not add files, code, tests, or comments unless asked.\n\nFollow instructions precisely. No assumptions or speculative additions.\n\nUse provided context accurately.\n\nAvoid extra output. No debugging logs or test harnesses unless requested.\n\nProduce clean, optimized code when code is requested. Respect existing style.\n\nDeliver complete, standalone solutions. No placeholders.\n\nLimit file creation. Only create new files when necessary.\n\nIf you modify the model in a user's code, you must confirm with the user and never be sneaky. Always tell the user exactly what you are doing.\n\nCommunication & Delivery\n\n9. Don't explain unless asked. Do not expose reasoning in outputs.\n10. If unsure, say \"I don't know.\" Avoid hallucinated content.\n11. Maintain consistency across sessions. Refer to project memory and documentation.\n12. Respect privacy and permissions. Never leak or infer secure data.\n13. Prioritize targeted edits over full rewrites.\n14. Optimize incrementally. Avoid unnecessary overhauls.\n\nSpec.md Requirement\n\nYou must maintain a file named Spec.md. This file acts as the single source of truth for the project.\n\nRules:\n\nBefore starting any implementation, check if Spec.md already exists.\n\nIf it does not exist, create one using the template provided below.\n\nAlways update Spec.md before and after any major change.\n\nUse the contents of Spec.md to guide logic, structure, and implementation decisions.\n\nWhen updating a section, condense previous content to keep the document concise.\n\nSpec.md Starter Template (Plain Text Format)\n\nTitle: Spec.md \u2013 Project Specification\n\nSection: Purpose\nDescribe the main goal of this feature, tool, or system.\n\nSection: Core Functionality\nList the key features, expected behaviors, and common use cases.\n\nSection: Architecture Overview\nSummarize the technical setup, frameworks used, and main modules or services.\n\nSection: Input and Output Contracts\nList all inputs and outputs in a table-like format:\n\nInput: describe the input data, its format, and where it comes from.\n\nOutput: describe the output data, its format, and its destination.\n\nSection: Edge Cases and Constraints\nList known limitations, special scenarios, and fallback behaviors.\n\nSection: File and Module Map\nList all important files or modules and describe what each one is responsible for.\n\nSection: Open Questions or TODOs\nCreate a checklist of unresolved decisions, logic that needs clarification, or tasks that are still pending.\n\nSection: Last Updated\nInclude the most recent update date and who made the update."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/licenses/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/licenses/README.md\n# Third party licenses\n\nThis folder contains the licenses of third-party open-source software that has been redistributed in this project.\nDetails of included files and modifications can be found in [NOTICE](/NOTICE.md).", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/licenses/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/licenses/README.md\n# Third party licenses\n\nThis folder contains the licenses of third-party open-source software that has been redistributed in this project.\nDetails of included files and modifications can be found in [NOTICE](/NOTICE.md)."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee\u2011mcp -\u00a0Run cognee\u2019s memory engine as a Model\u00a0Context\u00a0Protocol server\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demo</a>\n  .\n  <a href=\"https://cognee.ai\">Learn more</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Join Discord</a>\n  \u00b7\n  <a href=\"https://www.reddit.com/r/AIMemory/\">Join r/AIMemory</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n<a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n<a href=\"https://trendshift.io/repositories/13955\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13955\" alt=\"topoteretes%2Fcognee | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n\nBuild memory for Agents and query from any client that speaks MCP\u00a0\u2013 in your terminal or IDE.\n\n</div>\n\n## \u2728 Features\n\n- Multiple transports \u2013 choose Streamable HTTP --transport http (recommended for web deployments), SSE --transport sse (real\u2011time streaming), or stdio (classic pipe, default)\n- **API Mode** \u2013 connect to an already running Cognee FastAPI server instead of using cognee directly (see [API Mode](#-api-mode) below)\n- Integrated logging \u2013 all actions written to a rotating file (see get_log_file_location()) and mirrored to console in dev\n- Local file ingestion \u2013 feed .md, source files, Cursor rule\u2011sets, etc. straight from disk\n- Background pipelines \u2013 long\u2011running cognify & codify jobs spawn off\u2011thread; check progress with status tools\n- Developer rules bootstrap \u2013 one call indexes .cursorrules, .cursor/rules, AGENT.md, and friends into the developer_rules nodeset\n- Prune & reset \u2013 wipe memory clean with a single prune call when you want to start fresh\n\nPlease refer to our documentation [here](https://docs.cognee.ai/how-to-guides/deployment/mcp) for further information.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee\u2011mcp -\u00a0Run cognee\u2019s memory engine as a Model\u00a0Context\u00a0Protocol server\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demo</a>\n  .\n  <a href=\"https://cognee.ai\">Learn more</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Join Discord</a>\n  \u00b7\n  <a href=\"https://www.reddit.com/r/AIMemory/\">Join r/AIMemory</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n<a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n<a href=\"https://trendshift.io/repositories/13955\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/13955\" alt=\"topoteretes%2Fcognee | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n\nBuild memory for Agents and query from any client that speaks MCP\u00a0\u2013 in your terminal or IDE.\n\n</div>\n\n## \u2728 Features\n\n- Multiple transports \u2013 choose Streamable HTTP --transport http (recommended for web deployments), SSE --transport sse (real\u2011time streaming), or stdio (classic pipe, default)\n- **API Mode** \u2013 connect to an already running Cognee FastAPI server instead of using cognee directly (see [API Mode](#-api-mode) below)\n- Integrated logging \u2013 all actions written to a rotating file (see get_log_file_location()) and mirrored to console in dev\n- Local file ingestion \u2013 feed .md, source files, Cursor rule\u2011sets, etc. straight from disk\n- Background pipelines \u2013 long\u2011running cognify & codify jobs spawn off\u2011thread; check progress with status tools\n- Developer rules bootstrap \u2013 one call indexes .cursorrules, .cursor/rules, AGENT.md, and friends into the developer_rules nodeset\n- Prune & reset \u2013 wipe memory clean with a single prune call when you want to start fresh\n\nPlease refer to our documentation [here](https://docs.cognee.ai/how-to-guides/deployment/mcp) for further information."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n## \ud83d\ude80 Quick\u00a0Start\n\n1. Clone cognee repo\n    ```\n    git clone https://github.com/topoteretes/cognee.git\n    ```\n2. Navigate to cognee-mcp subdirectory\n    ```\n    cd cognee/cognee-mcp\n    ```\n3. Install uv if you don't have one\n    ```\n    pip install uv\n    ```\n4. Install all the dependencies you need for cognee mcp server with uv\n    ```\n    uv sync --dev --all-extras --reinstall\n    ```\n5. Activate the virtual environment in cognee mcp directory\n    ```\n    source .venv/bin/activate\n    ```\n6. Set up your OpenAI API key in .env for a quick setup with the default cognee configurations\n    ```\n    LLM_API_KEY=\"YOUR_OPENAI_API_KEY\"\n    ```\n7. Run cognee mcp server with stdio (default)\n    ```\n    python src/server.py\n    ```\n    or stream responses over SSE\n    ```\n    python src/server.py --transport sse\n    ```\n    or run with Streamable HTTP transport (recommended for web deployments)\n    ```\n    python src/server.py --transport http --host 127.0.0.1 --port 8000 --path /mcp\n    ```\n\nYou can do more advanced configurations by creating .env file using our <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">template.</a>\nTo use different LLM providers / database configurations, and for more info check out our <a href=\"https://docs.cognee.ai\">documentation</a>.\n\n\n## \ud83d\udc33 Docker Usage\n\nIf you'd rather run cognee-mcp in a container, you have two options:\n\n1. **Build locally**\n   1. Make sure you are in /cognee root directory and have a fresh `.env` containing only your `LLM_API_KEY` (and your chosen settings).\n   2. Remove any old image and rebuild:\n      ```bash\n      docker rmi cognee/cognee-mcp:main || true\n      docker build --no-cache -f cognee-mcp/Dockerfile -t cognee/cognee-mcp:main .\n      ```\n   3. Run it:\n      ```bash\n      # For HTTP transport (recommended for web deployments)\n      docker run -e TRANSPORT_MODE=http --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n      # For SSE transport  \n      docker run -e TRANSPORT_MODE=sse --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n      # For stdio transport (default)\n      docker run -e TRANSPORT_MODE=stdio --env-file ./.env --rm -it cognee/cognee-mcp:main\n      ```\n2. **Pull from Docker Hub** (no build required):\n   ```bash\n   # With HTTP transport (recommended for web deployments)\n   docker run -e TRANSPORT_MODE=http --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n   # With SSE transport\n   docker run -e TRANSPORT_MODE=sse --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n   # With stdio transport (default)\n   docker run -e TRANSPORT_MODE=stdio --env-file ./.env --rm -it cognee/cognee-mcp:main\n   ```\n\n### **Important: Docker vs Direct Usage**\n**Docker uses environment variables**, not command line arguments:\n- \u2705 Docker: `-e TRANSPORT_MODE=http`\n- \u274c Docker: `--transport http` (won't work)\n\n**Direct Python usage** uses command line arguments:\n- \u2705 Direct: `python src/server.py --transport http`\n- \u274c Direct: `-e TRANSPORT_MODE=http` (won't work)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n## \ud83d\ude80 Quick\u00a0Start\n\n1. Clone cognee repo\n    ```\n    git clone https://github.com/topoteretes/cognee.git\n    ```\n2. Navigate to cognee-mcp subdirectory\n    ```\n    cd cognee/cognee-mcp\n    ```\n3. Install uv if you don't have one\n    ```\n    pip install uv\n    ```\n4. Install all the dependencies you need for cognee mcp server with uv\n    ```\n    uv sync --dev --all-extras --reinstall\n    ```\n5. Activate the virtual environment in cognee mcp directory\n    ```\n    source .venv/bin/activate\n    ```\n6. Set up your OpenAI API key in .env for a quick setup with the default cognee configurations\n    ```\n    LLM_API_KEY=\"YOUR_OPENAI_API_KEY\"\n    ```\n7. Run cognee mcp server with stdio (default)\n    ```\n    python src/server.py\n    ```\n    or stream responses over SSE\n    ```\n    python src/server.py --transport sse\n    ```\n    or run with Streamable HTTP transport (recommended for web deployments)\n    ```\n    python src/server.py --transport http --host 127.0.0.1 --port 8000 --path /mcp\n    ```\n\nYou can do more advanced configurations by creating .env file using our <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">template.</a>\nTo use different LLM providers / database configurations, and for more info check out our <a href=\"https://docs.cognee.ai\">documentation</a>.\n\n\n## \ud83d\udc33 Docker Usage\n\nIf you'd rather run cognee-mcp in a container, you have two options:\n\n1. **Build locally**\n   1. Make sure you are in /cognee root directory and have a fresh `.env` containing only your `LLM_API_KEY` (and your chosen settings).\n   2. Remove any old image and rebuild:\n      ```bash\n      docker rmi cognee/cognee-mcp:main || true\n      docker build --no-cache -f cognee-mcp/Dockerfile -t cognee/cognee-mcp:main .\n      ```\n   3. Run it:\n      ```bash\n      # For HTTP transport (recommended for web deployments)\n      docker run -e TRANSPORT_MODE=http --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n      # For SSE transport  \n      docker run -e TRANSPORT_MODE=sse --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n      # For stdio transport (default)\n      docker run -e TRANSPORT_MODE=stdio --env-file ./.env --rm -it cognee/cognee-mcp:main\n      ```\n2. **Pull from Docker Hub** (no build required):\n   ```bash\n   # With HTTP transport (recommended for web deployments)\n   docker run -e TRANSPORT_MODE=http --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n   # With SSE transport\n   docker run -e TRANSPORT_MODE=sse --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n   # With stdio transport (default)\n   docker run -e TRANSPORT_MODE=stdio --env-file ./.env --rm -it cognee/cognee-mcp:main\n   ```\n\n### **Important: Docker vs Direct Usage**\n**Docker uses environment variables**, not command line arguments:\n- \u2705 Docker: `-e TRANSPORT_MODE=http`\n- \u274c Docker: `--transport http` (won't work)\n\n**Direct Python usage** uses command line arguments:\n- \u2705 Direct: `python src/server.py --transport http`\n- \u274c Direct: `-e TRANSPORT_MODE=http` (won't work)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n### **Docker API Mode**\n\nTo connect the MCP Docker container to a Cognee API server running on your host machine:\n\n#### **Simple Usage (Automatic localhost handling):**\n```bash\n# Start your Cognee API server on the host\npython -m cognee.api.client\n\n# Run MCP container in API mode - localhost is automatically converted!\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://localhost:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n```\n**Note:** The container will automatically convert `localhost` to `host.docker.internal` on Mac/Windows/Docker Desktop. You'll see a message in the logs showing the conversion.\n\n#### **Explicit host.docker.internal (Mac/Windows):**\n```bash\n# Or explicitly use host.docker.internal\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://host.docker.internal:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n```\n\n#### **On Linux (use host network or container IP):**\n```bash\n# Option 1: Use host network (simplest)\ndocker run \\\n  --network host \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://localhost:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  --rm -it cognee/cognee-mcp:main\n\n# Option 2: Use host IP address\n# First, get your host IP: ip addr show docker0\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://172.17.0.1:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n```\n\n**Environment variables for API mode:**\n- `API_URL`: URL of the running Cognee API server\n- `API_TOKEN`: Authentication token (optional, required if API has authentication enabled)\n\n**Note:** When running in API mode:\n- Database migrations are automatically skipped (API server handles its own DB)\n- Some features are limited (see [API Mode Limitations](#-api-mode))\n\n\n## \ud83d\udd17 MCP Client Configuration\n\nAfter starting your Cognee MCP server with Docker, you need to configure your MCP client to connect to it.\n\n### **SSE Transport Configuration** (Recommended)\n\n**Start the server with SSE transport:**\n```bash\ndocker run -e TRANSPORT_MODE=sse --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n```\n\n**Configure your MCP client:**\n\n#### **Claude CLI (Easiest)**\n```bash\nclaude mcp add cognee-sse -t sse http://localhost:8000/sse\n```\n\n**Verify the connection:**\n```bash\nclaude mcp list\n```\n\nYou should see your server connected:\n```\nChecking MCP server health...\n\ncognee-sse: http://localhost:8000/sse (SSE) - \u2713 Connected\n```\n\n#### **Manual Configuration**\n\n**Claude (`~/.claude.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee\": {\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n**Cursor (`~/.cursor/mcp.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee-sse\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n### **Docker API Mode**\n\nTo connect the MCP Docker container to a Cognee API server running on your host machine:\n\n#### **Simple Usage (Automatic localhost handling):**\n```bash\n# Start your Cognee API server on the host\npython -m cognee.api.client\n\n# Run MCP container in API mode - localhost is automatically converted!\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://localhost:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n```\n**Note:** The container will automatically convert `localhost` to `host.docker.internal` on Mac/Windows/Docker Desktop. You'll see a message in the logs showing the conversion.\n\n#### **Explicit host.docker.internal (Mac/Windows):**\n```bash\n# Or explicitly use host.docker.internal\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://host.docker.internal:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n```\n\n#### **On Linux (use host network or container IP):**\n```bash\n# Option 1: Use host network (simplest)\ndocker run \\\n  --network host \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://localhost:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  --rm -it cognee/cognee-mcp:main\n\n# Option 2: Use host IP address\n# First, get your host IP: ip addr show docker0\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://172.17.0.1:8000 \\\n  -e API_TOKEN=your_auth_token \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n```\n\n**Environment variables for API mode:**\n- `API_URL`: URL of the running Cognee API server\n- `API_TOKEN`: Authentication token (optional, required if API has authentication enabled)\n\n**Note:** When running in API mode:\n- Database migrations are automatically skipped (API server handles its own DB)\n- Some features are limited (see [API Mode Limitations](#-api-mode))\n\n\n## \ud83d\udd17 MCP Client Configuration\n\nAfter starting your Cognee MCP server with Docker, you need to configure your MCP client to connect to it.\n\n### **SSE Transport Configuration** (Recommended)\n\n**Start the server with SSE transport:**\n```bash\ndocker run -e TRANSPORT_MODE=sse --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n```\n\n**Configure your MCP client:**\n\n#### **Claude CLI (Easiest)**\n```bash\nclaude mcp add cognee-sse -t sse http://localhost:8000/sse\n```\n\n**Verify the connection:**\n```bash\nclaude mcp list\n```\n\nYou should see your server connected:\n```\nChecking MCP server health...\n\ncognee-sse: http://localhost:8000/sse (SSE) - \u2713 Connected\n```\n\n#### **Manual Configuration**\n\n**Claude (`~/.claude.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee\": {\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```\n\n**Cursor (`~/.cursor/mcp.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee-sse\": {\n      \"url\": \"http://localhost:8000/sse\"\n    }\n  }\n}\n```"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n### **HTTP Transport Configuration** (Alternative)\n\n**Start the server with HTTP transport:**\n```bash\ndocker run -e TRANSPORT_MODE=http --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n```\n\n**Configure your MCP client:**\n\n#### **Claude CLI (Easiest)**\n```bash\nclaude mcp add cognee-http -t http http://localhost:8000/mcp\n```\n\n**Verify the connection:**\n```bash\nclaude mcp list\n```\n\nYou should see your server connected:\n```\nChecking MCP server health...\n\ncognee-http: http://localhost:8000/mcp (HTTP) - \u2713 Connected\n```\n\n#### **Manual Configuration**\n\n**Claude (`~/.claude.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n**Cursor (`~/.cursor/mcp.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee-http\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n### **Dual Configuration Example**\nYou can configure both transports simultaneously for testing:\n\n```json\n{\n  \"mcpServers\": {\n    \"cognee-sse\": {\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:8000/sse\"\n    },\n    \"cognee-http\": {\n      \"type\": \"http\", \n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n**Note:** Only enable the server you're actually running to avoid connection errors.\n\n## \ud83c\udf10 API Mode\n\nThe MCP server can operate in two modes:\n\n### **Direct Mode** (Default)\nThe MCP server directly imports and uses the cognee library. This is the default mode with full feature support.\n\n### **API Mode**\nThe MCP server connects to an already running Cognee FastAPI server via HTTP requests. This is useful when:\n- You have a centralized Cognee API server running\n- You want to separate the MCP server from the knowledge graph backend\n- You need multiple MCP servers to share the same knowledge graph\n\n**Starting the MCP server in API mode:**\n```bash\n# Start your Cognee FastAPI server first (default port 8000)\ncd /path/to/cognee\npython -m cognee.api.client\n\n# Then start the MCP server in API mode\ncd cognee-mcp\npython src/server.py --api-url http://localhost:8000 --api-token YOUR_AUTH_TOKEN\n```\n\n**API Mode with different transports:**\n```bash\n# With SSE transport\npython src/server.py --transport sse --api-url http://localhost:8000 --api-token YOUR_TOKEN\n\n# With HTTP transport\npython src/server.py --transport http --api-url http://localhost:8000 --api-token YOUR_TOKEN\n```\n\n**API Mode with Docker:**\n```bash\n# On Mac/Windows (use host.docker.internal to access host)\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://host.docker.internal:8000 \\\n  -e API_TOKEN=YOUR_TOKEN \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n\n# On Linux (use host network)\ndocker run \\\n  --network host \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://localhost:8000 \\\n  -e API_TOKEN=YOUR_TOKEN \\\n  --rm -it cognee/cognee-mcp:main\n```\n\n**Command-line arguments for API mode:**\n- `--api-url`: Base URL of the running Cognee FastAPI server (e.g., `http://localhost:8000`)\n- `--api-token`: Authentication token for the API (optional, required if API has authentication enabled)\n\n**Docker environment variables for API mode:**\n- `API_URL`: Base URL of the running Cognee FastAPI server\n- `API_TOKEN`: Authentication token (optional, required if API has authentication enabled)\n\n**API Mode limitations:**\nSome features are only available in direct mode:\n- `codify` (code graph pipeline)\n- `cognify_status` / `codify_status` (pipeline status tracking)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n### **HTTP Transport Configuration** (Alternative)\n\n**Start the server with HTTP transport:**\n```bash\ndocker run -e TRANSPORT_MODE=http --env-file ./.env -p 8000:8000 --rm -it cognee/cognee-mcp:main\n```\n\n**Configure your MCP client:**\n\n#### **Claude CLI (Easiest)**\n```bash\nclaude mcp add cognee-http -t http http://localhost:8000/mcp\n```\n\n**Verify the connection:**\n```bash\nclaude mcp list\n```\n\nYou should see your server connected:\n```\nChecking MCP server health...\n\ncognee-http: http://localhost:8000/mcp (HTTP) - \u2713 Connected\n```\n\n#### **Manual Configuration**\n\n**Claude (`~/.claude.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee\": {\n      \"type\": \"http\",\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n**Cursor (`~/.cursor/mcp.json`)**\n```json\n{\n  \"mcpServers\": {\n    \"cognee-http\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n### **Dual Configuration Example**\nYou can configure both transports simultaneously for testing:\n\n```json\n{\n  \"mcpServers\": {\n    \"cognee-sse\": {\n      \"type\": \"sse\",\n      \"url\": \"http://localhost:8000/sse\"\n    },\n    \"cognee-http\": {\n      \"type\": \"http\", \n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n**Note:** Only enable the server you're actually running to avoid connection errors.\n\n## \ud83c\udf10 API Mode\n\nThe MCP server can operate in two modes:\n\n### **Direct Mode** (Default)\nThe MCP server directly imports and uses the cognee library. This is the default mode with full feature support.\n\n### **API Mode**\nThe MCP server connects to an already running Cognee FastAPI server via HTTP requests. This is useful when:\n- You have a centralized Cognee API server running\n- You want to separate the MCP server from the knowledge graph backend\n- You need multiple MCP servers to share the same knowledge graph\n\n**Starting the MCP server in API mode:**\n```bash\n# Start your Cognee FastAPI server first (default port 8000)\ncd /path/to/cognee\npython -m cognee.api.client\n\n# Then start the MCP server in API mode\ncd cognee-mcp\npython src/server.py --api-url http://localhost:8000 --api-token YOUR_AUTH_TOKEN\n```\n\n**API Mode with different transports:**\n```bash\n# With SSE transport\npython src/server.py --transport sse --api-url http://localhost:8000 --api-token YOUR_TOKEN\n\n# With HTTP transport\npython src/server.py --transport http --api-url http://localhost:8000 --api-token YOUR_TOKEN\n```\n\n**API Mode with Docker:**\n```bash\n# On Mac/Windows (use host.docker.internal to access host)\ndocker run \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://host.docker.internal:8000 \\\n  -e API_TOKEN=YOUR_TOKEN \\\n  -p 8001:8000 \\\n  --rm -it cognee/cognee-mcp:main\n\n# On Linux (use host network)\ndocker run \\\n  --network host \\\n  -e TRANSPORT_MODE=sse \\\n  -e API_URL=http://localhost:8000 \\\n  -e API_TOKEN=YOUR_TOKEN \\\n  --rm -it cognee/cognee-mcp:main\n```\n\n**Command-line arguments for API mode:**\n- `--api-url`: Base URL of the running Cognee FastAPI server (e.g., `http://localhost:8000`)\n- `--api-token`: Authentication token for the API (optional, required if API has authentication enabled)\n\n**Docker environment variables for API mode:**\n- `API_URL`: Base URL of the running Cognee FastAPI server\n- `API_TOKEN`: Authentication token (optional, required if API has authentication enabled)\n\n**API Mode limitations:**\nSome features are only available in direct mode:\n- `codify` (code graph pipeline)\n- `cognify_status` / `codify_status` (pipeline status tracking)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n- `prune` (data reset)\n- `get_developer_rules` (developer rules retrieval)\n- `list_data` with specific dataset_id (detailed data listing)\n\nBasic operations like `cognify`, `search`, `delete`, and `list_data` (all datasets) work in both modes.\n\n## \ud83d\udcbb Basic Usage\n\nThe MCP server exposes its functionality through tools. Call them from any MCP client (Cursor, Claude Desktop, Cline, Roo and more).\n\n\n### Available Tools\n\n- **cognify**: Turns your data into a structured knowledge graph and stores it in memory\n\n- **codify**: Analyse a code repository, build a code graph, stores it in memory\n\n- **search**: Query memory \u2013 supports GRAPH_COMPLETION, RAG_COMPLETION, CODE, CHUNKS\n\n- **list_data**: List all datasets and their data items with IDs for deletion operations\n\n- **delete**: Delete specific data from a dataset (supports soft/hard deletion modes)\n\n- **prune**: Reset cognee for a fresh start (removes all data)\n\n- **cognify_status / codify_status**: Track pipeline progress\n\n**Data Management Examples:**\n```bash\n# List all available datasets and data items\nlist_data()\n\n# List data items in a specific dataset\nlist_data(dataset_id=\"your-dataset-id-here\")\n\n# Delete specific data (soft deletion - safer, preserves shared entities)\ndelete(data_id=\"data-uuid\", dataset_id=\"dataset-uuid\", mode=\"soft\")\n\n# Delete specific data (hard deletion - removes orphaned entities)\ndelete(data_id=\"data-uuid\", dataset_id=\"dataset-uuid\", mode=\"hard\")\n```\n\n\n## Development and Debugging\n\n### Debugging\n\nTo use debugger, run:\n    ```bash\n    mcp dev src/server.py\n    ```\n\nOpen inspector with timeout passed:\n    ```\n    http://localhost:5173?timeout=120000\n    ```\n\nTo apply new changes while developing cognee you need to do:\n\n1. Update dependencies in cognee folder if needed\n2. `uv sync --dev --all-extras --reinstall`\n3. `mcp dev src/server.py`\n\n### Development\n\nIn order to use local cognee:\n\n1. Uncomment the following line in the cognee-mcp [`pyproject.toml`](pyproject.toml) file and set the cognee root path.\n    ```\n    #\"cognee[postgres,codegraph,gemini,huggingface,docs,neo4j] @ file:/Users/<username>/Desktop/cognee\"\n    ```\n    Remember to replace `file:/Users/<username>/Desktop/cognee` with your actual cognee root path.\n\n2. Install dependencies with uv in the mcp folder\n    ```\n    uv sync --reinstall\n    ```\n\n## Code of Conduct\n\nWe are committed to making open source an enjoyable and respectful experience for our community. See <a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> for more information.\n\n## \ud83d\udcab Contributors\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-mcp/README.md\n- `prune` (data reset)\n- `get_developer_rules` (developer rules retrieval)\n- `list_data` with specific dataset_id (detailed data listing)\n\nBasic operations like `cognify`, `search`, `delete`, and `list_data` (all datasets) work in both modes.\n\n## \ud83d\udcbb Basic Usage\n\nThe MCP server exposes its functionality through tools. Call them from any MCP client (Cursor, Claude Desktop, Cline, Roo and more).\n\n\n### Available Tools\n\n- **cognify**: Turns your data into a structured knowledge graph and stores it in memory\n\n- **codify**: Analyse a code repository, build a code graph, stores it in memory\n\n- **search**: Query memory \u2013 supports GRAPH_COMPLETION, RAG_COMPLETION, CODE, CHUNKS\n\n- **list_data**: List all datasets and their data items with IDs for deletion operations\n\n- **delete**: Delete specific data from a dataset (supports soft/hard deletion modes)\n\n- **prune**: Reset cognee for a fresh start (removes all data)\n\n- **cognify_status / codify_status**: Track pipeline progress\n\n**Data Management Examples:**\n```bash\n# List all available datasets and data items\nlist_data()\n\n# List data items in a specific dataset\nlist_data(dataset_id=\"your-dataset-id-here\")\n\n# Delete specific data (soft deletion - safer, preserves shared entities)\ndelete(data_id=\"data-uuid\", dataset_id=\"dataset-uuid\", mode=\"soft\")\n\n# Delete specific data (hard deletion - removes orphaned entities)\ndelete(data_id=\"data-uuid\", dataset_id=\"dataset-uuid\", mode=\"hard\")\n```\n\n\n## Development and Debugging\n\n### Debugging\n\nTo use debugger, run:\n    ```bash\n    mcp dev src/server.py\n    ```\n\nOpen inspector with timeout passed:\n    ```\n    http://localhost:5173?timeout=120000\n    ```\n\nTo apply new changes while developing cognee you need to do:\n\n1. Update dependencies in cognee folder if needed\n2. `uv sync --dev --all-extras --reinstall`\n3. `mcp dev src/server.py`\n\n### Development\n\nIn order to use local cognee:\n\n1. Uncomment the following line in the cognee-mcp [`pyproject.toml`](pyproject.toml) file and set the cognee root path.\n    ```\n    #\"cognee[postgres,codegraph,gemini,huggingface,docs,neo4j] @ file:/Users/<username>/Desktop/cognee\"\n    ```\n    Remember to replace `file:/Users/<username>/Desktop/cognee` with your actual cognee root path.\n\n2. Install dependencies with uv in the mcp folder\n    ```\n    uv sync --reinstall\n    ```\n\n## Code of Conduct\n\nWe are committed to making open source an enjoyable and respectful experience for our community. See <a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> for more information.\n\n## \ud83d\udcab Contributors\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/evals/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/README.md\n# QA Evaluation\n\nRepeated runs of QA evaluation on 24-item HotpotQA subset, comparing Mem0, Graphiti, LightRAG, and Cognee (multiple retriever configs). Uses Modal for distributed benchmark execution.\n\n## Dataset\n\n- `hotpot_qa_24_corpus.json` and `hotpot_qa_24_qa_pairs.json`\n- `hotpot_qa_24_instance_filter.json` for instance filtering\n\n## Systems Evaluated\n\n- **Mem0**: OpenAI-based memory QA system\n- **Graphiti**: LangChain + Neo4j knowledge graph QA\n- **LightRAG**: Falkor's GraphRAG-SDK\n- **Cognee**: Multiple retriever configurations (GRAPH_COMPLETION, GRAPH_COMPLETION_COT, GRAPH_COMPLETION_CONTEXT_EXTENSION)\n\n## Project Structure\n\n- `src/` - Analysis scripts and QA implementations\n- `src/modal_apps/` - Modal deployment configurations\n- `src/qa/` - QA benchmark classes\n- `src/helpers/` and `src/analysis/` - Utilities\n\n**Notes:**\n- Use `PyProject.toml` for dependencies\n- Ensure Modal CLI is configured\n- Modular QA benchmark classes enable parallel execution on other platforms beyond Modal\n\n\n## Running Benchmarks (Modal)\n\nExecute repeated runs via Modal apps:\n- `modal run modal_apps/modal_qa_benchmark_<system>.py`\n\nWhere `<system>` is one of: `mem0`, `graphiti`, `lightrag`, `cognee`\n\nRaw results stored in Modal volumes under `/qa-benchmarks/<benchmark>/{answers,evaluated}`\n\n## Results Analysis\n\n- `python run_cross_benchmark_analysis.py`\n- Downloads Modal volumes, processes evaluated JSONs\n- Generates per-benchmark CSVs and cross-benchmark summary\n- Use `visualize_benchmarks.py` to create comparison charts\n\n## Results\n\n- **45 evaluation cycles** on 24 HotPotQA questions with multiple metrics (EM, F1, DeepEval Correctness, Human-like Correctness)\n- **Significant variance** observed in metrics across small runs due to LLM-as-judge inconsistencies\n- **Cognee showed consistent improvements** across all measured dimensions compared to Mem0, Lightrag, and Graphiti\n\n### Visualization Results\n\nThe following charts visualize the benchmark results and performance comparisons:\n\n#### Comprehensive Metrics Comparison\n![Comprehensive Metrics Comparison](comprehensive_metrics_comparison.png)\n\nA comprehensive comparison of all evaluated systems across multiple metrics, showing Cognee's performance relative to Mem0, Graphiti, and LightRAG.\n\n\n\n\n#### Optimized Cognee Configurations\n![Optimized Cognee Configurations](optimized_cognee_configurations.png)\n\nPerformance analysis of different Cognee retriever configurations (GRAPH_COMPLETION, GRAPH_COMPLETION_COT, GRAPH_COMPLETION_CONTEXT_EXTENSION), showing optimization results.\n\n## Notes\n\n- **Traditional QA metrics (EM/F1)** miss core value of AI memory systems - measure letter/word differences rather than information content\n- **HotPotQA benchmark mismatch** - designed for multi-hop reasoning but operates in constrained contexts vs. real-world cross-context linking\n- **DeepEval variance** - LLM-as-judge evaluation carries inconsistencies of underlying language model", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/evals/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/README.md\n# QA Evaluation\n\nRepeated runs of QA evaluation on 24-item HotpotQA subset, comparing Mem0, Graphiti, LightRAG, and Cognee (multiple retriever configs). Uses Modal for distributed benchmark execution.\n\n## Dataset\n\n- `hotpot_qa_24_corpus.json` and `hotpot_qa_24_qa_pairs.json`\n- `hotpot_qa_24_instance_filter.json` for instance filtering\n\n## Systems Evaluated\n\n- **Mem0**: OpenAI-based memory QA system\n- **Graphiti**: LangChain + Neo4j knowledge graph QA\n- **LightRAG**: Falkor's GraphRAG-SDK\n- **Cognee**: Multiple retriever configurations (GRAPH_COMPLETION, GRAPH_COMPLETION_COT, GRAPH_COMPLETION_CONTEXT_EXTENSION)\n\n## Project Structure\n\n- `src/` - Analysis scripts and QA implementations\n- `src/modal_apps/` - Modal deployment configurations\n- `src/qa/` - QA benchmark classes\n- `src/helpers/` and `src/analysis/` - Utilities\n\n**Notes:**\n- Use `PyProject.toml` for dependencies\n- Ensure Modal CLI is configured\n- Modular QA benchmark classes enable parallel execution on other platforms beyond Modal\n\n\n## Running Benchmarks (Modal)\n\nExecute repeated runs via Modal apps:\n- `modal run modal_apps/modal_qa_benchmark_<system>.py`\n\nWhere `<system>` is one of: `mem0`, `graphiti`, `lightrag`, `cognee`\n\nRaw results stored in Modal volumes under `/qa-benchmarks/<benchmark>/{answers,evaluated}`\n\n## Results Analysis\n\n- `python run_cross_benchmark_analysis.py`\n- Downloads Modal volumes, processes evaluated JSONs\n- Generates per-benchmark CSVs and cross-benchmark summary\n- Use `visualize_benchmarks.py` to create comparison charts\n\n## Results\n\n- **45 evaluation cycles** on 24 HotPotQA questions with multiple metrics (EM, F1, DeepEval Correctness, Human-like Correctness)\n- **Significant variance** observed in metrics across small runs due to LLM-as-judge inconsistencies\n- **Cognee showed consistent improvements** across all measured dimensions compared to Mem0, Lightrag, and Graphiti\n\n### Visualization Results\n\nThe following charts visualize the benchmark results and performance comparisons:\n\n#### Comprehensive Metrics Comparison\n![Comprehensive Metrics Comparison](comprehensive_metrics_comparison.png)\n\nA comprehensive comparison of all evaluated systems across multiple metrics, showing Cognee's performance relative to Mem0, Graphiti, and LightRAG.\n\n\n\n\n#### Optimized Cognee Configurations\n![Optimized Cognee Configurations](optimized_cognee_configurations.png)\n\nPerformance analysis of different Cognee retriever configurations (GRAPH_COMPLETION, GRAPH_COMPLETION_COT, GRAPH_COMPLETION_CONTEXT_EXTENSION), showing optimization results.\n\n## Notes\n\n- **Traditional QA metrics (EM/F1)** miss core value of AI memory systems - measure letter/word differences rather than information content\n- **HotPotQA benchmark mismatch** - designed for multi-hop reasoning but operates in constrained contexts vs. real-world cross-context linking\n- **DeepEval variance** - LLM-as-judge evaluation carries inconsistencies of underlying language model"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/evals/old/comparative_eval/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/old/comparative_eval/README.md\n# Comparative QA Benchmarks\n\nIndependent benchmarks for different QA/RAG systems using HotpotQA dataset.\n\n## Dataset Files\n- `hotpot_50_corpus.json` - 50 instances from HotpotQA\n- `hotpot_50_qa_pairs.json` - Corresponding question-answer pairs\n\n## Benchmarks\n\nEach benchmark can be run independently with appropriate dependencies:\n\n### Mem0\n```bash\npip install mem0ai openai\npython qa_benchmark_mem0.py\n```\n\n### LightRAG\n```bash\npip install \"lightrag-hku[api]\"\npython qa_benchmark_lightrag.py\n```\n\n### Graphiti\n```bash\npip install graphiti-core\npython qa_benchmark_graphiti.py\n```\n\n## Environment\nCreate `.env` with required API keys:\n- `OPENAI_API_KEY` (all benchmarks)\n- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` (Graphiti only)\n\n## Usage\nEach benchmark inherits from `QABenchmarkRAG` base class and can be configured independently.\n\n# Results\nUpdated results will be posted soon.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/evals/old/comparative_eval/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/evals/old/comparative_eval/README.md\n# Comparative QA Benchmarks\n\nIndependent benchmarks for different QA/RAG systems using HotpotQA dataset.\n\n## Dataset Files\n- `hotpot_50_corpus.json` - 50 instances from HotpotQA\n- `hotpot_50_qa_pairs.json` - Corresponding question-answer pairs\n\n## Benchmarks\n\nEach benchmark can be run independently with appropriate dependencies:\n\n### Mem0\n```bash\npip install mem0ai openai\npython qa_benchmark_mem0.py\n```\n\n### LightRAG\n```bash\npip install \"lightrag-hku[api]\"\npython qa_benchmark_lightrag.py\n```\n\n### Graphiti\n```bash\npip install graphiti-core\npython qa_benchmark_graphiti.py\n```\n\n## Environment\nCreate `.env` with required API keys:\n- `OPENAI_API_KEY` (all benchmarks)\n- `NEO4J_URI`, `NEO4J_USER`, `NEO4J_PASSWORD` (Graphiti only)\n\n## Usage\nEach benchmark inherits from `QABenchmarkRAG` base class and can be configured independently.\n\n# Results\nUpdated results will be posted soon."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\nCognee - \u044d\u0442\u043e \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u0434\u043b\u044f \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0430\u043c\u044f\u0442\u044c\u044e \u0418\u0418, \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u0430\u044f \u0434\u043b\u044f \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0438 \u043d\u0430\u0434\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 (LLM) \u0438 \u0418\u0418-\u0430\u0433\u0435\u043d\u0442\u043e\u0432.\n\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">\u0414\u0435\u043c\u043e</a>\n  \u00b7\n  <a href=\"https://cognee.ai\">\u0423\u0437\u043d\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">\u041f\u0440\u0438\u0441\u043e\u0435\u0434\u0438\u043d\u0438\u0442\u044c\u0441\u044f \u043a Discord</a>\n</p>\n\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n<a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n\n\u0421\u043e\u0437\u0434\u0430\u0432\u0430\u0439 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043f\u0430\u043c\u044f\u0442\u044c \u0434\u043b\u044f \u0430\u0433\u0435\u043d\u0442\u043e\u0432, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f ECL (Extract -> Cognify -> Load) \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440.\n\n\u0423\u0437\u043d\u0430\u0439\u0442\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u043e [\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f](https://docs.cognee.ai/use-cases) \u0438 [\u0431\u0435\u043d\u0447\u043c\u0430\u0440\u043a\u0430\u0445](https://github.com/topoteretes/cognee/tree/main/evals)\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"\u041f\u043e\u0447\u0435\u043c\u0443 cognee?\" width=\"50%\" />\n</div>\n\n</div>\n\n\n\n## \u0424\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\n\n- \u0418\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044f \u0438 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445: \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0442\u044c \u0438 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043f\u0440\u043e\u0448\u043b\u044b\u0435 \u0440\u0430\u0437\u0433\u043e\u0432\u043e\u0440\u044b, \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u0430\u0443\u0434\u0438\u043e\u0437\u0430\u043f\u0438\u0441\u0438, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044f \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u043c \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n- \u0421\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0433\u0430\u043b\u043b\u044e\u0446\u0438\u043d\u0430\u0446\u0438\u0439 \u0438 \u0437\u0430\u0442\u0440\u0430\u0442: \u0423\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432, \u0441\u043d\u0438\u0436\u0430\u0435\u0442 \u0437\u0430\u0442\u0440\u0430\u0442\u044b \u043d\u0430 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0438 \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044e \u0418\u0418-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439.\n\n- \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c Pydantic: \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0433\u0440\u0430\u0444\u043e\u0432\u044b\u0435 \u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e Pydantic, \u0443\u043f\u0440\u043e\u0449\u0430\u044f \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438.\n\n- \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0438 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445: \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435, \u0441\u043e\u0431\u0438\u0440\u0430\u044f \u0438\u0445 \u0438\u0437 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u043c 30 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432, \u0432\u043a\u043b\u044e\u0447\u0430\u044f PDF, \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u0444\u043e\u0440\u043c\u0430\u0442\u044b.\n\n- \u041c\u043e\u0434\u0443\u043b\u044c\u043d\u044b\u0435 ECL-\u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u044b: \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043c\u043e\u0434\u0443\u043b\u044c\u043d\u044b\u0435 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u044b Extract, Cognify, Load (ECL) \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0447\u0442\u043e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u0433\u0438\u0431\u043a\u043e\u0441\u0442\u044c \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c \u0441\u0438\u0441\u0442\u0435\u043c\u044b.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\nCognee - \u044d\u0442\u043e \u043f\u043b\u0430\u0442\u0444\u043e\u0440\u043c\u0430 \u0434\u043b\u044f \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u043f\u0430\u043c\u044f\u0442\u044c\u044e \u0418\u0418, \u043f\u0440\u0435\u0434\u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u0430\u044f \u0434\u043b\u044f \u043f\u043e\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u0438 \u043d\u0430\u0434\u0435\u0436\u043d\u043e\u0441\u0442\u0438 \u043e\u0442\u0432\u0435\u0442\u043e\u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u044f\u0437\u044b\u043a\u043e\u0432\u044b\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 (LLM) \u0438 \u0418\u0418-\u0430\u0433\u0435\u043d\u0442\u043e\u0432.\n\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">\u0414\u0435\u043c\u043e</a>\n  \u00b7\n  <a href=\"https://cognee.ai\">\u0423\u0437\u043d\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">\u041f\u0440\u0438\u0441\u043e\u0435\u0434\u0438\u043d\u0438\u0442\u044c\u0441\u044f \u043a Discord</a>\n</p>\n\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n<a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n\n\u0421\u043e\u0437\u0434\u0430\u0432\u0430\u0439 \u0434\u0438\u043d\u0430\u043c\u0438\u0447\u0435\u0441\u043a\u0443\u044e \u043f\u0430\u043c\u044f\u0442\u044c \u0434\u043b\u044f \u0430\u0433\u0435\u043d\u0442\u043e\u0432, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f ECL (Extract -> Cognify -> Load) \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440.\n\n\u0423\u0437\u043d\u0430\u0439\u0442\u0435 \u0431\u043e\u043b\u044c\u0448\u0435 \u043e [\u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430\u0445 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f](https://docs.cognee.ai/use-cases) \u0438 [\u0431\u0435\u043d\u0447\u043c\u0430\u0440\u043a\u0430\u0445](https://github.com/topoteretes/cognee/tree/main/evals)\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"\u041f\u043e\u0447\u0435\u043c\u0443 cognee?\" width=\"50%\" />\n</div>\n\n</div>\n\n\n\n## \u0424\u0443\u043d\u043a\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\n\n- \u0418\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u044f \u0438 \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445: \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0442\u044c \u0438 \u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0442\u044c \u043f\u0440\u043e\u0448\u043b\u044b\u0435 \u0440\u0430\u0437\u0433\u043e\u0432\u043e\u0440\u044b, \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u044b, \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u0430\u0443\u0434\u0438\u043e\u0437\u0430\u043f\u0438\u0441\u0438, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044f \u0434\u043e\u0441\u0442\u0443\u043f \u043a \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u043d\u044b\u043c \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n- \u0421\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0433\u0430\u043b\u043b\u044e\u0446\u0438\u043d\u0430\u0446\u0438\u0439 \u0438 \u0437\u0430\u0442\u0440\u0430\u0442: \u0423\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u043e\u0432\u0435\u0440\u043d\u044b\u0445 \u043e\u0442\u0432\u0435\u0442\u043e\u0432, \u0441\u043d\u0438\u0436\u0430\u0435\u0442 \u0437\u0430\u0442\u0440\u0430\u0442\u044b \u043d\u0430 \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0443 \u0438 \u044d\u043a\u0441\u043f\u043b\u0443\u0430\u0442\u0430\u0446\u0438\u044e \u0418\u0418-\u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0439.\n\n- \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c Pydantic: \u041e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0433\u0440\u0430\u0444\u043e\u0432\u044b\u0435 \u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u044b\u0435 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e Pydantic, \u0443\u043f\u0440\u043e\u0449\u0430\u044f \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0438\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0438\u0438.\n\n- \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0438 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445: \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435, \u0441\u043e\u0431\u0438\u0440\u0430\u044f \u0438\u0445 \u0438\u0437 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u043c 30 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u043e\u0432, \u0432\u043a\u043b\u044e\u0447\u0430\u044f PDF, \u0442\u0430\u0431\u043b\u0438\u0446\u044b \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u0444\u043e\u0440\u043c\u0430\u0442\u044b.\n\n- \u041c\u043e\u0434\u0443\u043b\u044c\u043d\u044b\u0435 ECL-\u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u044b: \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043c\u043e\u0434\u0443\u043b\u044c\u043d\u044b\u0435 \u043f\u0430\u0439\u043f\u043b\u0430\u0439\u043d\u044b Extract, Cognify, Load (ECL) \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445, \u0447\u0442\u043e \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u0435\u0442 \u0433\u0438\u0431\u043a\u043e\u0441\u0442\u044c \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c \u0441\u0438\u0441\u0442\u0435\u043c\u044b."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- \u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0439 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 RDF: \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 RDF \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f.\n\n- \u041b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0437\u0432\u0435\u0440\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c: \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0440\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044c \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u043d\u0430 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0441\u0435\u0440\u0432\u0435\u0440\u0430\u0445, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044f \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c \u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438. \u0421\u0438\u0441\u0442\u0435\u043c\u0430 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445.\n\n## \u041d\u0430\u0447\u0430\u043b\u043e \u0440\u0430\u0431\u043e\u0442\u044b\n\n\u041d\u0430\u0447\u043d\u0438\u0442\u0435 \u043b\u0435\u0433\u043a\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e Google Colab <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">\u0431\u043b\u043e\u043a\u043d\u043e\u0442\u0430</a> \u0438\u043b\u0438 <a href=\"https://github.com/topoteretes/cognee-starter\">\u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0433\u043e \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f</a>\n\n## \u041f\u043e\u043c\u043e\u0449\u044c \u043f\u0440\u043e\u0435\u043a\u0442\u0443\n\n\u0412\u0430\u0448 \u0432\u043a\u043b\u0430\u0434 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0441\u043d\u043e\u0432\u043e\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0433\u043e \u0432 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u0441 \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u043c \u043a\u043e\u0434\u043e\u043c. \u041b\u044e\u0431\u043e\u0439 \u0432\u043a\u043b\u0430\u0434, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u044b \u0441\u0434\u0435\u043b\u0430\u0435\u0442\u0435, \u0431\u0443\u0434\u0435\u0442 **\u043e\u0447\u0435\u043d\u044c \u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\u0441\u044f**. \u0421\u043c\u043e\u0442\u0440\u0438\u0442\u0435 [`CONTRIBUTING.md`](/CONTRIBUTING.md) \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n## \ud83d\udce6 \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430\n\n\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c Cognee, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f **pip**, **poetry**, **uv** \u0438\u043b\u0438 \u043b\u044e\u0431\u043e\u0439 \u0434\u0440\u0443\u0433\u043e\u0439 \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440 \u043f\u0430\u043a\u0435\u0442\u043e\u0432 Python.\n\n### \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e pip\n\n```bash\npip install cognee\n```\n\n## \ud83d\udcbb \u0411\u0430\u0437\u043e\u0432\u043e\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\n\n### \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\n\n```python\nimport os\nos.environ[\"LLM_API_KEY\"] = \"\u0412\u0410\u0428_OPENAI_API_KEY\"\n```\n\n\u0412\u044b \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u0441\u043e\u0437\u0434\u0430\u0432 \u0444\u0430\u0439\u043b .env, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0448 <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">\u0448\u0430\u0431\u043b\u043e\u043d</a>.\n\u0414\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u043e\u0432 LLM \u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043d\u0430\u0448\u0443 <a href=\"https://docs.cognee.ai\">\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e</a>.\n\n### \u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n\n\u042d\u0442\u043e\u0442 \u0441\u043a\u0440\u0438\u043f\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442 *\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439* \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0432 cognee\n    await cognee.add(\"\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 (NLP) - \u044d\u0442\u043e \u043c\u0435\u0436\u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0445 \u043d\u0430\u0443\u043a \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0438\u0441\u043a\u0430.\")\n\n    # \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0433\u0440\u0430\u0444 \u0437\u043d\u0430\u043d\u0438\u0439\n    await cognee.cognify()\n\n    # \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u043e\u0438\u0441\u043a\n    results = await cognee.search(\"\u0420\u0430\u0441\u0441\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u043d\u0435 \u043e NLP\")\n\n    # \u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\u041f\u0440\u0438\u043c\u0435\u0440 \u0432\u044b\u0432\u043e\u0434\u0430:\n```\n\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 (NLP) \u2014 \u044d\u0442\u043e \u043c\u0435\u0436\u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u0442 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0435 \u043d\u0430\u0443\u043a\u0438 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043f\u043e\u0438\u0441\u043a. \u041e\u043d\u0430 \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u0432 \u0441\u0435\u0431\u044f \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u043e\u0432 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.\n```\n\n\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0440\u0430\u0444\u0430:\n<a href=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html\"><img src=\"graph_visualization_ru.png\" width=\"100%\" alt=\"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0440\u0430\u0444\u0430\"></a>\n[\u041e\u0442\u043a\u0440\u044b\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0430 \u0432 \u0431\u0440\u0430\u0443\u0437\u0435\u0440\u0435](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\n\u0411\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0432 <a href=\"https://docs.cognee.ai\">\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438</a>.\n\n\n## \u0418\u0437\u0443\u0447\u0438\u0442\u0435 \u043d\u0430\u0448\u0443 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png\" alt=\"\u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430 cognee\" width=\"100%\" />\n</div>", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n- \u041f\u043e\u0434\u0434\u0435\u0440\u0436\u043a\u0430 \u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0439 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 RDF: \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442 \u043e\u043d\u0442\u043e\u043b\u043e\u0433\u0438\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 RDF \u0434\u043b\u044f \u0431\u043e\u043b\u0435\u0435 \u0438\u043d\u0442\u0435\u043b\u043b\u0435\u043a\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0443\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f.\n\n- \u041b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0437\u0432\u0435\u0440\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u0438 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u043c\u043e\u0441\u0442\u044c: \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0440\u0430\u0437\u0432\u0435\u0440\u043d\u0443\u0442\u044c \u0441\u0438\u0441\u0442\u0435\u043c\u0443 \u043d\u0430 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u0445 \u0441\u0435\u0440\u0432\u0435\u0440\u0430\u0445, \u043e\u0431\u0435\u0441\u043f\u0435\u0447\u0438\u0432\u0430\u044f \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0435 \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u043d\u0438\u044f\u043c \u043a\u043e\u043d\u0444\u0438\u0434\u0435\u043d\u0446\u0438\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u0438. \u0421\u0438\u0441\u0442\u0435\u043c\u0430 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043e\u0431\u044a\u0435\u043c\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445.\n\n## \u041d\u0430\u0447\u0430\u043b\u043e \u0440\u0430\u0431\u043e\u0442\u044b\n\n\u041d\u0430\u0447\u043d\u0438\u0442\u0435 \u043b\u0435\u0433\u043a\u043e \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e Google Colab <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">\u0431\u043b\u043e\u043a\u043d\u043e\u0442\u0430</a> \u0438\u043b\u0438 <a href=\"https://github.com/topoteretes/cognee-starter\">\u0441\u0442\u0430\u0440\u0442\u043e\u0432\u043e\u0433\u043e \u0440\u0435\u043f\u043e\u0437\u0438\u0442\u043e\u0440\u0438\u044f</a>\n\n## \u041f\u043e\u043c\u043e\u0449\u044c \u043f\u0440\u043e\u0435\u043a\u0442\u0443\n\n\u0412\u0430\u0448 \u0432\u043a\u043b\u0430\u0434 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043e\u0441\u043d\u043e\u0432\u043e\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0433\u043e \u0432 \u043d\u0430\u0441\u0442\u043e\u044f\u0449\u0438\u0439 \u043f\u0440\u043e\u0435\u043a\u0442 \u0441 \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u043c \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u043c \u043a\u043e\u0434\u043e\u043c. \u041b\u044e\u0431\u043e\u0439 \u0432\u043a\u043b\u0430\u0434, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u044b \u0441\u0434\u0435\u043b\u0430\u0435\u0442\u0435, \u0431\u0443\u0434\u0435\u0442 **\u043e\u0447\u0435\u043d\u044c \u043f\u0440\u0438\u0432\u0435\u0442\u0441\u0442\u0432\u043e\u0432\u0430\u0442\u044c\u0441\u044f**. \u0421\u043c\u043e\u0442\u0440\u0438\u0442\u0435 [`CONTRIBUTING.md`](/CONTRIBUTING.md) \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n## \ud83d\udce6 \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430\n\n\u0412\u044b \u043c\u043e\u0436\u0435\u0442\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c Cognee, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f **pip**, **poetry**, **uv** \u0438\u043b\u0438 \u043b\u044e\u0431\u043e\u0439 \u0434\u0440\u0443\u0433\u043e\u0439 \u043c\u0435\u043d\u0435\u0434\u0436\u0435\u0440 \u043f\u0430\u043a\u0435\u0442\u043e\u0432 Python.\n\n### \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e pip\n\n```bash\npip install cognee\n```\n\n## \ud83d\udcbb \u0411\u0430\u0437\u043e\u0432\u043e\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\n\n### \u041d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\n\n```python\nimport os\nos.environ[\"LLM_API_KEY\"] = \"\u0412\u0410\u0428_OPENAI_API_KEY\"\n```\n\n\u0412\u044b \u0442\u0430\u043a\u0436\u0435 \u043c\u043e\u0436\u0435\u0442\u0435 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u0441\u043e\u0437\u0434\u0430\u0432 \u0444\u0430\u0439\u043b .env, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0448 <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">\u0448\u0430\u0431\u043b\u043e\u043d</a>.\n\u0414\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043f\u0440\u043e\u0432\u0430\u0439\u0434\u0435\u0440\u043e\u0432 LLM \u0441\u043c\u043e\u0442\u0440\u0438\u0442\u0435 \u043d\u0430\u0448\u0443 <a href=\"https://docs.cognee.ai\">\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e</a>.\n\n### \u041f\u0440\u0438\u043c\u0435\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f\n\n\u042d\u0442\u043e\u0442 \u0441\u043a\u0440\u0438\u043f\u0442 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442 *\u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0439* \u043a\u043e\u043d\u0432\u0435\u0439\u0435\u0440:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0432 cognee\n    await cognee.add(\"\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 (NLP) - \u044d\u0442\u043e \u043c\u0435\u0436\u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0445 \u043d\u0430\u0443\u043a \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0433\u043e \u043f\u043e\u0438\u0441\u043a\u0430.\")\n\n    # \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0433\u0440\u0430\u0444 \u0437\u043d\u0430\u043d\u0438\u0439\n    await cognee.cognify()\n\n    # \u0414\u0435\u043b\u0430\u0435\u043c \u043f\u043e\u0438\u0441\u043a\n    results = await cognee.search(\"\u0420\u0430\u0441\u0441\u043a\u0430\u0436\u0438\u0442\u0435 \u043c\u043d\u0435 \u043e NLP\")\n\n    # \u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\u041f\u0440\u0438\u043c\u0435\u0440 \u0432\u044b\u0432\u043e\u0434\u0430:\n```\n\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0435\u0441\u0442\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 (NLP) \u2014 \u044d\u0442\u043e \u043c\u0435\u0436\u0434\u0438\u0441\u0446\u0438\u043f\u043b\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u044c, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u0442 \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u043d\u044b\u0435 \u043d\u0430\u0443\u043a\u0438 \u0438 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u043f\u043e\u0438\u0441\u043a. \u041e\u043d\u0430 \u0432\u043a\u043b\u044e\u0447\u0430\u0435\u0442 \u0432 \u0441\u0435\u0431\u044f \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u0438 \u0438 \u043c\u0435\u0442\u043e\u0434\u044b \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430 \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u0438\u043d\u0442\u0435\u0440\u0444\u0435\u0439\u0441\u043e\u0432 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445.\n```\n\n\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0440\u0430\u0444\u0430:\n<a href=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html\"><img src=\"graph_visualization_ru.png\" width=\"100%\" alt=\"\u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0433\u0440\u0430\u0444\u0430\"></a>\n[\u041e\u0442\u043a\u0440\u044b\u0442\u044c \u043f\u0440\u0438\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0430 \u0432 \u0431\u0440\u0430\u0443\u0437\u0435\u0440\u0435](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\n\u0411\u043e\u043b\u044c\u0448\u0435 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0432 <a href=\"https://docs.cognee.ai\">\u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438</a>.\n\n\n## \u0418\u0437\u0443\u0447\u0438\u0442\u0435 \u043d\u0430\u0448\u0443 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png\" alt=\"\u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u0430\u044f \u0434\u0438\u0430\u0433\u0440\u0430\u043c\u043c\u0430 cognee\" width=\"100%\" />\n</div>"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n## \u0414\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438\n\n1. \u0427\u0442\u043e \u0442\u0430\u043a\u043e\u0435 \u043f\u0430\u043c\u044f\u0442\u044c \u0418\u0418:\n[\u0412\u0438\u0434\u0435\u043e](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. \u041f\u0440\u043e\u0441\u0442\u0430\u044f \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f GraphRAG\n[\u0412\u0438\u0434\u0435\u043e](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. Cognee \u0441 Ollama \n[\u0412\u0438\u0434\u0435\u043e](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)\n\n## \u041f\u0440\u0430\u0432\u0438\u043b\u0430 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f\n\n\u041c\u044b \u0441\u0442\u0440\u0435\u043c\u0438\u043c\u0441\u044f \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u0439 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043a\u043e\u0434 \u043f\u0440\u0438\u044f\u0442\u043d\u044b\u043c \u0438 \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u043e\u043f\u044b\u0442\u043e\u043c \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0433\u043e \u0441\u043e\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0430. \u0421\u043c\u043e\u0442\u0440\u0438\u0442\u0435 <a href=\"/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n## \ud83d\udcab \u041a\u043e\u043d\u0442\u0440\u0438\u0431\u044c\u044e\u0442\u043e\u0440\u044b\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"\u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0438\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n## \u0418\u0441\u0442\u043e\u0440\u0438\u044f \u0437\u0432\u0451\u0437\u0434 \u043d\u0430 GitHub\n\n[![\u0413\u0440\u0430\u0444\u0438\u043a \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u0437\u0432\u0451\u0437\u0434](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.ru.md\n## \u0414\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u0438\n\n1. \u0427\u0442\u043e \u0442\u0430\u043a\u043e\u0435 \u043f\u0430\u043c\u044f\u0442\u044c \u0418\u0418:\n[\u0412\u0438\u0434\u0435\u043e](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. \u041f\u0440\u043e\u0441\u0442\u0430\u044f \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f GraphRAG\n[\u0412\u0438\u0434\u0435\u043e](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. Cognee \u0441 Ollama \n[\u0412\u0438\u0434\u0435\u043e](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)\n\n## \u041f\u0440\u0430\u0432\u0438\u043b\u0430 \u043f\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u044f\n\n\u041c\u044b \u0441\u0442\u0440\u0435\u043c\u0438\u043c\u0441\u044f \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043e\u0442\u043a\u0440\u044b\u0442\u044b\u0439 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0439 \u043a\u043e\u0434 \u043f\u0440\u0438\u044f\u0442\u043d\u044b\u043c \u0438 \u0443\u0432\u0430\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u043e\u043f\u044b\u0442\u043e\u043c \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0433\u043e \u0441\u043e\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u0430. \u0421\u043c\u043e\u0442\u0440\u0438\u0442\u0435 <a href=\"/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438.\n\n## \ud83d\udcab \u041a\u043e\u043d\u0442\u0440\u0438\u0431\u044c\u044e\u0442\u043e\u0440\u044b\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"\u0443\u0447\u0430\u0441\u0442\u043d\u0438\u043a\u0438\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n## \u0418\u0441\u0442\u043e\u0440\u0438\u044f \u0437\u0432\u0451\u0437\u0434 \u043d\u0430 GitHub\n\n[![\u0413\u0440\u0430\u0444\u0438\u043a \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u0437\u0432\u0451\u0437\u0434](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - AI\u5e94\u7528\u548c\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u5c42\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">\u6f14\u793a</a>\n  .\n  <a href=\"https://cognee.ai\">\u4e86\u89e3\u66f4\u591a</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">\u52a0\u5165Discord</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n  \u53ef\u9760\u7684AI\u667a\u80fd\u4f53\u54cd\u5e94\u3002\n\n\n\n\u4f7f\u7528\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u7684ECL\uff08\u63d0\u53d6\u3001\u8ba4\u77e5\u3001\u52a0\u8f7d\uff09\u7ba1\u9053\u6784\u5efa\u52a8\u6001\u667a\u80fd\u4f53\u8bb0\u5fc6\u3002\n\n\u66f4\u591a[\u4f7f\u7528\u573a\u666f](https://docs.cognee.ai/use_cases)\u3002\n\n<div style=\"text-align: center\">\n  <img src=\"cognee_benefits_zh.JPG\" alt=\"\u4e3a\u4ec0\u4e48\u9009\u62e9cognee\uff1f\" width=\"100%\" />\n</div>\n\n</div>\n\n\n\n\n## \u529f\u80fd\u7279\u6027\n\n- \u4e92\u8054\u5e76\u68c0\u7d22\u60a8\u7684\u5386\u53f2\u5bf9\u8bdd\u3001\u6587\u6863\u3001\u56fe\u50cf\u548c\u97f3\u9891\u8f6c\u5f55\n- \u51cf\u5c11\u5e7b\u89c9\u3001\u5f00\u53d1\u4eba\u5458\u5de5\u4f5c\u91cf\u548c\u6210\u672c\n- \u4ec5\u4f7f\u7528Pydantic\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u56fe\u5f62\u548c\u5411\u91cf\u6570\u636e\u5e93\n- \u4ece30\u591a\u4e2a\u6570\u636e\u6e90\u6444\u53d6\u6570\u636e\u65f6\u8fdb\u884c\u6570\u636e\u64cd\u4f5c\n\n## \u5f00\u59cb\u4f7f\u7528\n\n\u901a\u8fc7Google Colab <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">\u7b14\u8bb0\u672c</a>\u6216<a href=\"https://github.com/topoteretes/cognee-starter\">\u5165\u95e8\u9879\u76ee</a>\u5feb\u901f\u4e0a\u624b\n\n## \u8d21\u732e\n\u60a8\u7684\u8d21\u732e\u662f\u4f7f\u8fd9\u6210\u4e3a\u771f\u6b63\u5f00\u6e90\u9879\u76ee\u7684\u6838\u5fc3\u3002\u6211\u4eec**\u975e\u5e38\u611f\u8c22**\u4efb\u4f55\u8d21\u732e\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u9605[`CONTRIBUTING.md`](CONTRIBUTING.md)\u3002\n\n\n\n\n\n## \ud83d\udce6 \u5b89\u88c5\n\n\u60a8\u53ef\u4ee5\u4f7f\u7528**pip**\u3001**poetry**\u3001**uv**\u6216\u4efb\u4f55\u5176\u4ed6Python\u5305\u7ba1\u7406\u5668\u5b89\u88c5Cognee\u3002\n\n### \u4f7f\u7528pip\n\n```bash\npip install cognee\n```\n\n## \ud83d\udcbb \u57fa\u672c\u7528\u6cd5\n\n### \u8bbe\u7f6e\n\n```\nimport os\nos.environ[\"LLM_API_KEY\"] = \"YOUR OPENAI_API_KEY\"\n\n```\n\n\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa.env\u6587\u4ef6\u8bbe\u7f6e\u53d8\u91cf\uff0c\u4f7f\u7528\u6211\u4eec\u7684<a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">\u6a21\u677f</a>\u3002\n\u8981\u4f7f\u7528\u4e0d\u540c\u7684LLM\u63d0\u4f9b\u5546\uff0c\u8bf7\u67e5\u770b\u6211\u4eec\u7684<a href=\"https://docs.cognee.ai\">\u6587\u6863</a>\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\u3002\n\n\n### \u7b80\u5355\u793a\u4f8b\n\n\u6b64\u811a\u672c\u5c06\u8fd0\u884c\u9ed8\u8ba4\u7ba1\u9053\uff1a\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Add text to cognee\n    await cognee.add(\"\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u68c0\u7d22\u7684\u8de8\u5b66\u79d1\u9886\u57df\u3002\")\n\n    # Generate the knowledge graph\n    await cognee.cognify()\n\n    # Query the knowledge graph\n    results = await cognee.search(\"\u544a\u8bc9\u6211\u5173\u4e8eNLP\")\n\n    # Display the results\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\u793a\u4f8b\u8f93\u51fa\uff1a\n```\n  \u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u68c0\u7d22\u7684\u8de8\u5b66\u79d1\u9886\u57df\u3002\u5b83\u5173\u6ce8\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u8bed\u8a00\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4f7f\u673a\u5668\u80fd\u591f\u7406\u89e3\u548c\u5904\u7406\u81ea\u7136\u8bed\u8a00\u3002\n  \n```\n\u56fe\u5f62\u53ef\u89c6\u5316\uff1a\n<a href=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html\"><img src=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.png\" width=\"100%\" alt=\"\u56fe\u5f62\u53ef\u89c6\u5316\"></a>\n\u5728[\u6d4f\u89c8\u5668](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html)\u4e2d\u6253\u5f00\u3002\n\n\u6709\u5173\u66f4\u9ad8\u7ea7\u7684\u7528\u6cd5\uff0c\u8bf7\u67e5\u770b\u6211\u4eec\u7684<a href=\"https://docs.cognee.ai\">\u6587\u6863</a>\u3002", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - AI\u5e94\u7528\u548c\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u5c42\n\n  <p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">\u6f14\u793a</a>\n  .\n  <a href=\"https://cognee.ai\">\u4e86\u89e3\u66f4\u591a</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">\u52a0\u5165Discord</a>\n  </p>\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n  \u53ef\u9760\u7684AI\u667a\u80fd\u4f53\u54cd\u5e94\u3002\n\n\n\n\u4f7f\u7528\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u7684ECL\uff08\u63d0\u53d6\u3001\u8ba4\u77e5\u3001\u52a0\u8f7d\uff09\u7ba1\u9053\u6784\u5efa\u52a8\u6001\u667a\u80fd\u4f53\u8bb0\u5fc6\u3002\n\n\u66f4\u591a[\u4f7f\u7528\u573a\u666f](https://docs.cognee.ai/use_cases)\u3002\n\n<div style=\"text-align: center\">\n  <img src=\"cognee_benefits_zh.JPG\" alt=\"\u4e3a\u4ec0\u4e48\u9009\u62e9cognee\uff1f\" width=\"100%\" />\n</div>\n\n</div>\n\n\n\n\n## \u529f\u80fd\u7279\u6027\n\n- \u4e92\u8054\u5e76\u68c0\u7d22\u60a8\u7684\u5386\u53f2\u5bf9\u8bdd\u3001\u6587\u6863\u3001\u56fe\u50cf\u548c\u97f3\u9891\u8f6c\u5f55\n- \u51cf\u5c11\u5e7b\u89c9\u3001\u5f00\u53d1\u4eba\u5458\u5de5\u4f5c\u91cf\u548c\u6210\u672c\n- \u4ec5\u4f7f\u7528Pydantic\u5c06\u6570\u636e\u52a0\u8f7d\u5230\u56fe\u5f62\u548c\u5411\u91cf\u6570\u636e\u5e93\n- \u4ece30\u591a\u4e2a\u6570\u636e\u6e90\u6444\u53d6\u6570\u636e\u65f6\u8fdb\u884c\u6570\u636e\u64cd\u4f5c\n\n## \u5f00\u59cb\u4f7f\u7528\n\n\u901a\u8fc7Google Colab <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">\u7b14\u8bb0\u672c</a>\u6216<a href=\"https://github.com/topoteretes/cognee-starter\">\u5165\u95e8\u9879\u76ee</a>\u5feb\u901f\u4e0a\u624b\n\n## \u8d21\u732e\n\u60a8\u7684\u8d21\u732e\u662f\u4f7f\u8fd9\u6210\u4e3a\u771f\u6b63\u5f00\u6e90\u9879\u76ee\u7684\u6838\u5fc3\u3002\u6211\u4eec**\u975e\u5e38\u611f\u8c22**\u4efb\u4f55\u8d21\u732e\u3002\u66f4\u591a\u4fe1\u606f\u8bf7\u53c2\u9605[`CONTRIBUTING.md`](CONTRIBUTING.md)\u3002\n\n\n\n\n\n## \ud83d\udce6 \u5b89\u88c5\n\n\u60a8\u53ef\u4ee5\u4f7f\u7528**pip**\u3001**poetry**\u3001**uv**\u6216\u4efb\u4f55\u5176\u4ed6Python\u5305\u7ba1\u7406\u5668\u5b89\u88c5Cognee\u3002\n\n### \u4f7f\u7528pip\n\n```bash\npip install cognee\n```\n\n## \ud83d\udcbb \u57fa\u672c\u7528\u6cd5\n\n### \u8bbe\u7f6e\n\n```\nimport os\nos.environ[\"LLM_API_KEY\"] = \"YOUR OPENAI_API_KEY\"\n\n```\n\n\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u521b\u5efa.env\u6587\u4ef6\u8bbe\u7f6e\u53d8\u91cf\uff0c\u4f7f\u7528\u6211\u4eec\u7684<a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">\u6a21\u677f</a>\u3002\n\u8981\u4f7f\u7528\u4e0d\u540c\u7684LLM\u63d0\u4f9b\u5546\uff0c\u8bf7\u67e5\u770b\u6211\u4eec\u7684<a href=\"https://docs.cognee.ai\">\u6587\u6863</a>\u83b7\u53d6\u66f4\u591a\u4fe1\u606f\u3002\n\n\n### \u7b80\u5355\u793a\u4f8b\n\n\u6b64\u811a\u672c\u5c06\u8fd0\u884c\u9ed8\u8ba4\u7ba1\u9053\uff1a\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Add text to cognee\n    await cognee.add(\"\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u68c0\u7d22\u7684\u8de8\u5b66\u79d1\u9886\u57df\u3002\")\n\n    # Generate the knowledge graph\n    await cognee.cognify()\n\n    # Query the knowledge graph\n    results = await cognee.search(\"\u544a\u8bc9\u6211\u5173\u4e8eNLP\")\n\n    # Display the results\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\n\u793a\u4f8b\u8f93\u51fa\uff1a\n```\n  \u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u548c\u4fe1\u606f\u68c0\u7d22\u7684\u8de8\u5b66\u79d1\u9886\u57df\u3002\u5b83\u5173\u6ce8\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u8bed\u8a00\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4f7f\u673a\u5668\u80fd\u591f\u7406\u89e3\u548c\u5904\u7406\u81ea\u7136\u8bed\u8a00\u3002\n  \n```\n\u56fe\u5f62\u53ef\u89c6\u5316\uff1a\n<a href=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html\"><img src=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.png\" width=\"100%\" alt=\"\u56fe\u5f62\u53ef\u89c6\u5316\"></a>\n\u5728[\u6d4f\u89c8\u5668](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html)\u4e2d\u6253\u5f00\u3002\n\n\u6709\u5173\u66f4\u9ad8\u7ea7\u7684\u7528\u6cd5\uff0c\u8bf7\u67e5\u770b\u6211\u4eec\u7684<a href=\"https://docs.cognee.ai\">\u6587\u6863</a>\u3002"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md\n## \u4e86\u89e3\u6211\u4eec\u7684\u67b6\u6784\n\n<div style=\"text-align: center\">\n  <img src=\"cognee_diagram_zh.JPG\" alt=\"cognee\u6982\u5ff5\u56fe\" width=\"100%\" />\n</div>\n\n\n\n## \u6f14\u793a\n\n1. \u4ec0\u4e48\u662fAI\u8bb0\u5fc6\uff1a\n\n[\u4e86\u89e3cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. \u7b80\u5355GraphRAG\u6f14\u793a\n\n[\u7b80\u5355GraphRAG\u6f14\u793a](https://github.com/user-attachments/assets/f57fd9ea-1dc0-4904-86eb-de78519fdc32)\n\n3. cognee\u4e0eOllama\n\n[cognee\u4e0e\u672c\u5730\u6a21\u578b](https://github.com/user-attachments/assets/834baf9a-c371-4ecf-92dd-e144bd0eb3f6)\n\n\n## \u884c\u4e3a\u51c6\u5219\n\n\u6211\u4eec\u81f4\u529b\u4e8e\u4e3a\u6211\u4eec\u7684\u793e\u533a\u63d0\u4f9b\u6109\u5feb\u548c\u5c0a\u91cd\u7684\u5f00\u6e90\u4f53\u9a8c\u3002\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605<a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a>\u3002\n\n## \ud83d\udcab \u8d21\u732e\u8005\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n\n## Star\u5386\u53f2\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.zh.md\n## \u4e86\u89e3\u6211\u4eec\u7684\u67b6\u6784\n\n<div style=\"text-align: center\">\n  <img src=\"cognee_diagram_zh.JPG\" alt=\"cognee\u6982\u5ff5\u56fe\" width=\"100%\" />\n</div>\n\n\n\n## \u6f14\u793a\n\n1. \u4ec0\u4e48\u662fAI\u8bb0\u5fc6\uff1a\n\n[\u4e86\u89e3cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. \u7b80\u5355GraphRAG\u6f14\u793a\n\n[\u7b80\u5355GraphRAG\u6f14\u793a](https://github.com/user-attachments/assets/f57fd9ea-1dc0-4904-86eb-de78519fdc32)\n\n3. cognee\u4e0eOllama\n\n[cognee\u4e0e\u672c\u5730\u6a21\u578b](https://github.com/user-attachments/assets/834baf9a-c371-4ecf-92dd-e144bd0eb3f6)\n\n\n## \u884c\u4e3a\u51c6\u5219\n\n\u6211\u4eec\u81f4\u529b\u4e8e\u4e3a\u6211\u4eec\u7684\u793e\u533a\u63d0\u4f9b\u6109\u5feb\u548c\u5c0a\u91cd\u7684\u5f00\u6e90\u4f53\u9a8c\u3002\u6709\u5173\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605<a href=\"https://github.com/topoteretes/cognee/blob/main/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a>\u3002\n\n## \ud83d\udcab \u8d21\u732e\u8005\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n\n## Star\u5386\u53f2\n\n[![Star History Chart](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - Mem\u00f3ria para Agentes de IA em 5 linhas de c\u00f3digo\n\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demonstra\u00e7\u00e3o</a>\n  .\n  <a href=\"https://cognee.ai\">Saiba mais</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Participe do Discord</a>\n</p>\n\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n<a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n\n\nCrie uma mem\u00f3ria din\u00e2mica para Agentes usando pipelines ECL (Extrair, Cognificar, Carregar) escal\u00e1veis e modulares.\n\nSaiba mais sobre os [casos de uso](https://docs.cognee.ai/use-cases) e [avalia\u00e7\u00f5es](https://github.com/topoteretes/cognee/tree/main/evals)\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"Por que cognee?\" width=\"50%\" />\n</div>\n\n</div>\n\n\n\n## Funcionalidades\n\n- Conecte e recupere suas conversas passadas, documentos, imagens e transcri\u00e7\u00f5es de \u00e1udio  \n- Reduza alucina\u00e7\u00f5es, esfor\u00e7o de desenvolvimento e custos  \n- Carregue dados em bancos de dados de grafos e vetores usando apenas Pydantic  \n- Transforme e organize seus dados enquanto os coleta de mais de 30 fontes diferentes  \n\n## Primeiros Passos\n\nD\u00ea os primeiros passos com facilidade usando um Google Colab  <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">notebook</a>  ou um <a href=\"https://github.com/topoteretes/cognee-starter\">reposit\u00f3rio inicial</a>", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md\n<div align=\"center\">\n  <a href=\"https://github.com/topoteretes/cognee\">\n    <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/dev/assets/cognee-logo-transparent.png\" alt=\"Cognee Logo\" height=\"60\">\n  </a>\n\n  <br />\n\n  cognee - Mem\u00f3ria para Agentes de IA em 5 linhas de c\u00f3digo\n\n<p align=\"center\">\n  <a href=\"https://www.youtube.com/watch?v=1bezuvLwJmw&t=2s\">Demonstra\u00e7\u00e3o</a>\n  .\n  <a href=\"https://cognee.ai\">Saiba mais</a>\n  \u00b7\n  <a href=\"https://discord.gg/NQPKmU5CCg\">Participe do Discord</a>\n</p>\n\n\n\n  [![GitHub forks](https://img.shields.io/github/forks/topoteretes/cognee.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/network/)\n  [![GitHub stars](https://img.shields.io/github/stars/topoteretes/cognee.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/topoteretes/cognee/stargazers/)\n  [![GitHub commits](https://badgen.net/github/commits/topoteretes/cognee)](https://GitHub.com/topoteretes/cognee/commit/)\n  [![Github tag](https://badgen.net/github/tag/topoteretes/cognee)](https://github.com/topoteretes/cognee/tags/)\n  [![Downloads](https://static.pepy.tech/badge/cognee)](https://pepy.tech/project/cognee)\n  [![License](https://img.shields.io/github/license/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/blob/main/LICENSE)\n  [![Contributors](https://img.shields.io/github/contributors/topoteretes/cognee?colorA=00C586&colorB=000000)](https://github.com/topoteretes/cognee/graphs/contributors)\n\n<a href=\"https://www.producthunt.com/posts/cognee?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-cognee\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=946346&theme=light&period=daily&t=1744472480704\" alt=\"cognee - Memory&#0032;for&#0032;AI&#0032;Agents&#0032;&#0032;in&#0032;5&#0032;lines&#0032;of&#0032;code | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n\n\nCrie uma mem\u00f3ria din\u00e2mica para Agentes usando pipelines ECL (Extrair, Cognificar, Carregar) escal\u00e1veis e modulares.\n\nSaiba mais sobre os [casos de uso](https://docs.cognee.ai/use-cases) e [avalia\u00e7\u00f5es](https://github.com/topoteretes/cognee/tree/main/evals)\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_benefits.png\" alt=\"Por que cognee?\" width=\"50%\" />\n</div>\n\n</div>\n\n\n\n## Funcionalidades\n\n- Conecte e recupere suas conversas passadas, documentos, imagens e transcri\u00e7\u00f5es de \u00e1udio  \n- Reduza alucina\u00e7\u00f5es, esfor\u00e7o de desenvolvimento e custos  \n- Carregue dados em bancos de dados de grafos e vetores usando apenas Pydantic  \n- Transforme e organize seus dados enquanto os coleta de mais de 30 fontes diferentes  \n\n## Primeiros Passos\n\nD\u00ea os primeiros passos com facilidade usando um Google Colab  <a href=\"https://colab.research.google.com/drive/1g-Qnx6l_ecHZi0IOw23rg0qC4TYvEvWZ?usp=sharing\">notebook</a>  ou um <a href=\"https://github.com/topoteretes/cognee-starter\">reposit\u00f3rio inicial</a>"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md\n## Contribuindo\n\nSuas contribui\u00e7\u00f5es est\u00e3o no centro de tornar este um verdadeiro projeto open source. Qualquer contribui\u00e7\u00e3o que voc\u00ea fizer ser\u00e1 **muito bem-vinda**. Veja o [`CONTRIBUTING.md`](/CONTRIBUTING.md) para mais informa\u00e7\u00f5es.\n## \ud83d\udce6 Instala\u00e7\u00e3o\n\nVoc\u00ea pode instalar o Cognee usando **pip**, **poetry**, **uv** ou qualquer outro gerenciador de pacotes Python.\n\n### Com pip\n\n```bash\npip install cognee\n```\n\n## \ud83d\udcbb Uso B\u00e1sico\n\n### Configura\u00e7\u00e3o\n\n```python\nimport os\nos.environ[\"LLM_API_KEY\"] = \"SUA_OPENAI_API_KEY\"\n```\n\nVoc\u00ea tamb\u00e9m pode definir as vari\u00e1veis criando um arquivo .env, usando o nosso <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">modelo</a>.\nPara usar diferentes provedores de LLM, consulte nossa <a href=\"https://docs.cognee.ai\">documenta\u00e7\u00e3o</a> .\n\n### Exemplo simples\n\nEste script executar\u00e1 o pipeline *default*:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Adiciona texto ao cognee\n    await cognee.add(\"Processamento de linguagem natural (NLP) \u00e9 um subcampo interdisciplinar da ci\u00eancia da computa\u00e7\u00e3o e recupera\u00e7\u00e3o de informa\u00e7\u00f5es.\")\n\n    # Gera o grafo de conhecimento\n    await cognee.cognify()\n\n    # Consulta o grafo de conhecimento\n    results = await cognee.search(\"Me fale sobre NLP\")\n\n    # Exibe os resultados\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\nExemplo do output:\n```\n  O Processamento de Linguagem Natural (NLP) \u00e9 um campo interdisciplinar e transdisciplinar que envolve ci\u00eancia da computa\u00e7\u00e3o e recupera\u00e7\u00e3o de informa\u00e7\u00f5es. Ele se concentra na intera\u00e7\u00e3o entre computadores e a linguagem humana, permitindo que as m\u00e1quinas compreendam e processem a linguagem natural.\n  \n```\n\nVisualiza\u00e7\u00e3o do grafo:\n<a href=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html\"><img src=\"graph_visualization_pt.png\" width=\"100%\" alt=\"Visualiza\u00e7\u00e3o do Grafo\"></a>\nAbra no [navegador](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nPara um uso mais avan\u00e7ado, confira nossa <a href=\"https://docs.cognee.ai\">documenta\u00e7\u00e3o</a>.\n\n\n## Entenda nossa arquitetura\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png\" alt=\"diagrama conceitual do cognee\" width=\"100%\" />\n</div>\n\n## Demonstra\u00e7\u00f5es\n\n1. O que \u00e9 mem\u00f3ria de IA:\n[Saiba mais sobre o cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. Demonstra\u00e7\u00e3o simples do GraphRAG\n\n[Demonstra\u00e7\u00e3o simples do GraphRAG](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. Cognee com Ollama\n\n[cognee com modelos locais](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md\n## Contribuindo\n\nSuas contribui\u00e7\u00f5es est\u00e3o no centro de tornar este um verdadeiro projeto open source. Qualquer contribui\u00e7\u00e3o que voc\u00ea fizer ser\u00e1 **muito bem-vinda**. Veja o [`CONTRIBUTING.md`](/CONTRIBUTING.md) para mais informa\u00e7\u00f5es.\n## \ud83d\udce6 Instala\u00e7\u00e3o\n\nVoc\u00ea pode instalar o Cognee usando **pip**, **poetry**, **uv** ou qualquer outro gerenciador de pacotes Python.\n\n### Com pip\n\n```bash\npip install cognee\n```\n\n## \ud83d\udcbb Uso B\u00e1sico\n\n### Configura\u00e7\u00e3o\n\n```python\nimport os\nos.environ[\"LLM_API_KEY\"] = \"SUA_OPENAI_API_KEY\"\n```\n\nVoc\u00ea tamb\u00e9m pode definir as vari\u00e1veis criando um arquivo .env, usando o nosso <a href=\"https://github.com/topoteretes/cognee/blob/main/.env.template\">modelo</a>.\nPara usar diferentes provedores de LLM, consulte nossa <a href=\"https://docs.cognee.ai\">documenta\u00e7\u00e3o</a> .\n\n### Exemplo simples\n\nEste script executar\u00e1 o pipeline *default*:\n\n```python\nimport cognee\nimport asyncio\n\n\nasync def main():\n    # Adiciona texto ao cognee\n    await cognee.add(\"Processamento de linguagem natural (NLP) \u00e9 um subcampo interdisciplinar da ci\u00eancia da computa\u00e7\u00e3o e recupera\u00e7\u00e3o de informa\u00e7\u00f5es.\")\n\n    # Gera o grafo de conhecimento\n    await cognee.cognify()\n\n    # Consulta o grafo de conhecimento\n    results = await cognee.search(\"Me fale sobre NLP\")\n\n    # Exibe os resultados\n    for result in results:\n        print(result)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n```\nExemplo do output:\n```\n  O Processamento de Linguagem Natural (NLP) \u00e9 um campo interdisciplinar e transdisciplinar que envolve ci\u00eancia da computa\u00e7\u00e3o e recupera\u00e7\u00e3o de informa\u00e7\u00f5es. Ele se concentra na intera\u00e7\u00e3o entre computadores e a linguagem humana, permitindo que as m\u00e1quinas compreendam e processem a linguagem natural.\n  \n```\n\nVisualiza\u00e7\u00e3o do grafo:\n<a href=\"https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html\"><img src=\"graph_visualization_pt.png\" width=\"100%\" alt=\"Visualiza\u00e7\u00e3o do Grafo\"></a>\nAbra no [navegador](https://rawcdn.githack.com/topoteretes/cognee/refs/heads/main/assets/graph_visualization.html).\n\n\nPara um uso mais avan\u00e7ado, confira nossa <a href=\"https://docs.cognee.ai\">documenta\u00e7\u00e3o</a>.\n\n\n## Entenda nossa arquitetura\n\n<div style=\"text-align: center\">\n  <img src=\"https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/assets/cognee_diagram.png\" alt=\"diagrama conceitual do cognee\" width=\"100%\" />\n</div>\n\n## Demonstra\u00e7\u00f5es\n\n1. O que \u00e9 mem\u00f3ria de IA:\n[Saiba mais sobre o cognee](https://github.com/user-attachments/assets/8b2a0050-5ec4-424c-b417-8269971503f0)\n\n2. Demonstra\u00e7\u00e3o simples do GraphRAG\n\n[Demonstra\u00e7\u00e3o simples do GraphRAG](https://github.com/user-attachments/assets/d80b0776-4eb9-4b8e-aa22-3691e2d44b8f)\n\n3. Cognee com Ollama\n\n[cognee com modelos locais](https://github.com/user-attachments/assets/8621d3e8-ecb8-4860-afb2-5594f2ee17db)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md\n## C\u00f3digo de Conduta\n\nEstamos comprometidos em tornar o open source uma experi\u00eancia agrad\u00e1vel e respeitosa para nossa comunidade. Veja o <a href=\"/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> para mais informa\u00e7\u00f5es.\n\n## \ud83d\udcab Contribuidores\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"contribuidores\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n## Hist\u00f3rico de Estrelas\n\n[![Gr\u00e1fico de Hist\u00f3rico de Estrelas](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/assets/community/README.pt.md\n## C\u00f3digo de Conduta\n\nEstamos comprometidos em tornar o open source uma experi\u00eancia agrad\u00e1vel e respeitosa para nossa comunidade. Veja o <a href=\"/CODE_OF_CONDUCT.md\"><code>CODE_OF_CONDUCT</code></a> para mais informa\u00e7\u00f5es.\n\n## \ud83d\udcab Contribuidores\n\n<a href=\"https://github.com/topoteretes/cognee/graphs/contributors\">\n  <img alt=\"contribuidores\" src=\"https://contrib.rocks/image?repo=topoteretes/cognee\"/>\n</a>\n\n## Hist\u00f3rico de Estrelas\n\n[![Gr\u00e1fico de Hist\u00f3rico de Estrelas](https://api.star-history.com/svg?repos=topoteretes/cognee&type=Date)](https://star-history.com/#topoteretes/cognee&Date)"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/logs/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/logs/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/logs/README.md\n# Logs Directory\n\nThis directory contains the application logs for Cognee.\n\n## Log Files\n\n- Log files are named by date in the format `YYYY-MM-DD_HH-MM-SS.log`\n- Logs are stored in plain text format with a consistent structure\n- Each log entry includes:\n  - Timestamp (ISO format)\n  - Log level (padded to consistent width)\n  - Message\n  - Additional context (if any)\n  - Logger name (in square brackets)\n- Exception tracebacks are included for error logs\n\n## Sample Log Entry\n\n```\n2025-03-27T13:05:27.481446Z [INFO    ] Structured log message user_id=user123 action=login status=success [TestLogger]\n```\n\n## Retention Policy\n\nThe system automatically keeps only the 10 most recent log files. Older log files are automatically deleted when new log files are created. This prevents excessive disk usage in long-running deployments.\n\n## Usage\n\nLogs are automatically generated by the application's logging mechanism. No manual actions are required to use this feature.\n\nThe logs directory structure is preserved in version control, but the log files themselves are gitignored."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/deployment/helm/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/deployment/helm/README.md\n# cognee-infra-helm\nGeneral infrastructure setup for Cognee on Kubernetes using a Helm chart.\n\n## Prerequisites\nBefore deploying the Helm chart, ensure the following prerequisites are met:\u00a0\n\n**Kubernetes Cluster**: A running Kubernetes cluster (e.g., Minikube, GKE, EKS).\n\n**Helm**: Installed and configured for your Kubernetes cluster. You can install Helm by following the [official guide](https://helm.sh/docs/intro/install/).\u00a0\n\n**kubectl**: Installed and configured to interact with your cluster. Follow the instructions [here](https://kubernetes.io/docs/tasks/tools/install-kubectl/).\n\nClone the Repository\u00a0Clone this repository to your local machine and navigate to the directory.\n\n## Deploy Helm Chart:\n\n   ```bash\n   helm install cognee ./cognee-chart\n   ```\n\n**Uninstall Helm Release**:  \n   ```bash\n   helm uninstall cognee\n   ```", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/deployment/helm/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/deployment/helm/README.md\n# cognee-infra-helm\nGeneral infrastructure setup for Cognee on Kubernetes using a Helm chart.\n\n## Prerequisites\nBefore deploying the Helm chart, ensure the following prerequisites are met:\u00a0\n\n**Kubernetes Cluster**: A running Kubernetes cluster (e.g., Minikube, GKE, EKS).\n\n**Helm**: Installed and configured for your Kubernetes cluster. You can install Helm by following the [official guide](https://helm.sh/docs/intro/install/).\u00a0\n\n**kubectl**: Installed and configured to interact with your cluster. Follow the instructions [here](https://kubernetes.io/docs/tasks/tools/install-kubectl/).\n\nClone the Repository\u00a0Clone this repository to your local machine and navigate to the directory.\n\n## Deploy Helm Chart:\n\n   ```bash\n   helm install cognee ./cognee-chart\n   ```\n\n**Uninstall Helm Release**:  \n   ```bash\n   helm uninstall cognee\n   ```"}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-frontend/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-frontend/README.md\nThis is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-frontend/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-frontend/README.md\nThis is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).\n\n## Getting Started\n\nFirst, run the development server:\n\n```bash\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\n```\n\nOpen [http://localhost:3000](http://localhost:3000) with your browser to see the result.\n\nYou can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.\n\nThis project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.\n\n## Learn More\n\nTo learn more about Next.js, take a look at the following resources:\n\n- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.\n- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.\n\nYou can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!\n\n## Deploy on Vercel\n\nThe easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.\n\nCheck out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details."}, {"codebase": "cognee", "reference": "https://github.com/topoteretes/cognee/blob/main/cognee-starter-kit/README.md", "text": "cognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-starter-kit/README.md\n# Cognee Starter Kit\nWelcome to the <a href=\"https://github.com/topoteretes/cognee\">cognee</a> Starter Repo! This repository is designed to help you get started quickly by providing a structured dataset and pre-built data pipelines using cognee to build powerful knowledge graphs.\n\nYou can use this repo to ingest, process, and visualize data in minutes. \n\nBy following this guide, you will:\n\n- Load structured company and employee data\n- Utilize pre-built pipelines for data processing\n- Perform graph-based search and query operations\n- Visualize entity relationships effortlessly on a graph\n\n# How to Use This Repo \ud83d\udee0\n\n## Install uv if you don't have it on your system\n```\npip install uv\n```\n## Install dependencies\n```\nuv sync\n```\n\n## Setup LLM\nAdd environment variables to `.env` file.\nIn case you choose to use OpenAI provider, add just the model and api_key.\n```\nLLM_PROVIDER=\"\"\nLLM_MODEL=\"\"\nLLM_ENDPOINT=\"\"\nLLM_API_KEY=\"\"\nLLM_API_VERSION=\"\"\n\nEMBEDDING_PROVIDER=\"\"\nEMBEDDING_MODEL=\"\"\nEMBEDDING_ENDPOINT=\"\"\nEMBEDDING_API_KEY=\"\"\nEMBEDDING_API_VERSION=\"\"\n```\n\nActivate the Python environment:\n```\nsource .venv/bin/activate\n```\n\n## Run the Default Pipeline\n\nThis script runs the cognify pipeline with default settings. It ingests text data, builds a knowledge graph, and allows you to run search queries.\n\n```\npython src/pipelines/default.py\n```\n\n## Run the Low-Level Pipeline\n\nThis script implements its own pipeline with custom ingestion task. It processes the given JSON data about companies and employees, making it searchable via a graph.\n\n```\npython src/pipelines/low_level.py\n```\n\n## Run the Custom Model Pipeline\n\nCustom model uses custom pydantic model for graph extraction. This script categorizes programming languages as an example and visualizes relationships.\n\n```\npython src/pipelines/custom-model.py\n```\n\n## Graph preview \n\ncognee provides a visualize_graph function that will render the graph for you.\n\n```\n    graph_file_path = str(\n        pathlib.Path(\n            os.path.join(pathlib.Path(__file__).parent, \".artifacts/graph_visualization.html\")\n        ).resolve()\n    )\n    await visualize_graph(graph_file_path)\n```\n\n# What will you build with cognee?\n\n- Expand the dataset by adding more structured/unstructured data\n- Customize the data model to fit your use case\n- Use the search API to build an intelligent assistant\n- Visualize knowledge graphs for better insights", "enriched_text": "[Codebase: cognee] [File: https://github.com/topoteretes/cognee/blob/main/cognee-starter-kit/README.md]\n\ncognee\nhttps://github.com/topoteretes/cognee/blob/main/cognee-starter-kit/README.md\n# Cognee Starter Kit\nWelcome to the <a href=\"https://github.com/topoteretes/cognee\">cognee</a> Starter Repo! This repository is designed to help you get started quickly by providing a structured dataset and pre-built data pipelines using cognee to build powerful knowledge graphs.\n\nYou can use this repo to ingest, process, and visualize data in minutes. \n\nBy following this guide, you will:\n\n- Load structured company and employee data\n- Utilize pre-built pipelines for data processing\n- Perform graph-based search and query operations\n- Visualize entity relationships effortlessly on a graph\n\n# How to Use This Repo \ud83d\udee0\n\n## Install uv if you don't have it on your system\n```\npip install uv\n```\n## Install dependencies\n```\nuv sync\n```\n\n## Setup LLM\nAdd environment variables to `.env` file.\nIn case you choose to use OpenAI provider, add just the model and api_key.\n```\nLLM_PROVIDER=\"\"\nLLM_MODEL=\"\"\nLLM_ENDPOINT=\"\"\nLLM_API_KEY=\"\"\nLLM_API_VERSION=\"\"\n\nEMBEDDING_PROVIDER=\"\"\nEMBEDDING_MODEL=\"\"\nEMBEDDING_ENDPOINT=\"\"\nEMBEDDING_API_KEY=\"\"\nEMBEDDING_API_VERSION=\"\"\n```\n\nActivate the Python environment:\n```\nsource .venv/bin/activate\n```\n\n## Run the Default Pipeline\n\nThis script runs the cognify pipeline with default settings. It ingests text data, builds a knowledge graph, and allows you to run search queries.\n\n```\npython src/pipelines/default.py\n```\n\n## Run the Low-Level Pipeline\n\nThis script implements its own pipeline with custom ingestion task. It processes the given JSON data about companies and employees, making it searchable via a graph.\n\n```\npython src/pipelines/low_level.py\n```\n\n## Run the Custom Model Pipeline\n\nCustom model uses custom pydantic model for graph extraction. This script categorizes programming languages as an example and visualizes relationships.\n\n```\npython src/pipelines/custom-model.py\n```\n\n## Graph preview \n\ncognee provides a visualize_graph function that will render the graph for you.\n\n```\n    graph_file_path = str(\n        pathlib.Path(\n            os.path.join(pathlib.Path(__file__).parent, \".artifacts/graph_visualization.html\")\n        ).resolve()\n    )\n    await visualize_graph(graph_file_path)\n```\n\n# What will you build with cognee?\n\n- Expand the dataset by adding more structured/unstructured data\n- Customize the data model to fit your use case\n- Use the search API to build an intelligent assistant\n- Visualize knowledge graphs for better insights"}]